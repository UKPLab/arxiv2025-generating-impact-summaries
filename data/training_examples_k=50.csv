e1,"…pre-trained language models to build dialogue tutors with less data supervision and higher coherence (Afzal et al., 2019; Demszky and Hill, 2023; Macina et al., 2023b), and can be further improved by integrating with pedagogical and learning science principles (Stasaski et al., 2020; Sonkar et…Aside from delivering effective and fluent dia-logic teaching, there is increased interest in exploring LLMs’ potential for personalized education (Stasaski et al., 2020; Macina et al., 2023b; Sonkar et al., 2023).",highlighting the potential of pre-trained language models in education,impact-revealing
e2,"Most existing studies focus on learning the pedagogical strategies to teach the students of the given exercises [39, 40], or generating high-quality responses in the tutoring dialogues [30, 44].As an advanced form of intelligent tutoring systems (ITSs), interactive ITSs has been extensively investigated as educational dialogue systems [30, 37, 47], as it can interatively provide adaptive instructions and real-time feedback to students.Recent studies develop interactive ITSs [20, 47] that can provide real-time feedback [7, 9], engage in natural conversations [30, 39, 40, 44], and customize their teaching content based on individual student needs [5, 28, 41].",acknowledge existing research and developments in interactive intelligent tutoring systems,other
e3,"Investigations have shown that no domain (thus far) is immune to this phenomenon; the scope of adversarial examples has been expansive, reaching into image processing [4, 12, 28, 33], malware detection [13, 23], text [9, 20], and even speech recognition [5].Research outside of image space (usually) provides their own definitions: perturbed malware must maintain its properties of malware [13, 23], perturbed audio must be nearly inaudible [5], perturbed text must preserve its semantics [9, 20], among other definitions.",highlighting the broad impact of adversarial examples across multiple domains,impact-revealing
e4,2 See Table 3 in Burrows et al. (2015) for a detailed list.,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing,other
e5,"…the difficulty of test questions for humans have been conducted in the domain of reading comprehension, where the readability of reading passages is associated with the difficulty of their corresponding comprehension questions (Huang et al., 2017; Beinborn et al., 2015; Loukina et al., 2016).the difficulty of their corresponding comprehension questions (Huang et al., 2017; Beinborn et al., 2015; Loukina et al., 2016).",acknowledging prior findings in reading comprehension,other
e6,"Large-scale pre-trained models were applied to various natural language processing tasks, such as text classiﬁcation [32], question answering [33] and natural language understanding [34], but not yet MQI extraction.",Highlight research gap in MQI extraction,impact-revealing
e7,"We use pretrained sentence-BERT (Reimers and Gurevych, 2019), an approach that modiﬁes BERT (Devlin et al., 2019) using siamese and triplet networks (Schroff et al., 2015) to obtain sentence embeddings.",method use for sentence embedding generation,impact-revealing
e8,"In recent years, the analysis of argumentation using Natural Language Processing methods, so-called argument mining [158], has gained a lot of attention in the Artificial Intelligence research community and has been applied to a number of domains, from student essays [290] to scientific articles [299] and online user-generated content [314, 165].",highlighting the growing interest and application of argument mining in various domains,impact-revealing
e9,Cross encoders [24] are a type of transformer-based models that aim to capture the relationship between input pairs.,providing context on transformer-based model type,other
e10,"Additionally, word embeddings have been shown to play an important role in boosting the generalization capabilities of neural systems (Goldberg, 2016; Camacho-Collados and Pilehvar, 2018).",highlighting the significance of word embeddings in neural systems,impact-revealing
e11,"It is based on the Argument Reasoning Comprehension Task (ARCS) [9, 10], which (after correcting for biases in the initial version of the task) has been shown to be very difficult even for state-of-the-art NLP systems [11].To systematically explore this, we propose the use of new benchmark datasets and tasks which build on two existing tasks in natural language processing: natural language inference [6, 7, 8] and argument reasoning comprehension [9, 10, 11].",motivating the development of new benchmarks and tasks in NLP,impact-revealing
e12,"We use the Java Wikipedia Library (JWPL) from Zesch et al. (2008), which converts a Wikipedia dump into a database that provides an easy-to-use access to the dump components.",tool used for data processing,impact-revealing
e13,"Additionally, with the development of social media, a growing number of researchers focus on user-generated web-based texts for claim detection [7], [8].",motivating research work on claim detection,impact-revealing
e14,Miller and Gurevych 2015 proposed a method based on word disambiguation for multiple meanings of semantic puns to detect pun.,reporting existing findings on word disambiguation for multiple meanings,other
e15,"Computational approaches to argument mining/understanding have become very popular (Persing & Ng, 2016; Cano-Basave & He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau & Moens, 2009; Habernal & Gurevych, 2016).",Acknowledge existing argument mining studies,other
e16,"In extending this work, we intend to test these methods on a variety of sequentially dependent tasks as well as incorporating the use of more efficient MTL methods including AdapterFusion (Pfeiffer et al., 2021) and AdapterDrop (Rücklé et al., 2021).",suggesting future research directions on multi-task learning,impact-revealing
e17,"Transfer learning over prompts (Vu et al., 2022) and Adapters (Pfeiffer et al., 2021) and multi-tasking over the latter (Mahabadi et al., 2021b) have also been explored, but they are not comparable to ours due to difference in task/problem settings., 2022) and Adapters (Pfeiffer et al., 2021) and multi-tasking",Highlight limitations in transfer learning approaches,impact-revealing
e18,"Automatic short answer grading (ASAG) has been an active area of research for nearly a decade, but it has historically been challenging to do easily and effectively enough, for widespread use in educational settings [7].",highlighting a significant challenge in the field of automatic short answer grading,impact-revealing
e19,"…2018; Mao et al., 2019; Su et al., 2020; Gong et al., 2020), generating metaphors (Yu and Wan, 2019; Stowe et al., 2020; Chakrabarty et al., 2020; Stowe et al., 2021) and paraphrasing between non-compositional expressions and their literal counterparts (Liu and Hwa, 2016; Agrawal et al., 2018;…Previously only a few of studies focus on metaphor generation (Yu and Wan, 2019; Chakrabarty et al., 2020; Stowe et al., 2021) whereas other types of non-compositional expressions remain under-IdiomHowever, owing to their non-compositionality and limitations on available and related data resources (Stowe et al., 2020, 2021), the task of generating non-compositional expressions including both idioms and metaphors remains challenging and under-explored.",highlighting research gap and need for further exploration of metaphor and idiom generation,impact-revealing
e20,"…systems that use neural attention, the one used in [29] integrate hierarchical attention and biGRU for the analysis of the quality of the argument, the one in [30] use attention to integrate sentiment lexicon, while in other works [31]–[33] attention modules are stacked on top of recurrent layers.Such is the case with Niculae et al. [23], whose CDCP dataset only consists of argumentative elements, and with others [31], [40], [58] who simply ignore the non-argumentative elements of the input text.",acknowledge variations in existing methods,other
e21,tation platform [55] and the WebAnno TSV v3.,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing,other
e22,"Other related shared tasks include the FEVER task (Thorne et al., 2018) on fact extraction and verification, the Fake News Challenge (Hanselowski et al., 2018), the FakeNews task at MediaEval (Pogorelov et al., 2020), as well as the NLP4IF tasks on propaganda detection (Da San Martino et al., 2019a) and on fighting the COVID-19 infodemic in social media (Shaar et al., 2021a).Other related shared tasks include the FEVER task (Thorne et al., 2018) on fact extraction and verification, the Fake News Challenge (Hanselowski et al., 2018), the FakeNews task at MediaEval (Pogorelov et al., 2020), as well as the NLP4IF tasks on propaganda detection (Da San Martino et al.,…",acknowledge related tasks,other
e23,"The former relates to purposefully misspelling or otherwise symbolically replacing text (e.g., fvk you , @ssh*l3 ) to subvert algorithms (Eger et al., 2019; Kurita et al., 2019).",providing context about a scientific term,other
e24,Pfeiffer et al. (2021) leverages the pre-trained embeddings of lexically overlapping tokens between the vocabulary of pre-trained model and that of unseen target language to initialize the corresponding embeddings of target language.,reporting prior findings,other
e25,"(5) Real-World Information Needs is a task with annotated claims based on search engine queries, representing real-world information needs [32].Consistent with this idea, some recent work has shown that a claim-checking system that queries Wikipedia performs poorly at checking claims from a climate-change-focused task [8], from scientific paper abstracts [38], or even from general-knowledge claims that are phrased more journalistically or colloquially [17, 32].Besides two tasks based on Wikipedia [9, 34] and one on scientific paper abstracts [38], we have a task on climate-change-related claims [8], one on statements from the 2016 presidential debates [13], and one on real-world information needs based on search-engine queries [32].",providing context and examples of related tasks,other
e26,"Next, to investigate the logical strength of an argument (Wachsmuth et al., 2017), we examine degree and depth .",providing context for argument examination,other
e27,"Given a claim, the goal is to find evidence and then to make a verdict about the claim’s veracity based on that evidence (Thorne and Vlachos, 2018; Glockner et al., 2022; Guo et al., 2022).Given a claim , the goal is to ﬁnd evidence and then to make a verdict about the claim’s veracity based on that evidence (Thorne and Vlachos, 2018; Glockner et al., 2022; Guo et al., 2022).",describing the process of evidence evaluation,other
e28,"However, the state-of-the-art supervised methods usually rely on multi-source labeled data to generalize to various downstream tasks (Reimers and Gurevych, 2019), while such large-scale annotated data across domains are not always available.",Highlighting limitation in supervised methods,impact-revealing
e29,"’ To date, these tasks have been mostly addressed separately through machine learning methods [39, 42], but recently, they have been jointly treated as sequence labelling tasks of NLP, addressed by specialized neural network models [44].",Shift from separate ML methods to joint NLP sequence labeling,impact-revealing
e30,We adopted the widely used document selection method from Hanselowski et al. (2018).,document selection method use,impact-revealing
e31,"The method, utilizing Sentence-bert [28] as an offline model to obtain the correlation degree between texts, thus the offline acquisition may contain too much noise and is not accurate enough.",Critique of offline noise in text correlation,impact-revealing
e32,"…costly to get high-quality annotations for ArgMin due to: (i) subjectivity of argumentation as well as divergent and competing ArgMin theories (Daxenberger et al., 2017; Schulz et al., 2018), leading to disagreement among crowd-workers as well as expert annotators (Habernal and Gurevych,…That is, while standard NLP tasks such as POS tagging are relatively stable across different domains, arguments may be very differently realized across different datasets (Daxenberger et al., 2017).",highlighting challenges in obtaining high-quality annotations,impact-revealing
e33,"Bigram set: [there-were, were-relieving, relievingfactors, factors-including, including-medication, medication-aids]
b. Word Bigram Frequency Filtering: The bigram repository is employed to filter out low-frequency sentences.",the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing,other
e34,"The related linguistic features appear with quite a number of variations across academia, without a naming convention (Zhu et al., 2009; Fitzsimmons et al., 2010; Tanaka-Ishii et al., 2010; Daowadung and Chen, 2011; Vajjala and Meur-ers, 2013; Ciobanu et al., 2015; Zhang et al., 2019; Blinova et…",acknowledge variations in existing research,other
e35,"Prior work in obligation extraction has primarily focused on classification tasks at the sentential level [4, 13], while our approach extracts a graph representation of the obligation.",Acknowledge limitations in obligation extraction,impact-revealing
e36,"However, argument search engines miss a reliable quality-based ranking so far (Wachsmuth et al., 2017c; Stab et al., 2018; Du-mani et al., 2020), likely due to the heterogeneity of argumentative domains and genres on the web.",Identifying knowledge gap in argument quality ranking,impact-revealing
e37,"In terms of improving faithfulness and factuality, researchers did not find natural language inference (NLI)4 models trained on standard NLI datasets to be robust enough for summarization-related downstream tasks (Falke et al., 2019).Falke et al. (2019) find that NLI models trained on
standard NLI datasets do not offer robust benefits to improving summarization factuality.AgreeSum is timely, given that recent works have shown difficulty of producing faithful abstractive summaries (Falke et al., 2019; Maynez et al., 2020; Kryscinski et al., 2020; Durmus et al., 2020; Zhou et al., 2021), though in the SDS setting.",highlighting the significance of recent findings in summarization,impact-revealing
e38,"In comparison, the only existing related dataset (Flekova and Gurevych, 2015) contains only 298 characters and focuses on a single dimension.Results and Analysis Following Flekova and Gurevych (2015), we use macro-averaged F1 as evaluation metric.…understanding of characters, such as coreference resolution (Chen and Choi, 2016) and character relationships (Iyyer et al., 2016), and few studies have explored deeper comprehension of characters’ persona (Flekova and Gurevych, 2015; Sang et al., 2022a), on which humans can generally do well.Latent Persona Induction Besides (Flekova and Gurevych, 2015) that is similar to our work in terms of the focus on personality classification, there is another line of related work on latent persona induction (Bamman et al., 2013).",Identifying limitations in existing persona datasets,impact-revealing
e39,"ral choice especially for component identiﬁcation (segmentation and classiﬁcation), which is a typical entity recognition problem for which BIO tagging is a standard approach, pursued in AM, e.g., by Habernal and Gurevych (2016). The challenge in the end-to-end setting is to also include relations into the tagging scheme, which we realize by coding the distances between linked components into the tag label. Since related entThis is a natural choice especially for component identification (segmentation and classification), which is a typical entity recognition problem for which BIO tagging is a standard approach, pursued in AM, e.g., by Habernal and Gurevych (2016).",providing context for a method,other
e40,"Our dataset includes more complaints with intense argumentation in comparison with other argument mining datasets, such as [15, 1, 11].",acknowledge dataset differences,other
e41,"…instead of always considering a large transformer model within the typical end-to-end machine learning loop, an active line of work (e.g., as in [19]) tries to benefit from them to embed text to vectors, while freezing the transformer weights and obviating the need to always backpropagate the…",acknowledge existing methods,other
e42,"With the popularity of online de-bating forums like convinceme 1 , debatepedia 2 and change my view (CMV) 3 , researchers pay increasing attention to evaluating the quality of debating or persuasive content (Tan et al., 2016; Wei and Liu, 2016; Wei et al., 2016; Habernal and Gurevych, 2016a).",Motivating research in debate content quality evaluation,impact-revealing
e43,"We use data for all translation directions involving da , de , en , is , nb , sv from the following parallel corpora from Opus: Bible (Christodouloupoulos and Steedman, 2014), Books (Tiedemann, 2012), Europarl (Koehn, 2005), Glob-alVoices (Tiedemann, 2012), JW300 (Agi´c and Vuli´c, 2019), MultiCCAligned (El-Kishky et al., 2020), Paracrawl (Esplà et al., 2019), TED2020 (Reimers and Gurevych, 2020), and WikiMatrix (Schwenk et al., 2019)., 2019), TED2020 (Reimers and Gurevych, 2020), and WikiMatrix (Schwenk et al.",reporting data sources,other
e44,"Also, in further related work in argumentation mining, the F1 scores we achieve in the here described work are absolutely comparable (e.g., F1 scores in the ranges of 60%–90% in [7], [10], [51], [52], and [53]).",Comparing argumentation mining F1 scores,impact-revealing
e45,"This similarity between the two tasks led us to draw inspiration from the UCL Machine Reading team’s submission to the FNC-1’s challenge, which performed 3 best among the 50 submissions to the challenge [9].Depending on the stance of the body of text with respect to the headline, the textheadline pairs were to be classified into any of the following classes:
• Agrees: The body of text agrees with the claim(s) made in the headline • Disagrees: The body of text disagrees with the claim(s) made in the headline • Discusses: The body of text and headline are referring to the same subject, but the
body does not take any stance or position on the claim(s) made in the headline • Unrelated: The body of text is not related to the claim(s) being made in the headline
FACTIFY’s not-entail class can be considered similar to a combination of Unrelated and Discusses classes of FNC-1, while entails and refutes classes can be considered similar to FNC-1’s Agrees and Disagrees classes respectively.This challenge was similar to the FACTIFY shared task, except FNC-1 only dealt with text entailment or stance detection, unlike FACTIFY which deals with multi-modal entailment.Given the similarity of the tasks being solved in FNC-1 and FACTIFY, we adopted the manner of concatenation of the cosine similarity and vector representations of the header and body as explained in [9].The resultant vector of length 10, 001 is then fed as input into a shallow Multi Layer Perceptron (MLP) network, which has a softmax output of length four, one for each class in the FNC-1 task.Stance Detection is an important part of Fake News detection and was notably used in the Fake News Challenge - 1 (FNC-1) [8].We then used these vector representations for classifications tasks of image and text entailment by adapting the approach introduced by Riedel et al. in their submission to the FNC-1 task.FNC-1 introduced a dataset which consisted of a headline and a body of text, which may be from the same article or different articles.The FNC-1 challenge had two other submissions which performed better than [9], however, we
concluded that those were more complicated architectures and might hamper the scalability and
time complexity of our model.",Drawing inspiration from successful stance detection models,impact-revealing
e46,"SR estimates are frequently utilized in NLP, AI, and IR [4, 9, 43], and have been applied in tasks such as information extraction [4], clustering [1, 30] and search [30].Even limiting our attention to Wikipedia-specific SR measures, which have been shown to be better [30] or as good as WordNet-based measures [43], there are still quite a few to consider (e.g. [9, 22, 30, 38]).",reporting prior findings and applications,other
e47,[13] investigates the performance of SVM [4] with SentenceBERT’s embeddings [17] (SBERT).,reporting prior findings,other
e48,"More recently, several text simplification papers have used this approach (Zhu et al., 2010; Coster & Kauchak, 2011; Woodsend & Lapata, 2011; Wubben et al., 2012).Zhu et al. (2010) evaluated their system on a set of 100 sentences from English Wikipedia, aligned with 131 sentences from Simple English Wikipedia., 2004), English (e.g., De Belder & Moens, 2010; Zhu et al., 2010; Coster & Kauchak, 2011; Siddharthan, 2011; Woodsend & Lapata, 2011; Wubben et al., 2012; Siddharthan & Angrosh, 2014; Narayan & Gardent, 2014), French (Seretan, 2012; Brouwers et al.…2012), Bulgarian (Lozanova et al., 2013), Danish (Klerke & Søgaard, 2013), Dutch (Daelemans et al., 2004), English (e.g., De Belder & Moens, 2010; Zhu et al., 2010; Coster & Kauchak, 2011; Siddharthan, 2011; Woodsend & Lapata, 2011; Wubben et al., 2012; Siddharthan & Angrosh, 2014; Narayan &…Both Zhu et al. (2010) and Woodsend & Lapata (2011) used the Stanford Parser (Klein & Manning, 2003) for syntactic structure, which does not provide morphological information.In contrast, Zhu et al. (2010) presented an approach based on syntax-based SMT (Yamada & Knight, 2001), consisting of a translation model, a language model and a decoder.",acknowledge prior work,other
e49,"Examples are OpenCyc1-4 [12], [13], Probase [78], [79], DBPedia [67], YAGO1-3 [34], [80], [81], CNKI [3], [8], [82], [83], [84].",the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing,other
e50,"…29, 67, 69, 92, 97], where the model architecture remains unchanged and a subset of the model parameters gets finetuned; adapter tuning [24, 28, 48, 65, 71], where additional trainable parameters are added to the original model; and prompt tuning [39, 42], where additional trainable prefix tokens…",describing model tuning methods,other
