id,citing_id_PST,citing_id_Semantic_scholar,citing_title,cited_id_PST,cited_title,context,intent_class,fine-grained_intent
2735,5bdc315017c44a1f58a05c5e,5201efab94c9376ef894f6f33cab06a5c5e00073,Learning Named Entity Tagger using Domain-Specific Dictionary,53e9b89bb7602d97044763cc,Distant supervision for relation extraction without labeled data.,"…labeled data, and has gained successes in various natural language processing tasks, including phrase mining (Shang et al., 2018), entity recognition (Ren et al., 2015; Fries et al., 2017; He, 2017), aspect term extraction (Giannakopoulos et al., 2017), and relation extraction (Mintz et al., 2009).###Originally, it was proposed to leverage knowledge bases to super-vise relation extraction tasks (Craven et al., 1999; Mintz et al., 2009).###Originally, it was proposed to leverage knowledge bases to supervise relation extraction tasks (Craven et al., 1999; Mintz et al., 2009).",other,reporting prior findings and applications
898,558c8f2684ae6766fdf3c27c,ada1841e93c294e5cc5218cc95eddc74899a2f95,Implicit hints: Embedding hint bits in programs without ISA changes,53e99ecbb7602d970279829e,Code placement for improving dynamic branch prediction accuracy,We refer to [14] for details of the algorithm.###Jiménez [14] proposes to encode branch hints in the program counter.,impact-revealing,reporting prior findings on encoding branch hints
972,,6304f54ff278f13388c09c556ba83403ec8271be,Reduced volume of the putamen in REM sleep behavior disorder patients.,,,"###Previous SPECT and PET studies have implicated the basal ganglia, especially the putamen, as being dysfunctional in RBD patients [7,8].###This sample size is not surprising given the difficulty in recruiting large numbers of these patients for research, and our sample size is similar in magnitude to those reported in the few other imaging studies of RBD [7].###This structural finding builds on previous reports of functional differences in the putamen of RBD patients assessed using PET imaging [7].",impact-revealing,highlighting previous findings related to RBD patients and imaging studies
2408,5db80dc83a55acd5c14a24b9,0af061849aa325b41a213e8730b3d1e84aa26c0d,CONNA: Addressing Name Disambiguation on the Fly,5736973c6e3b12023e62b744,Feature engineering and tree modeling for author-paper identification challenge.,"GBDT: Is a widely used model to solve KDD Cup 2013 challenge-1 [8], [20], [50].",other,reporting prior findings on GBDT model use
976,,24214fe90c6c652a797c7766b717c0925003c44b,Utopian methodology: Researching educational interventions to promote equity over multiple timescales,,,"###Recent research in the learning sciences has paved the way for shifting the orientation of our work from the given to radically new possibilities in designing for learning and equity (Stetsenko, 2016; Vossoughi, 2021).###Utopian designs are intended to challenge the existing power relations and status quo while transforming the institutions in which they are implemented (Gutiérrez et al., 2020; Sarason, 1990; Stetsenko, 2016).",impact-revealing,highlighting the transformative potential of utopian designs in learning and equity
1653,,42b53e990d980d10b1d63af7467f7c60c31ea31e,The politics of pronouns: how Trump framed the ingroup in the 2016 presidential election,,,"###The former is motivated by self-evaluation and takes place through social comparison, whereas the latter is based on ‘realistic’ self-interest and represents embryonic conflict” (Tajfel and Turner 1979).",impact-revealing,providing context for social comparison theory
2431,5ca600ae6558b90bfa4d76e9,7aa38b85fa8cba64d6a4010543f6695dbf5f1386,Towards Deep Learning Models Resistant to Adversarial Attacks,5c757d61f56def9798ad5986,Towards the first adversarially robust neural network model on MNIST.,"Indeed, subsequent work [20, 25] has found that PGD is in fact overestimating the `2-robustness of this model.",other,highlighting findings on the overestimation of model robustness
473,5e68b99493d709897cd373ed,d08b35243edc5be07387a9ed218070b31e502901,group normalization,53e9986eb7602d97020ab93b,Distinctive Image Features from Scale-Invariant Keypoints,"Classical features of SIFT [39], HOG [9], and GIST [41] are group-wise representations by design, where each group of channels is constructed by some kind of histogram.###We notice that many classical features like SIFT [39] and HOG [9] are group-wise features and involve group-wise normalization.###We notice that many classical features like SIFT [39] and HOG [9] are group-wise features and involve group-wise normalization .###However, in addition to orientations (SIFT [39], HOG [9], or [11, 8]), there are many factors that could lead to grouping, e.g ., frequency, shapes, illumination, textures.###However, in addition to orientations (SIFT [39], HOG [9], or [11, 8]), there are many factors that could lead to grouping, e.",impact-revealing,acknowledge characteristics of classical features in image processing
3239,5aed14d617c44a4438158f7c,ae1c89817a3a239e5344293138bdd80293983460,Attention U-Net:,5a260c8617c44a4ba8a31ec4,Ensembles of Multiple Models and Architectures for Robust Brain Tumour   Segmentation,"In particular, fully convolutional networks (FCN) [17] such as U-Net [22], DeepMedic [13] and holistically nested networks [15, 31] have been shown to achieve robust and accurate performance in various tasks including cardiac MR [3], brain tumours [12] and abdominal CT [24, 25] image segmentation tasks.",other,highlighting the effectiveness of fully convolutional networks in medical image segmentation
935,5c234870da562935fc1d4da6,94bd59e507ba8496b36605be0f6740e5731e91d5,CounterMiner: Mining Big Performance Data from Hardware Counters,558c75c5e4b0cfb70a1dd12a,Server Engineering Insights for Large-Scale Online Services,"2) Architecture and Compiler Optimization: Kozyrakis et al. employ hardware counters and other tools to analyze how large-scale online services use resources in data centers and then they provide several insights for server architecture design in data centers [22].###Therefore, performance counter based analysis is applied in a wide range of applications, including task scheduling [5], [6], workload characterization [7]–[14], performance optimization of applications [15]–[18], compiler optimization [19]–[21], architecture optimization [22], and many more.",impact-revealing,highlighting the broad applications of performance counter-based analysis
1631,,c69b2a463dd6a86fad54e99d8aebacacf99ab6ec,Improving diagnosis by Grouping Test Cases to Reduce Complexity,,,"###It has also been validated by Abreu et al [1].###Ochiai[2] will be used by default, but this could be substituted in future research by other metrics[1].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3772,5ee8986891e011e66831c3bc,965652c0e426c5b42d7218d7429025be7ac542bf,DeeperGCN: All You Need to Train Deeper GCNs,599c7985601a182cd2647a46,Deep Sets,Zaheer et al. [2017] propose DeepSets based on permutation invariance and equivariance to deal with sets as inputs.,other,reporting prior findings on DeepSets
444,5b67b4b417c44aac1c86789a,abd91aca4d78799492256b406f5abc199d3802e4,Multilingual End-to-End Speech Recognition with A Single Transformer on Low-Resource Languages,5a73cb6317c44a0b30358209,Multilingual Recurrent Neural Networks With Residual Learning For Low-Resource Speech Recognition,"Although multilingual speech recognition has been studied [1, 2, 3, 4, 5] for a long time, these researches are commonly limited to making acoustic model (AM) multilingual, which require language-speciﬁc pronunciation model (PM) and language model (LM).###The baseline systems come from our previous work [5] and all results are summarized in Table 5.###SHL-MLSTM [5] further explores long short-term memory (LSTM) [7] with residual learning as the shared hidden layer instead of DNN and achieves better results than SHL-MDNN.###Multilingual speech recognition has been investigated for many years [1, 2, 3, 4, 5].###A comparison with SHL-MLSTM [5] with residual learning is investigated on CALL-HOME datasets with 6 languages.",impact-revealing,highlighting limitations in multilingual speech recognition research
1306,,b2d52d31ecb2af8b6c2d25989a7f8b33cf69a984,"Applying cross-cultural values research to ""the Chinese""",,,"###Triandis on pushing social psychology toward considering crosscultural psychology in conceptually viable and methodologically rigorous ways (e.g., Triandis et al., 1986, including multi-method probes, Triandis et al., 1990) was enormous. One need only look at the ever-increasing number of articles that cite him, the Sage volume dedicated to his broad contribution (Social Psychology and Cultural Context, Adamopoulos & Kashima, 1999), and the growing “who’s who” of the field that were formerly his students (1999:xi). A key part of Triandis’ legacy was bringing “subjective culture” (1972) to the fore of comparative social psychological study.###Understanding this limitation is especially important for our analysis of societies undergoing rapid change, as proposed long before Kluckhohn & Strodtbeck (1961) by both Herder and Bastian.###Kluckhohn’s definition is certainly the most cited one in various disciplines regarding the conceptualization of values2 and is embedded in the later but also widely-cited psychological formulation by Milton Rokeach:
A value is an enduring belief that a specific mode of conduct or end-state of existence is personally or socially preferable to an opposite or converse mode of conduct or end-state of existence.###In their original text they clearly stated; (4) that even when these preferences are established, alternatives will exist; and (5) that “in societies undergoing change the ordering of preferences will not be clear-cut for some or even all the value orientations” (Kluckhohn & Strodtbeck, 1961:10).###Cognitive anthropology, especially the American culture and personality school (exemplified by Ruth Benedict, Alfred Kroeber, and Clyde Kluckhohn) where ""culture"" may be limited to the communicative and meaningful aspects of social life: from language to the meaning carried by symbols, persons, actions, and events26.###Thus his work tends to be more contemporary, rooted largely in Kluckhohn (1951), Rokeach (1973), Hofstede (1980, 2001), and the World Values Survey10 (though much of the literature analyzed in Chapter 3 appears periodically in his work, i.e., Schwartz & Bilsky, 1987).###Kerckhoff and Davis’s Filter theory (1962) was based on research that followed dating couples over an 8-month###Cultural Anthropologist Clyde K.M. Kluckhohn (1951:395)
At about the same time that the great scientific mind of Einstein was admitting human limits and appealing to common human values (at a lecture that reportedly influenced Karl Popper), the pioneering cultural anthropologist Clyde Kluckhohn was formulating scientific conceptions for the study of such values.###(Kroeber & Kluckhohn, 1952:357)
Note that this is a somewhat different and more extensive formulation than the one often cited from Kluckhohn (1951:395), which often appears in texts (e.g., Hofstede, 2001:9) and cited as the opening quote of this dissertation.###” But for most social psychologists, both the level of investigation and the methods applied at that level are important considerations. Peter Smith’s article (2002) is often cited as providing definitive guidelines on this issue (see the quote from his abstract above), as well as clarifying how levels can be confounded in research:###Though structuralism has been largely debunked, values studies still suffer some association with that academic paradigm, and assumptions of conflation with other behavior-related psychological constructs. Harry Triandis (1972), building on Charles Osgood’s (1964:171) conceptualization of “subjective culture” posited an intricate schema of elements interrelated with one another (in Figure 4.###Levitin agrees that Kluckhohn (1951) has offered one of the most comprehensive analyses “among social scientists who have attempted to classify values.###Defining culture has been both an integral task and increasingly divisionary debate among social scientists for most of the previous century, as demonstrated perhaps most saliently by the 164 definitions compiled by Alfred Kroeber and Clyde Kluckhohn (1952) then current in the anthropological literature.###Kroeber and Kluckhohn “unified various definitions of culture into a single formulation focused on both the symbolic and the behavioral inheritances of a cultural community” (Shweder in Borofsky et al., 2001:437):
Culture consists of patterns, explicit and implicit, of and for behavior acquired and transmitted by symbols, constituting the distinctive achievement of human groups, including their embodiments in artifacts; the essential core of culture consists of traditional (i.e., historically derived and selected) ideas and especially their attached values; culture systems may, on the one hand, be considered as products of action, on the other hand, as conditioning elements of further action.###Though this stream also describes itself as “cultural psychology,” it provides a much more historical and situated perspective (see Yoshihisa Kashima, 2000), a direction that some indigenous psychologists have sought to build on. Recently, Carl Ratner (2008) has sought to compare and contrast these divergent emphases with his new book, Cultural Psychology, Crosscultural Psychology and Indigenous Psychology, though he considers Vygotsky as part of the groundwork of the broader “cultural psychology” (2008:6-10).###86 2.6.3 State-sponsored Studies – Taylor, Leighton, Benedict, Hall, Kluckhohn ...................... 90 2.6.4 Describing Cultural Dimensions: Geertz, Inkeles, Douglas, Hsu ................................... 95 2.6.5 Summary of Anthropological Contributions ..................................................................###Cultural Anthropologists Alfred Kroeber and Clyde Kluckhohn
1.1.1.1 Values as a Cornerstone for Social Research and Transformations
This dissertation suggests that analyzing values is not merely an outdated enterprise which shaped the early development of culture-related studies, but, as Michael Harris Bond suggests (1986:208), continues to be a “touchstone” to which scholars or practitioners need to return to at salient points in the development of any socially- or culturally-oriented field, or at critical junctures in studying the development of, or cultural transitions faced by, any people5.###35 An overlapping, but more general review of how some of these same strands of thinking or research influenced or were incorporated into cross-cultural psychology is provided by Otto Kleinberg (1980).###Sociologists still deal with cognitive constructs like attitudes and values (e.g. Spates, 1983; Hitlin & Piliavin, 2004), and even Kluckhohn’s influential student Clifford Geertz, who, though adhering to the conceptual division of culture and society, was not willing to surrender ""society"" to the sociologists.###Lydon, Pierce, and O’Regan (1997) show now long-distance dating relationships that affirm core beliefs, values, and identities, even in adversity, have commitment staying power. Lydon and Zanna (1990) further show how values are affirmed and commitment is strengthened under adversity.###Many authors cite Kluckhohn and Strodtbeck’s three assumptions on which her/their five value orientations model was based; that (1) all human societies have had to deal with a limited set of common problems; (2) that there is also a limited range of human alternatives for dealing with these problems; and (3) that each culture develops a preferred ranking for how to do so.###This is particularly true for cultures undergoing rapid change, as Florence Rockwood Kluckhohn and Fred L. Strodtbeck seminally postulated.###But their work has met with some of the same critiques as Hofstede. And some new critiques, including one from Hofstede (2006b), pointed out errors in the###This is in keeping with other authors, who consider culture to consist of complex patterns, “…explicit and implicit, of and for behavior acquired and transmitted by symbols, constituting the distinctive achievements of human groups…the essential core of culture consists of traditional (i.e. historically derived and selected) ideas and especially their attached values” (Kroeber & Kluckhohn, 1952:357).",impact-revealing,Highlighting the significant influence of Triandis on cross-cultural psychology
595,5e3940c73a55ace46ed436d2,845b4941d8c016aa5f8967da2f86d38ef6c18fa3,a survey on knowledge graphs representation acquisition and applications,5a260c8b17c44a4ba8a32f1f,Knowledge Graph Embedding: A Survey of Approaches and Applications.,"The joint loss function is widely applied when incorporating KGE with textual description.###The collaborative CKE [203] jointly trains KGEs, item’s textual information, and visual content via translational KGE model and stacked auto-encoders.###Previous survey papers on knowledge graphs mainly focus on statistical relational learning [4], knowledge graph reﬁnement [11], Chinese knowledge graph construction [13], knowledge reasoning [14], KGE [5] or KRL [9].###[8] proposed a definition as a multi-relational graph in Definition 2.###For the combination of logic rules and embeddings, recent works [102], [103] combine Markov logic networks with KGE, aiming to leverage logic rules and handling their uncertainty.###Recent advances in knowledge-graph-based research focus on knowledge representation learning (KRL) or knowledge graph embedding (KGE) by mapping entities and relations into low-dimensional vectors while capturing their semantic meanings [5], [9].###3) Graph Convolutional Networks (GCNs): GCNs are utilized for encoding a dependency tree over sentences or learning KGEs to leverage relational knowledge for sentence encoding.###[8] gave a detailed review on these information.###Previous survey papers on knowledge graphs mainly focus on statistical relational learning [9], knowledge graph refinement [6], Chinese knowledge graph construction [10], KGE [8] or KRL [11].###KRL is also known as KGE, multi-relation learning, and statistical relational learning in the literature.###[8] categorized KRL according to scoring functions, and specifically focused on the type of information utilized in KRL.",impact-revealing,reporting on various approaches and methods in knowledge graph research
453,5c7a561ff56def9798e6a297,abe4e92d6e4868115aef57a4eba97ec4d48ac124,language-adversarial transfer learning for low-resource speech recognition,558be27ee4b02b9f07a3d209,Multilingual training of deep neural networks,"is trained jointly on several languages [19], [20].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3736,5dbebb7447c8f766462c22a6,e1fd81af050dbdc4232ff8b1ab71cf0973d530b6,graph convolutional networks with motif-based attention,53e9b5edb7602d970413db3d,Triangles to Capture Social Cohesion,3 where a node can benefit from using a neighborhood defined using triangle motifs to keep only neighbors connected via a stronger bond which is a well-known concept from social theory allowing us to distinguish between weaker ties and strong via the triadic closure [12].,other,providing context on social theory concepts
1983,,79f9cb825ccc51a07c2b7e67b11ecd3d1e7caac8,Health information exchange interventions can enhance quality and continuity of HIV care,,,"###Recently, policy makers, clinicians and researchers have emphasized the importance of expanding the continuum of care for people with HIV to include not only primary HIV care and support services, but also linkage, engagement and retention in HIV care [4,5].",impact-revealing,highlighting the importance of comprehensive care for people with HIV
3050,5f02f17c91e011ee5e0258c8,3da4626411d83c19c9919bb41dba94fff88da90e,Scaling Graph Neural Networks with Approximate PageRank,5c8a11324895d9cbc6121c34,Neural Message Passing for Quantum Chemistry.,"Graph Neural Networks (GNNs) excel on a wide variety of network mining tasks from semi-supervised node classification and link prediction [25, 32, 44, 55] to community detection and graph classification [3, 15, 22, 37].###[22] or other similar frameworks [6, 12, 50].###[42] and have since emerged as a powerful approach for solving many network mining tasks [1, 2, 9, 17, 22, 32, 42, 44].",other,highlighting the versatility and effectiveness of Graph Neural Networks in network mining tasks
2998,5eb3df3191e011cea6a7c3c8,86746ca4e3fb61cbcfb8dc29d6779d51b03692e0,Effect of Character and Word Features in Bidirectional LSTM-CRF for NER,5d9edc1647c8f76646032985,Named Entity Recognition with Bidirectional LSTM-CNNs,"This paper, we combined two techniques proposed in Chiu et al., 2015 (CNN + Bidirectional LSTM) and Huang et al., 2015 (Bidirectional LSTM + CRF), respectively, using public word embedding, character features and word features [6,4].###A combination of word embedding with word and character features is proven to be reliable for increasing the accuracy of NER system by Chiu et al., 2015 [6].",other,describing a method combination for improving NER accuracy
1150,,a28cdccba07dbf977795e15ff2c9b7ec80dac050,Score-Based Generative Modeling with Critically-Damped Langevin Diffusion,,,"###(56)
Note again that Lt is different for HSM and DSM.
Analogously to prior work (Ho et al., 2020; Vahdat et al., 2021; Song et al., 2021b) an objective better suited for high quality image synthesis can be obtained by “dropping the variance prefactor”:
HSM ( λ(t) = ( `HSMt )−2) = Et∼U…###The only models marginally outperforming CLD are LSGM (Vahdat et al., 2021) and NSCN++/VESDE with 2,000 step predictorcorrector (PC) sampling (Song et al., 2021c).###Our models are based on the NCSN++ and the DDPM++ architectures from Song et al. (2021c).###As has been shown in Song et al. (2021b), Eq .equation 40 can be written as a mixture of score matching losses.###Currently used SDEs (Song et al., 2021c; Kim et al., 2021) have drift and diffusion coefficients of the simple form f(xt, t)=f(t)xt andG(xt, t)=g(t)Id. Generally, f and G are chosen such that the SDE’s marginal, equilibrium density is approximately Normal at time T , i.e., p(uT )≈N (0, Id).###In particular, Song et al. (2021c) model data x, setting p(u0)=pdata(x).###In the seminal work by Song et al. (2021c), it has been shown that the score function that needs to be learnt by the neural network is uniquely determined by the forward diffusion process.###However, this expression corresponds to
xn+1 ∼ N (xn+1; √
1− β̂(t)xn, β̂(t)) (22) which is exactly the transition kernel of the VPSDE’s Markov chain (Ho et al., 2020; Song et al., 2021c).###We additionally present results for the VPSDE using the DDIM (Denoising Diffusion Implicit Models) sampler (Song et al., 2021a).###Following Song et al. (2021c), we focus on the widely used CIFAR-10 unconditional image generation benchmark.###As was observed by Song et al. (2021a), QS also helps for DDIM.###10For the ML objective of VPSDE, we refer the reader to Song et al. (2021b).###Score-based generative models (SGMs) and denoising diffusion probabilistic models have emerged as a promising class of generative models (Sohl-Dickstein et al., 2015; Song et al., 2021c;b; Vahdat et al., 2021; Kingma et al., 2021).###However, prior work (Song et al., 2021a; Kong & Ping, 2021; Watson et al., 2021) has shown that it can be beneficial to focus function evaluation on times t close to the “data”.###We focus on image synthesis and implement CLD-based SGMs using NCSN++ and DDPM++ (Song et al., 2021c) with 6 input channels (for velocity and data) instead of 3.###Analogously to prior work (Ho et al., 2020; Vahdat et al., 2021; Song et al., 2021b), an objective better suited for high quality image synthesis can be obtained by setting λ(t) = `−2t , which corresponds to “dropping the variance prefactor” ` 2 t .###If we assume a diffusion as above but with the potential term (ii) set to 0, we can similarly derive the VESDE Song et al. (2021c) as a high-friction limit of the corresponding diffusion.###We can use it to generate novel data by sampling the prior and solving this ODE, like previous works (Song et al., 2021c).###Different λ(t) result in different trade-offs between synthesis quality and likelihood in the generative model defined by sθ(xt, t) (Song et al., 2021b; Vahdat et al., 2021).###9 are better than those presented in Song et al. (2021a) itself, because we are relying on the DDPM++ model trained in Song et al. (2021c), whereas Song et al. (2021a) uses the DDPM model from Ho et al. (2020).###This means that for both CLD as well as VPSDE Song et al. (2021c) we can diffuse pdata(x) analytically.###Note that in practice we do not use SSCS to integrate all the way from t = 0 to t = T , but only up to t = T − , and perform a denoising step, similar to previous works (Jolicoeur-Martineau et al., 2021a; Song et al., 2021c).###Diffusions such as the VPSDE (Song et al., 2021c) correspond to overdamped Langevin dynamics with high friction coefficients Γ (see App.###All other results for VESDE and VPSDE are generated using the provided PyTorch code as well as the provided checkpoints from Song et al. (2021c)7.###Some works study SGMs for maximum likelihood training (Song et al., 2021b; Kingma et al., 2021; Huang et al., 2021).###…ūt := uT−t, a corresponding reverse-time diffusion process that inverts the above forward diffusion can be derived (Anderson, 1982; Haussmann & Pardoux, 1986; Song et al., 2021c) (with positive dt and t ∈ [0, T ]):
dūt = [ −f(ūt, T − t) +G(ūt, T − t)G(ūt, T − t)>∇ūt log pT−t(ūt) ]…###Following a similar derivation as Song et al. (2021b), we obtain the score matching (SM) objective (see App.###Recently, techniques to accelerate sampling from pre-trained SGMs have been proposed (San-Roman et al., 2021; Watson et al., 2021; Kong & Ping, 2021; Song et al., 2021a).###For SDEs acting in the data space directly, it has been reported that this denoising step is crucial to obtain good FID numbers Jolicoeur-Martineau et al. (2021b); Song et al. (2021c).###To derive the objective for training CLD-based SGMs, we start with a derivation that targets maximum likelihood training in a similar fashion to Song et al. (2021b).###For fair comparison, we train our models using the same t-sampling cutoff during training as is used for VESDE and VPSDE in Song et al. (2021c).###To simulate the SDE of the reverse-time diffusion process, previous works often relied on Euler-Maruyama (EM) (Kloeden & Platen, 1992) and related methods (Ho et al., 2020; Song et al., 2021c; Jolicoeur-Martineau et al., 2021a).###Note that in practice sθ(ut, t) won’t be a perfect model, though, such that the generative models defined by simulating the reverse-time SDE and the probability flow ODE are not exactly equivalent (Song et al., 2021b).###F.2 we also used DDIM (Song et al., 2021a) to sample the VPSDE.###…calculating a stochastic estimate of the logdeterminant of the Jacobian via Hutchinson’s trace estimator (and also calculating the probability of the output under the prior), as done in Normalizing flows (Chen et al., 2018; Grathwohl et al., 2019) and previous works on SGMs (Song et al., 2021c;b).###We also evaluate an upper bound on the negative loglikelihood (NLL): − log p(x0)≤−Ev0∼p(v0) log pε(x0,v0)−H , where H is the entropy of p(v0) and log pε(x0,v0) is an unbiased estimate of log p(x0,v0) from the probability flow ODE (Grathwohl et al., 2019; Song et al., 2021c).###(2)) or, alternatively, solve the corresponding probability flow ODE (Song et al., 2021c;b) (see App.###For λ(t) = Γβ, the objective corresponds to maximum likelihood learning (Song et al., 2021b) (see App.###Furthermore, when drawing a sample u0 to diffuse in
DSM, we are essentially placing an infinitely sharp Normal with unbounded score (Kim et al., 2021) at u0, which requires undesirable modifications or truncation tricks for stable training (Song et al., 2021c; Vahdat et al., 2021).###…θ
Et∈[0,T ]Ex0∼p0(x0)E 2d∼N ( 2d;02d,I2d) [ λ(t)`2t‖ d:2d − αθ(ut=µt(x0)+Lt 2d, t)‖22 ] , (10)
which corresponds to training the model to predict the noise only injected into the velocity during reparametrized sampling of ut, similar to noise prediction in Ho et al. (2020); Song et al. (2021c).###We build on Song et al. (2021c), which introduced the SDE framework for modern SGMs. Nachmani et al. (2021) recently introduced non-Gaussian diffusion processes with different noise distributions.###5 in Song et al. (2021c)).###We compare to Song et al. (2021c) and use EM to solve the generative SDE for their VPSDE and PC (reverse-diffusion + Langevin sampler) for the VESDE model.###For methods without adaptive stepsize (EM and SSCS), we use evaluation times chosen according to a quadratic function, like previous work (Song et al., 2021a; Kong & Ping, 2021; Watson et al., 2021) (indicated by QS).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3253,5ec49a639fced0a24b4de922,0d965ed237a3b4592ecefdb618c29f63adedff76,Towards Debiasing Sentence Representations,5a9cb66717c44a376ffb8ac1,Deep contextualized word representations.,"More recently, sentence-level representations such as ELMo (Peters et al., 2018), BERT (De-vlin et al., 2019), and GPT (Radford et al., 2019) have become the preferred choice for text sequence encoding.###More recently, sentence-level representations such as ELMo (Peters et al., 2018), BERT (Devlin et al.###, 2019) and ELMo (Peters et al., 2018), showing that our approach reduces the bias while preserving performance on downstream sequence tasks.###Our experiments are performed on two widely popular sentence encoders BERT (De-vlin et al., 2019) and ELMo (Peters et al., 2018), showing that our approach reduces the bias while preserving performance on downstream sequence tasks.###D EBIAS on two widely-used sentence encoders: BERT 2 (Devlin et al., 2019) and ELMo (Peters et al., 2018).",other,acknowledging the trend towards sentence-level representations in text encoding
2760,5f3268fb91e011bc1612aeab,dee8650c0a65588a09eb86751c600fb67a030bbc,Speech Driven Talking Face Generation From a Single Image and an Emotion Condition,573696116e3b12023e52461d,Emotion Classification: How Does an Automated System Compare to Naive   Human Coders?,This observation is similar to a speech emotion classification observation in [14].###Studies have shown that predicting emotions purely from speech audio is quite difficult for untrained people [14] and that we heavily rely on visual cues in emotion interpretation [15].,other,acknowledging challenges in predicting emotions from speech
1466,,85630fa72a780d81440f18e69fce59a62d826598,Concepts and Transformational Knowledge,,,"###One approach, following failures to observe ‘‘common features’’ of members of a category (e.g., Rosch, 1975b; Hampton, 1987), has been to posit perceived causality, naı̈ve theories, and/or goals as the basis for the coherence of members of a category.###Instead, the prediction concerning response time was motivated by the literature that shows that participants react faster to stimuli that are expected than ones that are not (e.g., Rosch, 1975a; 1975b).",impact-revealing,discussing theoretical approaches to category coherence
2411,5ee8986891e011e66831c3b6,38f93092ece8eee9771e61c1edaf11b1293cae1b,Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,53e9b1b6b7602d9703c474a1,DeCAF: A Deep Convolutional Activation Feature for Generic Visual   Recognition,"We have followed the approach used in [96]: This file defines datasets for 5 random splits of 25 training images per category, with 5 validation images per category and the remaining images used for testing.###[96] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell.",other,reporting methodology for dataset splits
1605,,4dcef4d040cdbc17eb8e7e39d1456c2a1ab691a0,SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models,,,"###However, our algorithm, which is inspired by Morris et al. (2020); Zhang et al. (2021), generates distractors that uncover model implicit biases.###Note that Morris et al. (2020); Zhang et al. (2021) propose algorithms that perturb text inputs to test model robustness; in contrast, our contribution is an automatic distractor generation algorithm designed to measure model demographic fairness.",impact-revealing,highlighting the distinction between existing algorithms and the proposed contribution
3689,57a4e92bac44365e35c9ab55,d98063b0eb446c99e98684b34cb53914ca6b7206,a survey of techniques for architecting dram caches,53e99a26b7602d970227edb5,Die Stacking (3d) Microarchitecture,"have led to significant improvement in these properties [6].###We refer the reader to prior work for more details on DRAM architecture [21], [22], [23], [24] and die-stacking technology [2], [6], [25].###As mentioned before, die-stacking provides significant latency and bandwidth benefits compared to conventional off-chip memory [6], [25], [33].###[6] evaluate different SRAM and DRAM 2D/ 3D cache designs from performance and thermal perspectives.###Although DRAM provides nearly 8 [6], [7] density advantage compared to SRAM, its relatively long latency had conventionally restricted its use to designingmainmemories only.",other,highlighting the benefits of die-stacking technology in memory architecture
1321,,ae9a08f2e019938fb468fad9426a3f2c13122a30,Does Stress-Related Growth Really Matter for Adolescents’ Day-to-Day Adaptive Functioning?,,,"###For continuous variables, these techniques are identical to those described and validated by Schafer and Graham (Graham & Hofer, 2000; Schafer & Graham, 2002).###This technique has been shown to perform well when data are missing at random and even acceptably under some cases of nonrandom missingness (Schafer & Graham, 2002).",impact-revealing,reporting prior findings on techniques for handling missing data
1351,,e508791b03ca3a4aaf6c9494ba05f609729778e1,Age-dependency in host-vector models: The global analysis,,,"###Introduction The emergence and re-emergence of vector-borne diseases, such as malaria, dengue fever, Chagas disease, yellow fever, Japanese encephalitis and many other, is driven by social and economical factors and, most impotently, the climate change, and widely recognized as a global problem [13, 14].",impact-revealing,acknowledging the global problem of vector-borne diseases and their drivers
3937,599c7982601a182cd2645d4e,88d346374f17189cebef7394ae5d39492443df89,Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution,58437725ac44360f108302aa,Real-Time Single Image and Video Super-Resolution Using an Efficient   Sub-Pixel Convolutional Neural Network,"SRCNN [7] LR + bicubic 3 No Direct L2 FSRCNN [8] LR 8 No Direct L2 SCN [33] LR + bicubic 5 No Progressive L2 ESPCN [28] LR 3 No Direct L2 VDSR [17] LR + bicubic 20 Yes Direct L2 DRCN [18] LR + bicubic 5 (recursive) No Direct L2 LapSRN (ours) LR 27 Yes Progressive Charbonnier###Several algorithms accelerate SRCNN by performing convolution on LR images and replacing the pre-defined upsampling operator with sub-pixel convolution [28] or transposed convolution [8] (also named as deconvolution in some of the literature).###To achieve real-time performance, the ESPCN network [28] extracts feature maps in the LR space and replaces the bicubic upsampling operation with an efficient sub-pixel convolution.###The FSRCNN network [8] adopts a similar idea and uses a hourglass-shaped CNN with more layers but fewer parameters than that in ESPCN.###Table 1: Comparisons of CNN based SR algorithms: SRCNN [7], FSRCNN [8], SCN [33], ESPCN [28], VDSR [17], and the proposed LapSRN.",other,comparing various CNN-based super-resolution algorithms
855,5b67b47917c44aac1c8639a8,957e4c03e7846a1f8670b250f5f9661c91dfcb75,Widar2.0: Passive Human Tracking with a Single Wi-Fi Link,53e9b54fb7602d9704086567,See through walls with WiFi!,"Various hardware [2–4 , 11] are manufactured manufactured to Since dedicated hardware are difficult to be generalized, re-searchesareshiftedtoubiquitousCOTSRFdevices,suchasRFID [26, 27, 33, 42], millimetre wave [35, 45] and Wi-Fi [21, 32].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3507,5a73cb4d17c44a0b3035672d,7cd5d7f8295b219b029a4231ae5cffb261e00ebe,Early Active Learning with Pairwise Constraint for Person Re-identification,5550441c45ce0a409eb4b537,Semi-Supervised Ranking For Re-Identification With Few Labeled Image Pairs,"The primary target of person re-identification (re-id) is to identify a person from camera shots across pairs of non-overlapping camera views, and research on this topic has attracted considerable attention in recent years [8,9,10,15,29].",other,highlighting the growing interest in person re-identification research
3396,5b67b45517c44aac1c860884,9fa3e53b5937a0ec92499ed415e339ede6c92010,DeepInf: Social Influence Prediction with Deep Learning,53e9bab4b7602d97046e48fb,Social influence locality for modeling retweeting behaviors,"Weibo [53, 54] Weibo 6 is the most popular Chinese microblogging service.###The dataset is from [53] and can be downloaded here.###Data Preparation We process the above four datasets following the practice in existing work [53, 54].###The above observations inspire a lot of user-level influence prediction models, most of which [27, 53, 54] consider complicated hand-crafted features, which require extensive knowledge of specific domains and are usually difficult to generalize to different domains.###Social Influence Locality[53] Social influence locality models the probability of v ’s action status conditioned on her r -ego network G rv and the action states of her r -neighbors.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2860,5f7fdd328de39f0828397ac2,a5fa6e7565dca654eab9372ace4b1ba7f63655f7,CogLTX: Applying BERT to Long Texts,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Pretrained language models, pioneered by BERT [12], have emerged as silver bullets for many NLP tasks, such as question answering [38] and text classification [22].###The direct and superficial obstacle for long texts is that the pretrained max position embedding is usually 512 in BERT [12].",other,highlighting the limitations of pretrained language models for long texts
3367,5db929e147c8f766461fc024,e03b5bc5edeb44d4b47d225c0c26ac54088fe528,An Improved Neural Baseline for Temporal Relation Extraction,59ae3c152bbe271c4c71e959,Classifying Temporal Relations By Bidirectional Lstm Over Dependency Paths,"There have indeed been such attempts, e.g., in clinical narratives (Dligach et al., 2017; Lin et al., 2017; Tourille et al., 2017) and in newswire (Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018).",other,acknowledge existing attempts in specific domains
1679,,5c80df00320ca8837051e0752977963c6c2aa9b6,Narratives of Being a Helper: The Presentation of Supporters and Victims in Victim Support Sweden,,,"###They build on social identity theory, in which social identity is understood as being built from a feeling of similarity to persons within the same group, which is strengthened by seeking differences from others (Tajfel, 1974; Tajfel & Turner, 1979).",impact-revealing,providing context on social identity theory
725,5c9df4643cb210d271bea0dd,62c13867cc9d80639100dc7bd4151f728c27d9ab,efficient load value prediction using multiple predictors and filters,5a260c3b17c44a4ba8a26007,Load value prediction via path-based address prediction: avoiding mispredictions due to conflicting stores.,"The result of any instruction type can be predicted, though in this paper we focus only on predicting load values since that is most effective with limited hardware resources [3], [4].###VPE provides the mechanism needed to communicate the predicted values from the value-predicted producers to their consumers [3].###Predicts Load values Load addresses Context agnostic Last Value Prediction (LVP) [1] Stride Address Prediction (SAP) [6] Context aware Context Value Prediction (CVP) [7], [8] Context Address Prediction (CAP) [3]###Readers may notice that our reported benefit for each design is sometimes significantly lower than previously published results; this is caused by different assumptions about the baseline ISA, microarchitecture, and storage constraints (Sheikh reports similar findings [3], [27]).###, 8KB of prediction state [3], [4]).###Sheikh reports similar findings in [3], [27].###To learn more about the two load value prediction approaches and the recent advances towards practical implementations of value prediction, we encourage the readers to visit prior art papers [3], [7], [8].###work confirmed the same is true for load instructions in particular [3], [4].###We use the state-of-the-art DLVP predictor as a reference design [3].###, DLVP [3]).",impact-revealing,providing context for load value prediction methods
1168,,303b77fe0347b92369ac1ce22030c4333d3ca49e,Extending Network Lifetime Using an Automatically Tuned Energy-Aware MAC Protocol,,,"###Asynchronous protocols [15, 8 ,9] are motivated by clock synchronization overhead.",impact-revealing,highlighting the motivation behind asynchronous protocols
291,5e5e18ad93d709897ce2654c,19605cad33f79d3070b7c4b24aa49653ab7c90da,Inductive Matrix Completion Based on Graph Neural Networks,599c798d601a182cd264a97a,Graph Convolutional Matrix Completion.,"This is reflected in that previous node-based approaches mainly use only one or two message passing layers (Berg et al., 2017; Ying et al., 2018a).###Our node labeling is also different from using the global node IDs as in GC-MC (Berg et al., 2017).###…IGMC with SumPooling replacing the proposed pooling layer, 2) IGMC without ARR by setting λ in (7) to 0, and 3) IGMC with content by concatenating target user and item’s content feature vectors to the graph representation g before feeding into the MLP (4), which is similar to (Berg et al., 2017).###Berg et al. (2017) propose graph convolutional matrix completion (GC-MC) which directly applies a GNN to the user-item bipartite graph to extract user and item latent features using a GNN.###In our experiments we only concatenate the target user and item’s content vectors with the final graph representation output by the GNN, similar to (Berg et al., 2017).###On the other hand, as Berg et al. (2017) and Zhang & Chen (2018) found, directly concatenating content with initial node features (the one-hot encoding vectors) as the input to GNN often led to worse performance due to information flow bottlenecks.###For these three datasets, we compare our IGMC with GRALS (Rao et al., 2015), sRGCNN (Monti et al., 2017), GC-MC (Berg et al., 2017), F-EAE (Hartford et al., 2018), and PinSage (Ying et al., 2018a).###, 2017), GC-MC (Berg et al., 2017), F-EAE (Hartford et al.",impact-revealing,acknowledge differences in node-based approaches and their limitations
1655,,5a8d1c4d557d30174ae0a7b1bf53270dade723e3,Can political speech foster tolerance of immigrants?,,,"###First, individuals self-categorize as members of a specific social group, leading to identification with that group and giving rise to a social identity (Tajfel and Turner, 1979).###Together, these three mechanisms—categorization, maintaining membership of a favored in-group, and out-group threat perceptions—reduce the tolerance that in-group members exhibit in their attitudes and actions toward out-group members (see, e.g., Tajfel and Turner, 1979).###These interventions build on individuals’ psychological need to maintain group membership (e.g., Tajfel and Turner, 1979), which requires following in-group norms that delineate the attitudes, behaviors, and traits of the in-group from those of out-groups (Hogg and Reid, 2006).###Third, due to a need for positive self-esteem, individuals extend positive regard, cooperation, and empathy to in-group members, but negatively evaluate the out-group (Tajfel and Turner, 1979; Turner and Reynolds, 2012).###First, we build on insights from social identity theory (SIT; Tajfel and Turner, 1979) to identify the mechanisms that account for the reduced tolerance of out-groups.###Tolerance toward out-groups is based on in- and out-group conceptions (Allport, 1954; Tajfel and Turner, 1979; Citrin et al., 1990), and we argue that these types of speech have the potential to shift natives’ perceptions of immigrants as an out-group.",impact-revealing,highlighting the mechanisms of social identity theory and its implications for out-group tolerance
1104,,0fcac1046a87715ff9129645c2bc08e740f81f2a,Hugin: a framework for awareness and coordination in mixed-presence collaborative information visualization,,,"###Additionally, Hugin builds on a large number of principles drawn from the literature in computer-supported collaborative work, such as personal territories for identity tracking and object ownership [21], information access through an object layer system [28], and awareness of other participants’ workspaces through features such as telefingers and a minimap [13, 14].###[13, 14], it provides the following functionality:",impact-revealing,acknowledge foundational principles in computer-supported collaborative work
1508,,d22475d371b0ec0ea9b7f0bc2b417ec2aa4c9807,Kinship Determination in Mobile Social Networks,,,###[7] emphasized the importance of network structure analysis on research of social networks.,impact-revealing,highlighting the significance of network structure analysis in social network research
1414,,7ed1566a286068f39effa67ae5c7489dbea06414,VRL3: A Data-Driven Framework for Visual Deep Reinforcement Learning,,,"###Following [37], and build on top of our framework, we use an additional conservative Q loss LQC term: LQC(D) = Eτ∼D [ log ∑ ãt exp(Q(st, ãt))−Q(st,at) ] .###On a high-level, this loss essentially reduces the Q value for actions proposed by the current policy, and increase the Q values for actions in the replay buffer [37].###Although off-policy algorithms do not naively work in offline settings [18, 49], many specialized algorithms are proposed to allow effective offline training [18, 37, 8, 63, 88, 36, 17, 87, 34, 92, 2, 20, 89].###Though this is not really discussed in the CQL paper [37], the number of random actions might have an effect on offline training.###[37] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.###Stage 2, conservative term weight (default: 1): this weight decides how hard the Q values are being affected by the conservative loss term [37].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3340,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,53e9b077b7602d9703ae1750,Semantic Annotation Of Multimedia Using Maximum Entropy Models,"[7], which have used maximum entropy model for multimedia analysis tasks, however in these works the authors used only single modality rather than multiple modalities.",other,acknowledge limitations in existing multimedia analysis approaches
3800,58d82fced649053542fd7453,2f85b7376769473d2bed56f855f115e23d727094,wasserstein gan,58437725ac44360f1082fca6,Energy-based Generative Adversarial Network,• Energy-based GANs (EBGANs) [25] can be thought of as the generative approach to the total variation distance.,other,providing context about a scientific term
2593,5b1642388fbcbf6e5a9b5740,3913d2e0a51657a5fe11305b1bcc8bf3624471c0,learning structured representation for text classification via reinforcement learning,53e9b221b7602d9703cb7c7e,Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions.,"• RAE: Recursive autoencoder which is defined on predefined parsing structure (Socher et al. 2011).###All rights reserved.
and recursive autoencoders (Socher et al. 2013; 2011; Qian et al. 2015) use pre-specified parsing trees to build structured representations.",other,reporting existing methods in recursive autoencoders
2879,57d063e0ac44367354294777,9c62532d78d6eb31f7b0489b6bcb4c2baf517860,CASH: Supporting IaaS Customers with a Sub-core Configurable Architecture,555045ee45ce0a409eb5ab6e,Towards Energy Proportionality For Large-Scale Latency-Critical Workloads,"PEGASUS [36], a feedback-based controller has been proposed to improve energy proportionality of warehouse scale computer systems.",other,reporting prior findings on PEGASUS
126,57a4e91aac44365e35c975d0,78aa018ee7d52360e15d103390ea1cdb3a0beb41,Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples,573696076e3b12023e51a63f,The Limitations of Deep Learning in Adversarial Settings,"Many classes of machine learning algorithms have been shown to be vulnerable to adversarial samples [22, 12, 19]; adversaries subtly alter legitimate inputs (call input perturbation) to induce the trained model to produce erroneous outputs.###Building on previous work [22, 12, 19] describing how adversaries can eﬃciently select perturbations leading deep neural networks to misclassify their inputs, we introduce new crafting algorithms for adversaries targeting Support Vector Machines (SVMs) and Decision Trees (DTs).###Learning substitute models approximating the decision boundaries of targeted classiﬁers alleviates the need of previous attacks [22, 12, 19] for knowledge of the target architecture and parameters.###To craft adversarial samples misclassiﬁed by DNNs, an adversary with knowledge of the model f and its parameters θ can use the fast gradient sign method introduced in [12] or the Jacobian-based iterative approach proposed in [19].###In one such attack, Papernot et al. trained a local deep neural network (DNN) using crafted inputs and output labels generated by the target “victim” DNN [19].",impact-revealing,highlighting vulnerabilities of machine learning algorithms to adversarial samples and introducing new crafting algorithms
1038,,49bf8d40a6584c4eef36abddadd0833dc5412f26,Design and Implementation of Client IP Notification Feature on DNS for Proactive Firewall System,,,"###After all a procedure called domain name resolution (as a simple example, it translates the hostname or domain name to the IP address, although it has many more features but we cut the details here) is required prior to the application layer communication and DNS is the most widely used for the…",impact-revealing,providing context for domain name resolution in networking
446,5ac1829d17c44a1fda9180a3,632d25ef7914ce962d258920460a9405b8c4553a,Sequence-Based Multi-Lingual Low Resource Speech Recognition,53e9b56cb7602d97040a65cc,The language-independent bottleneck features,"Unlike [5], we do not have any bottleneck layer, and the whole model is sequence trained based on CTC loss.###The standard approach is to train a context dependent Hidden Markov Model based Deep Neural Network acoustic model with a “bottleneck” layer using a frame based criterion on a large multilingual corpus [5, 6, 7].###The work by [5, 16] presented bottleneck features for multi-lingual systems where they showed feature porting is possible and gave competitive results when compared to systems with mono-lingual features.###been the so-called “shared hidden layer” model, in which data is passed through a series of shared feed-forward layers, before being separated into multiple language-specific softmax layers, which are trained using cross-entropy [13, 5, 14].",impact-revealing,comparing model architectures and training methods
3869,5c2c7a9217c44a4e7cf3189c,9ae43e25b04f5c35173b0bf490612015bd86c08f,Face-Focused Cross-Stream Network for Deception Detection in Videos,53e9a611b7602d9702f42fa8,Automatic deception detection in Italian court cases,"The change towards deception detection with real-life data was first advocated in [7], where the identification of deception in statements issued by witnesses and defendants is targeted using a corpus collected from hearings in Italian courts (i.",other,acknowledging the shift towards deception detection in real-life data
3040,5d1eb9b7da562961f0af38c9,c0f5e89cf9f1b4f3d9c76a2ae3b13315e691554b,Personalized Student Stress Prediction with Deep Multitask Network,555044bd45ce0a409eb507f3,Towards accurate non-intrusive recollection of stress levels using mobile sensing and contextual recall,"Other techniques do not depend on sensors but simply try to discover the user’s stress through self-reporting tools e.g., (Rahman et al., 2014) and surveys like the Perceived Stress Scale (Cohen et al., 1983).",other,acknowledge alternative techniques for stress detection
2297,5f9be24691e011dcf482d8d6,842bd2d15d53e3083c110e0af55ffd7447ad03e4,Prediction-Based Power Oversubscription in Cloud Platforms,53e9b1d1b7602d9703c64131,DeepDive: Transparently Identifying and Managing Performance Interference in Virtualized Environments.,"Many works select workload placements to reduce performance interference or energy usage, e.g. [3], [4], [8], [26], [30], [33].",other,acknowledging prior findings on workload placements
1257,,a950313c9820f3479012bf7667b1f06f074c7dee,Coordinating Multiple Double Integrator Robots on a Roadmap: Convexity and Global Optimality,,,"###We use solver MINLP [25], which implements a branch-and-bound algorithm searching a tree whose nodes correspond to continuous nonlinear optimization problems.###Here we develop a direct approach to solve the MINLP that builds on recent advances in efficiently solving convex NLP relaxations of MINLPs [10][25].###We use the MINLP Solver [25], which combines a branchand-bound algorithm with a filterSQP algorithm, to solve the MINLP coordination problems.###There is a literature on the theory, algorithms, and solvers to find the global optimum solution of a convex MINLP ([10],[25]).###The resulting models were solved by using the solver MINLP [25], which is a nonlinear programming solver that integrates a sequential quadratic programming algorithm and a branch-and-bound strategy for solving mixed integer nonlinear programming problems.",impact-revealing,describing the method used for solving MINLP problems
4031,5e79da4491e0115bb1157b77,8e74106415626a21bb909ff489ef310625c769e1,Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles,5843778eac44360f108445f0,Measuring the Similarity of Sentential Arguments in Dialogue.,"as the ones used in BERT [15] and XLNet [41]. The Transformer architecture allowed the efficient unsupervised pretraining of language models and led to significant improvements in many NLP benchmarks [26, 39, 43]. Reimers and Gurevych [34] proposed to combine BERT with a Siamese architecture [9] for semantic representations of sentences and their similarity [26]. In prior work [32], we also utilized a Siamese",other,highlighting the significance of the Transformer architecture in NLP advancements
2695,5d9ed30647c8f76646f7f04c,f160c69c428122e8fa7ba96f220b4ded5f8761f4,ARNOR: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification,53e9b89bb7602d97044763cc,Distant supervision for relation extraction without labeled data.,"To automatically obtain a large training dataset, DS has been proposed (Mintz et al., 2009).###Mintz et al. (2009) reports that distant supervision may lead to more than 30% noisy instances.###In order to cheaply obtain a large amount of labeled RC training data, Distant Supervision (DS) (Mintz et al., 2009) was proposed to automatically generate training data by aligning a knowledge base with an unlabeled corpus.",other,reporting prior findings on distant supervision and its implications
3457,5e15adca3a55ac47ab5b0729,97ebd482a78e6e6c1ba51da5e1b2f8e7640cc8b5,hyperbolic graph convolutional neural networks,57aa28de0a3ac518da9896d5,node2vec: Scalable Feature Learning for Networks,"Examples include matrix factorization [3, 24] and random walk methods [12, 31].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1515,,c0e0867371f33c8321f461674ae70b0eff78fda9,Prognostic value of biochemical variables for survival after surgery for metastatic bone disease of the extremities,,,"###We encourage future research to validate our findings of neutrophil count being an independent prognostic factor for survival in patients having surgery for MBD as has been done for patients with various cancers.(24,25) In general, this study is limited by a relatively small sample size and a high percentage of missing data for the neutrophil and lymphocyte count, which can be expected from the retrospective design of the study.",impact-revealing,suggesting future research directions based on current findings
2472,5ea16b2b91e011fa08b8f8d9,9cc444d4deb0a291059c43bc5657bb7743a846cc,torchgpipe: On-the-fly Pipeline Parallelism for Training Giant Models,5b67b4b417c44aac1c866ee2,PipeDream: Fast and Efficient Pipeline Parallel DNN Training.,"It is benefitting to combine different types of parallelization strategies [16, 14, 26, 12, 9, 11, 7], and recent lines of research questions how to find an optimal strategy###Among them, pipeline parallelism a way to accelerate neural network training by combining model parallelism with data pipelining, either in synchronous way as in GPipe [11] or in asynchronous way as in [12], PipeDream [9], and XPipe [7].",other,highlighting the benefits of combining parallelization strategies in neural network training
3365,53e99f3bb7602d970280aaa9,5b324db597c57cbed6b15baa757e3bf7611dd06c,Practical off-chip meta-data for temporal memory streaming,53e99ae2b7602d97023691f2,Temporal Instruction Fetch Streaming,"To support variable length temporal streams while maintaining storage efficiency, several designs separate the storage of address sequences and correlation data [ 10 ,21,27].###More recent addresscorrelating prefetchers use a single correlation to predict a sequence of successor misses [6,9, 10 ,21,23,27].###In contrast to stride-based approaches, addresscorrelating prefetchers [3,5,6,9, 10 ,13,16,18,21,23,27] are effective for repetitive, yet arbitrarily-irregular access patterns, such as the pointer-chasing access patterns of commercial workloads [5,6,26,27].###Addresscorrelating prefetchers associate a miss address with a set of possible successor misses, or, in Temporal Memory Streaming (TMS) [ 10 ,26,27] and similar recent proposals [6,9,21,23], a sequence of successors.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1892,,76c618c95f3a873c12a8b0c6fdd55bf2b9bc70b1,Perceived and Actual Cognitive Presence: A Case Study of an Intentionally-Designed Asynchronous Online Course,,,"###Social presence is categorized into three indicators: (1) open communication, where students have mutual trust and express ideas with risk-free; (2) affective expression, where students express emotions and camaraderie using personal expressions of feelings, beliefs, and values; and (3) group cohesion, where students build and maintain a sense of community with a feeling of belongingness and group commitment (Garrison et al., 2000; Swan & Shih, 2005; Tolu & Evans, 2013).###The primary responsibility of teaching presence is to enhance social and cognitive presence through design, facilitation learning, and direct instruction (Garrison et al., 2000).###Social presence refers to the extent that learners can present themselves as “real people” in an online learning environment (Garrison et al., 2000).###…students express emotions and camaraderie using personal expressions of feelings, beliefs, and values; and (3) group cohesion, where students build and maintain a sense of community with a feeling of belongingness and group commitment (Garrison et al., 2000; Swan & Shih, 2005; Tolu & Evans, 2013).###Figure 1 Community of Inquiry Framework (Garrison et al., 2000)###The authors adopted the survey from the CoI framework that was developed to understand the dynamics of online learning experiences in line with the traditional values of higher education to support discourse and reflection (Garrison et al., 2000), and the instrument developed to capture three areas of CoI framework was validated by the authors through a principal component analysis to be a valid measure for teaching, social, and cognitive presences (Arbaugh et al.###Teaching presence consists of three elements: design and organization, the facilitation of learning, and direct instruction in online courses (Garrison et al., 2000; Garrison & Akyol, 2013; Tolu & Evans, 2013).###…understand the dynamics of online learning experiences in line with the traditional values of higher education to support discourse and reflection (Garrison et al., 2000), and the instrument developed to capture three areas of CoI framework was validated by the authors through a principal…###as “real people” in an online learning environment (Garrison et al., 2000).###Community of Inquiry Framework (Garrison et al., 2000) Figure 1 shows that social presence, teaching presence, and cognitive presence are all interrelated to create an effective online educational experience for the learner.",impact-revealing,defining social presence and its indicators in online learning
1535,,40ce71fa1203a422dd1a377f1013b5160184ac39,Factors graduate students of color find supportive and challenging and the coping strategies they utilize,,,"###This finding is not surprising considering past studies have found that students of color often have less positive experiences and less feelings of belongingness than White students (Ancis et al., 2000; Sue et al., 2007).###Unfortunately, these findings build on previous studies, which has found that students of color experience high levels of stereotyping and racial micro-aggressions (Clark et al., 2012; Maton et al., 2011; Sue et al., 2007).###Racial micro-aggressions are described as “brief, everyday exchanges that send denigrating messages to people of color because they belong to a racial minority group…often unconsciously delivered in the form of subtle snubs, or dismissive looks, gestures, and tones” (Sue et al., 2007, p. 273).###often unconsciously delivered in the form of subtle snubs, or dismissive looks, gestures, and tones” (Sue et al., 2007, p. 273). In the Clark et al. (2012) study the ethnic minority students also reported lower levels of belongingness###levels of stereotyping and racial micro-aggressions (Clark et al., 2012; Maton et al., 2011; Sue et al., 2007).",impact-revealing,highlighting the significance of previous findings on racial micro-aggressions and their impact on students of color
190,5cd7fa07ced107d4c65bf34f,371c799bde8b162e7f8fa2b2a0a8cfb29765f89f,Knowledge Graph Convolutional Networks for Recommender Systems,5a260c8117c44a4ba8a30f54,Graph Attention Networks.,"xed-size set of neighbors as the support size [7]. Our work can be seen as a non-spectral method for a special type of graphs (i.e., knowledge graph). Our method also connects to PinSage [21] and GAT [15]. But note that both PinSage and GAT are designed for homogeneous graphs. The major difference between our work and the literature is that we offer a new perspective for recommender systems with the a###bability iteration D Es è ä å ááÐ- è è ä áÐ/ è è ä áÐ0 è :è; (b) Figure1:(a)Atwo-layerreceptivefield(greenentities)ofthe blue entity in a KG. (b) The framework of KGCN. •Neighbor aggregator [15] directly takes the neighborhood representation of entityvas the output representation: aддneiдhbor = σ  W ·vu S(v) +b  . (6) Aggregation is a key step in KGCN, because the representation of an item",impact-revealing,providing context for a new method in knowledge graphs
687,5d1eb9b7da562961f0af38c9,c0f5e89cf9f1b4f3d9c76a2ae3b13315e691554b,Personalized Student Stress Prediction with Deep Multitask Network,5a4aef9e17c44a2190f7a547,Towards Deep Learning Models for Psychological State Prediction using Smartphone Data: Challenges and Opportunities.,"Due to a heavy imbalance of class labels on a scale of 1-5, we follow (Mikelsons et al., 2018), converting the ﬁve stress label scale to a scale of three stress labels by deﬁning our classes as - below median stress, median stress and above median stress .###Due to a heavy imbalance of class labels on a scale of 1-5, we follow (Mikelsons et al., 2018), converting the five stress label scale to a scale of three stress labels by defining our classes as - below median stress, median stress and above median stress.###In all, our data comprises of 23 students, totaling to 1183 data points achieving roughly equal amount of training data in (Mikelsons et al., 2018).###In the work done by (Mikelsons et al., 2018), a Multilayer Perceptron (MLP) with 4 fully connected layers was employed to perform stress inference.###(Mikelsons et al., 2018) have tried to predict stress of students in the StudentLife dataset by novel feature engineering of location based features and Neural Networks.",impact-revealing,acknowledge prior work on stress inference methods
56,5edf5dd891e011bc656ded70,374f36c9081ab5dc686ab833c42a7297235cd13f,Denoising Implicit Feedback for Recommendation,5550415745ce0a409eb3a739,Adam: A Method for Stochastic Optimization.,"In addition, the batch size is always 1,024 andAdam [14] is applied to optimize all the parameters with the learning rate initialized as 0.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
310,5f0277e911dc830562231dac,72a5feb4835b3e6cab3754437bb033371c5df154,DVGAN: A Minimax Game for Search Result Diversification Combining Explicit and Implicit Features,59939d49ffdae9cf10039e6f,IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models,"(4) and the discrimi-nator 𝐷 is the estimated probability calculated by: Please note that different from IRGAN [19], DVGAN-doc has an additional component 𝑆 to represent the former selected documents.###Inspired by IRGAN [19], we generate negative document set 𝐷 ′ by selecting the documents from the candidate document set with the highest scores.###It is also used in the traditional information retrieval area, Wang proposed IR-GAN [19] which consists of two information retrieval models in it.###To tackle this problem, inspired by IRGAN [19], we introduce Generative Adversarial Network (GAN) [10] into search result diversification.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3829,5d9ed30647c8f76646f7f04c,f160c69c428122e8fa7ba96f220b4ded5f8761f4,ARNOR: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification,53e9aec4b7602d97038ec44a,Modeling Relations And Their Mentions Without Labeled Text,"First, multi-instance learning (Riedel et al., 2010; Lin et al., 2016; Surdeanu et al., 2012; Zeng et al., 2015) relaxes the DS assumption as at-least-one.###The ﬁrst widely studied method is based on multi-instance learning (Riedel et al., 2010; Lin et al., 2016; Sur-deanu et al., 2012; Zeng et al., 2015).",other,acknowledge existing methods in multi-instance learning
3087,5f0d85c69fced0a24be4f028,5d9073cfec34aea00247ec625fa94f6279ff580d,tailored page sizes,55323c6d45cec66b6f9dc0a7,Efficient Memory Virtualization: Reducing Dimensionality of Nested Page Walks.,"Prior work [8], [13], [23], [31], [32], [34] has demonstrated that some applications can spend up to 50% of their execution time servicing page table walks.###Prior work has shown that excessive page walks may signiﬁcantly degrade performance in applications suffering from limited TLB reach [8], [13], [23], [31], [32], [34].",other,highlighting performance issues related to page table walks in applications
1662,,1f1b324cf78d5a53321ec2014fe17523ad1668d8,Examining the normative and persuasive effects of televised U.S. Senate debates,,,"###Voters are motivated by partisan social identities and view their preferred party as the in-group and the opposition party as the out-group (Greene 2004; Hopkins 2018; Iyengar, Sood, and Lelkes 2012; Jennings 2019, Jennings et al. 2018; Mason 2018; Tajfel and Turner, 1979).###Individuals are biased toward their “in-group” relative to an “out-group” (Tajfel and Turner 1979).",impact-revealing,acknowledge social identity theory in voter behavior
3966,5f7d893591e011346ad27d16,ff8a988d88ebe5d5ca116340baa34ae00ce011e8,PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation,5d7f5d2e3a55acb4692ce2fb,CTRL: A Conditional Transformer Language Model for Controllable  Generation,"Specially designed control codes and auxiliary planning modules have been integrated into neural models (Keskar et al., 2019; Moryossef et al., 2019; Hua and Wang, 2019), yet those solutions require model architecture modification or retraining, making text generation with large models a very costly endeavor.###Specially designed con-trol codes and auxiliary planning modules have been integrated into neural models (Keskar et al., 2019; Moryossef et al., 2019; Hua and Wang, 2019), yet those solutions require model architecture modiﬁcation or retraining, making text generation with large models a very…",other,highlighting the challenges of integrating control codes in neural models
236,5e3d353b3a55ac4de4104f40,3024f58826a5bce3378af94f677e8fb90cbb49e0,LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation,5cf48a48da56291d582ab75a,Neural Graph Collaborative Filtering,"[39] recently proposes NGCF and achieves state-of-the-art performance for CF.###We first introduce NGCF [39], a representative and state-of-the-art GCN model for recommendation.###To reduce the experiment workload and keep the comparison fair, we closely follow the settings of the NGCF work [39].###, the matrix factorization model, results see [39]) to 1 leads to the largest performance gain, and using a layer number of 3 leads to satisfactory performance in most cases.###, propagation rule [39]) in LightGCN is defined as:###Motivated by the strength of graph convolution, recent efforts like NGCF [39], GC-MC [35], and PinSage [45] adapt GCN to the user-item interaction graph, capturing CF signals in high-hop neighbors for recommendation.###We first describe experimental settings, and then conduct detailed comparison with NGCF [39], the method that is most relevant with LightGCN but more complicated (Section 4.###As such, collaborative filtering (CF), which focuses on exploiting the past user-item interactions to achieve the prediction, remains to be a fundamental task towards effective personalized recommendation [10, 19, 28, 39].###, the first layer enforces smoothness on users and items that have interactions, the second layer smooths users (items) that have overlap on interacted items (users), and higher-layers capture higher-order proximity [39].###This is different from most existing graph convolution operations [14, 23, 36, 39, 48] that typically aggregate extended neighbors and need to handle the self-connection specially.",impact-revealing,reporting on recent advancements in collaborative filtering methods
3252,5fc75d8591e0114897921043,b62edbf6e619eeed886c63e51fdff2c3d94f998f,graph convolutions that can finally model local structure,5ee8986891e011e66831c3bc,DeeperGCN: All You Need to Train Deeper GCNs,"A first approach is to develop methods to train larger and deeper networks(Li et al., 2020), which can already lead to a considerable boost in performance.###A ﬁrst approach is to develop methods to train larger and deeper networks(Li et al., 2020), which can already lead to a considerable boost in performance.",other,suggesting methods for improving network performance
610,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,53e9a026b7602d970290cadc,"Semantic Indexing of Multimedia Content Using Visual, Audio, and Text Cues","[3] adopted a late fusion approach in order to detect semantic concepts (e.###[3] also used a Bayesian network in addition to SVM and showed the comparison of both for video shot retrieval.###This method is based on [3], where SVM is first used as a classifier for the individual modality and then super kernel non-linear fusion is applied for optimal combination of the individual classifier models.###The fusion of different modalities is generally performed at two levels: feature level or early fusion and decision level or late fusion [3, 45, 121].###3 SVM based score space classification of combined information from multiple intermediate concepts [3] Multimodal fusion for multimedia analysis 353",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3978,599c797d601a182cd2643e8a,cd8a9914d50b0ac63315872530274d158d6aff09,Modeling Relational Data with Graph Convolutional Networks,573695ea6e3b12023e4ff824,Substructure Counting Graph Kernels for Machine Learning from RDF Data,We consider two fundamental SRL tasks: link prediction (recovery of missing triples) and entity classification (assigning types or categorical properties to entities).,other,describing fundamental tasks in semantic role labeling
3029,599c797d601a182cd2643e8a,cd8a9914d50b0ac63315872530274d158d6aff09,Modeling Relational Data with Graph Convolutional Networks,573696026e3b12023e516133,Large-scale Simple Question Answering with Memory Networks,"Knowledge bases organize and store factual knowledge, enabling a multitude of applications including question answering (Yao and Van Durme 2014; Bao et al. 2014; Seyler, Yahya, and Berberich 2015; Hixon, Clark, and Hajishirzi 2015; Bordes et al. 2015; Dong et al. 2015) and information retrieval (Kotov and Zhai 2012; Dalton, Dietz, and Allan 2014; Xiong and Callan 2015b; 2015a).###…including question answering (Yao and Van Durme 2014; Bao et al. 2014; Seyler, Yahya, and Berberich 2015; Hixon, Clark, and Hajishirzi 2015; Bordes et al. 2015; Dong et al. 2015) and information retrieval (Kotov and Zhai 2012; Dalton, Dietz, and Al-lan 2014; Xiong and Callan 2015b; 2015a).",other,highlighting the applications of knowledge bases in question answering and information retrieval
872,5a260c8117c44a4ba8a30adf,ecf6c42d84351f34e1625a6a2e4cc6526da45c74,representation learning on graphs: methods and applications,58d82fd2d649053542fd75d8,Geometric deep learning: going beyond Euclidean data.,"|V| State-of-the-art approximations to these spectral approaches (e.g., using Chebyshev polynomials) are conceptually similar to Algorithm 1, with some minor variations, and we refer the reader to Bronstein et al. [7] for a thorough discussion of these techniques.###We refer the reader to [32], [42], [37], and [7] for comprehensive overviews of these areas.###…work, which we do not review in detail here—including latent space models of social networks [32], embedding methods for statistical relational learning [42], manifold learning algorithms [37], and geometric deep learning [7]—all of which involve representation learning with graph-structured data.",impact-revealing,providing context and references for spectral approaches and representation learning
749,58d82fced649053542fd692f,29e944711a354c396fad71936f536e83025b6ce0,categorical reparameterization with gumbel-softmax,58d82fcbd649053542fd6178,The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables.,"…approximation to arg max , and generate k -dimensional sample vectors y ∈ ∆ k − 1 where The density of the Gumbel-Softmax distribution (derived in Appendix B) is: This distribution was independently discovered by Maddison et al. (2016), where it is referred to as the concrete distribution.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3539,5edf5dd891e011bc656deed4,ae0988c55095827384bf70e346e192a89216e75c,IterefinE: Iterative KG Refinement Embeddings using Symbolic Knowledge,5b8c9f4a17c44af36f8b6a69,Towards Neural Theorem Proving at Scale,"It is worth noting that embeddings, with a few recent exceptions [Guo et al., 2016, Minervini et al., 2017, 2018, Fatemi et al., 2019], do not make use of rich taxonomic/ontological rules when available.",other,highlighting a limitation in the use of embeddings
3726,5e3be3c33a55ac29c4ae7e18,6dbdc34000b034b75b8ff70872fc7c35549e273a,Interpretable & Time-Budget-Constrained Contextualization For Re-Ranking,5cc8228fe1cd8e018e4cfd4f,On the Effect of Low-Frequency Terms on Neural-IR Models,"A key success factor are fine-tuned word representations, covering most of the indexed vocabulary [13].",other,highlighting the importance of fine-tuned word representations
1502,,37c9c4e7648f639c0b36f150fc6c6c90b3682f4a,Palette: Image-to-Image Diffusion Models,,,"###Conditional diffusion models [Chen et al. 2021a; Saharia et al. 2021] make the denoising process conditional on an input signal.###Autoregressive Models [Parmar et al. 2018; van den Oord et al. 2016], VAEs [Kingma and Welling 2013; Vahdat and Kautz 2020], and Normalizing Flows [Dinh et al. 2016; Kingma and Dhariwal 2018] have seen success in specific applications, but arguably, have not established the same level of quality and generality as GANs.###…and train a
neural network 𝑓𝜃 to denoise 𝒚 given 𝒙 and a noise level indicator 𝛾 , for which the loss is
E(𝒙,𝒚)E𝝐∼N(0,𝐼 )E𝛾 𝑓𝜃 (𝒙, √𝛾 𝒚 + √︁1−𝛾 𝝐︸ ︷︷ ︸ 𝒚 , 𝛾) − 𝝐 𝑝 𝑝 , (1)
[Chen et al. 2021a] and [Saharia et al. 2021] suggest using the 𝐿1 norm, i.e., 𝑝 = 1, whereas the…###…Nichol 2021; Ho et al. 2020, 2021], audio synthesis [Chen et al. 2021a; Kong et al. 2020], and image super-resolution [Kadkhodaie and Simoncelli 2021; Saharia et al. 2021], as well as unpaired imageto-image translation [Sasaki et al. 2021] and image editing [Meng et al. 2021; Sinha et al. 2021].###Some existing GAN-based models explicitly encourage diversity; [Yang et al. 2019b; Zhu et al. 2017b] propose methods for improving diversity of conditional GANs, and [Han et al. 2019; Zhao et al. 2020] explore diverse sample generation for image inpainting.###Consistent with previous works [Ho et al. 2020; Saharia et al. 2021], we use standard Adam optimizer with a fixed 1e-4 learning rate and 10k linear learning rate warmup schedule.###2020], and image super-resolution [Kadkhodaie and Simoncelli 2021; Saharia et al. 2021], as well as unpaired imageto-image translation [Sasaki et al.###GANs are widely used but often require auxiliary objectives on structures, context, edges, contours and hand-engineered features [Iizuka et al. 2017; Kim et al. 2021a; Liu et al. 2020; Nazeri et al. 2019; Yi et al. 2020; Yu et al. 2018b, 2019], and they lack diversity in their outputs [Zhao et al. 2021; Zheng et al. 2019].###Nevertheless, GANs can be challenging to train [Arjovsky et al. 2017; Gulrajani et al. 2017], and often drop modes in the output distribution [Metz et al. 2016;
ar X
iv :2
11 1.###Diffusion Hyper-parameters : Following [Chen et al. 2021a; Saharia et al. 2021] we use 𝛼 conditioning for training Palette.###On image super-resolution, they have delivered impressive face enhancement results, outperforming GANs [Saharia et al. 2021].###2021a] and [Saharia et al. 2021] suggest using the L1 norm, i.###These models have been applied to image super-resolution [Nichol and Dhariwal 2021; Saharia et al. 2021].###Despite these results, it is not clear whether diffusion models rival GANs in offering a versatile and general framework for image manipulation.###Palette uses a U-Net architecture [Ho et al. 2020] with several modifications inspired by recent work [Dhariwal and Nichol 2021; Saharia et al. 2021; Song et al. 2021].###While existing conditional diffusion
models, SR3 [Saharia et al. 2021] and WaveGrad [Chen et al. 2021a], have found 𝐿1 norm to perform better than the conventional 𝐿2 loss, there has not been a detailed comparison of the two.###[Dong et al. 2015] applied deep CNN architectures for JPEG restoration, and [Galteri et al. 2017, 2019] successfully applied GANs for artifact removal, but they have been restricted to quality factors above 10.###2020] with several modifications inspired by recent work [Dhariwal and Nichol 2021; Saharia et al. 2021; Song et al. 2021].###Our work is inspired by Pix2Pix [Isola et al. 2017a], which explored myriad image-to-image translation tasks with GANs.###Generative Adversarial Networks (GANs) [Goodfellow et al. 2014; Radford et al. 2015] have emerged as the model family of choice for many image-to-image tasks [Isola et al. 2017a]; they are capable of generating high fidelity outputs, are broadly applicable, and support efficient sampling.###The two main differences between our architecture and theirs are (i) absence of class-conditioning, and (ii) additional conditioning of the source image via concatenation, following [Saharia et al. 2021].###While existing conditional diffusion models, SR3 [Saharia et al. 2021] and WaveGrad [Chen et al.",impact-revealing,highlighting the capabilities and applications of GANs in image-to-image tasks
504,5fd8acf991e0119b22c1f38d,5b9d8bcc46b766b47389c912a8e026f81b91b0d8,Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting,5e09a806df1a9c0c41680baa,Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel,"Following the formulation in (Tsai et al. 2019), the i -th query’s attention is deﬁned as a kernel smoother in a probability form: where p ( k j | q i ) = k ( q i , k j ) l k ( q i , k l ) and k ( q i , k j ) selects the asymmetric exponential kernel exp( The self-attention combines the values and…###Following the formulation in (Tsai et al. 2019), the i-th query’s attention is defined as a kernel smoother in a probability form:",impact-revealing,providing context for attention mechanism formulation
113,5c04967517c44a2c7470926f,c2d40522eaa5523d67a0de5e4098e7031fdccb3d,Pitfalls of Graph Neural Network Evaluation,5d9edc1947c8f76646032f41,Troubling Trends in Machine Learning Scholarship,"This makes it difficult to identify whether the improved performance comes from (a) a superior architecture of the new model, or (b) a better-tuned training procedure and / or hyperparameter configuration that unfairly benefits the new model [Lipton and Steinhardt, 2018].###Such diverse experimental setups makes it hard to empirically identify the driver behind the improved performance [Lipton and Steinhardt, 2018].###This makes it difﬁcult to identify whether the improved performance comes from (a) a superior architecture of the new model, or (b) a better-tuned training procedure and / or hyperparameter conﬁguration that unfairly beneﬁts the new model [Lipton and Steinhardt, 2018].",impact-revealing,highlighting challenges in identifying performance drivers in model improvements
1181,,f0c8be3f721b45612fa1116899bdb3531930bcb7,A Two-Nearest Wireless Access Point-Based Fingerprint Clustering Algorithm for Improved Indoor Wireless Localization,,,"###To solve this localization time and accuracy trade-off, fingerprint database clustering algorithms are used [7].###A number of studies have proposed and used various clustering algorithms with the objective of reducing localization time and improving localization accuracy [7–13].###…these fingerprint clustering algorithms is constrained by a number of factors, including the choice of the best initial hyperparameters, such as cluster number and cluster centroids, as well as the type of fingerprint similarity measure employed and being extremely computationally intensive [7–9].",impact-revealing,highlighting challenges and factors affecting fingerprint clustering algorithms
129,5db9298647c8f766461f8ecd,ce177672b00ddf46e4906157a7e997ca9338b8b9,Attention is not not Explanation,5cede10bda562983788eb4c8,Attention is not Explanation,"…greatly exacerbated when performed independently on each instance.2 Thus, it is no surprise that Jain and Wal-
2Indeed, the most open-ended task, question answering over CNN data, produces considerable difficulty to manipulate its scores by random permutation (Figure 6e in Jain and Wallace (2019)).###A recent paper (Jain and Wallace, 2019) points to possible pitfalls that may cause researchers to misapply attention scores as explanations of model behavior, based on a premise that explainable attention distributions should be consistent with other feature-importance measures as well as exclusive given a prediction.###N: random seed; ⌅: uniform weights; dotted line: our adversarial setup as is varied; +: adversarial setup from Jain and Wallace (2019).###A recent paper claims that ‘Attention is not Explanation’ (Jain and Wallace, 2019).###…via the data distribution, which is heavily skewed towards positive examples (see Table 1 in the Appendix), together with the fact (conceded in Jain and Wallace (2019)’s section 4.2.1) that positive instances in detection datasets such as MIMIC tend to contain a handful of indicative tokens,…###in Jain and Wallace (2019)).###In this section, we briefly describe the experimental design of Jain and Wallace (2019) and look at the results they provide to support their claim that ‘Attention is not explanation’.###Figure 1: Schematic diagram of a classification LSTM model with attention, including the components manipulated or replaced in the experiments performed in Jain and Wallace (2019) and in this work (by section).###This can be explained via the data distribution, which is heavily skewed towards positive examples (see Table 1 in the Appendix), together with the fact (conceded in Jain and Wallace (2019)’s section 4.###A recent paper (Jain and Wallace, 2019) points to possible pitfalls that may cause researchers to misapply attention scores as explanations of model behavior, based on a premise that explainable attention distributions should be consistent with other feature-importance measures as well as exclusive…###As a more direct examination of models, and as a complementary approach to Jain and Wallace (2019)’s measurement of backward-pass gradient flows through the model for gauging token importance, we introduce a post-hoc training protocol of a non-contextual model guided by pre-set weight distributions.###Indeed, the most open-ended task, question answering over CNN data, produces considerable difficulty to manipulate its scores by random permutation (Figure 6e in Jain and Wallace (2019)).###In this section, we briefly describe the experimental design of Jain and Wallace (2019) and look at the results they provide to support their claim",impact-revealing,highlighting potential pitfalls in attention-based models and referencing prior work
114,5ca600ae6558b90bfa4d76e9,7aa38b85fa8cba64d6a4010543f6695dbf5f1386,Towards Deep Learning Models Resistant to Adversarial Attacks,53e9a93eb7602d97032928b5,Intriguing properties of neural networks.,"Computer vision presents a particularly striking challenge: very small changes to the input image can fool state-of-the-art neural networks with high probability (Szegedy et al., 2013; Goodfellow et al., 2014; Nguyen et al., 2015; Sharif et al., 2016; Moosavi-Dezfooli et al., 2016).###While trained models tend to be very effective in classifying benign inputs, recent work (Szegedy et al., 2013; Goodfellow et al., 2014; Nguyen et al., 2015; Sharif et al., 2016) shows that an adversary is often able to manipulate the input so that the model produces an incorrect output.",impact-revealing,highlighting the vulnerability of neural networks to adversarial examples in computer vision
1921,,f8e078774548d3460f1c0ce072f4ca55c8350035,Generation of a transgenic zebrafish model of Tauopathy using a novel promoter element derived from the zebrafish eno2 gene,,,"###NSE expression is generally considered a feature of cells with neuronal differentiation (24).###The mammalian ENO2 gene encodes the neuron-specific g-enolase isoenzyme (24), which is expressed in mature neurons throughout the neuraxis; consequently, neuron-specific enolase (NSE) is frequently used as a marker to identify cells with neuronal differentiation in human histopathology samples (24).",impact-revealing,highlighting the significance of neuron-specific enolase in identifying neuronal differentiation
3784,5e3940c73a55ace46ed436d2,845b4941d8c016aa5f8967da2f86d38ef6c18fa3,a survey on knowledge graphs representation acquisition and applications,5d4d46fb3a55acff992fdc1c,Relation Extraction Using Supervision from Topic Knowledge of Relation Labels.,"TK-MF [162] enriches sentence representation learning by matching sentences and topic words.###[132] Transfer learning + sub-tree parse + attention position embedding CORD [133] BiGRU + hierarchical attention + cooperative module position embedding, logic rules TK-MF [134] Topic modeling + multi-head self attention position embedding, topic words HATT-Proto [135] Prototypical networks + CNN + hybrid attention position embedding###TK-MF [134] enriches sentence representation learning by matching sentences and topic words.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
999,,5d2415916d7d9acfcab1cc1f466a0ff7b7848c0e,Incloodle-Classroom: Technology for Inclusive Joint Media Engagement in a Neurodiverse Kindergarten Classroom,,,"###As an example, researchers have explored the potential of robot technologies to promote inclusive education and collaborative learning among students with diverse visual abilities [57, 62, 63].###In this paper, we describe research that builds upon our prior formative work in inclusive play [78] to design, develop, and evaluate a technology for promoting Inclusive JME among kindergarten students in an inclusive classroom setting.",impact-revealing,highlighting the potential of robot technologies in inclusive education
2082,,5bf23bc341dd7ac3ca3fabb30f765428e5923603,HybridAvatar: Efficient Mesh-based Human Avatar Generation from Few-Shot Monocular Images with Implicit Mesh Displacement,,,"###Since the human body are highly articulated, classical parametric models are widely used for reconstruction [15, 17, 18].###Early approaches mainly use parametric models ( e.g. , SMPL [15], SMPL-X [20], and STAR [18]) to represent human shapes, which effectively model human bodies in various heights and fatness, but still remain a large gap between the modeled shapes and the real human identities due to the lack of…",impact-revealing,highlighting the limitations of classical parametric models in accurately representing human identities
203,5d1eb9beda562961f0af981f,934d7bffdba0b560a80a518b99a791a16b3e198c,A Fourier Perspective on Model Robustness in Computer Vision,5b3d98cc17c44a510f801e6a,AutoAugment: Learning Augmentation Policies from Data.,"4 , low pass ﬁlter front end with bandwidth 45 , high pass ﬁlter front end with bandwidth 223 , and AutoAugmentation.###AutoAugment applies a learned mixture of image transformations during training and achieves the state-of-the-art performance on CIFAR-10 and ImageNet.###On ImageNet-C, we compare the robustness of the naturally trained model, AutoAugment (Auto) and the Stylized-ImageNet augmentation strategy (SIN+IN) (Geirhos et al., 2018a).###The fact that AutoAugment was tuned speciﬁcally for clean test error, and transfers well even after removing the contrast and brightness parts of the policy (as these corruptions appear in the benchmark) gives us hope that this is a step towards more useful domain invariant features.###In all of our experiments with AutoAugment, we remove the brightness and constrast sub-policies as they explicitly appear in the CIFAR-10-C and ImageNet-C benchmarks.###To-wards this end, we investigated the learned augmentation policy AutoAugment (Cubuk et al., 2018).###Using the mean corruption error (mCE) metric proposed by Hendrycks & Dietterich (2019), we observe that AutoAugment achieves the best mCE of 66 , and in comparison, SIN+IN achieves an mCE of 69 (see a formal deﬁnition of mCE in the appendix).###In particular we demonstrate that the recently proposed AutoAug-ment data augmentation policy (Cubuk et al., 2018) achieves state-of-the-art results on the CIFAR-10-C and ImageNet-C benchmarks, with improvements on 14 out of 15 of the corruptions.###The robustness problem is certainly far from solved, and our Fourier analysis shows that the AutoAugment model is not strictly more robust than the baseline — there are frequencies for which robustness is degraded rather than improved.###We evaluate 6 models on CIFAR-10-C, each trained differently — natural training, Gaussian data augmentation, adversarial training, trained with a low pass ﬁlter front end (bandwidth 15 ), trained with a high pass ﬁlter front end (bandwidth 31 ), and trained with AutoAug-ment (see a more detailed discussion on AutoAugment in Section 4.3).",impact-revealing,highlighting the effectiveness of AutoAugment in achieving state-of-the-art performance on image benchmarks
629,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",558c091e84ae6766fdf0af3f,Discrete Signal Processing on Graphs: Frequency Analysis,"When data labels are presented as signals on a (nearest neighbor) graph, graph signal regularization techniques can be used in the process of estimating labels [5], optimizing the prediction of unknown labels in classification [51] or semisupervised learning problems [195].###For the total variation criterion it has been shown experimentally and justified theoretically that the frequency bases obtained from the shift operator tend to be ordered [51].###Other norms could be used to define the total variation, see [51][3].###ASP led to the introduction of, possibly weighted, graph adjacency matrices as shifts that generate the graph signal model for signals indexed by nodes of an arbitrary directed or undirected graph [2], [51].###Such a smooth graph signal model makes it possible to detect outliers or abnormal values by highpass filtering and thresholding [51], [155], or to build effective signal reconstruction methods from sparse set of sensor readings, as in [156], [157], [158], which can potentially lead to",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
174,5c04967517c44a2c74709354,04c131293bf64c67972baa0053e85a510c4aa725,Intervention Harvesting for Context-Dependent Examination-Bias Estimation,58437725ac44360f1082fa29,Unbiased Learning-to-Rank with Biased Feedback,"Note that knowing the relative propensities with respect to a single ""anchor"" position (e.g. p k p 1 ) is sufﬁcient, since the learning objective in [Joachims et al., 2017] is invariant to multiplicative scaling.###As shown by Joachims et al. [2017], the parameters of the PBM can serve as propensity estimates, enabling the use of inverse propensity score (IPS) weighting for unbiased learning-to-rank.###While P ( E = 1 | k ) can be used as an estimate of the examination propensity [Joachims et al., 2017], it is a rather simplistic model since it assumes that examination does not vary across queries.###In this case, online swap interventions can be directly employed for propensity estimation [Joachims et al., 2017].###To address this bias problem, Joachims et al. [2017] proposed a counterfactual inference approach, providing an unbiased LTR framework via Empirical Risk Minimization.###There are two key limitations of existing propensity estimation methods for LTR [Agarwal et al., 2018b, Wang et al., 2018, Joachims et al., 2017].###…results at positions k and k ′ before presenting the ranking makes the expected relevance of results at the two positions equal, and thus the ratio of the observed click-through rates with swap-ping gives a consistent estimate of the relative examination bias, i.e., [Joachims et al., 2017].###The resulting deep network can then be used to compute context-dependent propensities for learning to rank (LTR) in algorithms like [Joachims et al., 2017, Agarwal et al., 2018a].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
930,573697796e3b12023e66024b,de2ddd662aa91c335f2733e586a23c249581c077,redundant memory mappings for fast access to large memories,556fb0872401b4b38c23789b,Increasing TLB reach by exploiting clustering in page translations,"…2 described the qualitative differences between RMM and the most closely re-lated work on multipage mappings (sub-blocked TLBs [47], CoLT [39], Clustered TLBs [38]), huge pages [1, 6, 36], and direct segments [10, 23], and Section 8 showed quantitatively that RMM substantially improves over them.###Nonethe-less, the simple eager paging algorithm generates large range translations for a variety of block sizes and exploits the clustering behavior of the buddy allocator [38, 39].###Multipage Mapping approaches, such as sub-blocked TLBs [47], CoLT [39] and Clustered TLBs [38], pack multiple Page multiple of translations (e.g., 8-16) per entry, which limits their potential to reduce page-walks for large working sets.###Each entry indexes up to an 8-page cluster, shown best by Clustered TLB [38].###The CTLB bars show Clustered TLB [38] results.###The effectiveness of huge pages is limited by the size-alignment requirement: huge pages must have size-aligned physical addresses, and thus the OS can only allocate them when the available memory is size-aligned and contiguous [38, 39].###Multipage mappings use one TLB entry to map multiple pages (e.g., 8-16 pages per entry) [38, 39, 47].###We implement the Clustered TLB approach [38] of Pham et al., conﬁgured with 512 fully-associative entries.",impact-revealing,highlighting the improvements of RMM over related work
3655,5ce2d0feced107d4c63dd498,d07284a6811f1b2745d91bdb06b040b57f226882,Decoupled Weight Decay Regularization,53e9ba23b7602d970462d60d,Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,"Adaptive gradient methods, such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2014) and most recently AMSGrad (Reddi et al., 2018) have become a default method of choice for training feed-forward and recurrent neural networks (Xu et al., 2015; Radford et al., 2015).###Adaptive gradient methods, such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2014) and most recently AMSGrad (Reddi et al.###While we focused our experimental analysis on Adam, we believe that similar results also hold for other adaptive gradient methods, such as AdaGrad (Duchi et al., 2011) and AMSGrad (Reddi et al.###Adaptive gradient methods, such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2014) and most recently AMSGrad (Reddi et al., 2018) have become a default method of choice for training feed-forward and recurrent neural networks (Xu et al., 2015; Radford et…###While we focused our experimental analysis on Adam, we believe that similar results also hold for other adaptive gradient methods, such as AdaGrad (Duchi et al., 2011) and AMSGrad (Reddi et al., 2018).",other,acknowledge the prevalence and importance of adaptive gradient methods in neural network training
515,57d063d3ac44367354292258,db645d8a227d024121dc95a1c0eeec1b15c7fe53,A multi-player Markov stopping game for delay-tolerant and opportunistic resource sharing networks,53e9a2c7b7602d9702bcfaeb,Markov stopping games with random priority,"However, [17] does not provide a systematic method to deal with a general number of players and hence is still not readily applicable to practical DT-ORS-Net.###To derive the optimal strategy for each player in such situations, the two-player MSG framework developed in [17] may serve as a basis.###However, a systematic method for handling a general number of players in a MSG is missing from [17].###To this end, the two-player Markov stopping game (MSG) developed in [17] can serve as a good starting point, which extends the classic optimal stopping theory into a game theoretic setting so as to handle the potential conflicts between the two players.###To further handle the dynamism in the number of players as the game evolving, the concept of selection time is proposed in [17].###Particularly, when two players coexist in the MSG, the randomized stopping time is used in [17] to deal with the potential competition from the other player.",impact-revealing,highlighting the limitations of existing methods in handling multiple players
2674,5c04967517c44a2c7470927f,b9015d4f1e591eba7ea21c3566c919f80f7c2afe,attentive long short-term preference modeling for personalized product search,53e9b991b7602d9704583bfb,Towards a theory model for product search.,"Online product search has become an indispensable part of our daily life, as more and more users search and purchase products on the Internet [31].###The decision mechanism that underlies the process of buying a product is different from locating relevant documents or objects [31].",other,highlighting the significance of online product search in daily life
2303,53e9a42bb7602d9702d44c0b,2d6f191fd9b08d2a53498f0ed9b4f1a411d83cdf,Temporal Streams In Commercial Server Applications,53e9b6d6b7602d97042617b9,Piranha: A Scalable Architecture Based On Single-Chip Multiprocessing,"Because of the allocation policies of the Piranha intra-chip coherence protocol, coherence misses may be satisfied by a peer L1 or the shared L2.###The L1s and shared L2 implement a MOSI coherence protocol based closely on Piranha [2].",other,providing context on coherence protocols
366,5fdc8e9d91e01104c91811a8,849b88ddc8f8cabc6d4246479b275a1ee65d0647,A Generalization of Transformer Networks to Graphs,599c7987601a182cd2648373,Attention Is All You Need.,"Since Laplacian PEs are generalization of the PE used in the original transformers (Vaswani et al. 2017) to graphs and these better help encode distance-aware information ( i.e., nearby nodes have similar positional features and farther nodes have dissimilar positional features), we use Laplacian…###There has been a tremendous success in the ﬁeld of nat-ural language processing (NLP) since the development of Transformers (Vaswani et al. 2017) which are currently the best performing neural network architectures for handling long-term sequential datasets such as sentences in NLP.###Transformers based models have led to state-of-the-art performance on several NLP applications (Devlin et al. 2018; Radford et al. 2018; Brown et al. 2020).###This kind of information injection is not seen to be explored much, or applied in NLP Transformers as there is usually no available feature information between two words.###…our architecture emerges as a fresh and improved attention based GNN baseline surpassing GAT (see Table 2), which employs multi-headed attention inspired by the original transformer (Vaswani et al. 2017) and have been often used in the literature as a baseline for attention-based GNN models.###The Graph Transformer is closely the same transformer architecture initially proposed in (Vaswani et al. 2017), see Figure 1 (Left).###As stated earlier, we take into account two key aspects to develop Graph Transformers – sparsity and positional encodings which should ideally be used in the best possible way for learning on graph datasets.###It thereby makes sense to have each word attending to each other word in a sentence, as followed by the Transformer architecture (Vaswani et al. 2017).",impact-revealing,highlighting the evolution and significance of transformer architectures in NLP
1457,,cfcfe9fd697e98ad2508b4bf95dad16390f66938,Chaining and the temporal dynamics of scientists’ publishing behaviour,,,"###The prototype model is motivated by Rosch’s work in categorization [30].###We focus on investigating three related issues: 1) whether there is systematic predictability in the historical order of emergence of scientists’ papers; 2) how the exemplar-based chaining account fares against alternative competing accounts including the prominent prototype-based view that predicts growth to stem from a central core [30, 31]; 3) whether mechanisms of chaining are domain-general or differ across scientific fields, or between award-winning (or prominent) and randomly-sampled (control) scientists.",impact-revealing,highlighting the motivation and research focus based on prior work in categorization
3188,5f8d00a29e795ea21aee8001,34d6bc3dc0a4811eb262508379fc74f600671687,a collective approach to scholar name disambiguation,5e3a93a93a55ac06c6119dc9,Improving Spectral Clustering with Deep Embedding and Cluster Estimation,"As mentioned in the framework in Section 3, the estimated number of authors for each name is used as the stop condition, essentially a cluster estimation problem [10], [26] Specifically, a name is considered as fully disambiguated if the number of authors of this name reaches the estimated one.",other,providing context for author disambiguation
260,5e3a93a93a55ac06c6119df5,cad9e682ddec3b1dd532cb8301737109d9eda7d7,Collaborative Distillation for Top-N Recommendation,5550417545ce0a409eb3b767,Distilling the Knowledge in a Neural Network.,"Then, we explain the concept of knowledge distillation (KD) [10] and present rank distillation (RD) [11] that applies knowledge distillation to recommender models.###To tackle this issue, [10] introduces the notion of a temperature T .###KD [10] is to find a proper balance between the soft targets and hard labels.###In this paper, we employ knowledge distillation (KD) [10] which is a network compression technique by transferring the distilled",impact-revealing,describing the concept and application of knowledge distillation in recommender models
1718,,87700815bcc8d5bba2dff23cc88262868b35926e,To Give or Not to Give?,,,"###…people who are motivated by communal norms and have compassionate caregiving goals
find great pleasure in giving to others, even when doing so is construed as a duty and comes at the expense of the self (Crocker & Canevello, 2008; Kogan et al., 2010; Le, Impett, Kogan, Webster, & Cheng, in press).",impact-revealing,providing context on motivations related to communal norms
1401,,86a0131e04ff6db5105db8bdb7a079f326971c8f,Universal Black-Box Reward Poisoning Attack against Offline Reinforcement Learning,,,"###(Bhardwaj et al., 2024; Kumar et al., 2020; Fujimoto & Gu, 2021; Kidambi et al., 2020; Wu et al., 2019; Cheng et al., 2022).###We empirically test the efficiency of our attack on various standard datasets from the D4RL benchmark (Fu et al., 2020) learned by different state-of-the-art offline RL algorithms (Kumar et al., 2020; Fujimoto & Gu, 2021; Kostrikov et al., 2021).###For the learning algorithms, we choose IQL Kostrikov et al. (2021), CQL Kumar et al. (2020), and TD3-BC Fujimoto & Gu (2021), which are frequently used as baselines in current offline RL studies.###To verify that our attack is universal against different learning algorithms, we take the Hopper-medium-expert dataset as an example, apply the policy contrast attack using the same setup as in the main results, and run four different learning algorithms including TD3 BC, CQL, AWAC, and ReBRAC on the corrupted dataset.",impact-revealing,testing the efficiency of an attack on various standard datasets
1532,,dfc8010dca2c2980d67398d6a860e93a29d9a77e,"Involuntary ""Whiteness"": The Acculturation of Black Doctoral Female Students in the Field of Clinical Psychology",,,"###Sue et al. (2007) specified three types of racial microaggressions experienced by ethnic minorities:###Additionally, they cited that previous studies and literature (i.e., Robinson-Wood, 2009; Sue et al., 2007; Yosso, Ceja, Smith, & Solorzano, 2009) propound that Black women at PWIs in higher education and other professional experiences experience a plethora of experiences ranging from personal to…###Sue et al. (2007) specified three types of racial microaggressions experienced by ethnic minorities:
Microassaults, which refer to “explicit, racially derogatory statements of actions intended
to hurt the victim”, which may or may not be violent “but are motivated by the conscious and purposeful…###Additionally, they cited that previous studies and literature (i.e., Robinson-Wood, 2009; Sue et al., 2007; Yosso, Ceja, Smith, & Solorzano, 2009) propound that Black women at PWIs in higher education and other professional experiences experience a plethora of experiences ranging from personal to environmental shifts that contribute to “feelings of isolation, loneliness, and a sense of not belonging” (p.",impact-revealing,highlighting the types of racial microaggressions and their impact on Black women in higher education
1166,,8bf0a2a543eab3305eb1977e479f09fe9bfb2a55,On multipacket reception based neighbor discovery in low-duty-cycle wireless sensor networks,,,"###Since the neighbor discover process can last up to weeks [5], it is still unrealistic to use a protocol which keeps all nodes awake for weeks (batteries will still be used up quickly).###, birthday protocols), the optimal transmission probability can be easily determined to be 1/n, where n is the clique size [5].###Y • In [5,7], SPR model is used and nodes are always awake, which implies that in each run we can only choose at most one coupon, and it only needs to be picked out once.###first extended the results of [5,7] to the k-MPR situation.###By setting k = 1 and m = 1, we get O(n log n) which is the time complexity of birthday protocols [5,7].###They are widely adopted by many revious works [5,7,8,11,12].###The milestone of the randomized protocols of ND is the Birthay Protocol proposed in [5] by McGlynn and Borbash, who consider he randomized strategy in a synchronous system to avoid collisions n a clique.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2676,5cf48a45da56291d582a8448,f937d6482ad162f55022c7dce5857855ece27c1e,Knowledge Graph Convolutional Networks for Recommender Systems with Label Smoothness Regularization,53e99aacb7602d9702328c69,Factorization Machines with libFM,• LibFM [15] is a widely used feature-based factorization model for CTR prediction.,other,reporting prior findings on a widely used model
1618,,b19dcafdc4be48bd92ef47608e543baf71358247,Contributions from Gallese's neurophysiology and Bruner's psychology to the understanding of social learning,,,"###For Gallese (2007) – author referenced in the dis-###For Gallese (2007) there is a strict relation between language, action and embodied simulation.###According to Gallese (2003), these mirror neuron circuits would be the organic foundation for our ability to share our actions, emotions and sensations with others in a prereflective and implicit level.###In other words, unconscious functional precursors are highlighted as constituting the self and bonding with another human being (Metzinger & Gallese, 2003). Such mirror neurons would show a human’s organic tendency for detecting intentionality of action in others by inducing a process of internal simulation of actions, emotions and sensations from other people. This ability to detect the intentionality of actions in others by internally simulating them contributes to the understanding of this perceived action. This reveals a human being’s tendency to be social in order to be able to relate to other members of its species. As is exposed, there is an emphasis on the automatic, unconscious, pre-reflective, and therefore, pre-linguistic character in the construction of social knowledge. When defending such a perspective, Gallese (2006) opposes the traditional view from cognitive sciences.###This mechanism engenders intersubjective experiences by being activated when determined actions are executed, or emotions and sensations are felt, for example, when actions, emotions or sensations are observed in other people (Gallese, 2003). According to Gallese (2003), these mirror neuron circuits would be the organic foundation for our ability to share our actions, emotions and sensations with others in a prereflective and implicit level.###In other words, unconscious functional precursors are highlighted as constituting the self and bonding with another human being (Metzinger & Gallese, 2003).###This mechanism engenders intersubjective experiences by being activated when determined actions are executed, or emotions and sensations are felt, for example, when actions, emotions or sensations are observed in other people (Gallese, 2003).###Or even: “Once a rudimentary subjective perspective has been established with help of the motor system, inter-subjectivity can follow” (Metzinger & Gallese, 2003, p. 567), concluding that “behavior-reading is transformed into mind-reading” (Metzinger & Gallese, 2003, p. 568).###Thus, for us, there is a question that underlies the discussion undertaken until the moment and that could be formulated in the following way: analogous to the relation between mind and culture, in which culture is not only created by, but also creates the mind, would it be admissible to consider the biological substrate not only as a producer of inter-subjectivity and cognition, but also as its product? Gallese highlights the importance of experience, at the level of a phenomenon, or even, of an intentional attunement besides an embodied simulation and the organic base of mirror neurons. But when problems that may come up in the functioning of intersubjectivity, most specifically when dealing with autism, his hypothesis is that “these deficits . . . are to be ascribed to a deficit or a malfunctioning of the ‘intentional attunement’ because of a malfunctioning of embodied simulation mechanisms, in turn produced by a dysfunction of the mirror neuron systems” (2006, p. 7). It is worth remembering that Gallese’s hypothesis mentioned above is based on a few evidences, most specifically regarding the relation between autism and structural or functional anomalies in the insula, limbic system and amygdala. In the literature they are pronounced as the inability of autistic people to attribute intention to the other, understand different points of view, and to have a theory of mind (Butman & Allegri, 2001). It is assumed that these areas are functionally connected to mirror neuron areas, created by an ample network that serves as a basis for a form of empathy based on simulation. Activity detected in this network serves as a marker of sociability and empathy. Empirical evidences presented by Iacoboni (2009) show that the more severe the autism, the less amount of activity in these areas during observation and imitation of facial expressions by autistic children in comparison to children that have a typical development.###…the neurophysiological perspective confer greater importance to the role of observing an action or the manifestation of the other’s emotions for the individual’s learning: “Action is . . . a suitable candidate principle enabling social bonds to be initially established” (Gallese, 2003, p. 174).###In other words, unconscious functional precursors are highlighted as constituting the self and bonding with another human being (Metzinger & Gallese, 2003). Such mirror neurons would show a human’s organic tendency for detecting intentionality of action in others by inducing a process of internal simulation of actions, emotions and sensations from other people. This ability to detect the intentionality of actions in others by internally simulating them contributes to the understanding of this perceived action. This reveals a human being’s tendency to be social in order to be able to relate to other members of its species. As is exposed, there is an emphasis on the automatic, unconscious, pre-reflective, and therefore, pre-linguistic character in the construction of social knowledge. When defending such a perspective, Gallese (2006) opposes the traditional view from cognitive sciences. The author believes that in these sciences, social cognition becomes almost the same as mind reading abilities, due to their concepts of “Folk Psychology” and Theory of Mind. According to these concepts, human beings are capable of understanding the behavior of others by attributing mental states – intentions, beliefs and desires. On the other hand, Gallese (2006) believes that social cognition is not only a “social metacognition,” that is, to explicitly think about the contents of another person’s mind through embodied representations.",impact-revealing,highlighting the role of mirror neurons in social cognition and empathy
1288,,2a5f0b3b9e354bd8517a8e32bacbecfa9e02cee8,Magnetic Resonance Spectroscopy of Cancer Metabolism and Response to Therapy,,,"###Computer programs are available that automatically measure individual metabolites, either by fitting to the peaks in the spectrum or to the untransformed original signal. jMRUI (10, 11) (which uses a range of methods from simple exponentials to quantum mechanical models to fit time domain data), and LCModel (12) (which uses model spectra of commonly observed metabolites to fit the spectrum), are commonly used for quantitating tumor metabolite data.###Several groups have used minor modifications of existing software packages
such as LCModel (26), jMRUI (27, 28) and TARQUIN (29), to fit the observed peaks.###jMRUI (10, 11) (which uses a range of methods from simple exponentials to quantum mechanical models to fit time domain data), and LCModel (12) (which uses model spectra of commonly observed metabolites to fit the spectrum), are commonly used for quantitating tumor metabolite data.",impact-revealing,reporting commonly used software for metabolite quantification
1552,,5be8a0c0d9a081500af732ba3de99cc3bf411772,"Job Satisfaction , Cost Management Knowledge , Budgetary Participation , and Their Impact on Performance",,,"###As highlighted by Blumberg and Pringle (1982), one of the willingness aspect that determines individual performance is job satisfaction.###The research about participative budgeting (opportunity dimension) conducted by Setiawaty (2002) included the motivation variable into the willingness dimension in regards of the framework of Blumberg and Pringle (1982).###Using the theoretical framework of Blumberg and Pringle (1982), the job satisfaction variable is more suited to moderate the effects of budgetary participation on managerial performance.###…as an addition to the literature in researches on individual performance by providing empirical evidence for the theoretical framework proposed by Blumberg and Pringle (1982), that individual performance is affected by three dimensions in interaction which are capacity, willingness and…###In accordance with the theoretical framework of
Blumberg and Pringle (1982) as well as Shields and Shields (1998), participative budgeting would yield the highest benefit in strengthening managerial performance when combined with high level of cost management knowledge and job satisfaction.###According to Blumberg and Pringle (1982), individual performance is affected by three dimensions which are capacity, willingness, and opportunity.###This research strives to explore variables in relation to individual factors, utilizing the theoretical framework from Blumberg and Pringle (1982) that individual performance is affected by three interactive dimensions which are the opportunity dimension (participative budgeting), capacity…###The interaction between three variables around inividual performance can be explained using the theoretical model of Blumberg and Pringle (1982) as previously mentioned.###Blumberg and Pringle (1982) criticized research investigating individual performance for having only associated performance with one or two variables.###On the other hand, Blumberg and Pringle (1982) explained in their article about the performance theory, that there are three dimensions in
interaction, which ultimately determines individual performance.###Referring to the theories used by Blumberg and Pringle (1982) regarding the factors which affect performance, that performance is influenced by three dimensions of opportunity, capacity, and willingness.###Research outcomes have shown that the interaction between three variables which represent the dimensional interaction in the theory of Blumberg and Pringle (1982) has no significant influence on managerial performance.###By using the theoretical framework of Blumberg and Pringle (1982), the job satisfaction examined in the research of Lopez et, al (2009) becomes something that motivates employees to improve their performances, which is specifically categorized into the willingness dimension.###…Agbejule and Saarikoski (2006) about the infuence of cost management knowledge on the relationship between participative budgeting and managerial performance, by adding the
willingness dimension, which is job satisfaction according to the theoretical framework given by Blumberg and Pringle (1982).###Blumberg and Pringle (1982) criticizes that many researches on individual performance have failed to consider these dimensions which are capacity, willingness, and opportunity.###The interaction between the three variables in the participative budgeting research which inclines to the theoretical framework of Blumberg and Pringle (1982) can be explained using the psychology theory.###This research utilizes the theoretical framework of Blumberg and Pringle (1982) in order to explain the influenc of participative budgeting on managerial performance.",impact-revealing,highlighting the contribution of Blumberg and Pringle's framework to understanding individual performance
3515,573697796e3b12023e66024b,de2ddd662aa91c335f2733e586a23c249581c077,redundant memory mappings for fast access to large memories,53e9bb01b7602d9704739803,Going the distance for TLB prefetching: an application-driven study,"For instance, the hardware can prefetch PTEs in to the TLB in advance of their use [14, 30, 42].",other,providing context on hardware prefetching
530,5aed148b17c44a4438154fae,fa54b47df8641dff1579b5e8e0f18f057de68e73,DRN: A Deep Reinforcement Learning Framework for News Recommendation,55a6bae665ce054aad73115b,Human-level control through deep reinforcement learning.,"Here, we use the experience replay technique [31] to update the network.###Third, we propose to apply Dueling Bandit Gradient Descent exploration strategy [16, 49] to our algorithm which can both improve recommendation diversity and avoid the harm to recommendation accuracy induced by classical exploration strategies like ϵ - greedy [31] and Upper Confidence Bound [23].###State-of-art reinforcement learning methods usually apply the simple ϵ - greedy strategy [31] or Upper Confidence Bound ( UCB ) [23, 43] (mainly for Multi-Armed Bandit methods).###First, in order to better model the dynamic nature of news characteristics and user preference, we propose to use Deep Q-Learning (DQN) [31] framework.###Considering the previous mentioned dynamic feature of news recommendation and the need to estimate future reward, we apply a Deep Q-Network (DQN) [31] to model the probability that one user may click on one specific piece of news.###The most straightforward strategies to do exploration in reinforcement learning are ϵ - greedy [31] and UCB [23]. ϵ - greedy will randomly recommend new items with a probability of ϵ , while UCB will pick items that have not been explored for many times (because these items may have larger…",impact-revealing,describing the application of reinforcement learning techniques in news recommendation
3468,5eccb534e06a4c1b26a8358b,8a8a5f327ead63fa56d72e8e75a647e3e6154bc8,Residual Feature Aggregation Network for Image Super-Resolution,58437725ac44360f1082f7f7,Accelerating The Super-Resolution Convolutional Neural Network,"ness of our RFANet, we compare RFANet with 12 stateof-the-art image SR methods: SRCNN [4], FSRCNN [5], VDSR [13], LapSRN [15], MemNet [25], EDSR [18], SRMD [36], NLRN [19], DBPN [6], RDN [40], RCAN [38] and SAN [3].###We compare our RFANet with 10 state-of-the-art methods: SPMSR [22], SRCNN [4], FSRCNN [5], VDSR [13], IRCNN [35], SRMD [36], RDN [40], SRFBN [17], RCAN [38], and SAN [3].###9663 FSRCNN [5] ×2 37.###9 shows the comparisons about model size and performance with 11 stae-of-the-art SR methods:SRCNN [4], FSRCNN [5], VDSR [13], LapSRN [15], MemNet [25], NLRN [19], SRMD [36], DBPN [6], RDN [40], RCAN [38] and SAN [3].",other,comparing performance of RFANet with state-of-the-art image super-resolution methods
943,5c04967517c44a2c74709321,54c4642d017830e1faddbb49f0377228d2b01493,HAQ: Hardware-Aware Automated Quantization With Mixed Precision,5bdc315017c44a1f58a05e13,Amc: Automl For Model Compression And Acceleration On Mobile Devices,[11] leveraged the reinforcement learning to automatically prune the convolution channels.,impact-revealing,reporting prior findings on reinforcement learning application
97,5f3f917891e011d38f9242d9,14156438bafed28a626738630b5181b83ed5d79c,Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters,5d270b1b3a55ac3e2533fd48,Label Aware Graph Convolutional Network -- Not All Edges Deserve Your  Attention,"Inspired by LAGCN [4] which uses a Multi-layer Perceptron (MLP) as the edge label predictor, we employ a one-layer MLP as the node label predictor at each layer and use the l1-distance between the prediction results of two nodes as their similarity measure.###Though some recent works [4, 9, 13, 25, 41] have noticed similar challenges, their solutions either fail to fit the fraud detection problems or break the end-to-end learning fashion of GNNs.###Meanwhile, theoretical studies prove the limitations and vulnerabilities of GNNs when graphs have noisy nodes and edges [3, 4, 13, 33].###To save the computational cost, we only take the embedding of the node itself as the input instead of using combined embeddings like the LAGCN [4].###Among those works, [4] proposes a neural network to predict the labels of neighboring nodes.###Another approach is the metric learning [4, 13].",impact-revealing,drawing inspiration from existing models while highlighting their limitations
191,5cd7fa07ced107d4c65bf34f,371c799bde8b162e7f8fa2b2a0a8cfb29765f89f,Knowledge Graph Convolutional Networks for Recommender Systems,5b67b45517c44aac1c860876,Graph Convolutional Neural Networks for Web-Scale Recommender Systems.,Our method also connects to PinSage [21] and GAT [15].,impact-revealing,acknowledge connections to existing methods
3538,5550443b45ce0a409eb4c3b9,0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f,Towards End-To-End Speech Recognition with Recurrent Neural Networks,53e9bb2fb7602d970476d2a2,Learning precise timing with lstm recurrent networks,"For the version of LSTM used in this paper (Gers et al., 2002) H is implemented by the following composite function: where σ is the logistic sigmoid function, and i , f , o and c are respectively the input gate , forget gate , output gate and cell activation vectors, all of which are the same size…",other,providing context for the implementation of LSTM
3333,5fae6daad4150a363cec035c,413117826eecb3fd1491e0665e4b644a521d3bc3,Spanet: Spatial Pyramid Attention Network for Enhanced Image Recognition,5a73cb9817c44a0b3035c553,Attentive Systems: A Survey,Attention Mechanism [4] has been prevailed in computer vision for years [16].,other,acknowledge the use of attention mechanisms in computer vision
288,5db929b747c8f766461fa94f,2a6d160b529272964ce1a6707adf52f3d6ba4861,Diffusion Improves Graph Learning,5b16426b8fbcbf6e5a9b5cd3,TopPPR: Top-k Personalized PageRank Queries with Precision Guarantees on Large Graphs.,"Most importantly, there are fast approximations for both PPR [3, 77] and the heat kernel [34], with which GDC achieves a linear runtimeO(N).",impact-revealing,highlighting the efficiency of GDC with fast approximations
1250,,9550291efbb603fe6428dd43da4d25fd04a056bb,Chromosome diversity and evolution in Liliaceae.,,,"###Recently, however, this changed with the proposal for a relatively broad circumscription of Liliaceae comprising 15 genera and an improved understanding of the evolutionary relationships between them.###Total form % TF% Huziwara (1962) Index of chromosome size resemblance Rec Greilhuber and Speta (1976) Index of karyotype symmetry Syi Greilhuber and Speta (1976) Karyotype dispersion index DI Lavania and Srivastava (1992) Degree of karyotype asymmetry A Watanabe et al.###Chromosome diversity and evolution in Liliaceae
L. Peruzzi1,*, I. J. Leitch2 and K. F. Caparelli1
1Dipartimento di Biologia, Unità di Botanica Generale e Sistematica, Università di Pisa, via Luca Ghini 5, 56126 Pisa, Italy and 2Jodrell Laboratory, Royal Botanic Gardens, Kew, Richmond, Surrey TW9 3AB, UK
Received: 15 May 2008 Returned for revision: 4 August 2008 Accepted: 20 October 2008 Published electronically: 25 November 2008
† Background and Aims There is an extensive literature on the diversity of karyotypes found in genera within Liliaceae, but there has been no attempt to analyse these data within a robust phylogenetic framework.",impact-revealing,highlighting the lack of previous analysis on karyotype diversity within a phylogenetic framework
1335,,332b1250eb38575b4768bb6036369abc1d48ea25,Structure Preserving Non-negative Feature Self-Representation for Unsupervised Feature Selection,,,"###In light of the above mentioned limitations of RSR, the mentioned limitations of the RSR, in this study, we propose a novel unsupervised feature selection algorithm termed Structure Preserving Nonnegative Feature SelfRepresentation (SPNFSR), which simultaneously exploits the self-representation and structure preserving ability of features to guide the procedure of feature selection.###Motivated by the success of low-rank representation in subspace clustering, Zhu et al. [21] proposed a novel unsupervised feature selection algorithm called Regularized self-representation (RSR) model, in which each feature is represented as the linear combination of its relevant features.###Besides, labeling all training data often requires expensive human labor and costing large amount of time, many semisupervised feature selection approaches including Locality sensitive semi-supervised feature selection (LSFS) [11], Discriminative semi-supervised feature selection via manifold regularization (FS-manifold) [12], and Semi-supervised feature selection via spline regression (S2FS2R) [13] have been proposed to utilize both the limited labeled data and the abundant unlabeled data to improve the performance of feature selection.###Similar to the RSR, each feature is first represented by all other features in our SPNFSR.###Although the experimental results in [21] show that RSR outperforms some state-of-the-art algorithms, the structure preserving ability of features is neglected.###EXPERIMENTAL RESULTS AND ANALYSIS In this section, we conduct two experiments including image recognition and clustering to evaluate the effectiveness of our proposed SPNFSR and compare our proposed approach with other state-of-the-art unsupervised feature selection algorithms (SPEC [4], RSR [12], LS [14], MCFS [15], UDFS [18], RUFS [19] NDFS [20] and GRNSR [30]) on###EXPERIMENTAL RESULTS AND ANALYSIS In this section, we conduct two experiments including image recognition and clustering to evaluate the effectiveness of our proposed SPNFSR and compare our proposed approach with other state-of-the-art unsupervised feature selection algorithms (SPEC [4], RSR [12], LS [14], MCFS [15], UDFS [18], RUFS [19] NDFS [20] and GRNSR [30]) on
VOLUME 5, 2017 8797
six standard databases including Extended YaleB [22], CMU PIE[23], AR [24], JAFFE [25], ORL [26] and COIL20 [27].###Moreover, RSR does not consider the nonnegative property of the learned feature weights, which may reduce its physical significance during feature selection.###Therefore, the structure preserving ability of features should be taken into account to improve the feature selection performance of RSR.###By introducing the l2,1-norm for regularization, RSR can effectively select the most representative features.###(3) Comparing to the existing feature selection methods, RSR and GRNSR, our method achieves better performance by taking the advantage of the self-representation ability of features, which benefits to select the most representative features.",impact-revealing,proposing a novel feature selection algorithm and highlighting its advantages over existing methods
1215,,ab19ce59f736c42e2504324160d7ed104b72cd45,Determinants of stem cell enrichment in healthy tissues and tumors: implications for non-genetic drug resistance,,,"###This adds to the mathematical literature quantifying the role of feedback regulation for tissue and tumor dynamics [2,13,17,23,26,30-34], and builds upon the wider mathematical literature concerned with the dynamics of hierarchically structured cell populations, e.###Homeostasis is ensured by complex regulatory mechanisms including negative feedback loops [2,3].###An ordinary differential equation model has been used to describe tissue hierarchy dynamics in a healthy tissue [2,26], and the models presented here build on these approaches.###, in the olfactory epithelium, where GDF11 and Activin βB negatively regulate cell division rates in progenitor and stem cells [2,18].",impact-revealing,highlighting the contribution to mathematical literature on tissue and tumor dynamics
924,573698426e3b12023e70bf59,1917bfe805b46fe3a45903e803b27bd41719cf3c,Symbiotic job scheduling on the IBM POWER8,53e9b89ab7602d97044701fa,Simultaneous multithreading: maximizing on-chip parallelism,"As pointed out by Tullsen et al. [21], this is a situation that should be avoided, and we suspect that current SMT implementations (including POWER8) have mechanisms to prevent this to happen.###Simultaneous multithreading (SMT) was proposed by Tullsen et al. [22] as a way to improve the utilization and throughput of a single core.",impact-revealing,acknowledging prior work on simultaneous multithreading
3143,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",558b9f65e4b0cfb70a18e6d5,"A Tour of Modern Image Filtering: New Insights and Methods, Both Practical and Theoretical","This type of image-dependent graph representation is strongly connected to popular image processing techniques, such as the bilateral filter and related methods [73], which also apply signal dependent filtering and are widerly used in applications such as image###images as graphs for segmentation [71], [72] and popular image-dependent filtering methods can be interpreted from a graph perspective [73].",other,providing context on image processing techniques
1779,,661959a7d2a4d458bc5b578152b0495c5546e66c,A MULTI-SITE CASE STUDY: ALGEBRAIC CONTENT AND PEDAGOGICAL KNOWLEDGE OF SIXTH GRADE MATHEMATICS TEACHERS.,,,###Hancock and Algozzine (2006) emphasized the importance of having such an interview guide for case study research.,impact-revealing,reporting prior findings on the importance of interview guides in case study research
3491,5e85c28491e0114016e821dc,2db6b9621862b4293c1433c91ee8fb45e394f376,DeepGS: Deep Representation Learning of Graphs and Sequences for Drug-Target Binding Affinity Prediction,5cc03429ced107d4c60ed491,An Efficient Framework for Sentence Similarity Modeling.,"Since the label/one-hot encodings of amino acids and atoms often neglect the context information, and motivated by the idea of Word2Vec [20], we leverage advanced techniques, Smi2Vec and Prot2Vec , to encode the amino acids and atoms to a distributed representation, before plug-ging them to CNN and BiGRU.###Speciﬁcally, motivated by Word2Vec [20], in the ﬁrst step we encode the symbols in the sequence of the target/protein and the drug to a distributed representation, by using Prot2Vec and Smi2Vec , respectively.###To remedy this, we propose to use Smi2Vec [26], a method similar to Word2Vec [20, 15, 27], to represent the symbols in the SMILES sequence.",other,highlighting the motivation for using advanced encoding techniques in protein and drug representation
1872,,acd10d1062aa2f3c9100363798aac3c58b8d72a8,Experiencing Participation in Health Care : “Through the Eyes of Older Adults”,,,"###According to Whittemore and Knafl [19], using a mixed method technique provides the possibility of reducing errors and thus helping to deepen and capture the concept of interest [19].###Moreover, integrated reviews incorporate defining concepts [19].###The primary research methods of analysis, which were developed for qualitative design, are applicable to the integrative review method [19].###Whittemore and Knafl confirm that qualitative research also gives less room for errors and more room for confirmation of the findings [19].###The integrative approach proved the possibility of retrieving a comprehensive understanding of the concept of interest [19].###This study used the methodology of Whittemore and Knafl during the whole process, including problem identification stage, literature search stage, data evaluation stage, and data analysis stage.###The authors sought to be systematic and rigorous, and the integrative review approach emphasized the importance of these two qualities [19].",impact-revealing,highlighting the significance of mixed method techniques in research methodology
3706,5f7fdd328de39f0828397e7f,9fa283d4f9c2ed991383c0434ef6043bee0dc8e2,Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification,5c8f69874895d9cbc647064f,MnasNet: Platform-Aware Neural Architecture Search for Mobile,", MnasNets [49], ShuffleNets-V2 [36], MobileNets-V2 [43], CondenseNets [21], FBNets [59], ProxylessNAS [5], SkipNet [55], SACT [10], GoogLeNet [48] and MSDNet [20].###We also compare GFNet with a number of highly competitive baselines, i.e., MnasNets [49], ShuffleNets-V2 [36], MobileNets-V2 [43], CondenseNets [21], FBNets [59], ProxylessNAS [5], SkipNet [55], SACT [10], GoogLeNet [48] and MSDNet [20].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1993,,fcc77af10fae572dd79f11e41ba0b25ffeb985de,Ecosystem consequences of species richness and composition in pond food webs,,,"###Thus our approach does not consider how likely each of the manipulated species is to go extinct, and we emphasize that in situations of environmental concern, compositional effects will depend on additional factors that determine which species are most likely to go extinct",impact-revealing,highlighting limitations in the approach regarding species extinction likelihood
3729,5f576c1591e011f4c3d5dd7e,c36571ac50808c75fa8a5d37f1041af22e89e6ee,GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training,58d82fc8d649053542fd59aa,Geometric deep learning on graphs and manifolds using mixture model CNNs,"Modern GNNs follow a neighborhood aggregation strategy (Sukhbaatar et al., 2016; Kipf & Welling, 2017; Hamilton et al., 2017; Velickovic et al., 2018; Monti et al., 2017; Ying et al., 2021), where the representation of a node is iteratively updated by aggregating the representation of its neighbors.",other,providing context on modern graph neural networks
2480,5e943b8091e0113448664118,b26f2037f769d5ffc5f7bdcec2de8da28ec14bee,dense passage retrieval for open-domain question answering,5dca89783a55ac77dcb01f15,Knowledge Guided Text Retrieval and Reading for Open Domain Question  Answering,"Augmenting text-based retrieval with external structured information, such as knowledge graph and Wikipedia hyperlinks, has also been explored recently (Min et al., 2019b; Asai et al., 2020).###In practice, the sparse vector space model like TF-IDF or BM25 is a very strong baseline for retrieval and has been viewed as the standard method applied broadly to various QA tasks (e.g., Chen et al., 2017; Nie et al., 2019; Min et al., 2019a; Wolfson et al., 2020).###Augmenting text-based retrieval with externalstructured information, such as knowledge graph and Wikipedia hyper-links, has also been explored recently (Min et al., 2019b; Asai et al., 2020).",other,acknowledge recent explorations in text-based retrieval methods
1411,,bd3a0bbabae3260098e06bfb615147fb6d34e55a,Latent Plans for Task-Agnostic Offline Reinforcement Learning,,,"###TACO-RL outperforms both LMP and CQL as it is able to stitch plans through dynamic programming as shown in Table 5.###Our Q-learning approach corresponds to the following Bellman error optimization objective:
min λ
Eτ∼D,z∼qφ(z|τ),sg∼S [ Qλ(st, zt, g)− Q̂(st, zt, g) ]2 where: Q̂(st, zt, g) = ( 1st+k−1=sg + γ1st+k−1 6=sg maxzt+k−1 Qλ(st+k−1, zt+k−1, sg)
) (3) We can then learn a high-level policy πθ(z|sc, sg) with an off-the-shelf offline actor-critic method.###This change is required as CQL needs to take isolated decisions every time step.###In TACO-RL, we use Conservative Q-Learning (CQL) [6] where we initialize the actor weights with the previously learned prior policy πδ(z|sc, sg), as the prior policy is already a good starting point that is able to solve short-horizon tasks.###We use the following hyperparameters:
• Batch size of 64
• Learning rates − Q-function: 3e− 4, Policy: 1e− 4, • Target network update rate of 0.005
• Ratio of policy to Q-function updates of 1 : 1
• Number of Q-functions: 2 Q-functions, min(Q1, Q2) used for Q-function backup and policy update
• Automatic entropy tuning: True, with target entropy set to − log |A| • CQL version: CQL(H) (Using deterministic backup) • α in CQL: 1.0 (we used the non-Lagrange version of CQL(H)) • Number of negative samples used for estimating logsumexp: 4 • Initial BC warmstart epochs: 5 • Discount factor of 0.95
It is important to mention that the model performance is robust to slight changes in the hyperparameter selection.###The CQL transitions are (st, a, st+1) instead of (st, zt, st+k−1), therefore when we sample a goal from discrete-time offset ∆ ∼ Geom(p), we use the observations at the time step t + ∆ as a goal instead of the observation at the timestep t+ ∆ ∗ (k− 1), where k is the window size and p is the same value for both implementations.###To learn the high level policy we use CQL.###CQL+HER is trained on the derived reward as explained in Section 4.2 and introduces a penalty for out-ofdistribution actions to limit the respective values of unseen actions.###As we aim to combine the complementary strengths of both paradigms, imitation and offline RL, we compare TACO-RL to representatives of the two extrema of the spectrum: the offline RL method Conservative Q-learning [6] extended by hindsight relabeling (CQL+HER) and the imitation learning method Play-supervised Latent Motor Plan (LMP) [13].###Conservative Q-learning [6], on the other hand, imposes a penalty for actions not covered in the dataset making out-of-distribution actions non-optimal.###TACO-RL successfully performs long-horizon tasks that require reasoning over sequential behaviors with an final success rate of 27% which corresponds to an order of magnitude improvement upon the LMP and CQL+HER baselines.###On the other hand, CQL+HER is not able to achieve the desired goal image and LMP has a strong bias towards the end-effector position ignoring the changes in the environment.###While we make use of Conservative Q-learning in our experiments – which recently emerged as one of the most widely used benchmarks in offline RL – we want to point out that any other sophisticated improvement upon the classical offline RL objective is orthogonal to our work and could in principle be incorporated in our framework.###Implicit Q-learning [10] modifies the Bellman optimality update towards a SARSA-like update, maximizing only over actions in the data-set.###Related prior work attempted to solve this via a goal-conditioned reformulation [12] of Conservative Q-learning [6].###Especially offline RL [5, 6, 7, 8, 9, 10] with its appealing property to estimate (close-to) optimal policies from previously collected and fixed datasets yielded a strong current in robot control research.",impact-revealing,describing the performance and methodology of TACO-RL in comparison to other methods
2848,5a260c0c17c44a4ba8a1e113,c751ab01aedc2888a7fe6e8b4f77ab1afa94072f,protein interface prediction using graph convolutional networks.,5550410f45ce0a409eb384f8,Sequence to Sequence Learning with Neural Networks.,"The remarkable success of deep neural networks in a wide range of challenging machine learning tasks from computer vision [14, 15] and speech recognition [12] to machine translation [24] and computational biology [4], has resulted in a resurgence of interest in this area.",other,highlighting the broad impact and success of deep neural networks across various fields
1680,,99f825f43f9ea1b3050aa0f1a966c46c48983013,Intra-group Tension Under Inter-group Conflict: A Generative Model Using Group Social Norms and Identity,,,"###This is motivated by Social Identity Theory and contrasts with a conventional modeling approach where individuals are the dominant modeling element, from which groups are then defined based on a collection of common characteristics.###Over the years numerous significant theoretical insights have been made [1,2,3], but particularly profound contributions have originated from Social Identity Theory [4].###In this paper, we use Social Identity Theory to develop a new and generative model of group behavior, specifically focusing on the tensions associated with affiliation to multiple groups.",impact-revealing,highlighting the significance of Social Identity Theory in understanding group behavior
3439,5db929b747c8f766461fa94f,2a6d160b529272964ce1a6707adf52f3d6ba4861,Diffusion Improves Graph Learning,53e9b851b7602d9704417407,Wavelets on Graphs via Spectral Graph Theory,"A common choice for gξ in the literature is a polynomial filter of order J , since it is localized and has a limited number of parameters [16, 27]:",other,providing context for a common choice in literature
3785,57d063e0ac44367354294777,9c62532d78d6eb31f7b0489b6bcb4c2baf517860,CASH: Supporting IaaS Customers with a Sub-core Configurable Architecture,558b4f2684ae84d265c2ab1a,Scale-out processors.,"Microservers [4, 37, 59], data center optimized accelerators [19, 31, 43], and fine-grain reconfigurable processors [64] are all examples.",other,providing examples of computing architectures
2070,,634d48731e6ffa0cfcbf8a02b01dd22687ec49d1,Potency of Omadacycline against Mycobacteroides abscessus Clinical Isolates In Vitro and in a Mouse Model of Pulmonary Infection,,,"###abscessus reference strain ATCC 19977 (47) was purchased from ATCC (Manassas, VA) and authenticated by sequencing its genome (48).###abscessus strain and is widely used by laboratories studying this organism (47), was also included.",impact-revealing,reporting the use of a specific reference strain in research
611,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,53e9bd98b7602d9704a46071,A comparative evaluation of fusion strategies for multimodal biometric verification,[4] provided a comparison between the rule-based fusion and learning-based fusion (trained) strategy.,impact-revealing,reporting prior findings on fusion strategies
708,53e99e61b7602d97027252c8,e3369553f399fb63b9f24261247afe64fb6c3f65,Virtual Program Counter (VPC) Prediction: Very Low Cost Indirect Branch Prediction Using Conditional Branch Prediction Hardware,53e99c8bb7602d970253dff0,Optimizing dynamically-dispatched calls with run-time type feedback,"Devirtualization is the substitution of an indirect method call with direct method calls in object-oriented languages [11], [24], [21], [6], [28].###The VPC prediction algorithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) [11], [24], [21], [6] or devirtualization [28].###These approaches include the method cache in Smalltalk-80 [11], polymorphic inline caches [23], and type feedback/devirtualization [24], [28].",impact-revealing,providing context on devirtualization and its applications in compiler optimization
2825,5bdc31b417c44a1f58a0bbc4,27e98e09cf09bc13c913d01676e5f32624011050,U-Net: Machine Reading Comprehension with Unanswerable Questions,5550456245ce0a409eb55cee,Glove: Global Vectors for Word Representation.,"Glove embedding (Pennington, Socher, and Manning 2014) represented as a d -dim embedding by combining the features/embedding described above.",other,reporting prior findings on GloVe embedding representation
841,58d82fcbd649053542fd67e0,6b183d2297cb493a57dbc875689ab2430d870043,task-guided and path-augmented heterogeneous network embedding for author identification,57d063c7ac443673542906db,Entity Embedding-Based Anomaly Detection for Heterogeneous Categorical Events.,"Network embedding also attracts lots of attentions in recent years [17, 26, 25, 6, 7].###network mining problems, good representations of data are very important, as demonstrated by many previous work [16, 15, 17, 26, 7].",impact-revealing,highlighting the growing interest in network embedding and its importance in network mining
3699,5d0b00ed8607575390fedd4e,493d5f344eea1468260946b29a80dc81b2be409c,HopSkipJumpAttack: A Query-Efficient Decision-Based Attack,5a260c8b17c44a4ba8a3287e,Mitigating adversarial effects through randomization.,Multiple variants have been proposed to randomize the gradients [35–39].,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2327,5d9edc8347c8f76646042a37,7e71eedb078181873a56f2adcfef9dddaeb95602,simplifying graph convolutional networks,5b1643998fbcbf6e5a9bc241,Semi-Supervised User Geolocation Via Graph Convolutional Networks,"Rahimi et al. (2018) apply GCNs with highway connections on this task and achieve close to state-of-the-art results.###Table 5 shows that SGC outperforms GCN with highway connections on GEOTEXT (Eisenstein et al., 2010), TWITTERUS (Roller et al., 2012), and TWITTER-WORLD (Han et al., 2012) under Rahimi et al. (2018)’s framework, while saving 30+ hours on TWITTER-WORLD.###We replace the 4- layer, highway-connection GCN with a 3rd degree propagation matrix (K = 3) SGC and use the same set of hyperparameters as Rahimi et al. (2018).",other,reporting prior findings and comparisons in graph convolutional networks
2439,5f8d6be69fced0a24bbab005,93e513de7bc55a6d9319b7861435f435ed85a03f,dimension relation modeling for click-through rate prediction,5a260c8617c44a4ba8a3203a,Deep & Cross Network for Ad Click Predictions,"Meanwhile, embedding based methods have the ability to improve model generalization and explore different level (low-order or high-order) feature interactions thus making it the base of many models in CTR prediction, [1, 3–6, 8, 10].###…0.4436 0.7869 0.3755 0.6685 0.1432 AFM [11] 0.8038 0.4478 0.7817 0.3792 0.6618 0.1457 DeepFM [3] 0.8091 0.4423 0.7878 0.3753 0.6708 0.1372 DCN [10] 0.8093 0.4420 0.7875 0.3759 0.6702 0.1361 PNN [7] 0.8094 0.4414 0.7879 0.3752 0.6705 0.1360 xDeepFM [5] 0.8096 0.4412 0.7877 0.3753 0.6710 0.1356…###It should be explained that increase at 10 − 3 level in Criteo dataset is already clear compared with recent works such as xDeepFM [5] and DCN [10].",other,highlighting the effectiveness of embedding-based methods in CTR prediction
758,5ce3cd34e1cd8e3f7932b9af,5073bf182ee7b1cbe64128d12cc474b8cee43b76,Fast and Low-Precision Learning in GPU-Accelerated Spiking Neural Network,56d8d6ecdabfae2eeed2f2fd,Unsupervised learning of digit recognition using spike-timing-dependent plasticity.,"Several research efforts have explored STDP algorithms to enable learning in SNN [3] [4] [5].###The key innovation of this paper is to demonstrate stochastic STDP for unsupervised learning in SNN, instead of well-explored deterministic STDP algorithms [3] [4] used in prior simulators.###We next verify that deterministic STDP (defined as baseline) in ParallelSpikeSim shows comparable accuracy with the stateof-the-art SNN design with deterministic STDP from Diehl [3].",impact-revealing,highlighting the innovation of stochastic STDP for unsupervised learning in SNN
1965,,ca9e9acba8c14da7396583c3ec8ac510a724fa30,Moderate hypofractionated radiotherapy with volumetric modulated arc therapy and simultaneous integrated boost for pelvic irradiation in prostate cancer,,,"###However, it has to be considered that the trial included only 45% of patients with risk higher than 15% of lymph node involvement and that the upper limit of the pelvic fields in the trial was located at S1/S2, slightly lower than the limit commonly used for whole pelvic irradiation (Pommier et al. 2007).###Successively, the GETUG trial showed no differences between the two arms in terms of PFS and OS, but with the limitation of the low total dose administered to the prostate (Pommier et al. 2007).",impact-revealing,highlighting limitations in the trial design and results
1673,,59e058e6d6d230a6edf3ba60709b1397c380cdcb,"Fair Is Fair, or Is It? Territorial Identity Triggers Influence Ultimatum Game Behavior",,,"###Our experiment builds upon the insights of social identity theory (Tajfel & Turner, 1979, 1986) and the
related social categorization theory (Tajfel, Billig, Bundy, & Flament, 1971; Turner, Hogg, Oakes, Reicher, & Wetherell, 1987) to try to understand the complex relationship between the range of…",impact-revealing,building on social identity theory to explore complex relationships
3450,5ce3aebeced107d4c65ebaaf,7223fb56eeece5806f8d25718bbc78386e17f19f,SinGAN-GIF: Learning a Generative Video Model from a Single GIF,5d9edbf747c8f7664602da0d,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,"They have been used for various image processing applications including super resolution, image editing, and style transfer [5, 19, 41].",other,reporting applications of image processing techniques
3045,5ec49a639fced0a24b4de7d4,9eda533cf0badf8dbed5c8240bb828b622328183,Fine-grained Fact Verification with Kernel Graph Attention Network,5aed14d117c44a4438158a77,AllenNLP: A Deep Semantic Natural Language Processing Platform.,"For a given claim, it first utilizes the constituency parser in AllenNLP (Gardner et al., 2018) to extract all phrases which potentially indicate entities.",other,describing the process of entity extraction
2808,5c2c7a9217c44a4e7cf317b4,61946177ea5d44ad18ff09de7929556d7b34cd8b,simd-x: programming and processing of graph algorithms on gpus.,555048bc45ce0a409eb70dcf,Computation and communication efficient graph processing with distributed immutable view.,"Recent advance in graph computing falls in algorithm innovation [39, 72], framework developments [37, 14, 33, 28, 30, 75, 77, 18, 53, 51, 49, 19, 42, 48, 61, 6, 66, 68, 52, 73, 62, 43, 74, 70, 69, 3, 64, 40, 17, 8] and accelerator optimizations [63, 29, 38, 25, 71, 47].",other,acknowledge advancements in graph computing
322,5eda19c991e01187f5d6d814,befc296197edcf5431e6042ee58f48d5dc2cf970,Convergence and Stability of Graph Convolutional Networks on Large Random Graphs,53e99800b7602d970201082e,Group Invariant Scattering,"Under this light, a relevant notion is that of stability: since GCNs are trained then tested on different (large) graphs, how much does a change in the graph structure affect its predictions? In the context of signals defined on Euclidean domains, including images or audio, convolutional representations such as scattering transforms or certain CNN architectures have been shown to be stable to spatial deformations [34, 5, 40].###Graph Convolutional Networks (GCNs [8, 14, 25]) are deep architectures deﬁned on graphs inspired by classical Convolutional Neural Networks (CNNs [27]).###When P is proportional to the Lebesgue measure, since NP (τ) is controlled by ‖∇τ‖∞, the GCN is invariant to translations and stable to deformations, similar to Euclidean domains [34].###The study of stability to deformations has been pioneered by Mallat [34] in the context of the scattering transform for signals on Euclidean domains such as images or audio signals [8, 2], and was later extended to more generic CNN architectures [5, 40].###In contrast, we have shown that combining them with random models of large graphs allows us to define intuitive notions of deformations and stability in the continuous world like the Euclidean case [34, 5, 40], with direct applications in community-based social networks or shape analysis on point clouds.###In contrast, our continuous setup allows us to define more intuitive geometric perturbations based on deformations of random graph models and to obtain deformation stability bounds that are similar to those on Euclidean domains [34].###Similar to CNNs [34, 5], studying GCNs in the continuous world allows us to define intuitive notions of model deformations and characterize their stability.###Once again we focus on invariant c-GCNs with pooling, similar to classical scattering transform [34].###d‖∇τ‖∞, recovering the more standard quantity of Mallat [34].###The study of stability to deformations has been pioneered by Mallat [32] in the context of the scattering transform for signals on Euclidean domains such as images or audio signals [7, 2], and was later extended to more generic CNN architectures [4, 38].###Mallat [34] studied the stability to small deformations of the wavelet-based scattering transform, which was extended to more generic learned convolutional network, e.###Mallat [32] studied the stability to small deformations of the wavelet-based scattering transform, which was extended to more generic learned convolutional network, e.g., [4, 38], and tries to establish bounds of the following form for a signal representation Φ( ): where − ) is the deformed signal and N ( τ ) quantiﬁes the size of the deformation, typically through norms of its jacobian ∇ τ , such as k∇ τ k ∞ = sup x k∇ τ ( x ) . the deformation k As we have seen in the introduction, it is not clear how to extend notion of on discrete graphs [16, 18].###Similar to CNNs [32, 4], studying GCNs in the continuous world allows us to deﬁne intuitive notions of model deformations and characterize their stability.###In particular, when P is proportional to the Lebesgue measure and k∇ τ k ∞ < 1, we have q τ ( x ) = det( I − ∇ τ ( x )) − 1 ; then, for small enough k∇ τ k ∞ , we obtain N P ( τ ) . d k∇ τ k ∞ , recovering the more standard quantity of Mallat [32]. we also 2 d if In this case, have the bound In the rest of the section, we will assume for simplicity that the considered GCNs Φ have zero bias at each layer.",impact-revealing,discussing the stability of Graph Convolutional Networks in relation to deformations
2983,5d08be648607575390f908ca,b30b850e726ee41f80a28c2a81da9c640a8541fc,QoSMT: supporting precise performance control for simultaneous multithreading architecture,53e9ad2cb7602d970370cef1,Learning-Based Smt Processor Resource Distribution Via Hill-Climbing,"There are many studies [7][30][9][10][27] providing solutions on improving overall SMT throughput and fairness, but they did not take performance control into account.###Although there is previous literature [9] [10]on hardware design for guarantee of quality-of-service (QoS) on SMT processors, most of their designs require profiling in-advance.",other,highlighting gaps in existing studies on SMT throughput and fairness
7,5e5f7c4791e011df604ecbed,0ca7d8c3250d43d14fdde46bf6fc299654d861ef,Heterogeneous Graph Transformer,599c797d601a182cd2643e8a,Modeling Relational Data with Graph Convolutional Networks.,"GNN Models GCN [6] RGCN [11] GAT [17] HetGNN [22] HAN [18] HGTnoHeter HGT###We compare HGT with several state-of-the-art GNNs, including both homogeneous—GCN [6] and GAT [17]—and heterogeneous GNNs—RGCN [11], HetGNN [22], and HAN [18].###Recently, in view of graph neural networks’ (GNNs) success [4, 6, 17], there are several attempts to adopt GNNs to learn with heterogeneous networks [11, 18, 21, 22].###Moreover, HGT has fewer parameters and comparable batch time than all the heterogeneous graph neural network baselines, including RGCN, HetGNN, and HAN.",impact-revealing,comparing HGT with state-of-the-art GNNs
1877,,4d0f5f599b4d0a65349187bbeb21586084a6c0da,"Danish University Colleges Parental divorce and parental death-An integrative systematic review of children’s double bereavement Marcussen,",,,"###The analyses were otherwise carried out according to principles of systematic integrative review, outlined by Whittemore and Knafl.###When data were conceptualized to a higher level of abstraction, the primary source was reviewed to verify the synthesis.[1]###Data display is shown in the review matrix (see Table 2).[1, 23] The included articles cover expressions of the double loss and be-###Seven studies were rated as high and four as low in terms of methodology/theoretical rigour, whereas four studies were rated as high and seven as low regarding relevance.[1]###This ensured transparency in the review process, consistency with the stated aim and replicability of the research.[1]###We were inspired by Whittemore and Knafl’s strategy for conducting an integrative review, evaluating and analyzing the data, and qualifying the synthesis of the results.[1] The review used the matrix method, which follows a structured approach and process.###In the discussion and conclusion of the data, the data comparison also included, as described by Whittemore and Knaf[1] and inspired by Miles and Huberman: Subsuming particulars into general, noting relations between variability, finding intervening factors and building the logical chain of evidence.###In the discussion and conclusion of the data, the data comparison also included, as described by Whittemore and Knaf[1] and inspired by Miles and Huberman: Subsuming particulars into general, noting relations between variability, finding intervening factors and building the logical chain of evidence.[1, 24] This was done through discussion of the themes and subcategories.###None of the articles were excluded in this process.[1, 23]###This article presents an integrative, systematic literature review[1] of research on children and adolescents who experience double bereavement.###4 The analysis process The analysis process is described by Whittemore and Knafl[1] (see Figure 1).###The analysis process is described by Whittemore and Knafl[1] (see Figure 1).###We were inspired by Whittemore and Knafl’s strategy for conducting an integrative review, evaluating and analyzing the data, and qualifying the synthesis of the results.",impact-revealing,acknowledging the methodology for systematic integrative review
3775,5e71f49891e0115656f5d0a5,fc99c6cc77f12cf252a9dcaa4dd2f2987e029375,closed-loop matters: dual regression networks for single image super-resolution,5c2c7a9217c44a4e7cf31404,Unsupervised Degradation Learning for Single Image Super-Resolution.,"In this case, existing SR models often incur the severe adaptation problem [43, 54].###Second, it is hard to obtain a promising SR model when the paired data are unavailable [43, 54].###More critically, if we directly apply existing SR models to real-world data, they often incur a severe adaptation problem and yield poor performance [43, 54].###There is an increasing interest in learning super-resolution models without paired data in the unsupervised setting [43, 54].",other,highlighting challenges and limitations in existing super-resolution models
778,53e9a396b7602d9702ca5bc7,faed220527ee0144847018c7cc5ba3156513dd1f,memory bypassing: not worth the effort,53e9a4fab7602d9702e1d422,Memory dependence prediction using store sets,"Chrysos and Emer proposed store sets for memory dependence prediction which we used in this study [3].###In particular, Chrysos and Emer proposed using Store Setsto predict memory dependences [3].###In both cases, we also use the set merging optimization described in [3], which we found to improve performance slightly.###The store set implementation presented in [3] uses a pair of tables to predict memory dependences.###We used an aggressive processor model that is comparable to the configuration used in the original store sets study [3] and the 8-wide configuration from the Stack Value File study [7].",impact-revealing,acknowledge the use of store sets for memory dependence prediction
112,5c04967517c44a2c7470926f,c2d40522eaa5523d67a0de5e4098e7031fdccb3d,Pitfalls of Graph Neural Network Evaluation,573695fe6e3b12023e511794,Revisiting semi-supervised learning with graph embeddings,"We run the 4 models on the datasets and respective splits from [Yang et al., 2016].",impact-revealing,reporting model evaluation on datasets
3755,53e9a232b7602d9702b3a1a9,327722247ffc70a0d51f5c2246bc9a53c0e7daa3,Accurate branch prediction for short threads,53e9b768b7602d9704309249,Mitosis compiler: an infrastructure for speculative threading based on pre-computation slices,"During the thread spawning event, register values and even memory inputs of the new speculative thread may be predicted by using a prediction algorithm [40, 25, 22, 32].###Using the terminology from [32], a speculative thread is started when the first address from a spawning pair is fetched by the processor.###Some models rely primarily on hardware prediction of liveins [24] and others use software to compute register and memory live-ins [32].###Specific examples include speculative multithreading [40, 7, 28, 30, 32, 43, 45, 47], or thread-level speculation, which executes multiple not-necessarily-independent short threads in parallel, and detects unpredicted dependence violations – squashing those threads where parallelism was not found.###For SpMT systems with a more general thread spawning strategy [32], this result may still hold.###Those proposed include models that work best when threads are largely independent [44, 7] and those that work even in the presence of dependences [24, 32].",other,describing prediction algorithms and models for speculative threading
177,5bdc31b817c44a1f58a0c039,abfa95058fa50c55a0b923a6c35830f470c125ad,Adaptive sampling towards fast graph representation learning,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"We evaluate the performance of our method on four popular benchmarks for node classification, including Cora, Citeseer, Pubmed [11] and Reddit [3].###This includes examples of social networks [3], protein interfaces [4], and 3D meshes [5].###More recently, two kinds of sampling-based methods including GraphSAGE [3] and FastGCN [20] were developed for fast representation learning on graphs.###We evaluate the performance of our methods on the following benchmarks: (1) categorizing academic papers in the citation network datasets–Cora, Citeseer and Pubmed [11]; (2) predicting which community different posts belong to in Reddit [3].###For the Reddit dataset, the hidden dimensions are selected to be 256 as suggested by [3].###The codes of GraphSAGE [3] and FastGCNN [20] provided by the authors are implemented inconsistently; here we re-implement them based on our framework to make the comparisons more fair.###We contrast our approach with GraphSAGE [3] and FastGCN [20] regarding the following aspects:",impact-revealing,reporting performance evaluation on popular benchmarks for node classification
2587,5fe30a2291e01125d4b5b5e3,07fd366a8ebdefe54cdb57d87c81dcd22de25a91,A DISTRIBUTIONAL APPROACH TO CONTROLLED TEXT GENERATION,5e5e18ca93d709897ce315f0,Residual Energy-Based Models for Text Generation,"Some current applications to text generation include Parshakova et al. (2019a) and Deng et al. (2020), who augment a standard autoregressive LM with an additional global factor in order to get a lower perplexity on the training data.",other,reporting current applications in text generation
1041,,b24c78580145b92d5de9729c22b51a1f2b3b3cbe,Principled dictionary pruning for low-memory corpus compression,,,"###Hash-based fingerprint technology has been widely used for the tasks of duplicate identification and redundancy elimination [12, 13, 21, 22, 23, 26].###Algorithm 3 identifies redundant chunks using the Rabin-Karp rolling hash function [12].",impact-revealing,reporting prior findings and applications
2309,5f8d6be69fced0a24bbab01e,a87e4124f7305a97a8efaa574c1b270dccf4a563,DisenHAN: Disentangled Heterogeneous Graph Attention Network for Recommendation,58d82fcbd649053542fd669e,Deep Variational Information Bottleneck,", bring enhanced generalization ability as well as improved robustness to adversarial attack[2], but also make the downstream process more interpretable which can directly find applications in various fields with semantic data, such as images[5, 6, 13], texts[16] and user behaviors[22].",other,highlighting the benefits and applications of enhanced generalization and robustness in various fields
1527,,afc79b2d49fcd87bf87916002566136f5490f821,Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts,,,"###One signiﬁcant class of subtle-but-offensive comments includes microag-gressions (Sue et al., 2007, MA S ), deﬁned in Merriam-Webster as “a comment or action that Figure 1 : Existing state-of-the-art tools for hate speech detection and sentiment analysis cannot identify the veiled of-fensiveness of…###Speciﬁ-cally, our new typology builds upon the work of Sue et al. (2007) with three goals in mind: the typology should be (1) generalizable across different axes of discrimination, (2) sufﬁciently represented in our corpus, and (3) comprehensive over all distinct microaggression types in the corpus.###As originally speciﬁed in Sue et al. (2007), MA S were categorized into 13 themes across speciﬁc axes of discrimination such as racism and sexism.",impact-revealing,highlighting the significance of microaggressions in the context of hate speech detection
1093,,e6cfb7910cfe049301629fb394463ee70be17fc3,DOGMA-MESS: A Tool for Fact-Oriented Collaborative Ontology Evolution,,,"###In order to support the perspective uniﬁcation process, we are currently implementing a meaning negotiation and argumentation module, that is inspired by re-lated tools such as HCOME [14] and Diligent [18].",impact-revealing,drawing inspiration from related tools for implementing a new module
2940,58437722ac44360f1082f15c,cc16e43cce64b649da00892d1493425620c2d61c,Learning to Match Using Local and Distributed Representations of Text for Web Search.,5736977f6e3b12023e665ee9,Improving Document Ranking with Dual Word Embeddings.,"[28] used dual embeddings, one for document terms and one for query terms, then ranked based on the all-pairs similarity between vectors.###Among the baseline models, including both traditional and neural network based models, CDSSM and DESM achieve the highest NDCG at position one and ten, respectively, on the weighted test set.###In IR, a significant number of these works have focused on word embeddings [6, 8, 10, 11, 27, 28, 34, 41] and modelling short-text similarities [15, 16, 29, 35–37].###In particular, we use the DESM IN-OUT model, which was reported to have the best performance on the retrieval task, as a baseline in this paper.###The dual embedding space model (DESM) [27, 28] computes a document relevance score by comparing every term in the document with every query term using pre-trained word embeddings.###These anecdotal examples agree with the results in in Table 2 that show that on the weighted test set all the neural models whose main focus is on learning distributed representations of text (duet model, distributed model, DESM, DSSM, and CDSSM) perform better than the models that only look at patterns of term matches (local model and DRMM).",other,reporting prior findings and methods in information retrieval
672,5ce2d184ced107d4c6438f01,cd26a0ae3c5a65d807b8fa7134f3a44cfd0392bd,exploiting edge features for graph neural networks,5a260c8117c44a4ba8a30f54,Graph Attention Networks.,"The original GAT model [27] is only able to handle one dimensional binary edge features, i.###Indeed, the essential difference between GCN [18] and GAT [27] is whether we use the attention coefficients (i.###[27] adapts an attention mechanism to graph learning and proposes a graph attention network (GAT), achieving current state-of-the-art performance on several graph node classification problems.###In existing attention mechanisms [27], the attention coefficient depends on Xi· and Xj· only.###Since taking the power of a DSM preserves the three mentioned properties, it prevents the edge matrix from exploding or shrinking to zero during diffusion, thus can help stabilize the process, compared with the previously used row normalization as in GAT [27]:###The three citation network datasets are also used in [32] [18] [27].###The baseline methods we used are GCN [18] and GAT [27].###[27] adopt attention mechanism into graph learning, and propose a graph attention network (GAT).###Following the experiment settings of [18][27], we use two layers of EGNN in all of our experiments for fair comparison.",impact-revealing,describing the differences and improvements in graph attention networks
1719,,9f617afb48fa91c0bc3c6212904d86351f0d6c44,Healthy social bonds: A necessary condition for well-being,,,"###Research by Crocker and Canevello (2008) has examined two types of interpersonal goals, compassionate goals and self-image goals, that reflect distinct perspectives and systems of motivation.###…and are motivated by concern for the well-being of others, while self-image goals focus on constructing and maintaining desirable public and private images of the self and are motivated out of a concern for one’s own needs and desires (Crocker & Canevello, 2008, 2012, Canevello & Crocker, 2015).###This perspective supports the ideas that actions towards others will harm or aid the self because of the interconnected nature of relationships and that cooperation with others can lead to the fulfillment of one’s needs (Crocker & Canevello, 2008, Crocker & Canevello, 2015).###…individuals view relationships as zero-sum affairs in which they can only gain benefits if others pay costs and others are only considered in terms of how they might help meet one’s own needs and desires (Crocker & Canevello, 2008, Crocker, Canevello, & Lewis, 2017, Canevello & Crocker, 2015).###Crocker and Canevello (2008) found that greater support provision by those who adopt compassionate goals leads to greater perceptions of perceived support from family, friends, and romantic partners as well as the receiving of more support from roommates.###…goals show greater consideration of others needs in their decision making, are more constructive in how they address relationship problems (Canevello, Crocker, Lewis, & Hartsell, 2014), and are more supportive (Crocker & Canevello, 2008) and responsive (Canevello & Crocker, 2011a) to others.###…upon how compassionate goals serve as a mechanism within relationships by which relevant interpersonal and personal outcomes can be improved (Crocker & Canevello, 2008, Canevello & Crocker, 2011a), but also reveal how, in the case of self-image goals, relationships can lead to worse…",impact-revealing,highlighting the distinct perspectives of interpersonal goals and their implications on relationships
3358,5ee9f15b91e01152af022eaf,a83902f8b3aadfda633968a840ca1738bedef837,modeling graph structure via relative position for text generation from knowledge graphs,5aed14d617c44a4438159341,Self-Attention with Relative Position Representations.,"Shaw et al. (2018) introduced position-aware self-attention in the Trans-former by (i) adding a relative position embedding A K ∈ R M × M × d to X ’s key representation, when computing the softmax-normalized attention scores α i between X i ∈ R d and the complete input embedding matrix X ∈ R M × d…###Like Graformer, these approaches incorporate information about the graph topology with some variant of relative position embeddings (Shaw et al., 2018).###…of implementation, our ﬁnal data structure for the KG is the hyper-graph’s incidence graph, a bipartite graph where hyper-arcs are represented as nodes and edges are unlabeled: where p differentiates between different relative positions in the original entity string, similar to (Shaw et al., 2018).",other,providing context on position-aware self-attention in the Transformer
2755,5f00587b9fced0a24b1fbbf1,1803c317dbd25d64210026fc4181faa31e521719,USMPep: universal sequence models for major histocompatibility complex binding affinity prediction,573695db6e3b12023e4f276a,Gapped sequence alignment using artificial neural networks: application to the MHC class I system.,NetMHC4 [6] Input: 9mer fixed length BLOSUM encoding plus additional features; multilayer perceptron with one hidden layer,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
405,5bdc315017c44a1f58a05e7e,7a71941e60894ae7e1f5af8e79c37cec6cd6c6ad,Multi-scale Residual Network for Image Super-Resolution,59ae3be32bbe271c4c71ba2e,Enhanced Deep Residual Networks for Single Image Super-Resolution,"EDSR [9] is an outstanding model gained amazing results.###It can be seen that our results are slightly lower than EDSR [9].###EDSR [9] was the champion of the NTIRE2017 SR Challenge.###In contrast, the specifications of our model is much smaller than EDSR [9], which makes it easier to reproduce and promote.###But it is worth noting that EDSR [9] use RGB channels for training, meanwhile, the data augment methods are different.###3 Comparisons with State-of-the-art Methods We compare our model with 10 state-of-the-art SR methods, including Bicubic, A+ [23], SelfExSR [20], SRCNN [1], ESPCN [2], FSRCNN [3], VDSR [4], DRCN [5], LapSRN [6] and EDSR [9].###After that, many SR models have been proposed, including DRCN [5], DRNN [7], LapSRN [6], SRResNet [8], and EDSR [9].###For fair, we retrain most of these models (except for EDSR [9], the results of EDSR provided by their original papers).###In this work, we have reconstructed some classic SR models, such as SRCNN [1], EDSR [9] and SRResNet [8].###To better illustrate the difference with EDSR [9], we show a comparison of model specifications in Table 3.",impact-revealing,comparing results with a state-of-the-art model and discussing model specifications
3184,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",58437735ac44360f1083224c,Dual Graph Regularized Dictionary Learning.,The design of representations that adapt to the specific properties of graph signal classes has further been addressed from the viewpoint of dictionary learning [115]–[117].,other,acknowledge existing research on graph signal representations
3132,5aed14d617c44a4438158f7c,ae1c89817a3a239e5344293138bdd80293983460,Attention U-Net:,5550414745ce0a409eb39ec6,Deeply-Supervised Nets.,"All models are trained using the Adam optimiser [15], batch-normalisation, deep-supervision [16], and standard data-augmentation techniques (afﬁne transformations, axial ﬂips, random crops).###We use deep-supervision [16] to force the intermediate feature-maps to be semantically discriminative at each image scale.###In particular, fully convolutional networks (FCN) [18] such as U-Net [24], DeepMedic [13] and holistically nested networks [16, 35] have been shown to achieve robust and accurate performance in various tasks including cardiac MR [3], brain tumours [12] and abdominal CT [26, 27] image segmentation…",other,reporting training methods and techniques used in models
3751,5db92a1a47c8f76646200974,8330c7c98c6c3e338ca578e0ab47f41bc18d0019,HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs,5a260c8617c44a4ba8a32449,Diffusion Operator and Spectral Analysis for Directed Hypergraph   Laplacian,"• directed hypergraphs (idea is to consider supremum in tail, infimum in head) [49, 9].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
263,5f8d6be69fced0a24bbaaf7b,6427b12aa3ddb4c89b7879c43267cd4a9f0ad1c7,DE-RRD: A Knowledge Distillation Framework for Recommender System,53e9b76eb7602d9704310d8f,Listwise approach to learning to rank: theory and algorithm,"To this end, RRD adopts the list-wise learning-to-rank approach [29] and learns to ensure the student to preserve the ranking orders predicted by the teacher.###For more details about the list-wise approach, please refer to [29].###Then, RRD defines a relaxed permutation probability motivated by [29].###To this end, RRD adopts the classical list-wise learning-to-rank approach [29].",impact-revealing,describing the method used in ranking
2250,558aed5ce4b0b32fcb39b5f6,ebe508e6a460e35e5c7a66d03c041d97e9145607,high-performance routing at the nanometer scale,53e99827b7602d970204aea9,Probabilistic congestion prediction.,"In fact, more recent work by Westra and Groeneveld has questioned the usefulness of probabilistic congestion estimators [33], including the technique presented in [32].###Empirical studies [32] show that in a fully-routed design a majority of all 2-pin nets take on Lshapes.###Recent work on probabilistic congestion estimation used the assumption that L- and Z-shaped routes were the most common routes taken by industrial routers [32].",other,highlighting recent critiques and findings in probabilistic congestion estimation
3705,5c04967517c44a2c7470927f,b9015d4f1e591eba7ea21c3566c919f80f7c2afe,attentive long short-term preference modeling for personalized product search,5c5c55bfe1cd8e03e716894c,Review-Driven Answer Generation for Product-Related Questions in E-Commerce.,"With the popularity of the online review platforms in recent years, researchers have attempted to extract information from reviews to represent products [7, 8, 10] by using representation learning techniques (e.g., word2vec [32, 33, 39]).",other,motivating research in information extraction from online reviews
1465,,500923d2513d30299350a6a0e9b84b077250dc78,Determining Semantic Similarity among Entity Classes from Different Ontologies,,,"###In this sense, studies have shown that the perceived similarity from a class to its superclass is greater than the perceived similarity from the superclass to the class, and that the superclass is commonly used as base(1) of the similarity evaluation [44, 49].###the relative size and salience of distinctive features sets [20], by potential stimulus biases, such as density and prototypicality [44, 50], by a natural reference point or landmark for members of a category [49], and by the direction of maximum informativiness [51].",impact-revealing,highlighting findings on perceived similarity in classification
1155,,464df3e306a5b9a6a7131a582637aaef3de058f2,Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation,,,"###In addition, we wish to obtain loss guidance terms that allow us to leverage existing developments on accelerated diffusion sampling methods (Song et al., 2021a; Zhang & Chen, 2022; Lu et al., 2022), unlike in Graikos et al. (2022), which finds point estimates via stochastic optimization.###In this paper, we directly apply fast first-order ODE / SDE solvers, such as DDIM.###• Efficiency D-PnP may take many iterations ( e.g. , DreamFusion (Poole et al., 2022) uses 15,000 iterations for each result) whereas CG and LGD are much faster since they can inherit the developments of accelerated diffusion model sampling ( e.g. , 10 to 1000 iterations), such as DDIM (Song et al., 2021a).###With the same underlying DDIM sampler, LGD-MC has much better histograms and KL divergence than LGD-DPS, as shown in Fig.###This sampling-based approach can leverage efficient diffusion model sampling algorithms (Song et al., 2021a; Liu et al., 2022; Zhang & Chen, 2022; Lu et al., 2022).###Score-based diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) represent a class of models that solve a stochastic differential equation (Song et al., 2021c) using the neural network D θ ; see App.###Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021c) are widely used as foundation models (Bommasani et al., 2021) for generative modeling.###…in a plug-and-play manner, and are applied to natural images (Kawar et al., 2021; Choi et al., 2021; Kawar et al., 2022a), medical images (Jalal et al., 2021; Chung & Ye, 2022; Song et al., 2021b), speech (Sawata et al., 2022), audio (Saito et al., 2022), and astronomy (Karchev et al., 2022).###Recently, there is a plethora of sampling methods developed for diffusion models, such as ones based on first-order ODE/SDE solvers (Song et al., 2021a), higher-order ODE solvers (Zhang & Chen, 2022; Karras et al., 2022; Lu et al., 2022; Dockhorn et al., 2022), or knowledge distillation models…###…D-PnP may take many iterations ( e.g. , DreamFusion (Poole et al., 2022) uses 15,000 iterations for each result) whereas CG and LGD are much faster since they can inherit the developments of accelerated diffusion model sampling ( e.g. , 10 to 1000 iterations), such as DDIM (Song et al., 2021a).",impact-revealing,highlighting the advancements in diffusion model sampling methods and their applications
2925,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,53e9af07b7602d970393fbe3,Efficient sampling of training set in large and noisy multimedia data,"For instance, to employ some noise filter mechanisms to smooth the noisy data or to apply an appropriate sampling technique to differentiate the noisy data from the input data before the fusion takes place [137].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2586,5f7d8a8391e011346ad27d2b,3bb1e24eb3429f807397833105d1e137d9927767,SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup,5cede107da562983788e6ba1,EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks,"Zhang et al. (2015); Wei and Zou (2019) utilize heuristic approaches including synonym replancement, random insertion, swap and deletion for text augmentation, Kaﬂe et al. (2017); Silfverberg et al. (2017) employ heuristic rules based on speciﬁc task, Hu et al. (2017) pro-pose to augment text data…###…data augmentation methods such as context-based words substitution (Kobayashi, 2018), synonym replacement, random insertion, swap, and deletion (Wei and Zou, 2019), paraphrasing (Cho et al., 2019) or back translation (Xie et al., 2019), because label composition is complex for sequence…",other,acknowledge various heuristic approaches for text augmentation
490,58437785ac44360f108432a7,92527ace7f75188b5ec209ff7d59f431343075e4,Video-based emotion recognition using CNN-RNN and C3D hybrid networks,573696f46e3b12023e5f12ae,Learning Spatiotemporal Features with 3D Convolutional Networks,C3D can model appearance and motion information simultaneously and the C3D features with a linear classifier can achieve good performance on different video analysis benchmarks [6].###The confusion matrices of our submissions on the validation and testing sets are given in Figure 5.###The category with the highest score is taken to be the final recognition result.###And we found that the CNN-RNN and C3D hybrid network can further improve the performance.,impact-revealing,highlighting the performance capabilities of C3D in video analysis
3319,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,53e99cfdb7602d97025b3938,Learning query-class dependent weights in automatic video retrieval.,"[151] Text (closed caption, video OCR), audio, video (color, edge and texture histogram), motion Video retrieval",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3455,5f0d85c69fced0a24be4f052,0dd3e9f581c617eb826bc0fabac5ae1394f9cef1,Data Compression Accelerator on IBM POWER9 and z15 Processors : Industrial Product,5736985a6e3b12023e7201f0,Spark Sql: Relational Data Processing In Spark,"TPC-DS benchmark interfaces with Spark SQL [62], a Spark module for relational data processing.",other,providing context for the TPC-DS benchmark
3290,5ee8986891e011e66831c3b6,38f93092ece8eee9771e61c1edaf11b1293cae1b,Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,5550414245ce0a409eb39c3f,Semi-supervised Learning with Deep Generative Models.,"In the semi-supervised setting [55, 56], an unsupervised loss is combined with a classification loss over a handful of labels to ground the training [19, 20, 57, 58, 59, 60, 61, 62].",other,describing the semi-supervised training approach
3169,5e85c28491e0114016e821dc,2db6b9621862b4293c1433c91ee8fb45e394f376,DeepGS: Deep Representation Learning of Graphs and Sequences for Drug-Target Binding Affinity Prediction,53e9b0dfb7602d9703b5c4ed,Drug Target Identification Using Side-Effect Similarity,"As a result, DTA prediction has received much attention in recent years [3, 14, 38].",other,highlighting the increasing interest in DTA prediction
1490,,e8e744ee4d1d46bcb8710d219c4c598e7c9d02ba,GroupDiff: Diffusion-based Group Portrait Editing,,,"###Diffusion models have been also widely applied in multiple tasks including video generation [14,19,25,54,64], image enhancement [18,51] and image composition [6,39,58,70] To take advantage of the generation power of diffusion models, our work also builds on top of this learning framework.",impact-revealing,highlighting the application of diffusion models in various tasks
694,5736982b6e3b12023e6fd154,5061f0c3637f22b1776f013f28f1bdc518a5c304,MORC: A manycore-oriented compressed cache,555045ee45ce0a409eb5ab81,SC2: a statistical compression cache scheme,"Even the recent work [24] which does compress across cache lines, is optimized for single-stream performance.###While one recent scheme [24] compresses inter-line, it still prioritizes single-stream performance over throughput.###MORC was evaluated against three best-of-breed cache compression schemes: Adaptive [18], Decoupled [19], and SC2 [24].###Prior work has shown that compressed caches are more efficient than larger uncompressed caches thanks to significantly lower static and dynamic energy power [24].###In addition to uncompressed caches, we also compare to Adaptive [18], Decoupled [19], and SC2 [24] compressed caches.###Finally, SC2 [24] is most similar to MORC because it maintains a system-wide dictionary which can compress data across cache lines.###SC2 needs 18KB to support the Huffman compression flow [24].",impact-revealing,acknowledge existing cache compression schemes and their performance
1741,,c0a225ea0d9f4971d97305766a54b37c9c20c4ac,Scalable Uniform Graph Sampling by Local Computation,,,"###In 1997, Paxson and Floyd [46] identified the difficulty in characterizing Internet topology and attributed it to the constant change and growth of the network.###In 2003, Floyd and Kohler [25] discussed the need for better models of the Internet.###Their research on Internet simulation is motivated by the possibility of approaching “complicated scenarios that would be either difficult or impossible to analyze” [46].",impact-revealing,acknowledging prior findings and motivations in Internet research
741,5f53599a91e0110c40a7bc91,5e7047851d05b2ecef5de451dda5404acda726de,learning from protein structure with geometric vector perceptrons,5d3c234c3a55acd386d4e112,Generative Models for Graph-Based Protein Design.,"These methods vary in their representation of geometry: while some, such as ProteinSolver and GraphQA, represent edges as a function of their length, others, such as Structured Transformer, indirectly encode the 3D geometry of the proximity graph in terms of relative orientations and other scalar features.###Following Ingraham et al. (2019), we frame this as an autoregressive task and use a masked encoder-decoder architecture to capture the joint distribution over all positions: for each i , the network models the distribution at i based on the complete structure graph, as well as the sequence…###Following Ingraham et al. (2019), we report evaluation on short (100 or fewer amino acid residues) and single-chain subsets of the CATH 4.2 test set, containing 94 and 103 proteins, respectively, in addition to the full test set.###…importance and difﬁculty, such problems, which we broadly refer to as learning from structure , have recently developed into an exciting and promising application area for deep learning (Graves et al., 2020; Ingraham et al., 2019; Pereira et al., 2016; Townshend et al., 2019; Won et al., 2019).###Recent state-of-the-art GNNs include Structured Transformer (Ingraham et al., 2019) on CPD, ProteinSolver (Strokach et al., 2020) on CPD and mutation stability prediction, and GraphQA (Baldassarre et al., 2020) on MQA.###2, representing a substantial improvement both in terms of perplexity and sequence recovery over Structured Transformer [15], which was trained on the same training set (Table We also tried learning a weighted average of nodes, but this led to increased overfitting.###There-5 Table 1: GVP-GNN outperforms Structured Transformer and sets a new state-of-the art on the CATH 4.2 protein design test set (and its short and single-chain subsets) in terms of per-residue perplexity (lower is better) and recovery (higher is better).###We use the CATH 4.2 dataset curated by Ingraham et al. (2019) in which all available structures with 40% nonredudancy are partitioned by their CATH (class, architecture, topology/fold, homologous superfamily) classiﬁcation.###Although Structured Transformer leverages an attention mechanism on top of a graph-structured representation of proteins, the authors note in ablation studies that removing attention appeared to increase performance.###[15] also evaluate the model perplexity on held-out native sequences.###We show and discuss the interpretability of such features in Appendix F. Ablation studies The methods we have compared against include a number of GNNs (Structured Transformer/GNN, ProteinSolver, GraphQA).###3 Structured Transformer [15] 8.###Thanks to their importance and difficulty, such problems, which we broadly refer to as learning from structure, have recently developed into an exciting and promising application area for deep learning [12, 13, 15, 23, 27, 31].###We therefore retrain and compare against a version of Structured Transformer with the attention layers replaced with standard graph propagation operations (Structured GNN).###Protein design GVP-GNN achieves state-of-the-art performance on CATH 4.2, representing a substantial improvement both in terms of perplexity and sequence recovery over Structured Trans-former (Ingraham et al., 2019), a GNN method which was trained using the same training and validation sets (Table 1).###The CPD method Structured Transformer [15] additionally uses a quaternion representation of pairwise relative orientations.###Drawing an analogy between sequence design and language modelling, Ingraham et al. (2019) also evaluate the model perplexity on held-out native sequences.###…design GVP-GNN achieves state-of-the-art performance on CATH 4.2, representing a substantial improvement both in terms of perplexity and sequence recovery over Structured Trans-former (Ingraham et al., 2019), a GNN method which was trained using the same training and validation sets (Table 1).",impact-revealing,highlighting the significance of GVP-GNN's performance improvements in protein design
2960,5b67b45517c44aac1c86078b,e62ddf27659bc131968d2dcc3e2bd59de98c6917,"Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop.",5a260c3517c44a4ba8a2529f,Name Disambiguation in Anonymized Graphs using Network Embedding.,"However, by incorporating both global supervision and local linkage structure, our method (AMiner) outperforms the baselines in terms of F1-score (+7.93% over Zhang et al., +34.96% over GHOST and +7.43% over Louppe et al. relatively).###[33]: This method constructs three local graphs for a candidate set based on coauthors and document similarity.###GHOST [5]: The second method is purely based on coauthor names.###Our experiments show that the proposed solution achieves significantly better performance than several state-of-the-art methods including GHOST [33], Zhang et al. [33], and Louppe et al. [17] (+7-35% in terms of F1-score).###GHOST directly partitions the coauthor graph by a nity propagation.###Our experiments show that the proposed solution achieves significantly better performance than several state-of-the-art methods including GHOST [33], Zhang et al.###We carefully evaluate the proposed framework on real-world large data and experimental results show that the proposed solution achieves clearly better performance (+7-35% in terms of F1-score) than several state-of-the-art methods including GHOST [5], Zhang et al. [33], and Louppe et al. [17].###Both GHOST and Zhang et al. leverage the structure of coauthor graphs.###Following the idea of linkage based disambiguation methods proposed in [1, 9, 12, 33], we construct a graph for each candidate set.###[33] solve this problem by learning graph embedding from three constructed graphs based on document similarity and coauthor relationship.###GHOST [5] builds document graph for each ambiguous name by co-authorship only.",other,highlighting the performance improvement of the proposed method over state-of-the-art methods
1640,,2d0bac4f8d74837faa7878e83b7ba8bb8ff4a0af,GREEN HRM AND GREEN COMPETITIVE ADVANTAGE IN HOTEL AND TOURISM INDUSTRY: A MEDIATED MODERATION MODEL USING ECO-INNOVATION AND GREEN PSYCHOLOGICAL CLIMATE,,,"###…Eco-innovation, and green competitive advantage Social identity theory suggests that individuals develop a positive self-concept by classifying themselves into groups and identifying as members of that group, reinforced by positive images and shared perceptions and actions (Tajfel et al., 1979).",impact-revealing,providing context on social identity theory
3023,5d0616bd8607575390f86730,635b079447ee46acc813a08305988e77dfe3d1dd,Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi,599c7eef601a182cd28d9bf2,Position and Orientation Agnostic Gesture Recognition Using WiFi.,"Tempts to adapt recognition schemes in various domains fall into two categories: virtually generating features for target domains [39, 40, 50, 53] and developing domain-independent features [9, 20, 37].###In the former type, WiAG [39] derives translation functions between CSIs from different domains, and generates virtual training data accordingly.###Due to complexity of human activity, existing approaches extract signal features, either statistical [14, 15, 23, 28, 30, 45, 49] or physical [6, 31, 34, 38, 39, 44, 51, 52] ones, and map them to discrete activities.###Another solution, WiAG [39], derives a translation function to generate signal features of the target domain for model re-training.",other,acknowledge existing approaches in recognition schemes
108,5cf48a34da56291d58296d51,3c64b7a74c749d43b1a4b96dd1a00620ba613ee0,Representation Learning for Attributed Multiplex Heterogeneous Network,5b67b46417c44aac1c86132c,Scalable Multiplex Network Embedding.,"The compared methods include PMNE [22], MVE [30], MNE [43].###MNE [43] uses one common embedding and several additional embeddings of each edge type for each node, which are jointly learned by a unified network embedding model.###We chooseMNE [43], a recent representative work for MHEN, as the base model for multiplex heterogeneous networks to discuss the connection between our proposed model and previous work.###Single Multi / MVE [30] MNE [43] mvn2vec [32] GATNE-T Multi Multi Attributed MHEN (AMHEN) GATNE-I Multi Multi Attributed###, MNE [43]).",impact-revealing,reporting methods compared in the study
3351,5a260bfb17c44a4ba8a1c61e,a55970013b984f344dfbbbba677d89dce0ba5f81,Image Super-Resolution via Deep Recursive Residual Network,573697846e3b12023e66ac4b,Self-tuned deep super resolution,"The deep models (d ≤ 8) include SRCNN [2], DJSR [33], CSCN [32], ESPCN [25] and FSRCNN [3].###To address this issue, the Deep Joint Super Resolution (DJSR) jointly utilizes both the wealth of external examples and the power of self examples unique to the input.",other,reporting on deep models for super resolution
3165,5e68b99493d709897cd373ed,d08b35243edc5be07387a9ed218070b31e502901,group normalization,5550415945ce0a409eb3a820,ImageNet Large Scale Visual Recognition Challenge,"The effectiveness of GN in ImageNet, COCO, and Kinetics demonstrates that GN is a competitive alternative to BN that has been dominant in these tasks.###We note that the pre-trained GN model is slightly worse than BN in ImageNet (24.1% vs .###With a batch size of 2 samples, GN has 10.6% lower error than its BN counterpart for ResNet-50 [20] in ImageNet [50].###We experiment in the ImageNet classiﬁcation dataset [50] with 1000 classes.###%) in ImageNet.###We replace BN * with GN during ﬁne-tuning, using the corresponding models pre-trained from ImageNet.###5 To our knowledge, our numbers ( 41.0 box AP and 36.4 mask AP) are the best from-scratch results in COCO reported to date; they can even compete with the ImageNet-pretrained results in Table 6.###We experiment in the ImageNet classification dataset [50] with 1000 classes.###6% lower error than its BN counterpart for ResNet-50 [20] in ImageNet [50].###The models are pre-trained from ImageNet.",other,highlighting the competitive performance of GN compared to BN in various tasks
1495,,04a0f3f39c5a6aeddcd2841457d0b500adfcb534,Tell2Design: A Dataset for Language-Guided Floor Plan Generation,,,"###Recently, text-conditional generative AI models (Nichol et al., 2022; Saharia et al., 2022b; Ramesh et al., 2022; Dhariwal and Nichol, 2021; Ho et al., 2022) have demonstrated impressive results in generating high-fidelity images.###…2021; Sa-haria et al., 2022c; Dhariwal and Nichol, 2021; Ho et al., 2022; Saharia et al., 2022a; Rombach et al., 2022; Nichol et al., 2022; Saharia et al., 2022b; Ramesh et al., 2022) and received wide success in image generation, outperforming other approaches in fidelity and diversity,…###More recently, some works have applied diffusion models (Ho et al., 2020; Nichol and Dhariwal, 2021; Saharia et al., 2022c; Dhariwal and Nichol, 2021; Ho et al., 2022; Saharia et al., 2022a; Rombach et al., 2022; Nichol et al., 2022; Saharia et al., 2022b; Ramesh et al., 2022) and received wide success in image generation, outperforming other approaches in fidelity and diversity, without training instability and mode collapse issues (Brock et al.###…works have applied diffusion models (Ho et al., 2020; Nichol and Dhariwal, 2021; Sa-haria et al., 2022c; Dhariwal and Nichol, 2021; Ho et al., 2022; Saharia et al., 2022a; Rombach et al., 2022; Nichol et al., 2022; Saharia et al., 2022b; Ramesh et al., 2022) and received wide success in image…###• Imagen (Saharia et al., 2022b) is one of the state-of-the-art text-to-image generation models that build upon both large language models (e.g., T5) for text understanding and diffusion models for high-fidelity image generation.###Imagen (Saharia et al., 2022b) exhibits its strong ability to generate realistic images in the target domain.",impact-revealing,highlighting the impressive results and advancements in text-conditional generative AI models for image generation
3136,5bdc315017c44a1f58a05e7e,7a71941e60894ae7e1f5af8e79c37cec6cd6c6ad,Multi-scale Residual Network for Image Super-Resolution,573695fd6e3b12023e510ff5,Deeply-Recursive Convolutional Network for Image Super-Resolution,"3 Comparisons with State-of-the-art Methods We compare our model with 10 state-of-the-art SR methods, including Bicubic, A+ [23], SelfExSR [20], SRCNN [1], ESPCN [2], FSRCNN [3], VDSR [4], DRCN [5], LapSRN [6] and EDSR [9].###After that, many SR models have been proposed, including DRCN [5], DRNN [7], LapSRN [6], SRResNet [8], and EDSR [9].###We compare our model with 10 state-of-the-art SR methods, including Bicubic, A+ [23], SelfExSR [20], SRCNN [1], ESPCN [2], FSRCNN [3], VDSR [4], DRCN [5], LapSRN [6] and EDSR [9].",other,comparing model performance with state-of-the-art methods
971,,efe77d95559ee3fdb054d20cb17e986e95e2250f,Why Act Morally? Economic and Philosophical Reasons,,,"###Therefore in the I-treatment, player A will choose 6−=a because he only loses if he chooses 0>a and has nothing to fear if 0 a .###In the Intention-treatment (I-treatment) A himself determines a , whereas in the Nointention treatment (NI-treatment) A’s move was determined by a random device in which the probabilities resembled a “human choice distribution”.###Let us discuss in more detail a simple example taken from Falk et al. (2000) and Falk (2003) which characterizes some features of the Homo Reciprocans.###In the NI-treatment b also increases with a but less than in the I-treatment (Falk et al. 2000, p. 10).###If all players are rational and selfish, the Homo Oeconomicus prediction would be as follows (Falk et al. 2000, p. 9): “In both treatments B will always choose 0=b , i.e., she will neither punish nor reward, because any other choice is costly.###…game” (Abbink et al. 2000), shows that behaviour is driven by the perceived fairness (or unfair-
ness) of intentions and outcomes and can be described as follows (Falk et al. 2000, p. 6; see also Figure 2):
“The ‘moonlighting game’ is a two-player sequential move game that consists of two stages.###In the NI-treatment, player A’s move is determined by a random device.”###However, if behaviour is motivated by people’s desire to be fair or if they dislike unfair intentions and/or outcomes, then the prediction would be that in the I-treatment b increases with a .",impact-revealing,providing context for experimental treatments in a game theory study
566,58437722ac44360f1082f160,8aa3358a34a17abd0a65622aad8c85317b851af4,very deep convolutional networks for end-to-end speech recognition,573696126e3b12023e5246d6,End-to-End Attention-based Large Vocabulary Speech Recognition,"Recently, very deep CNNs architectures [15] have also been shown to be successful in ASR [16, 17], using more non-linearities, but fewer parameters.###Our baseline model follows [4] using the skip connection technique in its time reduction.###As a result, a single end-to-end model can jointly accomplish the ASR task within one single large neural network.###In this subsection, we extend on Section 3.2 and describe experiments in which we build deeper encoders by stacking convolutional layers and residual blocks in the acoustic encoder before the BLSTM. Unlike computer vision applications or truncated BPTT training in ASR, seq2seq models need to handle very long utterances (i.e., > 2000 frames).###We experimented with the Wall Street Journal (WSJ) ASR task.###The foundational work on seq2seq models, however, has relied on simple neural network encoder and decoder models using recurrent models with LSTMs [6] or GRUs [4].###The sequence-to-sequence (seq2seq) model with attention [1] has recently demonstrated a promising new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR [2, 3, 4, 5, 6].###CNNs have shown improvement over traditional fully-connected deep neural networks on many ASR tasks [14, 12], we investigate the effect of convolutional layers in seq2seq models.###While very deep CNNs have been successfully applied to ASR, recently there have been several advancements in the computer vision community on very deep CNNs [15, 18] that have not been 1.###The foundational work on seq2seq models, however, has relied on simple neural network encoder and decoder models using recurrent models with LSTMs [4, 6] or GRUs [4].###5% absolute improvement over published best result [4].###On the WSJ ASR task, we obtained 10 .###In this paper, we use very deep CNN techniques to significantly improve over previous shallow seq2seq speech recognition models [4].###Convolutional Neural Networks (CNNs) [9] have been successfully applied to many ASR tasks [10, 11, 12].",impact-revealing,highlighting advancements in ASR using deep CNN architectures
1907,,315156150e70822271ba1e20ea117b77b438665b,Collaborative mind mapping to support online discussion in teacher education,,,"###…construction, a lot of research has highlighted the importance of online discussions in facilitating learning as a result of student interaction (e.g. Garrison et al., 1999; Henri, 1992; Means et al., 2014), and many have proposed models that evidence learning and knowledge construction processes.###Still, research studies have consistently shown that students rarely engage in critical discourse or higher stages of communicative processes through online threaded forums (Anderson, 2018; Fahy, 2005; Garrison et al., 1999; Lucas et al., 2014).###Since its publication, the article Critical inquiry in a text-based environment: Computer conferencing in higher education (Garrison et al., 1999) has generated significant interest amid online learning researchers, and the CoI as a framework has been used in many publications (See…###Second, the cognitive presence is “the extent to which learners are able to construct and confirm meaning through sustained reflection and discourse” (Garrison et al., 1999, p. 11), and it is shaped by the four phases of inquiry: triggering event, exploration,
45
integration, and resolution.###The cognitive presence is “the extent to which learners are able to construct and confirm meaning through sustained reflection and discourse” (Garrison et al., 1999, p. 11), and it is only achievable through communicative relationships.###Since its publication, the article Critical inquiry in a text-based environment: Computer conferencing in higher education (Garrison et al., 1999) has generated significant interest amid online learning researchers, and the CoI as a framework has been used in many publications (See https://coi.",impact-revealing,highlighting the significance of online discussions in learning and the challenges in student engagement
2319,58d82fced649053542fd692f,29e944711a354c396fad71936f536e83025b6ce0,categorical reparameterization with gumbel-softmax,573696ce6e3b12023e5ceb9b,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.",", 2014), image regions (Xu et al., 2015), and memory locations (Graves et al.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
339,5c8bd2e54895d9cbc6af9826,d14afc470cd90521147130e153c0d3e1324cd104,Learning Language Representations for Typology Prediction,59ae3c3a2bbe271c4c71fc83,"URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors.","We calculate these feature vectors using an NMT model trained on 1017 languages, and use them for typlogy prediction both on their own and in composite with feature vectors from previous work based on the genetic and geographic distance between languages (Littell et al., 2017).###ained on 1017 languages, and use them for typlogy prediction both on their own and in composite with feature vectors from previous work based on the genetic and geographic distance between languages (Littell et al., 2017). Results show that the extracted representations do in fact allow us to learn about the typology of languages, with particular gains for syntactic features like word order and the presence of case ma###ume III and Campbell,´ 2007; Takamura et al., 2016; Coke et al., 2016). As an alternative that does not necessarily require pre-existing knowledge of the typological features in the language at hand, Littell et al. (2017) have proposed a method for inferring typological features directly from the language’s k nearest neighbors (k-NN) according to geodesic distance (distance on the Earth’s surface) and genetic distance### mismatch of needs has motivated various proposals to reconstruct missing entries, in WALS and other databases, from known entries (Daume III and´ Campbell, 2007; Daume III, 2009; Coke et al.,´ 2016; Littell et al., 2017). In this study, we examine whether we can 2For example, each chapter of WALS aims to provide a statistically balanced set of languages over language families and geographical areas, and so many langu###As an alternative that does not necessarily require pre-existing knowledge of the typological features in the language at hand, Littell et al. (2017) have proposed a method for inferring typological features directly from the language’s k nearest neighbors ( k -NN) according to geodesic distance…###Typology Database: To perform our analysis, we use the URIEL language typology database (Littell et al., 2017), which is a collection of binary features extracted from multiple typological, phylogenetic, and geographical databases such as WALS (World Atlas of Language Structures) (Collins and…###This mismatch of needs has motivated various proposals to reconstruct missing entries, in WALS and other databases, from known entries (Daum´e III and Campbell, 2007; Daum´e III, 2009; Coke et al., 2016; Littell et al., 2017).",impact-revealing,reporting on the use of feature vectors for typology prediction
1676,,77356676c68ad703ee853055b372dc5e2cf19d91,Flying with Nicole Kidman or Jennifer Aniston? Brand funnel stages’ influence on brand personality,,,"###The idea is inspired by social identification theory (Tajfel & Turner, 1979).",impact-revealing,providing context for the theoretical inspiration
1588,,1a27c1d8f4ff0865daea85a7d554717cb0e0cf90,Lion Optimization Algorithm (LOA): A nature-inspired metaheuristic algorithm,,,###Dolphin Partner Optimization [39] and Dolphin echolocation algorithm [40] were inspired by dolphins' behaviors.,impact-revealing,highlighting the inspiration drawn from dolphin behaviors for optimization algorithms
3892,5da1a6d447c8f7664606888c,404d82a8da658f16c46714442155490d79a413f1,temporal prefetching without the off-chip metadata,53e999cbb7602d9702212df6,Efficient Representations And Abstractions For Quantifying And Exploiting Data Reference Locality,"find that tables are not ideal for organizing off-chip metadata because temporal streams can have highly variable lengths [9, 45].",other,highlighting limitations of tables for organizing off-chip metadata
3275,5f06e5e591e0117f54657c19,f6d32ed0eee5fb3f6ac518f3aebc8ceff2aae397,NVAE: A Deep Hierarchical Variational Autoencoder,58d82fc8d649053542fd5c3d,On the Quantitative Analysis of Decoder-Based Generative Models,"[32] observe that the marginal loglikelihood, estimated by non-encoder-based methods, is not sensitive to the encoder overfitting (see also Fig.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2865,599c7945601a182cd262a009,6727f574ad8b1c3763be8d58eeaf82c551aa33ef,Generative and Discriminative Text Classification with Recurrent Neural Networks,5c873b4d4895d9cbc6f504ad,Overcoming Catastrophic Forgetting In Neural Networks,"Discriminative models are known to suffer from catastrophic forgetting when learning sequentially from examples from a single class at a time, and specialized techniques are actively being developed to minimize this problem (Rusu et al., 2016; Kirkpatrick et al., 2017; Fernando et al., 2017).",other,acknowledging challenges in discriminative models and ongoing solutions
1978,,962533faed417ab3c9dcec58579f4cb922333870,"Vertex Nomination, Consistent Estimation, and Adversarial Modification",,,"###We note here that this adversarial contamination model is similar to the contamination model considered in [8].###The adversarial framework we consider is similar to the model considered in [8], and it is motivated by the example in the previous section in which the addition of the vertices without correspondences to G2 negatively impacted VN performance.###There has been significant recent attention towards better understanding the impact of adversarial attacks on machine learning methodologies (see, for example, [24, 8, 36, 15, 50]).",impact-revealing,highlighting the significance of understanding adversarial attacks on machine learning methodologies
1474,,82afe0e464f667fd8657c881bbfa6fca9417853f,Soil Moisture Prediction Using NDVI and NSMI Satellite Data: ViT-Based Models and ConvLSTM-Based Model,,,"###Finally, as Exponential Linear Unit [38] is widely used for satellite data, we have evaluated the model performance by adding it after each convolutional layer instead of ReLU (Rectified Linear Unit).",impact-revealing,highlighting the use of Exponential Linear Unit in model evaluation
3973,5c7572b7f56def97988385ce,c342c71cb23199f112d0bc644fcce56a7306bf94,active learning for convolutional neural networks: a core-set approach,53e9aa95b7602d9703411c82,Using Document Summarization Techniques for Speech Data Subset Selection.,"The most similar algorithm to ours is the unsupervised subset selection algorithm in (Wei et al., 2013).",other,comparing algorithms for subset selection
218,5eb789d3da5629cf24430b41,1739466ac1411788cf1de60a3a6b59d739dc41ff,Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder,5c2c7a9217c44a4e7cf318b1,Feature Denoising for Improving Adversarial Robustness,"We will compare the performance between FPD-enhanced CNN and the CNN enhanced by [29] in Section 4.###Comparison with the Related Work As mentioned in Section 2, the denoising approach proposed in [29] is similar to our denoising layers in FPD.###In the meantime, little attention has been given to the direct design of robust frameworks in an attack-agnostic manner, except a few touches on denoising [25, 29] and obfuscating gradients [11, 25] that aim to directly enhance a target network in order to cope with any potential attacks.###Moreover, we compare with the most related work [29] as well.###In Table 1, X represents the enhanced CNN by [29].###Nevertheless, different from [29], the principle behind our FPD is to improve the intrinsic robustness, regardless of conducting adversarial training or not.###Therefore, we conduct a comparison experiment with [29] as well.###Our proposal is partially related to [29], as the denoising layers in our FPD are inspired by their feature denoising approach.###Compared with the Gaussian filtering operator, the dot product operator helps improve the adversarial robustness [29].",impact-revealing,comparing performance and methodologies with related work
4034,5e15adca3a55ac47ab5b0729,97ebd482a78e6e6c1ba51da5e1b2f8e7640cc8b5,hyperbolic graph convolutional neural networks,5ac1829d17c44a1fda917eb3,Link Prediction Based on Graph Neural Networks.,"We did not compare link prediction GNNs based on shallow embeddings such as [49] since they are not inductive.###Compared to previous compilations [49], our dataset has larger size (2,236 nodes).",other,highlighting dataset size and limitations of previous models
843,5aed148b17c44a4438154efb,2de0f24f91d86725a26a983776ed9f27c370fa4c,higher-order network representation learning,573697296e3b12023e61afcd,Efficient Graphlet Counting for Large Networks,"The termmotif is used generally and may refer to graphlets or orbits (graphlet automorphisms) [1, 6].",impact-revealing,providing context for the term motif
134,5a260c8117c44a4ba8a30f54,33998aff64ce51df8dee45989cdca4b6b1329ec4,Graph Attention Networks,599c7987601a182cd2648373,Attention Is All You Need.,"However, Vaswani et al. (2017) showed that not only self-attention can improve a method based on RNNs or convolutions, but also that it is sufﬁcient for constructing a powerful model obtaining state-of-the-art performance on the machine translation task.###…corresponding to them, to serve as the ﬁnal output features for every node (after potentially applying a nonlinearity, σ ): To stabilize the learning process of self-attention, we have found extending our mechanism to employ multi-head attention to be beneﬁcial, similarly to Vaswani et al. (2017).",impact-revealing,highlighting the significance of self-attention in improving model performance
3448,58d82fc8d649053542fd5862,2a94c84383ee3de5e6211d43d16e7de387f68878,Feature Pyramid Networks For Object Detection,573698016e3b12023e6da477,U-Net: Convolutional Networks for Biomedical Image Segmentation,"rarchy without combining features or scores. There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation. Ghiasi et al. [8] present a Laplacian pyramid presenta",other,acknowledge recent methods in feature association for various tasks
1988,,3a1e0554ae5c3d2f6fbb5df68fc8d3e09f16022a,A universal strategy to interpret DNA profiles that does not require a definition of low-copy-number.,,,"###Probabilistic determinations can be made using graphical models (or Bayes nets) [44–47] but these require the utilisation of prior probabilities, which is problematic for scientists to usewithin theUK courts [48,49].",impact-revealing,highlighting the challenges of using probabilistic models in legal contexts
2218,5e09caba3a55ac662f721afe,36ad06e6f9b39192e7668634eadd6fcf9593e922,Efficient Adversarial Training With Transferable Adversarial Examples,5b1642388fbcbf6e5a9b57aa,Learning to Attack: Adversarial Transformation Networks.,"The first metric is error rate transferability used in [1, 19], which is the ratio of the number of adversarial examples misclassified by source model to that of the targeted model.",other,providing context for a specific metric
3236,5c0495fa17c44a2c747059aa,172f096eecc0290442b35908fbef01d62e668e0a,RFUZZ: Coverage-Directed Fuzz Testing of RTL on FPGAs,53e9a6fdb7602d97030336a4,A Functional Validation Technique: Biased-Random Simulation Guided by Observability-Based Coverage,"work [2, 14, 15] uses functional coverage models manually specified by verification engineers, these test suites are expensive to create and thus generally unavailable to the broader research community.###[14] analyse the circuit in order to improve the biases for an existing random input generator.",other,highlighting the challenges of creating functional coverage models
1453,,c4d1af0c17c9330a7597b7b8330689dfb4bcc8c3,Research Paper: The Role of Domain Knowledge in Automating Medical Text Report Classification,,,"###We created report representations from the output of a natural language processor, specifically, the Medical
Language Extraction and Encoding system (MedLEE) developed by Friedman at Columbia University.2 MedLEE is a semantic parser that takes narrative text as input and uses a clinical vocabulary to map words and phrases to standard terms.###Medical text reports contain substantial and essential clinical data.(2,3) For example, a recent study distinguishing between planned and unplanned readmissions found that information available in structured, coded format alone was not sufficient for classifying admissions and that information in text reports significantly improved this task.###We created report representations from the output of a natural language processor, specifically, the Medical Language Extraction and Encoding system (MedLEE) developed by Friedman at Columbia University.(2) MedLEE is a semantic parser that takes narrative text as input and uses a clinical vocabulary to map words and phrases to standard terms.",impact-revealing,describing the functionality and significance of the MedLEE system in clinical data processing
521,5dcbd5da3a55ac789b0dbbdd,d6b414487787d0b6efd735a3236a690ad13aae70,TENER: Adapting Transformer Encoder for Named Entity Recognition,599c7987601a182cd2648373,Attention Is All You Need.,"For any ﬁxed offset k , P E t + k can be represented by a linear transformation of P E t (Vaswani et al., 2017).###Recently, Transformer (Vaswani et al., 2017) began to prevail in various NLP tasks, like machine translation (Vaswani et al., 2017), language modeling (Radford et al., 2018), and pretraining models (Devlin et al., 2018).###The output of the multi-head attention will be further processed by the position-wise feed-forward networks, which can be represented as follows, where Other components of the Transformer encoder includes layer normalization and Residual connection, we use them the same as (Vaswani et al., 2017).###Instead of using the sinusoidal position embedding (Vaswani et al., 2017) and learned absolute position embedding, Shaw et al.###For any fixed offset k, PEt+k can be represented by a linear transformation of PEt (Vaswani et al., 2017).###, 2017) began to prevail in various NLP tasks, like machine translation (Vaswani et al., 2017), language modeling (Radford et al.###In order to solve this problem, (Vaswani et al., 2017) suggested to use position embeddings generated by sinusoids of varying frequency.###We ﬁrst introduce the Transformer encoder proposed in (Vaswani et al., 2017).###Recently, Transformer (Vaswani et al., 2017) began to prevail in various NLP tasks, like machine translation (Vaswani et al.###Instead of using the sinusoidal position embedding (Vaswani et al., 2017) and learned absolute position embedding, Shaw et al. (2018) argued that the distance between two tokens should be considered when calculating their attention score.###We first introduce the Transformer encoder proposed in (Vaswani et al., 2017).###Other components of the Transformer encoder includes layer normalization and Residual connection, we use them the same as (Vaswani et al., 2017).###Since the self-attention mechanism used in the Transformer is unaware of positions, to avoid this shortage, position embeddings were used (Vaswani et al., 2017; Devlin et al., 2018).###Transformer was introduced by (Vaswani et al., 2017), which was mainly based on self-attention.",impact-revealing,providing context on the Transformer model and its applications
1660,,658fba7ef2341322f29acd1ee3ec2cc539ab5501,"Do leaders condone unethical
 pro‐organizational
 employee behaviors? The complex interplay between leader organizational identification and moral disengagement",,,"###The social identity approach further argues that this self-definition in terms of a group has important behavioral consequences (Tajfel & Turner, 1979; Turner, Hogg, Oakes, Reicher, & Wetherell, 1987).###While organizational research has largely focused on the effects of organizational identification on people's own behaviors, the social identity approach also provides important insights into perceptions of and responses to other people's behaviors (Tajfel & Turner, 1979; Turner et al., 1987).",impact-revealing,highlighting the significance of the social identity approach in understanding behavioral consequences
3954,5c2c7a9217c44a4e7cf317b4,61946177ea5d44ad18ff09de7929556d7b34cd8b,simd-x: programming and processing of graph algorithms on gpus.,5d0e098a8607575390005efe,XBFS: eXploring Runtime Optimizations for Breadth-First Search on GPUs,"Recent advance in graph computing falls in algorithm innovation [51, 87, 15], framework developments [49, 18, 45, 39, 42, 90, 92, 22, 66, 63, 61, 23, 54, 60, 74, 7, 80, 84, 65, 88, 75, 55, 89, 86, 85, 3, 78, 52, 21, 9, 81] and accel-",other,highlighting advancements in graph computing
1913,,84879a324c304e569c5f31605fa9ea36eefd3c93,"An Inquiry into Relationships between Demographic Factors and Teaching, Social, and Cognitive Presence",,,"###Social presence sets the climate of the learning environment and supports discourse about content between students (Garrison, Anderson, and Archer 2000).###Three main components, or presences, provide the structure of the CoI Framework: teaching, social, and cognitive (Garrison, Anderson, and Archer 2000; Swan et al. 2008).###Social presence is the degree to which participants in computer-mediated communication feel socially and emotionally connected (Garrison, Anderson, and Archer 2000, Swan et al. 2008).###Activities within the course, the architectural framework of the discussion, and flow of facilitation, as well as contact with students through direct instruction, focusing, and resolving issues, complete the presence (Garrison, Anderson, and Archer 2000).###Additionally, reflection of content and discourse with fellow students and faculty on subject matter further scaffold learning (Garrison, Anderson, and Archer 2000; Swan et al. 2008).###A product of the interaction between classmates, this presence builds on cognitive learning by encouraging discourse and critical thinking (Garrison, Anderson, and Archer 2000).",impact-revealing,providing context for the CoI Framework and its components
224,5cf48a3eda56291d582a1174,05c4eb154ad9512a69569c18d68bc4428ee8bb83,Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks,573695aa6e3b12023e4c879a,A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs,"sier to implement than the neighborhood search procedure used in previous SGD-based training methods. We use graph clustering algorithms to partition the graph. Graph clustering methods such as Metis [8] and Graclus [4] aim to construct the partitions over the vertices in the graph such that withinclusters links are much more than between-cluster links to better capture the clustering and community s###rtition than nodes in different partitions. Based on the graph clustering idea, we proposed Cluster-GCN, an algorithm to design the batches based on efficient graph clustering algorithms (e.g., METIS [8]). We take this idea further by proposing a stochastic multi-clustering framework to improve the convergence of Cluster-GCN. Our strategy leads to huge memory and computational benefits. In terms of m",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1785,,ec7d83f22e389c5719dfa03aa9ce31d45e943988,Too Many Cooks: Exploring How Graphical Perception Studies Influence Visualization Recommendations in Draco,,,"###For example, Wongsuphasawat et al. [43, 44] use Mackinlay’s expressiveness and effectiveness principles [20] to produce recommendations.###For example, CompassQL [42] and Voyager [43, 44] build on Mackinlay’s APT rules [20] to refine recommendations.###Unlike machine-learning recommendation algorithms which require extensive human-labeled corpora to train models [13, 18, 24], rule-based algorithms use existing theoretical guidelines [21, 43, 44] or propose new ranking metrics [6, 15, 38] to compare candidate visualizations.",impact-revealing,highlighting the differences between rule-based and machine-learning recommendation algorithms
3269,5f1022a091e01168a7d6fc4f,04e234c7e4f300b4e8a16370728d875f9f484b39,learning visual context by comparison,5b67b4b417c44aac1c866e25,ChestNet: A Deep Neural Network for Classification of Thoracic Diseases on Chest Radiography.,"Other works use attention module to indirectly align the class-level prediction with the potentially abnormal location [35,32,10] without the text reports on the location of the disease.###[35] introduces an attention mechanism to focus on regions of diseases.###1 Wang and Xia [35] ResNet-151 CE 224 78.",other,acknowledge variations in existing methods
1550,,7cfaf639d6e5b3e5638cfc47193bcae828a44e24,"When do international human capital enhancing practices benefit the bottom line? An ability, motivation, and opportunity perspective",,,"###This configuration asserts that all three dimensions must be present (i.e., none can ensure performance in isolation), as well as that low values on any of the dimensions would result in markedly lower outcomes (Blumberg & Pringle, 1982).###As the above arguments suggest, firms are expected to benefit from the presence of individual employees
Journal of International Business Studies
who possess IHC arising from these three types of work and life experiences.###Scholars have suggested multiple configurations of these factors, such as the additive model, the combination model, and the multiplicative model (Blumberg & Pringle, 1982; Bos-Nehles, van Riemsdijk, & Looise, 2013; Cummings & Schwab, 1973).###In order to do so, we use the ability–motivation–opportunity (AMO) framework (Blumberg & Pringle, 1982) as our main theoretical lens.###Eight items were identified by at least
Journal of International Business Studies
two of the three authors for an agreement rate of 96.2%.###While others have examined the importance of
Journal of International Business Studies
foreign nationality and education at the TMT level (Nielsen & Nielsen, 2011; Piaskowska & Trojanowski, 2014), our results show that they add value for other employees as well.###Journal of International Business Studies###Firms gain when their employees develop global competence through their IHC, but employees also
Journal of International Business Studies
enhance their general as well as firm-specific human capital through an improved understanding of international and cross-cultural contexts.###Firms can
Journal of International Business Studies
benefit from motivating employees through a collaborative climate, to transfer and share their international expertise; which can be achieved through effective human resource management (HRM) practices (Cabrera & Cabrera, 2005; Caligiuri, 2014; Lado & Wilson, 1994; Minbaeva et al., 2003).###The survey consisted of two parts, one that was carried out at the organizational level, and one that was administered
Journal of International Business Studies
at the individual employee level.###Drawing upon the AMO framework (Blumberg & Pringle, 1982), we posited that in addition to investing in employees’ IHC, firms must maintain a collaborative climate that motivates employees to share knowledge, and a level of internationalization that enables them to apply their expertise.###The extent to which knowledge and work practices spread out quickly throughout the firm via the
information system Individual employees
The extent to which employees freely suggest any ideas/opinions to their senior employees Individual employees The extent to which a cooperative and trust-based relationship is built within teams Individual employees The extent to which communication within teams goes well Individual employees The extent to which employees actively participate in problem-solving and decision-making Individual employees The extent to which human resources development activities improve employees’ morale Human resources manager The percentage of the team-based incentives in the monthly salary Human resources manager The percentage of the department-based incentives in the monthly salary Human resources
manager
Journal of International Business Studies###For this measure, strategic planning managers in each organization rated the degree
Journal of International Business Studies
to which the following changed during the past 3 years: (1) demands for the organization’s major products, (2) development and introduction of new products, and (3) the organization’s branches and line facilities.###Theoretical Implications Firms invest substantial resources into developing, maintaining and adding to their employees’ IHC,
Journal of International Business Studies
and a large number of scholars, particularly in the expatriation literature, have pointed to the potential advantages of such international expertise (e.g., Bird et al., 2010; Caligiuri, 2006; Furuya et al., 2009; Johnson et al., 2006; Levy et al., 2007).###…Opportunity “consists of the particular configuration of the field of forces surrounding a person and his or
Journal of International Business Studies
her task that enables or constrains that person’s task performance and that are beyond the person’s direct control” (Blumberg & Pringle, 1982: 565).###The multiplicative model represents the traditional view that ability, motivation and opportunity operate in a complementary, or interactive manner (i.e., performance depends on a model of the type P= f (A×M×O); Blumberg & Pringle, 1982; Reinholt, Pedersen, & Foss, 2011).###Nevertheless, future studies should apply a
Journal of International Business Studies
longitudinal research design to better address the emergence of firm-level IHC.###Empirical tests of the relationship between firm-level human capital and firm performance have often yielded conflicting results (Crook et al., 2011; Newbert, 2007; Nyberg et al., 2014), suggesting that research on unit-level human capital should be conducted with greater precision (Crook et al., 2011;
Journal of International Business Studies
Nyberg et al., 2014).###Firm level of internationalization as opportunity Opportunity “consists of the particular configuration of the field of forces surrounding a person and his or
Journal of International Business Studies
her task that enables or constrains that person’s task performance and that are beyond the person’s direct control” (Blumberg & Pringle, 1982: 565).###Journal of International Business Studies
Blumberg and Pringle (1982) point out that the multiplicative model also indicates a reciprocal relationship between entities (individuals or firms), their behavior, and the environment.###Classic theories of work performance (Blumberg & Pringle, 1982; Cummings & Schwab, 1973) assert that optimal performance arises from a combination of ability (A), motivation (M), and opportunity (O).###It is also crucial for the firm to put in place measures tomotivate employees to share and use their competencies to the firm’s
Journal of International Business Studies
advantage (Minbaeva, Pedersen, Björkman, Fey, & Park, 2003), as well as to focus on providing the opportunity for them to apply relevant knowledge in the international context, through a high enough level of internationalization.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2919,57d063f6ac44367354296741,908f7931de8768786d9ef7d64f5a8156860709dd,Dynamic Pricing and Traffic Engineering for Timely Inter-Datacenter Transfers,53e9ae69b7602d9703886b6c,Incentive Compatibility And Dynamics Of Congestion Control,"Incentive issues in routing have also been explored on the supply side, where individual routing nodes may act strategically [13, 15].",other,acknowledge prior work on incentive issues in routing
421,53e9bb36b7602d970477616b,d2cbad37a383b894cc0c0f904c76b06a73dbb738,a survey of flash translation layer,53e99dabb7602d970266b72e,Algorithms and data structures for flash memories,Toledo [ 7 ] also provided algorithms and data structures for flash memory systems.,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
185,5f0bde8e9e795ea206ff8ef5,0feea94f89d395436bf41bd10c797447eecbc128,Unsupervised data augmentation for consistency training,5bbacb9e17c44aecc4eaff70,Understanding Back-Translation at Scale,"When used as an augmentation method, backtranslation [16, 17] refers to the procedure of translating an existing example x in language A into another language B and then translating it back into A to obtain an augmented example x̂.",impact-revealing,providing context on backtranslation as an augmentation method
3690,5c3ff3ecdf5b8c0b3cd013d4,8e68fdceb150d1c93ec9923ffcf766454201fbd3,Advanced recurrent network-based hybrid acoustic models for low resource speech recognition,599c7c01601a182cd277d718,Gated recurrent units based hybrid acoustic models for robust speech recognition,"In this section, we extend conventional LSTM and investigate in more depth the gated recurrent unit (GRU) element introduced in our previous work [18].",other,extending previous work on LSTM and GRU
2449,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e99c8bb7602d970253d070,On the huge benefit of quasi-random mutations for multimodal optimization with application to grid-based tuning of neurocontrollers,"for their Go program MOGO [55], and neural networks have been used to tune the parameters of MOGO [57], using information about the current search as input.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3092,5ea2b8c391e01167f5a89e2d,38643c2926b10f6f74f122a7037e2cd20d77c0f1,Supervised Contrastive Learning,573696f46e3b12023e5f160a,Unsupervised Visual Representation Learning by Context Prediction,"In the image domain, predictive approaches have also been used to learn embeddings [13, 57, 58, 35]: the typical setup is that some part of the signal is left out and we try to predict that portion from other parts of the signal.",other,acknowledge predictive approaches in image domain
949,,7711291a0c8f7ec3e69f58880c42ea730db18ae6,District-wide outcomes from a bullying prevention programming,,,"###Although bystanders’ efforts to terminate bullying are usually successful, bystanders often reinforce bullying behaviors with increased respect, attention, and encouragement of the bullying perpetrator (Salmivalli et  al., 2011).###Literature suggests that classmates play key roles either impeding or encouraging bullying behaviors as bullying is frequently reinforced by peer attention (Salmivalli et  al., 2011).###Applied research on the reinforcement of bullying behaviors has primary narrowed in on the reinforcement provided by peers to bullying perpetrators, and the support provided by peers to bullying recipients (O’Connell et  al., 1999; Salmivalli et  al., 2011).",impact-revealing,highlighting the role of bystanders in bullying dynamics
170,5cf48a2cda56291d5828e868,c42816f497d663c681df20d48a6e66a5632600d8,Mixmatch: A holistic approach to semi-supervised learning,57a4e91dac44365e35c98218,Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning.,"As baselines, we consider the four methods considered in [35] (Π-Model [25, 40], Mean Teacher [44], Virtual Adversarial Training [31], and Pseudo-Label [28]) which are described in section 2.###Using data augmentation to obtain an artificial target for an unlabeled example is common in consistency regularization methods [25, 40, 44].###In the simplest case, for unlabeled points x, prior work [25, 40] adds the loss term ‖pmodel(y | Augment(x); θ)− pmodel(y | Augment(x); θ)‖ 2 2.",impact-revealing,reporting baseline methods used in the study
1682,,1d63d28806159c5f604073333e71ea91a8b9e2ee,Logics of Common Ground,,,"###The first three logics build on existing informal descriptions of common ground, namely on (1) Stalnaker’s (2002) straightforward characterisation of common ground as common belief; (2) Stalnaker’s more general definition of common ground as collective acceptance and Tuomela’s (2003) account of collective acceptance; and (3) Kashima et al.###This is in line with the broader ideas of social identity theory (Tajfel & Turner, 1979) and self-categorisation theory (Turner, 1982), according to which different social identities are salient during any social interaction.",impact-revealing,providing context on the foundations of common ground in social interaction
892,5e6cae3493d709897ccff2f9,804f23ac1a4a56b8dc5bb7201dab7b8cece76a70,classifying memory access patterns for prefetching,53e99bb1b7602d97024584db,A Self-Repairing Prefetcher in an Event-Driven Dynamic Optimization Framework,"Zhang [53] performs dynamic prefetch optimization based on profiling, however, this work requires an extra thread while running the main application.",impact-revealing,reporting prior findings on dynamic prefetch optimization
183,5cede0f2da562983788d0d44,aecddd82840323e5bd43f9c73a32fed88ee93c8c,An Effective Approach To Unsupervised Machine Translation,5c89f3ae4895d9cbc606aa32,Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks,"Inspired by the previous work on CycleGANs (Zhu et al., 2017) and dual learning (He et al., 2016), our method takes two initial models in opposite directions, and defines an unsupervised optimization objective that combines a cyclic consistency loss and a language model loss over the two…###(Zhu et al., 2017) and dual learning (He et al.###Inspired by the previous work on CycleGANs (Zhu et al., 2017) and dual learning (He et al., 2016), our method takes two initial models in opposite directions, and defines an unsupervised optimization objective that combines a cyclic consistency loss and a language model loss over the two monolingual corpora E and F :
L = Lcycle(E) + Lcycle(F ) + Llm(E) + Llm(F )
The cyclic consistency loss captures the intuition that the translation of a translation should be close to the original text.",impact-revealing,drawing inspiration from previous work on CycleGANs and dual learning
3090,5da2f8a647c8f76646083cd9,b789abc47b7a92596050f6055a93c8fe1929db2a,Dynamic Multicontext Segmentation of Remote Sensing Images Based on Convolutional Networks,55d06634696322190568b85f,Deep learning,", from image pixels to semantic labels) is the great advantage of deep learning when compared to previous state-of-the-art methods [12], such as low-level [13]–[15] and midlevel (e.",other,highlighting the advantages of deep learning over previous methods
3360,59ec02da0cf22f5df7319dc3,c27db32efa8137cbf654902f8f728f338e55cd1c,mastering the game of go without human knowledge,53e9a0d1b7602d97029ba7b7,Empirical Comparison Of Various Reinforcement Learning Strategies For Sequential Targeted Marketing,"However, very similar algorithms have subsequently proven highly effective in video games 6–8,10 , robotics 60 , industrial control 61–63 and online recommendation systems 64,65 .",other,highlighting the broad applicability of similar algorithms across various domains
2076,,fbc8720e903ba88a99f4b56ad872bef4d9c69ea3,Timing Analysis of Safety-Critical Automotive Software: The AUTOSAFE Tool Flow,,,"###The tool chronVAL is based on real-time calculus [7], a method for schedulability analysis for distributed systems.###Real-Time Calculus: The concept of RTC was firstly proposed by Thiele [7], and was later extended by approximative methods, as described in [8] and [9].",impact-revealing,providing context and background on the chronVAL tool and its foundational method
2105,,6d2e2ef9f3b742234e3fba585b743dd3aebcc0f4,SKAIT: A Parameterized Key Assignment Scheme for Wireless Networks,,,"###In the face of such constraints, symmetric key pre-distribution has shown itself to be an appealing solution, and several schemes have been proposed [5], [7]– [15] to distribute these keys to nodes in the network.###build upon that scheme by using deployment knowledge to increase the probability that neighboring nodes will share a key in common [7].",impact-revealing,highlighting the appeal and development of symmetric key pre-distribution schemes
1143,,076d4a8e4ee5c68d81c3d7fedd10552dc185b063,DiffusionAD: Norm-guided One-step Denoising Diffusion for Anomaly Detection,,,"###However, as a class of likelihood-based models, diffusion models [20, 44] generally require a large number of denoising iterations (typically about 50 to 1000 steps) to obtain optimal reconstructions from randomly sampled Gaussian noise, which is much slower than the real-time requirements in…###A family of generative models called denoising diffusion models [20, 35, 43, 44] are inspired by equilibrium thermodynamics [46, 47].###Diffusion models [20, 44, 47], a class of generative models inspired by non-equilibrium thermodynamics [43], define a paradigm in which the forward process slowly adds random noise to the data, and the reverse constructs the desired data samples from the noise.",impact-revealing,highlighting the characteristics and challenges of diffusion models
4014,53e99967b7602d97021ac42b,41721de035c15528a7e35d3ab4d79b053633d763,Feedback-directed memory disambiguation through store distance analysis,5390981d20f70186a0e0696f,Reuse-distance-based miss-rate prediction on a per instruction basis,"A constant store distance allows store distance analysis to be applied to a single training run, rather than the multiple training runs used in previous memory distance analysis [7, 8, 13, 29].###[8, 9] introduce the notion of memory distance to encompass reuse distance, access distance and value distance.###Both whole-program [7, 28] and instructionbased [8, 9, 13] reuse distances have been predicted accurately across all program inputs using a few profiling runs.",other,highlighting advancements in memory distance analysis
827,5eabf3cd91e011664efc496f,916595fc4e701ac6125725408912a1fac3d7a60b,CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows,5c9df4643cb210d271bea0de,Freeway: Maximizing MLP for Slice-Out-of-Order Execution,"However, the various shapes and sizes of dependence chains could restrict their ability to exploit ILP [16], [17].###To address this limitation, Free-way [16] introduces a dependence-aware slice scheduling policy.###Instructions in a performance-critical slice are scheduled in order but independently from the rest of the application using multiple parallel IQs [15], [16].###To address this issue, another work has proposed slice-based MLP exploitation techniques built upon an energy-efﬁcient stall-on-use InO core [15], [16].###However, the slice-based approaches could experience a severe slowdown because the various shapes and sizes of dependence chains may impede the exploitation of ILP and MLP in such parallel InO scheduling windows [16], [17].###A. Performance 1) Comparison to InO and OoO: Figure 6 shows the performance of Load Slice Core (LSC) [15], Freeway core [16], CASINO core, and OoO core, normalized to that of an InO core.###A simple way to address this issue is to wait for all previous stores to resolve the target addresses and then issue the following loads, thereby eliminating the need for associative LQ searches [15], [16].",impact-revealing,highlighting limitations in dependence chain exploitation and proposing solutions
676,5f576c1591e011f4c3d5dd7e,c36571ac50808c75fa8a5d37f1041af22e89e6ee,GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training,57a4e921ac44365e35c98ea1,Instance Normalization: The Missing Ingredient for Fast Stylization.,", 2020); Instance normalization (InstanceNorm) has been found effective for style transfer tasks (Ulyanov et al., 2016) .###…is a standard component in computer vision (Ioffe & Szegedy, 2015); Layer normalization (LayerNorm) is popular in natural language processing (Ba et al., 2016; Xiong et al., 2020); Instance normalization (InstanceNorm) has been found effective for style transfer tasks (Ulyanov et al., 2016) .###…methods shift and scale the hidden representations and are shown to help the optimization for deep neural networks (Ioffe & Szegedy, 2015; Ulyanov et al., 2016; Ba et al., 2016; Salimans & Kingma, 2016; Xiong et al., 2020; Salimans et al., 2016; Miyato et al., 2018; Wu & He, 2018;…###Closely related to our work, InstanceNorm (Ulyanov et al., 2016) is originally proposed for real-time image generation.###Normalization methods shift and scale the hidden representations and are shown to help the optimization for deep neural networks (Ioffe & Szegedy, 2015; Ulyanov et al., 2016; Ba et al., 2016; Salimans & Kingma, 2016; Xiong et al., 2020; Salimans et al., 2016; Miyato et al., 2018; Wu & He, 2018; Santurkar et al., 2018).",impact-revealing,acknowledge the effectiveness of normalization methods in deep learning
2738,5736986b6e3b12023e730129,424561d8585ff8ebce7d5d07de8dbf7aae5e7270,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,53e9a508b7602d9702e2bcf5,Rectified Linear Units Improve Restricted Boltzmann Machines,"ReLUs [15] are applied to the output of the n × n conv layer.###dimensional feature (256-d for ZF and 512-d for VGG, with ReLU [33] following).",other,providing context for the application of ReLUs in convolutional layers
2657,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,599c7cda601a182cd27e0aed,Language-level persistency.,"For example, microsecond-scale overheads that arise from accesses to Flash [40], emerging memory technologies like 3D XPoint by Intel and Mi-cron [41–43], or 40-100 Gb/s Inﬁniband and Ethernet network interactions [44] can signiﬁcantly degrade the request latency of microsecond-scale microservices…",other,highlighting the impact of overheads on microservice latency
3610,53e99804b7602d970201668d,5bda0d60efb98013537d9edd9edfaf59fe809e7b,fetching instruction streams,53e9a2b2b7602d9702bb9cc3,Code Layout Optimizations For Transaction Processing Workloads,"Our previous work presents a detailed analysis [25] of the effects of these optimizations, concluding that the improvements on the instruction cache performance are due to an increase in the sequential execution of code, and a better packing of useful code to cache lines.###These results show that even a very long 128-byte line (32 instructions) is usually fully used before being replaced [25].",other,reporting prior findings on instruction cache performance
1102,,58fa22ee1f000e30fe2a9ce1f97632ccd8b77f31,Accelerate Learning of Deep Hashing With Gradient Attention,,,"###Note that though in some hashing methods [14, 34], supervision is provided in the form of triplet ranking or list ranking, the ranking information still depends on pairwise similarities.###ŝij = 〈bi,bj〉 or by the relaxed Hamming distance between binary codes [5, 14], i.###We follow [2, 35, 14, 32] to construct the training set by randomly sampling 500 images per class and the query set by randomly sampling 100 images per class.###However, when the code length is large, its performance may be weaker than DNNH, DPSH or GH.###We compare the performance of the proposed GAH with nine classical or state-of-the-art hashing methods: two are unsupervised methods LSH [4] and ITQ [10]; two are supervised shallow methods ITQ-CCA [10] and KSH [20]; five are deep learning based methods CNNH [32], DNNH [14], DPSH [16], HashNet [3] and GH [24].###In order to learn feature representation and hashing codes simultaneously, DHHN [14] was proposed.###With the development of deep learning, most recently developed supervised hashing models are build on top of deep neural networks to provide an end-to-end solution for image retrieval [14, 35, 3, 5, 2, 24].",impact-revealing,acknowledge variations in existing hashing methods
4049,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e9b5c8b7602d970410ffa3,Finding the Needle in the Haystack with Heuristically Guided Swarm Tree Search.,"If m = 0 then only actions selected in the tree are used to update nodes, similarly if m is larger than the number of moves in the simulation, this is equivalent to the AMAF algorithm.###This is related to P-game trees (7.3).###The purpose of Cutoff AMAF is to warm-up the tree with AMAF data, then use the more accurate UCT data later in the search.",other,providing context for the Cutoff AMAF algorithm
988,,1ba892bc68ace60265f0b2660b36dfb4f52a8752,Knowledge-based New Product Development: fostering innovation through knowledge co-creation,,,"###, 1985; Takeuchi and Nonaka, 1986) already discussed the issues of creating and transferring knowledge in product development projects more than 20 years ago, and the theory of organisational knowledge creation is thoroughly grounded in and backed up by empirical research on such projects (e.g., Dyck et al., 2005; Nonaka et al., 1994; Nonaka and Takeuchi, 1995; Schulze and Hoegl, 2006).",impact-revealing,acknowledging historical foundations of knowledge creation theory
3316,5c757586f56def9798a004b1,99b722f4ff8614b3e2a541a59a9ee6e1ed322f93,temporal point processes and the conditional intensity function,53e9ab6fb7602d970350e20b,Approximate Simulation of Hawkes Processes,"So the Hawkes process can be simulated either by the inverse method or Ogata’s modified thinning algorithm (but in fact there are simpler methods for simulating the Hawkes process, see e.g. Møller and Rasmussen (2005, 2006)).",other,providing context on methods for simulating the Hawkes process
1486,,482a274a17efcb2f1abdc7fbac4507afc114f16a,Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing,,,"###Building upon the established capabilities of ChatGPT, recent research has investigated the application of LLMs in annotating stances of social media texts (Gilardi et al., 2023; Zhang et al., 2023).###These studies demonstrate that ChatGPT attains state-of-the-art performance in multiple stance detection benchmarks (Zhang et al., 2023) and, in certain annotation tasks, surpasses human annotators, as validated by expert assessments (Gilardi et al., 2023).",impact-revealing,highlighting the effectiveness of ChatGPT in stance detection tasks
111,5cede0fcda562983788dbed8,6f5b1076ebacd30849d86e5f5787e3d43b65911f,Adversarial Attacks on Graph Neural Networks via Meta Learning,599c7974601a182cd263f01c,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.,"…(or learning to learn) tries to make the process of learning machine learning models more time and/or data efﬁcient, e.g. by ﬁnding suitable hyperparameter conﬁgurations (Bengio, 2000) or initial weights that enable rapid adaptation to new tasks or domains in few-shot learning (Finn et al., 2017).###To alleviate this issue, Finn et al. (2017) propose a ﬁrst-order approximation, leading to We denote by ˜ θ t the parameters at time t independent of the data A (and ˜ θ t − 1 ), i.e. ∇ A ˜ θ t = 0 ; the gradient is thus not propagated through ˜ θ t .###Our approach is based on the principle of meta learning, which has traditionally been used for hyperparameter optimization (Bengio, 2000), or, more recently, few-shot learning (Finn et al., 2017).###First-order refers to the approximation proposed by Finn et al. (2017), i.e. ignoring all second-order derivatives.",impact-revealing,describing the concept of meta learning and its applications
1806,,bf457a30094445635caaaff72d66671b5b049128,Conceptual and Epistemic Aspects of Students' Scientific Explanations,,,"###Students suggested that explanations had to be backed up by relevant evidence and they had to be causally coherent, consistent with research showing adolescents’ sensitivity to causal mechanism (Koslowski, 1996).###The guides should focus them on articulating a claim about the trait being selected for, and prior work on people’s apparent concern with plausible causal mechanisms (Brem & Rips, 2000; Koslowski, 1996) suggested that most groups would make some claim for selective advantage.###Students’ performance here is consistent with recent work showing that adolescents and adults prefer plausible causal explanations even in the face of scarce evidence (Brem & Rips, 2000) or contrary evidence that does not suggest a plausible mechanism (Koslowski, 1996).###An alternative view proposes that children and adults can, in fact, distinguish causal claims from evidence in various settings (Brem & Rips, 2000; Koslowski, 1996; Sodian, Zaitchik, & Carey, 1991; Tschirgi, 1980).",impact-revealing,highlighting the importance of causal coherence in explanations and its relevance to students' understanding
1468,,2b63a63b681fe186459e2e28dafb81562dd85d0b,Coin it up: Generalization of creative constructions in the wild,,,"###…generalize over these different approaches, we employ the same set of models used in (Ramiro et al., 2018): a prototype model, an exemplar model, and k nearest-neighbor chaining models for k = 1 . . .5, which are inspired by theoretical frameworks of categorization (e.g. Rosch, 1975; Lakoff, 1987).",impact-revealing,describing the models used for generalization
626,5f76f20a91e011f31b98056c,645bd6eadc247989abc5e0b0aa0be79ec8b11ea6,CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,5b1642d68fbcbf6e5a9b7e77,Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods.,"One line of work explores evaluation grounded to specific downstream tasks, such as coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Dinan et al., 2020) and relation extraction (Gaut et al., 2019).###One line of work explores evaluation grounded to specific downstream tasks, such as coreference resolution (Rudinger et al., 2018; Webster et al., 2018; Dinan et al., 2020) and relation extraction (Gaut et al.",impact-revealing,acknowledge existing evaluation methods in NLP
2485,599c7988601a182cd2648a09,6b7d6e6416343b2a122f8416e69059ce919026ef,Inductive Representation Learning on Large Graphs.,5550456245ce0a409eb55cee,Glove: Global Vectors for Word Representation.,"For features, we use off-the-shelf 300-dimensional GloVe CommonCrawl word vectors [27]; for each post, we concatenated (i) the average embedding of the post title, (ii) the average embedding of all the post’s comments (iii) the post’s score, and (iv) the number of comments made on the post.",other,describing the feature extraction process for posts
3585,5c0f87a5da562944ac95a190,4d157bec76125b1121a999ab2b79730540ab7aad,"Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases",573695886e3b12023e4a8df4,"InChI, the IUPAC International Chemical Identifier","The most popular line notations are SMILES [58] and InChI [59]
notations (for detailed information, please refer to the supplementary material).###In chemical space, similarities are calculated by searching molecular substructure and isomorphism based on the representations of molecules such as SMILES and InChI.###The most popular line notations are SMILES [58] and InChI [59] 1882 | Rifaioglu et al.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
785,5a9cb60d17c44a376ffb35be,b8bbef8e62b4ef342243666f39b205de9f20eb8c,MSTM: A novel map matching approach for low-sampling-rate trajectories,53e9b2b1b7602d9703d59f62,Map-matching for low-sampling-rate GPS trajectories,"ST-Matching algorithm[4] is the first one solving the problem about low-sampling-rate map matching.###In [4], the author combines the spatial structures of the road network and the temporal/speed constraints of the trajectories.",impact-revealing,reporting prior findings on a specific algorithm
330,5843777eac44360f108417ec,455afd748e8834ef521e4b67c7c056d3c33429e2,Hierarchical Attention Networks for Document Classification,573696ce6e3b12023e5ceb9b,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.","Xu et al. (2015) uses the attention mechanism in image caption generation to select the relevant image regions when generating words in the captions.###To include sensitivity to this fact, our model includes two levels of attention mechanisms (Bahdanau et al., 2014; Xu et al., 2015) — one at the word level and one at the sentence level — that let the model to pay more or less attention to individual words and sentences when constructing the…",impact-revealing,reporting prior findings on attention mechanisms in image caption generation
3680,5f2e715791e011ecdac9c1bc,110ef8f751d1abf3f18b10ee90f883f2709f2312,DeText: A Deep Text Ranking Framework with BERT,5550412045ce0a409eb38b4c,Convolutional Neural Networks for Sentence Classification.,We follow the previous work [14] to generate the text embedding from word embedding matrix E.,other,reporting prior findings on text embedding generation
1019,,b25b2564c5d0891a64c429582ccbd12591b59517,The health status of the Dutch population as assessed by the EQ-6D,,,"###The effect of this extension of the EQ-5D was that the content validity of the instrument improved, while the reliability remained unaltered [8].###Recently however, the EQ-5D has been extended with an additional dimension: cognitive functioning (meaning memory, concentration, coherence, and IQ) [8, 9].###The EQ-6D is the EQ-5D extended by a sixth dimension: cognitive functioning (memory, concentration, coherence, IQ) [8, 9] with three possible answers.",impact-revealing,highlighting the improvements in content validity of the EQ-5D instrument
300,5fb24ee191e01186d3f5decc,050fecf6e3e21faeeb7629f179ba839c4dfd1ead,EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints,5e09a9d7df1a9c0c416afc7f,Mask-Predict: Parallel Decoding of Conditional Masked Language Models,"Edit operations such as substitution (Ghazvininejad et al., 2019) and insertion-deletion (Gu et al., 2019) have reduced the quality gap between non-autoregressive and autoregressive models.###These issues have been addressed via partially parallel decoding (Wang et al., 2018; Stern et al., 2018) or multi-pass decoding (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019).###Our model is an Edi t-Based T ransf O rmer with R epositioning ( EDITOR ), which builds on recent progress on non-autoregressive sequence generation (Lee et al., 2018; Ghazvininejad et al., 2019).###We stop reﬁning if 1) the output sequences from two consecutive iterations are the same (Gu et al., 2019), or 2) the maximum number of decoding steps is reached (Lee et al., 2018; Ghazvininejad et al., 2019).",impact-revealing,highlighting advancements in non-autoregressive sequence generation methods
854,5bdc315017c44a1f58a05a1d,f9fb0ad85f903e3ecf5d4bafdf7c74f647e75f58,LEMNA: Explaining Deep Learning based Security Applications,5c8b13614895d9cbc64b4fbc,A unified approach to interpreting model predictions,"However, this sacrifices the fidelity of the explanation [34].###More advanced methods [34, 45] produce explanations under a “blackbox” setting where no knowledge of classifier details is available.###Explanation Method Support RNN/MLP Local Non-linear Support Blackbox Representative Works Whitebox method (forward) G# # G# Occlusion [17, 32, 74, 76], AI2 [19], Whitebox method (backword) G# # # Saliency Map [3, 54, 57], Grad-carm [50], DeepLIFT [53] Blackbox method G# # LIME [45], SHAP [34], Interpretable Decision Set [31] Our method LEMNA LEMNA###We find that SHAP is very slow and its performance is worse than LIME for our applications.###A recent work SHAP [34] tries to extend LIME by adding weights to the artificially generated data samples.###To help readers to understand the testing process, we use 4We have tested SHAP [34], which is an extension of LIME.###These are two key assumptions made by existing models [34, 45] which are often violated in security applications, causing a poor explanation fidelity.###4We have tested SHAP [34], which is an extension of LIME.",impact-revealing,highlighting the limitations of existing explanation methods in security applications
151,5d9edbfc47c8f7664602eba5,dde65325dc7600d02983a76bd54693f0050946a4,integrating both visual and audio cues for enhanced video caption,573696f46e3b12023e5f0ee9,Sequence to Sequence - Video to Text,"…as pooling over frames (Venugopalan et al. 2014), holistic video representations (Gua ; Rohrbach et al. 2015; Rohrbach et al. 2013), sub-sampling on a ﬁxed number of input frames (Yao et al. 2015) and extracting the last hidden state of recurrent visual feature encoder (Venugopalan et al. 2015).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
568,5e7345fd91e011a051ebf85f,9c6dccf7e17221adc3b02bfc202a0e0e061fe28a,deliberation model based two-pass end-to-end speech recognition,5aed14d617c44a44381594e0,Achieving Human Parity on Automatic Chinese to English News Translation.,"The deliberation model has been used in state-of-the-art machine translation [17], or generating intermediate representation in speech-to-text translation [18].",impact-revealing,acknowledging the application of the deliberation model in machine translation and speech-to-text translation
3928,5e85c28491e0114016e821dc,2db6b9621862b4293c1433c91ee8fb45e394f376,DeepGS: Deep Representation Learning of Graphs and Sequences for Drug-Target Binding Affinity Prediction,56d8f57edabfae2eee906dfa,Low-Quality Structural and Interaction Data Improves Binding Affinity Prediction via Random Forest.,[16] proposed a docking method based on random forrest (RF).,other,reporting prior findings on docking method
540,5da2f8a647c8f76646083cd9,b789abc47b7a92596050f6055a93c8fe1929db2a,Dynamic Multicontext Segmentation of Remote Sensing Images Based on Convolutional Networks,57a4e91aac44365e35c97589,"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.","learning context (as an alternative to deconvolution layers) mainly for semantic segmentation [20], [28], [39].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
485,5db929e947c8f766461fd005,11ead744b2edbdabe174258a8dfe4023be8e501d,Hierarchical Attention Prototypical Networks for Few-Shot Text Classification,5ce2d0bbced107d4c63af966,Hybrid Attention-based Prototypical Networks for Noisy Few-Shot Relation Classification,", 2017) has achieved excellent performance in few-shot image classification and few-shot text classification (Han et al., 2018; Gao et al., 2019) tasks respectively, so our model is based on prototypical networks and aims to get promotion.###(2017) which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by Gao et al. (2019) which includes Proto and PHATT.###, 2017) and PHATT (Gao et al., 2019) respectively.###For FewRel dataset, we cite the results reported by Snell et al. (2017) which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by Gao et al. (2019) which includes Proto and PHATT.###At the training stage, we also compare the convergence speed between Proto, PHATT, and HAPN on the 10 way 5 shot and 10 way 15 shot FewRel task.###So we compare our model’s SSA with other models such as Proto and PHATT on the 10 way FewRel task, and the shot number ranges from 5 to 25.###…task such as intention classiﬁcation, Han et al. (2018) present a relation classiﬁcation dataset - FewRel, and adapt most recent state-of-the-art few-shot learning methods for it, Gao et al. (2019) propose a hybrid attention-based prototypical networks for noisy few-shot relation classiﬁcation.###Firstly, we compare our model with several traditional models such as Finetune and kNN, Then we compare our model with ﬁve state-of-the-art few-shot learning models based on neural networks, they are MetaN (Munkhdalai and Yu, 2017), GNN (Garcia and Bruna, 2018), SNAIL (Mishra et al., 2018), Proto (Snell et al., 2017) and PHATT (Gao et al., 2019) respectively.###So we apply a CNN-based feature attention mechanism similar to Gao et al. (2019) proposed as a class feature extractor.###The prototypical networks (Snell et al., 2017) has achieved excellent performance in few-shot image classiﬁcation and few-shot text classiﬁcation (Han et al., 2018; Gao et al., 2019) tasks respectively, so our model is based on prototypical networks and aims to get promotion.###…such as Finetune and kNN, Then we compare our model with ﬁve state-of-the-art few-shot learning models based on neural networks, they are MetaN (Munkhdalai and Yu, 2017), GNN (Garcia and Bruna, 2018), SNAIL (Mishra et al., 2018), Proto (Snell et al., 2017) and PHATT (Gao et al., 2019) respectively.###(2018) and ◇ reported by Gao et al. (2019).###For a fair comparison, in our model, we use the same word embeddings and hyperparameters of instance encoder as PHATT proposed.",impact-revealing,reporting comparisons and results of few-shot learning models
59,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,5736960c6e3b12023e51f0a0,End-To-End Sequence Labeling Via Bi-Directional Lstm-Cnns-Crf,"Many recent sequence labeling frameworks (Ma and Hovy, 2016b; Misawa et al., 2017) share a very basic structure: a bidirectional LSTM network followed by a CRF tagging layer (i.e. BLSTM-CRF).###Recent research efforts in neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) (Ma and Hovy, 2016a) or bidirectional long short-term memory (BLSTMs) (Lam-ple et al., 2016) can largely eliminate human-crafted features.",impact-revealing,acknowledge recent developments in sequence labeling frameworks
3983,5e2ac357df1a9c0c41e7fa48,73bc990757d54f2161eceacd330e0aeb189393bf,Building attention and edge message passing neural networks for bioactivity and physical–chemical property prediction,5966c6720cf2aff42d51b898,Convolutional embedding of attributed molecular graphs for physical property prediction.,"It has been shown that the MPNN framework has a high generalizability such that several popular graph neural network algorithms [24–26, 28, 29] can be translated into the MPNN framework.###Other molecular graph convolution methods were reported by Kearnes et al. [25] and Coley [26] as extensions to Duvenaud’s method.",other,highlighting the generalizability of the MPNN framework and its relation to other methods
2597,5eccb534e06a4c1b26a83a46,e582444763f8b1e3e2c725aab1f37c5d64b69123,Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations,5ca600ae6558b90bfa4d76e9,Towards Deep Learning Models Resistant to Adversarial Attacks,"The perturbation was crafted with PGD [25] though the image features appear to be more dominant than the perturbation.###However, this linearity hypothesis is not fully compatible with the existence of adversarial examples which violate local linearity [25].###However, reasonably robust DNNs of high-dimensional inputs can be trained in practice [25, 37].",other,discussing the limitations of the linearity hypothesis in relation to adversarial examples
1055,,a8147b5cfcc1fb2a9cc6e460639b8d72a50800c5,QueueLinker: A Framework for Parallel Distributed Data-Stream Processing,,,"###Data stream management systems (DSMSs) [2,3,4,5,6,7] are designed to process data streams through continuous query; however, operator execution mechanisms such as Chain [8], Eddy [9], and Teddy [10] do not consider latency in parallel computing and fail to consider communication latency between threads or CPU cores.###Aurora [2,25,26], Borealis [3,27,28,29], STREAM [4,30], TelegraphCQ [5], NiagaraCQ [6] and Gigascope [7] are examples of such systems.###It then provides a review of Data Stream Management Systems (DSMSs) [2,3,4,5,6,7] developed especially for executing so-called “continuous queries” [14] of data streams in real-time.###Previously, data stream management systems (DSMSs) [2,3,4,5,6,7] had been developed to process data streams using continuous query, as DSMSs have scheduling algorithms for operator execution.",impact-revealing,providing context on data stream management systems and their challenges
1122,,e3aff87d2ac728f084bf5ec85e838297945bc51e,Video Generation with Consistency Tuning,,,"###Denoising Diffusion Probabilistic Model (DDPM) [7] and its variant Denoising Diffusion Implicit Model (DDIM) [12] have been widely used for text-to-image generation, including MCVD [13], FDM [4], LVDM [5], PVDM [16], Gen-L-Video [14] and so on.###Then, denoising diffusion implicit model is learned to (DDIM) [12] generalize the framework of denoising diffusion probabilistic model (DDPM) [7] and propose a deterministic ODE process, achieving faster sampling speed.",impact-revealing,acknowledge the use of diffusion models in text-to-image generation
1659,,d3a2f42cf0c6cfe7f38e5ef6c48466998d91327d,Targeting Personal Recovery of People With Complex Mental Health Needs: The Development of a Psychosocial Intervention Through User-Centered Design,,,"###Indeed, literature suggests that intergroup identity, which is an important part of self-identity, is something that often arises from a social context (18, 19).###Interpersonal identity builds upon personal interests, attitudes, and behavior that differs from other individuals, whereas intergroup identity builds upon social group memberships (18, 19).###Self-identity is considered to be an integrated construct and encompasses interpersonal and intergroup identities [Social Identity Approach; (18, 19)].###Third, (self) stigma negatively affects self-identity if someone considers himself/herself part of a group that is devalued by society (18, 29).",impact-revealing,providing context on identity concepts
1541,,0a7ada05a9c0cfc493f9eb0f2428cd3dda74b368,Corticosterone regulates fear memory via Rac1 activity in the hippocampus,,,"###Contextual fear conditioning is widely used as a hippocampus-dependent model of PTSD (Desmedt et al., 2015; Eichenbaum, 2000; Grillon et al., 1996; Jiang et al., 2015) The key intersection of stress and memory regulation is located in the hippocampus.###Fear learning is beneficial for survival in stressful situations by guiding subsequent behavior, but fear learning is also a critical initiating factor for stress-related disorders, such as PTSD.###Stressful events can generate enduring memories, which may induce certain psychiatric disorders such as post-traumatic stress disorder (PTSD).###Abbreviations: CORT, corticosterone; FSs, foot shocks; GPCR, G-protein coupled receptors; GR, glucocorticoid receptor; HPA, hypothalamic-pituitary-adrenocortical, MR, mineralocorticoid receptor
Introduction
The molecular mechanisms involved in fear are key points in gaining a better understanding of fearrelated disorders such as posttraumatic stress disorder (PTSD) (Desmedt et al., 2015).",impact-revealing,highlighting the significance of understanding molecular mechanisms in PTSD
743,5f7fdd328de39f0828397ac2,a5fa6e7565dca654eab9372ace4b1ba7f63655f7,CogLTX: Applying BERT to Long Texts,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Pretrained language models, pioneered by BERT [12], have emerged as silver bullets for many NLP tasks, such as question answering [38] and text classification [22].###The direct and superficial obstacle for long texts is that the pretrained max position embedding is usually 512 in BERT [12].",impact-revealing,highlighting the significance of pretrained language models in NLP tasks
4016,5f75eed591e0111c1eb4da5f,400389ca8b23ff77fa9ee96717fe6447df7469af,Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning Attacks,5bdc31b817c44a1f58a0bf75,Fast Gradient Attack on Network Embedding.,F AST G RADIENT A TTACK (FGA) Chen et al. (2018) FGAs are created based on gradient information in GNNs and they belong to the category of targeted attacks.###Fast Gradient Attack (FGA) (Chen et al. 2018) FGAs are created based on gradient information in GNNs and they belong to the category of targeted attacks.###2018) or targeting specific nodes either directly or indirectly for their misclassification (Chen et al. 2018).,other,providing context on Fast Gradient Attack in GNNs
2194,5ca600ae6558b90bfa4d76e9,7aa38b85fa8cba64d6a4010543f6695dbf5f1386,Towards Deep Learning Models Resistant to Adversarial Attacks,5bdc31b817c44a1f58a0beb0,Second-Order Adversarial Attack and Certifiable Robustness.,"Indeed, subsequent work [20, 25] has found that PGD is in fact overestimating the `2-robustness of this model.",other,highlighting a limitation in the robustness assessment of a model
2036,,4028e081dbfc605e9355686b7e6b61c96dd17dc1,Comparative Genomics of Klebsiella pneumoniae Strains with Different Antibiotic Resistance Profiles,,,"###However, the siderophore transport iroNBCD cluster found in K. pneumoniae NTUH-K2044 and reportedly missing from K. pneumoniae MGH 78578 is also absent from both JH1 and 1162281 (53).###The majority of the unique gene clusters had homologs in other K. pneumoniae strains, including the acetoin catalysis aco operon and the capsular polysaccharide and lipopolysaccharide gene clusters, which have the highest similarity to those of K. pneumoniae NTUH-K2044 (53).###pneumoniae NTUH-K2044 (54) 79 was isolated from a Taiwanese patient with a liver abscess and shows sensitivity to most 80 clinically used antibiotics.###pneumoniae isolates which were 479 indicative of antibiotic susceptibilities (54).###While these size estimates are smaller than those published for the complete genomes of K. pneumoniae strains MGH 78578 (5,315,120 bp), NTUH-K2044 (5,248,520 bp), and 342 (5,641,239 bp), the overall size differences are comparable.###pneumoniae strains is considerable with many strain 72 specific genes and genomic re-arrangements being reported, often involving specific plasmids 73 and mobile elements (29,54,56).###pneumoniae MGH 78578 is also absent from both 289 KpJH1 and Kp1162281 (54).###While most matches were to one of the other sequenced K. pneumoniae strains, NTUH-K2044 or 342, JH1 also had several genes that are possible first reports for this species.###The cluster of K. pneumoniae NTUH-K2044 and K. pneumoniae subsp. rhinoscleromatis ATCC 13884 had weaker support.###Similar magnitudes of variation in genome content, as well as a smaller genome size, were also reported by Wu et al. when they compared the genome of pathogenic strain K. pneumoniae NTUH-K2044 with that of K. pneumoniae MGH 78578 (53).###K. pneumoniae NTUH-K2044 (53) was isolated from a Taiwanese patient with a liver abscess and shows sensitivity to most clinically used antibiotics.###pneumoniae genome, the strain MGH 78578 was widely used as the comparator sequence in 251 other recent genomic studies (13,54).",impact-revealing,highlighting genomic variations and comparisons among K. pneumoniae strains
2068,,6592e6fa8429c2a279855970f3ab489f1f4063cb,Pituitary adenomas: Surgery and radiotherapy in the age of molecular diagnostics and pathology.,,,"###To treat erectile dysfunction, oral PDE-5 inhibitors are commonly used as the first-line treatment as they are effective and less invasive than other delivery mechanisms.(127)",impact-revealing,highlighting the common treatment for erectile dysfunction
1432,,6c6db44a2739e9215e319c94fa426e29893843af,Attentive fine-tuning of Transformers for Translation of low-resourced languages @LoResMT 2021,,,"###08 55
6v 2
[ cs
(Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013) are the widely adopted as the standard approach by both industrial and research communities (Jadhav, 2020a; Bojar et al., 2016; Cheng et al., 2016).###, 2014; Kalchbrenner and Blunsom, 2013) are the widely adopted as the standard approach by both industrial and research communities (Jadhav, 2020a; Bojar et al., 2016; Cheng et al., 2016).",impact-revealing,acknowledge widely adopted approaches in the field
3827,5c8b99db4895d9cbc69c7956,add350d0c5605c98d285b87493fc77c1d68281df,architectural support for server-side php processing,558ad7ab612c41e6b9d3b590,Feedback directed optimization of TCMalloc,"A significant fraction of execution time in these applications come from memory allocation and deallocation, despite significant efforts to optimize them in software [37, 51].",other,highlighting performance issues related to memory management in applications
3272,5ef3247091e0110c353da5ff,06bf758b7e7fd675ceb2d008520db51631716d42,Embedding-based Retrieval in Facebook Search,53e9b929b7602d9704515d97,Unicorn: a system for searching the social graph,"of the existing system, such as realtime updates, efficient query planning and execution, and support for multi-hop queries (see [3]).###In order to integrate embedding-based retrieval into our serving stack, we implemented first-class support for NN search in Unicorn [3], a retrieval engine powering most search products at Facebook.",other,providing context on system features and integration
413,5c8d4bf34895d9cbc64e3332,a60c69c2fae27ebbb73c87f7f2a4765556bd7f9f,Stochastic Training of Graph Convolutional Networks with Variance Reduction,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"We cannot set D ( l ) = 1 because GraphSAGE explicitly need the activation of a node itself besides the average of its neighbors.###Hamilton et al. (2017a) propose to perform the approximate forward propagation as Eq.###Since most GCNs only have two graph convolution layers (Kipf & Welling, 2017; Hamilton et al., 2017a), this gives a signiﬁcant reduction of the receptive ﬁeld size and speeds up the computation.###On the largest Reddit dataset, the training time of our algorithm is 7 times shorter than that of the best-performing competitor among the exact algorithm (Kipf & Welling, 2017), neighbor sampling (Hamilton et al., 2017a) and importance sampling (Chen et al., 2018) algorithms.###Hamilton et al. (2017a) make an initial attempt to develop stochastic training algorithms for GCNs via a scheme of neighbor sampling (NS).###To reduce the receptive ﬁeld size, Hamilton et al. (2017a) propose a neighbor sampling (NS) algorithm.###GCNs and their variants (Hamilton et al., 2017a; Veliˇckovi´c et al., 2017) have been applied to semi-supervised node classiﬁcation (Kipf & Welling, 2017), inductive node embedding (Hamilton et al., 2017a), link prediction (Kipf & Welling, 2016; Berg et al., 2017) and knowledge graphs…###Hamilton et al. (2017a) choose D (1) = 10 and D (2) = 25 , and the receptive ﬁeld size D (1) × D (2) = 250 is much larger than that of MLP, which is 1 , so the training is still expensive.###• PPI and Reddit: We use the mean pooling architecture GraphSAGE-mean proposed by [3].###Our accuracy result for IS+PP can match the result reported by Chen et al. (2018), while their NS baseline, GraphSAGE (Hamilton et al., 2017a), does not implement the preprocessing technique in Sec.###Our algorithm is applicable to other models (Hamilton et al., 2017a) and tasks (Kipf & Welling, 2016; Berg et al., 2017; Schlichtkrull et al., 2017; Hamilton et al., 2017b) that involve computing the average activation of neighbors.###• PPI and Reddit: We use the mean pooling architecture GraphSAGEmean proposed by [3].###…et al., 2017a; Veliˇckovi´c et al., 2017) have been applied to semi-supervised node classiﬁcation (Kipf & Welling, 2017), inductive node embedding (Hamilton et al., 2017a), link prediction (Kipf & Welling, 2016; Berg et al., 2017) and knowledge graphs (Schlichtkrull et al., 2017),…###The model is GCN for the former 4 datasets and GraphSAGE (Hamilton et al., 2017a) for the latter 2 datasets, see Appendix E for the details on the architectures.###We examine the variance and convergence of our algorithms empirically on six datasets, including Citeseer, Cora, PubMed and NELL from Kipf & Welling (2017) and Red-dit, PPI from Hamilton et al. (2017a), as summarized in Table 1, with the same train / validation / test splits.###Training with the CV estimator is similar as with the NS estimator (Hamilton et al., 2017a).",impact-revealing,providing context on the application of GraphSAGE and GCNs in various tasks
2885,5eccb534e06a4c1b26a83a46,e582444763f8b1e3e2c725aab1f37c5d64b69123,Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations,5736960a6e3b12023e51d5fb,Identity Mappings In Deep Residual Networks,"While the working mecha-L nism of DNNs is not fully understood, one widely accepted interpretation considers DNNs as feature extractors [16], which inspires the recent work [17] to link the existence of adversarial examples to non-robust features in the training dataset.###Deep neural networks (DNNs) have shown impressive performance in numerous applications, ranging from image classiﬁcation [16, 48] to motion regression [8, 47].",other,highlighting the role of DNNs as feature extractors and their implications for adversarial examples
3549,5feb068b91e011f5d3420813,e339c5d31ffc7029c1f72d567ac07b4606701c72,ALP-KD: Attention-Based Layer Projection for Knowledge Distillation,53e9b8b4b7602d9704494109,Automatically Constructing a Corpus of Sentential Paraphrases,The task is to decide whether sentences are semantically equivalent (Dolan and Brockett 2005).,other,describing the task of semantic equivalence
3925,5d04e8dbda56295d08db13cf,56e3ce0ff4cbd05e404214d19ae264fe6c457a16,cif: continuous integrate-and-fire for end-to-end speech recognition,5736978a6e3b12023e6705e8,Librispeech: An Asr Corpus Based On Public Domain Audio Books,"We experiment on three public ASR datasets including the popular English read-speech corpus (Librispeech [17]), current largest Mandarin read-speech corpus (AISHELL-2 [18]) and the Mandarin telephone ASR benchmark (HKUST [19]).",other,reporting datasets used for experiments
519,55a6bae665ce054aad73115b,340f48901f72278f6bf78a04ee5b01df208cc508,Human-level control through deep reinforcement learning,53e9a1bdb7602d9702ab61f2,Neural fitted q iteration – first experiences with a data efficient neural reinforcement learning method,"Because Q maps history–action pairs to scalar estimates of their Q-value, the history andtheactionhave beenusedasinputstothe neuralnetwork by some previous approaches 24,26 .###Whileotherstablemethodsexistfortrainingneuralnetworksinthe reinforcementlearningsetting,suchasneuralfittedQ-iteration 24 , these methodsinvolvetherepeatedtrainingofnetworks denovo onhundreds of iterations.###Incontrasttopreviouswork 24,26 , our approach incorporates ‘end-to-end’ reinforcement learning that uses reward to continuously shape representations within the convolutional network towards salientfeaturesoftheenvironmentthatfacilitatevalueestimation.",impact-revealing,highlighting the novelty of the proposed approach in reinforcement learning
230,5e5e18de93d709897ce37796,0a6a9e6d4e3efd7c69357769305b70097281655f,DropEdge: Towards Deep Graph Convolutional Networks on Node Classification,5bdc31b817c44a1f58a0c039,Adaptive sampling towards fast graph representation learning,"We apply the full-supervised training fashion used in Huang et al. (2018) and Chen et al. (2018) on all datasets in our experiments.###Comparison with SOTAs We select the best performance for each backbone with DropEdge, and contrast them with existing State of the Arts (SOTA), including GCN, FastGCN, AS-GCN and GraphSAGE in Table 2; for the SOTA methods, we reuse the results reported in Huang et al. (2018).###DropEdge vs. DropNode Another related vein belongs to the kind of node sampling based methods, including GraphSAGE (Hamilton et al., 2017), FastGCN (Chen et al., 2018), and ASGCN (Huang et al., 2018).###Recently, several sampling-based methods have been proposed for fast graph representation learning, including the node-wise sampling methods (Hamilton et al., 2017), the layer-wise approach (Chen et al., 2018) and its layer-dependent variant (Huang et al., 2018).###, 2018) and its layer-dependent variant (Huang et al., 2018).",impact-revealing,reporting methodology and comparisons with state-of-the-art methods
1601,,02e415430297a6eb86830c2b080647d527508f97,Perturbed examples reveal invariances shared by language models,,,"###We build upon the ""textattack"" library (Morris et al., 2020) In this section, we aim to investigate whether a design choice (i.e., distillation) that has a nominal impact on the IID accuracy, preserves shared invariances along different linguistic capabilities.",impact-revealing,acknowledge the use of a specific library for investigation
1534,,b76c27245a421576a29edce13a8ccf54b061edec,STIGMA CONSCIOUSNESS AS A MODERATOR OF THE RELATIONSHIP BETWEEN RACIAL,,,"###To be inclusive of all ethnic minorities, a more recent definition of racial microaggressions has been developed – “brief, everyday exchanges that send denigrating
6     
messages to people of color because they belong to a racial minority group” (Sue et al., 2007, p. 273).###If experiences of microaggressions are ongoing, these experiences may accumulate and contribute to negative psychological health outcomes (Sue et al., 2007).###recognizing racial microaggressions, how these subtle messages influence ethnic minority clients, as well as be accountable for and be active when holding their own personal biases related to clients of color (Sue et al., 2007).###…providers should receive education and clinical training related to recognizing racial microaggressions, how these subtle messages influence ethnic minority clients, as well as be accountable for and be active when holding their own personal biases related to clients of color (Sue et al., 2007).###Scholars have emphasized the importance of increasing quantitative
41     
racial microaggressions research, as well as studying how racial microaggressions negatively influence mental health (Nadal, 2011; Sue et al., 2007).###41 racial microaggressions research, as well as studying how racial microaggressions negatively influence mental health (Nadal, 2011; Sue et al., 2007).",impact-revealing,highlighting the significance of understanding racial microaggressions and their impact on mental health
4052,53e9a42bb7602d9702d44c0b,2d6f191fd9b08d2a53498f0ed9b4f1a411d83cdf,Temporal Streams In Commercial Server Applications,53e9af67b7602d97039a85ee,Last-Touch Correlated Data Streaming,"Although the term “temporal stream” was introduced in [25], a wide variety of recent prefetchers rely on the same underlying phenomenon, including hot data stream prefetching [7], the global history buffer [19], the user-level memory thread [21], epoch-based correlation prefetching [8], and last-touch correlated data streaming [9].###Furthermore, prior studies have considered only a single system organization, focusing either on uniprocessors [7, 8, 9, 21] or multi-chip distributed-shared-memory systems [25].###Building on this line of research, the latest proposals prefetch extended sequences of memory accesses that recur over the course of program execution [7, 9, 19, 21, 25].###To improve performance for these access patterns, over a decade of research has lead to the development of address-correlating prefetchers, which exploit correlation between consecutive memory accesses and are highly-effective for pointerbased structures [6, 7, 8, 9, 13, 14, 15, 19, 21, 25].",other,highlighting the evolution and effectiveness of memory access prefetching techniques
154,5b67b45517c44aac1c86078b,e62ddf27659bc131968d2dcc3e2bd59de98c6917,"Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop.",573697846e3b12023e66ab35,Facenet: A Unified Embedding For Face Recognition And Clustering,"Instead of projecting to a single point, triplet loss enables documents with the same identity to reside on a manifold [20], and at the same time maintain a distance from other documents.",impact-revealing,describing the functionality of triplet loss in document representation
2220,5ce3afafced107d4c65f7c4c,d9f6ada77448664b71128bb19df15765336974a6,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,5b8c9f4a17c44af36f8b6977,SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense   Inference,"Similarly, given the similarity of COPA to SWAG (Zellers et al., 2018), we first fine-tune BERT on SWAG.###Using SWAG as a transfer task for COPA sees an 8 point improvement.###Similarly, given the similarity of COPA to SWAG (Zellers et al., 2018), we ﬁrst ﬁne-tune BERT on SWAG.",other,highlighting the effectiveness of transfer learning from SWAG to COPA
514,573697c96e3b12023e6a9ac3,c8d4d601e1677ab93fa8c0a3392b152c48b94d80,How to bid the cloud,53e9b78ab7602d970432f081,Statistical Modeling of Spot Instance Prices in Public Cloud Environments,"We can gain a basic statistical understanding of Amazon’s prevailing spot prices by studying the two-month history made available by Amazon [1,15].###Joint user-provider interactions for cloud services are considered in [26], but auction-speciﬁc works on both provider and user actions are limited to statistical studies of historical spot prices [1,15].###Complicating this tradeoﬀ is the fact that user jobs can have long runtimes spanning many changes in the spot price [15].",impact-revealing,highlighting the limitations of existing auction-specific works on cloud services
1845,,713d87772cec9caaf419dbbf80e09dc75008d6bf,Technical Report : A Cryptographically Sound Dolev-Yao Style Security Proof of the Otway-Rees Protocol via a Symbolic Security Proof ( Long Version ) ∗,,,"###, in [55, 43, 56], and various new approaches and formal proof tools for the analys is of security protocols were validated by showing that they can prove the protocol in the Dolev-Yao mod el (respectively that they can find the wellknown type-flaw attack if the underlying model does not provi de sufficient typing itself; the model that our proof is based upon excludes this attack).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
956,,5947d4944f1aed028ed809b52dc80664f1a7192c,Peripheral stimuli generate different forms of inhibition of return when participants make prosaccades versus antisaccades to them,,,"###It bears noting that only one of the 12 experiments included in the megaanalysis reported monitoring eye movements or actively discouraging oculomotor responding (Lupiáñez et al., 1997— Experiment 5).###…& Taylor, 1994, for review; Terry, Valdes, & Neill, 1994); however, as first demonstrated by Hartley and Kieley (1995) and later reinforced by Lupiáñez et al. (1997), IOR can be reliably measured in non-spatial discrimination tasks at cue-target onset asynchronies greater than 400 ms.…",impact-revealing,highlighting the significance of eye movement monitoring in experiments
3397,573696026e3b12023e516718,f8e79ac0ea341056ef20f2616628b3e964764cfd,"You Only Look Once: Unified, Real-Time Object Detection",573697156e3b12023e60c553,Real-Time Grasp Detection Using Convolutional Neural Networks,"Our work is similar in design to work on
grasp detection by Redmon et al [26].###Our work is similar in design to work on grasp detection by Redmon et al [26].",other,acknowledge similarity in design to prior work on grasp detection
51,5f8d6be69fced0a24bbab01e,a87e4124f7305a97a8efaa574c1b270dccf4a563,DisenHAN: Disentangled Heterogeneous Graph Attention Network for Recommendation,599c7987601a182cd2648373,Attention Is All You Need.,"Inspired by the architecture design of Transformer[31], we leverage attention mechanism to learn the weight of source nodes under different aspects.",impact-revealing,describing the method inspired by Transformer architecture
913,53e9af67b7602d97039a85ee,529e8c6e6b5a6cb4f1cf202c47d9d42f5889ec1d,Last-Touch Correlated Data Streaming,53e9afd3b7602d9703a24a4b,Dead-block prediction & dead-block correlating prefetchers,"As in DBCP [12], LT-cords constructs signatures using the history table.###The realistic DBCP is implemented with a 2MB on-chip correlation table as in [12].###proposed Dead-Block Correlated Prefetching [12] to predict the last touch to a cache block prior to that block’s eviction, and replace the block by prefetching a subsequently accessed block.###1 The figure corroborates prior results [12,13,26], showing that over 85% of all cache-block dead-times are longer than the memory access latency.###A recent proposal, the Dead-Block Correlating Prefetcher (DBCP) [12], achieves maximal prefetch lookahead through correlation to “last touch” accesses—the last access to each cache block prior to eviction.###Figure 3 depicts the anatomy of DBCP [12].###storage [12] achieves only 17% performance improvement.###Cache misses are frequently clustered [12], corresponding to bursts of last touches.",impact-revealing,reporting prior findings and methods in cache prefetching
955,,ddd668143c46cf7f3c3a6deaf68de557b90a6582,Inhibition of Return Is Modulated by Negative Stimuli: Evidence from Subliminal Perception,,,"###However, SOA shorting than 700–1000 ms is not suitable for the generation of IOR effect (Lupiáñez et al., 1997).",impact-revealing,highlighting limitations in SOA duration for IOR effect generation
2244,5db9297247c8f766461f6d13,9ec95c1130a6ac4238ac2e5c7b2b66047511ea92,long and diverse text generation with planning-based hierarchical variational model,5a260c2817c44a4ba8a237ba,Generating High-Quality and Informative Conversation Responses with Sequence-to-Sequence Models,"To keep the decoder aware of the crucial information in the already generated preﬁx, Shao et al. (2017) appended the generated preﬁx to the encoder, and Guo et al. (2018) leaked the extracted features of the generated preﬁx from the discriminator to the generator in a Generative Adversarial Nets…",other,describing methods to enhance decoder awareness in generative models
2158,,5d8f71dd3eac5db979d1cae758dc47d0dcc97fc6,Rel-CNN: Learning Relationship Features in Time Series for Classification,,,"###In addition, we observe that the three traditional feature-based baselines, LS , BOSS and WEASEL , have average accuracy as 78.74%, 81.02% and 83.34%, respectively (higher than 1NN-DTW 73.78###More-over, LRFC-Cos is ranked No. 1, Top 2 and Top 3 for 26, 37 and 51 times versus 19, 34 and 50 times by WEASEL, WEA-SEL and Inception, respectively.###LRFC-Eud, even more impressive, is ranked No. 1, Top 2 and Top 3 for 31, 43 and 57 times versus 18, 32 and 48 times by WEASEL, WEASEL and Inception (the second best models), respectively.###As shown in the last two columns in Table 4, we observe that all the three Rel-CNN models outperform 1NN-DTW, LS, BOSS, WEASEL and OS-CNN in terms of both recall and precision.###More speciﬁ-cally, we compare Rel-CNN models with 1-NN DTW (Multi-Dimension), WEASEL, ResNet, Inception and OS-CNN.###ing window for time series classification [15].###Among them, WEASEL performs particularly well, bringing the performance in all metrics up a notch.###The average rankings of these models also demonstrate their strengths against traditional models (even though they do not win the No. 1 rank in as many datasets as WEASEL).###To improve the efﬁciency of BOSS, Schafer et al. propose an advanced version, called word extraction for time series classi-ﬁcation (WEASEL) , to extract more discriminative features by an aggressive feature selection algorithm from each sliding window for time series classiﬁcation [15].###designing feature extraction algorithms [13], [14], [15].###Word ExtrAction for time SEries cLassification (WEASEL) [15]: An upgraded version of BOSS; widely used for TSC.###18, where LRFC-Eud outperforms all the baselines, while signiﬁcantly over OS-CNN, 1NN-DTW (MD) and WEASEL, in terms of average rank.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2773,5a260c0917c44a4ba8a1e00e,93f9607034c9b7b7693c60e9d2631adc15a2a524,learning to model the tail,573696086e3b12023e51b855,Image Question Answering Using Convolutional Neural Network with Dynamic Parameter Prediction,"Such techniques have also been recently explored in the context of regressing classifier weights from training sample [40, 41, 42].",other,acknowledge recent explorations of techniques in classifier weight regression
1883,,72fe1bc87891b86bb5a94430d7497cf21e8d440a,Electrochemical Biosensing of Dopamine Neurotransmitter: A Review,,,"###However, carbon-fiber electrodes are the most widely used for the electrochemical characterization of dopamine oxidation [40,41] because they possess numerous advantages.",impact-revealing,highlighting the advantages of carbon-fiber electrodes in dopamine oxidation characterization
2802,5edf5ddc91e011bc656defd7,c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87,linformer: self-attention with linear complexity,5d3ed2653a55ac61d998598b,RoBERTa: A Robustly Optimized BERT Pretraining Approach,"This operation is key for retaining long-term information, giving Transformers the edge over recurrent models on long sequences.###In this work, we introduce a novel approach for tackling the self-attention bottleneck in Transformers.###This quadratic dependency on the sequence length has become a bottleneck for Transformers.###We ﬁrst compare the pretraining performance of our proposed architecture against RoBERTa (Liu et al., 2019), which is based on the Transformer.###The most common techniques for model efﬁciency that can be applied to Transformers (some speciﬁc to Transformers, others more general-purpose) include: Mixed Precision (Micikevicius et al., 2017): Using half-precision or mixed-precision representations of ﬂoating points is popular in deep learning, and is also widely used in training Transformers (Ott et al., 2019).###One predominant application of Transformers, that has seen the most gains, is using them as pretrained language models, whereby models are ﬁrst pretrained with a language modeling objective on a large corpus, then ﬁnetuned on target tasks using supervised data (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2019).###The RoBERTa-base model here is pretrained with same corpus as BERT.###…Transformers (some speciﬁc to Transformers, others more general-purpose) include: Mixed Precision (Micikevicius et al., 2017): Using half-precision or mixed-precision representations of ﬂoating points is popular in deep learning, and is also widely used in training Transformers (Ott et al., 2019).###There has been much prior literature on improving the efﬁciency of Transformers, especially the self-attention bottleneck.###We ﬁnetune our Linformer on IMDB (Maas et al., 2011) and SST-2 (Socher et al., 2013) (sentiment classiﬁcation), as well as QNLI (natural language inference) (Rajpurkar et al., 2016), and QQP (textual similarity) (Chen et al., 2018) We do the same with RoBERTa, 12-layer BERT-base and 6-layer distilled BERT.###We observe that the Linformer model ( n = 512 , k = 128 ) has comparable downstream performance to the RoBERTa model, and in fact even slightly outperforms it at k = 256 .###We use two pretrained trans-former models, RoBERTa-base (12-layer stacked transformer) and RoBERTa-large (24-layer stacked transformer) (Liu et al., 2019) on two tasks: masked-language-modeling task on Wiki103 (Merity et al., 2016) and classiﬁcation task on IMDB (Maas et al., 2011).###Our work focuses on making Transformers more efﬁcient by introducing a mechanism that reduces self-attention to linear-time complexity.###…of Transformers, that has seen the most gains, is using them as pretrained language models, whereby models are ﬁrst pretrained with a language modeling objective on a large corpus, then ﬁnetuned on target tasks using supervised data (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2019).",other,highlighting the advancements and challenges in improving Transformer efficiency
4011,5f350c4191e011d4254d01da,d592e8ca7ad6096a7c0d590f0602ebee8b70a197,Transfer Learning Approaches for Streaming End-to-End Speech Recognition System,599c7965601a182cd263864d,Transfer Learning for Speech Recognition on a Budget,"Successful strategies include transfer learning [17, 18], that leverage a well trained AM from high-resource language to bootstrap the low-resource AM; multi-task training [19, 20] and ensemble learning [21, 22] that aim to utilize multilingual data and share the model parameters.###Several methods have been proposed to improve the performance of low-resource ASR models [13, 14, 15, 16, 17, 18, 19, 20, 21, 22].",other,acknowledge existing strategies for improving low-resource ASR models
63,5ebe685391e0117693a52241,99314a532a3358cb86064fc8917ed2c283227539,NAT: Noise-Aware Training for Robust Neural Sequence Labeling,5b3d98cc17c44a510f801f2c,Towards Robust Neural Machine Translation,"Inspired by recent research in computer vision (Zheng et al., 2016), Neural Machine Translation (NMT; Cheng et al., 2018), and ASR (Sperber et al., 2017), we propose two Noise-Aware Training (NAT) objectives that improve the accuracy of sequence labeling performed on noisy input without reducing efﬁciency on the original data.###Cheng et al. (2018) continued their work and developed the adversarial stability training method for NMT by adding a discriminator term to the objective function.###Inspired by recent research in computer vision (Zheng et al., 2016), Neural Machine Translation (NMT; Cheng et al., 2018), and ASR (Sperber et al., 2017), we propose two Noise-Aware Training (NAT) objectives that improve the accuracy of sequence labeling performed on noisy input without reducing…###, 2016), Neural Machine Translation (NMT; Cheng et al., 2018), and ASR (Sperber et al.",impact-revealing,drawing inspiration from recent advancements in related fields to propose new objectives
1452,,30bafa2d2f9dbe9e89c0efae1e4571809d383328,The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text,,,"###MedLEE [20,21] builds on semantic models derived from the linguistic string project (LSP) [22] and is guided by a semantic grammar that consists of patterns of semantic classes, such as degree + change + ﬁnding, which would match mild increase in congestion .",impact-revealing,reporting on the development and structure of MedLEE
2475,5f02f25491e011ee5e0258e0,7d4dcee4628745f79f5e352859172817fece3e83,Adaptive Graph Encoder for Attributed Graph Embedding,53e998f6b7602d9702130ab8,Relational Topic Models for Document Networks,"[2, 5] model features as latent variables in Bayesian networks.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2295,5dde4b463a55ac4c42972afc,43a8b2fd651c3783723f4265d7641f9601a5a6f4,One Man’s Trash Is Another Man’s Treasure: Resisting Adversarial Examples by Adversarial Examples,5ca600ae6558b90bfa4d76e9,Towards Deep Learning Models Resistant to Adversarial Attacks,"Finding an adversarial example amounts to searching for a local minimum on a highly fluctuated objective landscape [26].###By far the most successful defense against white-box attacks is adversarial training [26, 13, 38], and a rich set of methods has been proposed to accelerate its training speed or further improve its robustness [51, 54, 21, 13, 46, 35, 53, 29, 27].###PGDtype methods are considered the strongest attacks based on first-order information, namely the network’s gradient with respect to the input [26].###[26] further formalized the problem of adversarial attacks and proposed Projected Gradient Descent (PGD) method, which further inspires many subsequent attacking methods [11, 8, 28, 19].",other,highlighting the significance of adversarial training and its methods in defense against attacks
3295,5ede0553e06a4c1b26a841e6,9a772646ef9ed9c917f45fa592d5f89f7d5f8542,bayesian graph neural networks with adaptive connection sampling,5c757274f56def9798814733,Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking,"We preprocess and split the dataset as done in (Kipf & Welling, 2017) and (Bojchevski & Gunnemann, 2018).###We consider Cora , Citeseer and Cora-ML datasets, and preprocess and split them same as Kipf & Welling (2017) and Bojchevski & Gunnemann (2018).",other,acknowledge preprocessing methods used in dataset preparation
1460,,ff042571d750a08e8929973af7a11ef3bd72ed53,A Study of Representational Similarity: The Emergence of Object Concepts in Rapid Serial Visual Presentation Streams,,,"###…and Feature-Based Approaches Attribute-based approaches were motivated by the notion that correlations between attributes and properties distributed across varying objects would reveal important latent structures in conceptual knowledge (Rosch, 1975; Rosch et al., 1976; Collin & Quillian, 1969).###between attributes and properties distributed across varying objects would reveal important latent structures in conceptual knowledge (Rosch, 1975; Rosch et al., 1976; Collin & Quillian, 1969).###between attributes and properties distributed across varying objects would reveal important latent structures in conceptual knowledge (Rosch, 1975; Rosch et al., 1976; Collin & Quillian, 1969). This idea was articulated in the works of Quillian (1967)###The notion of a distributed featural correlation also underpinned Rosch's (1976) 'typicality effect' theory.###59 REPRESENTATIONAL SIMILARITY This argument leads back to the classic feature-based accounts of semantic representation and semantic memory (Rosch et al., 1976; Smith, Shoben & Rips, 1974). Notably, in work by Smith et al. (1974), the representation of a concept is construed as a binary list of characteristic features in a multidimensional feature space where the features correlate to their external referent object.",impact-revealing,highlighting foundational theories in conceptual knowledge representation
797,5ee9f15b91e01152af022eb9,6360aaece0d6bf153183b9ecd075f42f7b127cc9,Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting,5e5e18a093d709897ce21291,What graph neural networks cannot learn: depth vs width,"based GNNs [33], [37], [38] that obtain universality at the###From a different perspective, [68] and [33] showed the connections between GNNs and distributed local algorithms [69], [70], [71] and suggested more powerful alternatives based on either local orderings or###Our method draws inspiration from [33], where it was shown that GNNs become universal when the vertices in the graph are uniquely identified, i.",impact-revealing,drawing inspiration from prior work on GNNs
772,5ed7796e91e011e6e91120f0,8ade0f3bbe8d8251c5de9ef39886a13055f463e5,TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding,58d82fced649053542fd6bdd,Ideology Detection for Twitter Users with Heterogeneous Types of Links.,"Even though some realized the importance of links [9, 13], they failed to provide an embedding.###On the other hand, on social network datasets, it is quite intuitive trying to extract information from text data to do ideology-detection [5, 8, 15–17], only a few paid attention to links [9, 13].###Emerging from social science, probabilistic models have been widely used for such kinds of analysis since the early 1980s [2, 13, 28].###The work conducted on ordinary citizens could also be categorized into two types according to the source of data being used: intentionally collected via strategies like survey [1, 20], and directly collected such as from news articles [2] or from social networks [13, 15, 17].###Some others [2, 13, 17, 29] noticed the advantages of neural networks, but seldom do they focus on links.",impact-revealing,highlighting the gap in research regarding the importance of links in ideology detection
2332,5f8cf5159e795ea21aee7f07,0ee0801ba010a441403f9ed666ef9bf006b3aa07,Adaptive Universal Generalized PageRank Graph Neural Network,53e99b16b7602d97023a6228,Scaling personalized web search.,"The authors of this study use the fact that the PPR propagation will converge to ΠpprH(0), where Πppr = α(In− (1−α)Ãsym)−1 is independent on the node label information provided in the training data.###This is similar to the PPR weights used in APPNP, despite that the decaying speed is different.###When the graph is strong homophilic (φ = 0.75), the learnt GPR weights are increasing which is significantly different from the PPR weights.###In contrast, recent GNN models that utilize Personalized PageRanks (PPR) with fixed weights (Wu et al., 2019; Klicpera et al., 2018; 2019) inevitably act as low-pass filters.###The recent work (Klicpera et al., 2018) showed that fixed PPR weights (APPNP) can also provably resolve the over-smoothing problem.###Fixing the GPR weights makes the model unable to adaptively learn the optimal propagation rules which is of crucial importance: As we will show in Section 4, the fixed PPR weights corresponds to low-pass graph filters which makes them inadequate for learning on heterophilic graphs.###For the GPR weights, we use different initializations including PPR with α ∈ {0.1, 0.2, 0.5, 0.9}, γk = δ0k or δKk and the default random initialization in pytorch.###These two weight choices correspond to Personalized PageRank (PPR) (Jeh & Widom, 2003), which is known to be suboptimal compared to the IPR framework when applied to homophilic node classification (Li et al., 2019).",other,discussing the limitations of fixed PPR weights in GNN models
666,5f50ba4291e01182e69239cb,20454697ea082975db2503a52418efb8f65b8ae6,clocs: camera-lidar object candidates fusion for 3d object detection,5a73cbcc17c44a0b3035f319,Joint 3d Proposal Generation And Object Detection From View Aggregation,"MV3D [11] and AVOD [12] project the raw point cloud into bird’s eye view (BEV) to form a multi-channel BEV image.###While early and deep fusion have greatest potential to leverage cross modality information, they suffer from sensitivity to data alignment, often involve complicated architectures [11], [12],",impact-revealing,acknowledge existing methods and their limitations in data alignment
3369,5a260c8117c44a4ba8a30adf,ecf6c42d84351f34e1625a6a2e4cc6526da45c74,representation learning on graphs: methods and applications,57a4e91dac44365e35c9830c,Learning Convolutional Neural Networks for Graphs,"The most prominent application domain is for classifying the properties of graphs corresponding to different molecules [16, 21, 43, 33].###Other variants of the convolutional idea are proposed by Neipert et al. [43] and Kearnes et al. [33].",other,acknowledge application of graph classification in molecular properties
3584,5cf48a3cda56291d5829eb69,a0852cd9a026bc90168fa85fa422cb0e48f98394,Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss,599c7953601a182cd2630e0d,Voxceleb: A Large-Scale Speaker Identification Dataset,"For example, in web videos [5, 24] (e.g., LRW and VoxCeleb datasets), speakers move signiﬁcantly when they are talking.###We evaluate our model along with state-of-the-art methods on several popular datasets (e.g., GRID [6], LRW [5], VoxCeleb [24] and TCD [13]).###The ground truth videos are selected from different sources: we randomly select samples from the testing set of LRW [5], VoxCeleb [24], TCD [13], GRID [6] and real-world samples from YouTube (in total 38 videos).",other,providing context for evaluating model performance on various datasets
2796,5eede0b791e0116a23aafe75,150f95f9c73820e0a0fa1546140e9f2bdfd25954,temporal graph networks for deep learning on dynamic graphs,5f0574209e795e2a332c5cfd,Streaming graph neural networks,"Sequence-based approaches for CTDGs [36, 58, 59, 38] use RNNs to update representations of the source and destination node each time a new edge appears.",other,providing context on sequence-based approaches for CTDGs
1177,,fa1c9db0d8143706a9d27ad07066daa7b98fc4e8,The HERD Project: Human-Multi-Robot Interaction in Search & Rescue and in Farming,,,"###A common method for creating coverage plans of a polygonal area is the boustrophedon/snake pattern [12], and has been widely used for pre-operation planning in SAR operations [8], [13], [14].",impact-revealing,highlighting a common method used in SAR operations
858,5f2e715791e011ecdac9c1bc,110ef8f751d1abf3f18b10ee90f883f2709f2312,DeText: A Deep Text Ranking Framework with BERT,5b67b45517c44aac1c8607cb,Real-time Personalization using Embeddings for Search Ranking at Airbnb.,"With representation based approaches, existing work uses embedding pre-computing, either for documents [23, 29] or for member profiles (personalization) [10].",impact-revealing,acknowledge existing methods in representation-based approaches
2734,5ec49a639fced0a24b4de82e,724d182e2b0217ce10ff05e3ad0b2f548fb39b9f,Iterative Edit-Based Unsupervised Sentence Simplification,5b67b46b17c44aac1c861ff6,A Language Model Based Evaluator For Sentence Compression,"2 Second, we use a syntax-aware LM, i.e., in addition to words, we use part-of-speech (POS) and dependency tags as inputs to the LM (Zhao et al., 2018b).###Zhao et al. (2018a) integrated the transformer architecture and paraphrasing rules to guide simpliﬁcation learning.###, in addition to words, we use part-of-speech (POS) and dependency tags as inputs to the LM (Zhao et al., 2018b).###Next, we compare our method with neural machine translation (NMT) systems: EncDecA , which is a vanilla Seq2Seq model with attention (Nisioi et al., 2017); Dress and Dress-Ls , which are based on deep reinforcement learning (Zhang and Lapata, 2017); DMass (Zhao et al., 2018a), which is a transformer-based model with external simpliﬁcation rules; EncDecP , which is an encoder-decoder model with a pointer-mechanism; EntPar , which is based on multi-task learning (Guo et al., 2018); S2S-All-FA , which a reranking based model focussing on lexical sim-pliﬁcation (Kriz et al., 2019); and Access , which is based on the transformer architecture (Martin et al., 2019).###…model with attention (Nisioi et al., 2017); Dress and Dress-Ls , which are based on deep reinforcement learning (Zhang and Lapata, 2017); DMass (Zhao et al., 2018a), which is a transformer-based model with external simpliﬁcation rules; EncDecP , which is an encoder-decoder model with a…",other,acknowledging various methods and models in simplification learning
1750,,a5438144a67015a2d96ad8c860e6117979939db5,PD-L2 based immune signature confers poor prognosis in HNSCC,,,"###However, patients with HNSCC harboring PD-L1-expressing tumors responded poorly to anti-PD-1 blockade treatment in several clinical trials.(17,18) Additionally, the clinical benefits of PD-1 mAb were also seen in PD-L1-negative patients in some cancer types, such as in patients with lung squamous cell carcinoma (LUSC) and renal cell carcinoma (RCC), suggesting that only evaluating PD-L1 was not sufficient for patient selection.",impact-revealing,highlighting the limitations of PD-L1 as a biomarker for treatment response
3845,5cede0e5da562983788c40d8,e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b,Unsupervised Embedding Learning via Invariant and Spreading Instance Feature,53e99b50b7602d97023f2c5c,Robust Boltzmann Machines for recognition and denoising,"Bolztmann Machines (RBMs) [24, 40], Auto-encoders [20, 42] and generative adversarial network (GAN) [7, 10, 11] are widely studied.",other,acknowledge existing models in machine learning
2514,5736982b6e3b12023e6fd21d,0344f9ac92006155f6fa464cab19925ac481dea4,Microarchitectural implications of event-driven server-side web applications,53e999eeb7602d970223102c,The Yin and Yang of power and performance for asymmetric hardware and managed software,"While there are numerous studies that have focused on various aspects of dynamic languages on hardware, such as garbage collection [2], type checking [3], exploiting parallelisms [4, 5], and leveraging hardware heterogeneity [6], we study the implications of the programming model that is emerging in server-side JavaScript applications, i.",other,highlighting the focus on programming model implications in server-side JavaScript applications
1755,,41120a220d4c6c14d2bb8a01a450526dfa7e54fd,Rate-Distortion Optimized Coding for Efficient CNN Compression,,,"###The hardware simulation results demonstrated that our approach can bring considerable speedup for the inference rate on two deep learning accelerators Google TPU and MIT Eyeriss.###We explore the impact our approach has on the inference 2. rate on two hardware platforms, MIT Eyeriss [8] and Google TPU [9], which are inspired by current embedded and high-performance neural-network-targeted hardware accelerators.###Our rate-distortion optimized bit allocation scheme, together with dead zone quantization and Tunstall coding, achieves up to 4.3 × higher inference rate on Google TPU and 2.8 × higher inference rate on MIT Eyeriss, compared with the full precision model.###Our approach achieves high compression ratio on ResNet [1] and MobileNet-v2 [7] and obtains considerable inference speedup on TPU [8] and Eyeriss [9].###rate on two hardware platforms, MIT Eyeriss [8] and Google TPU [9], which are inspired by current embedded and high-performance neural-network-targeted hardware accelerators.",impact-revealing,highlighting the significant performance improvements of the proposed approach on deep learning accelerators
1110,,0d9d467f3470edbda67cae9006bb5d1da7f11f05,Varying Manifolds in Diffusion: From Time-varying Geometries to Visual Saliency,,,"###We further notice an alternative way of defining the generation rate, which is inspired by the generation state derived from [8].###…equation (ODE) [7]: In this paper, we adopt this deterministic approach and use its specific discrete form from [8]: where α t is a time-dependent variable as defined in [8] and ϵ tθ ( X t ) is a neural network with parameter θ trained to approximate the score function ∇ x log p t ( X t ) .###…as an ordinary differential equation (ODE) [7]: In this paper, we adopt this deterministic approach and use its specific discrete form from [8]: where α t is a time-dependent variable as defined in [8] and ϵ tθ ( X t ) is a neural network with parameter θ trained to approximate the score…",impact-revealing,providing context for the generation rate definition
3155,53e9a9a9b7602d97033084f4,2c2e32267c43161f80241a2e1ba21d1f0f871dd4,Express virtual channels: towards the ideal interconnection fabric,53e99ab2b7602d970232ba15,Flit-reservation flow control,Flit-reservation flow control [28] sends out control flits in advance to schedule buffers and channels along the way for subsequent data flits.,other,providing context for flow control method
3336,5e09caba3a55ac662f721afe,36ad06e6f9b39192e7668634eadd6fcf9593e922,Efficient Adversarial Training With Transferable Adversarial Examples,599c796f601a182cd263cd58,Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection   Methods,"To verify the robustness of our method, we evaluate models in the previous section with other attacks: PGD-100 [13], FGSM [8], CW-20 [3].###To verify the robustness of our method, we evaluate models in the previous section with other attacks: PGD-100 [16], FGSM [10], CW-20 [3].",other,describing the evaluation of model robustness against various attacks
3504,5a9cb66717c44a376ffb87ea,fe9b8aac9fa3bfd9724db5a881a578e471e612d7,efficient neural architecture search via parameter sharing,573696086e3b12023e51b8a3,Multi-task Sequence to Sequence Learning,"The idea has apparent complications, as different child models might utilize their weights differently, but was encouraged by previous work on transfer learning and multitask learning, which established that parameters learned for a particular model on a particular task can be used for other models on other tasks, with little to no modifications (Razavian et al., 2014; Zoph et al., 2016; Luong et al., 2016).###…encouraged by previous work on transfer learning and multitask learning, which established that parameters learned for a particular model on a particular task can be used for other models on other tasks, with little to no modiﬁca-tions (Razavian et al., 2014; Zoph et al., 2016; Luong et al., 2016).",other,highlighting the relevance of previous work on transfer learning and multitask learning
1066,,a31801668966799e98d7dae2671b33ecc6d16778,Determinants of Firm´s Innovation,,,"###The CDM model has been frequently applied by scholars using data from the Community Innovation Survey (CIS) launched by Eurostat, such as Lööf and Heshmati (2003) for Norway, Finland and Sweden, Janz et al. (2004) for Germany and Sweden, or Griffi th et al. (2006) for France, Germany, Spain and the UK.###…CDM model has been frequently applied by scholars using data from the Community Innovation Survey (CIS) launched by Eurostat, such as Lööf and Heshmati (2003) for Norway, Finland and Sweden, Janz et al. (2004) for Germany and Sweden, or Griffi th et al. (2006) for France, Germany, Spain and the UK.",impact-revealing,acknowledge frequent application of the CDM model in various studies
2901,5d04e8d7da56295d08daef06,448b64df68335d3695a37c54770e7d5cd5f6fe68,Context Attentive Document Ranking and Query Suggestion,5a260c3517c44a4ba8a25359,"Learning to Attend, Copy, and Generate for Session-Based Query Suggestion.","As we filtered out queries that do not have any associated clicks when constructing the experiment dataset, we lost some longer tasks; otherwise our test data distribution is similar to [8].",other,acknowledging limitations in dataset construction
1412,,5b93eb7af42d546c9d2d7ac249fee7a2d238df32,Offline Reinforcement Learning at Multiple Frequencies,,,"###In particular, we use CQL in our experiments.###One popular instantiation of this idea is conservative Q-learning (CQL) [32], which optimizes the following objective: Qk+1 = arg minQ [ α · ( Es∼D,a∼µ(a|s)[Q(s, a)]− Es∼D,a∼π̂(a|s)[Q(s, a)] ) + 1 2Es,a,s′∼D [( Q(s, a)− r(s, a)− γEπ(a|s)[Qk(s, a)] )] ] , where D is the offline dataset of states, actions, and rewards, µ is a wide action distribution close to the uniform distribution, and π̂ is the behavior policy that collects the offline data.###This results in the complete objective:
LCQL = max µ αEs∼Dδt [Ea∼µ(a|s)[Qδt(st+Nδt , at+Nδt )]− Ea∼π̂(a|s,δt)[Q δt(st+Nδt , at+Nδt )]] +R(µ) LN-Step = Eδt∼∆ [ LCQL + 1
2 Est,at,st+1∼Dδt
[( Qδt(st, at)−Qδttarget )2] ] If Nδt is not an integer, we sample the number of steps from the nearest digits (i.e., rounding up or down) with a Bernoulli distribution where the success rate parameter p is the fractional part of Nδt .###One popular instantiation of this idea is conservative Q-learning (CQL) [32], which optimizes the following objective: Q = arg minQ [ α · ( Es∼D,a∼μ(a|s)[Q(s, a)]− Es∼D,a∼π̂(a|s)[Q(s, a)] ) + 1 2Es,a,s′∼D [( Q(s, a)− r(s, a)− γEπ(a|s)[Q(s, a)] )] ] , where D is the offline dataset of states, actions, and rewards, μ is a wide action distribution close to the uniform distribution, and π̂ is the behavior policy that collects the offline data.###We use CQL as our offline RL algorithm of choice, but our analysis is applicable to other value-based offline RL methods.###We modify the CQL objective described in Sec.###We build on top of the CQL implementation from Geng [44], and use the following CQL hyperparameters: all of our models use a CQL alpha term of 5, a policy learning rate of 3e − 5, and a Q-function learning rate of 3e − 4.###We will show how we can use a modified version of CQL to incorporate data from different discretizations in offline RL.###Prior offline RL approaches focus on mitigating the distributional shift between the learned policy and the data-collecting policy [24] via either explicit or implicit policy regularization [25, 26, 27, 28, 29, 30, 31], penalizing value backup errors [32], uncertainty quantification [26, 33, 34], and model-based methods [35, 36, 37, 38, 39].",impact-revealing,describing the use of conservative Q-learning in experiments
1300,,fdfc5707962f13f0b07c53f214e7b1b561117bd7,Effects of Different Doses of Bone Morphogenetic Protein 4 on Viability and Proliferation Rates of Mouse Embryonic Stem Cells,,,"###This finding reinforced by other researcher's studies that observed the mutations in the BMP type I receptors (Bmpr1a and Acvr1) (15, 16, 24), the BMP type II receptor (Bmpr2) (18), and signaling component, Smads, (Smad1, Smad5, Yakhteh Medical Journal, Vol 11, No 1, Spring 2009, Pages: 29-34 Original Article###In the mouse, the roles of BMPs in the formation of skeletal system (11-14), heart, nervous system, urogenital system, mesoderm induction (15-19), as well as formation and early proliferation of primordial germ cells (PGCs) (20-22) have confirmed with targeted or spontaneous mutations in various BMPs, their receptors and Smads.###This finding reinforced by other researcher's studies that observed the mutations in the BMP type I receptors (Bmpr1a and Acvr1) (15, 16, 24), the BMP type II receptor (Bmpr2) (18), and signaling component, Smads, (Smad1, Smad5,
Yakhteh Medical Journal, Vol 11, No 1, Spring 2009, Pages: 29-34
29
and Smad4) (25-27) precludes analysis of BMP signaling during organogenesis due to the early embryonic lethality.",impact-revealing,highlighting the significance of BMP mutations in organogenesis
2192,555045d745ce0a409eb59fe4,460d131e081486491af532604351a0691b52b11f,User-level psychological stress detection from social media using deep neural network,53e9a41cb7602d9702d367f9,Modeling Spread of Disease from Social Interactions.,Sadilek et al. [9] leverage Tweeter postings to identify the spread of flu symptoms.,other,reporting prior findings on flu symptom identification
933,5d245bb5da56295a28fcca5f,4efb9a950f252138a30eeb942ed02663a3ea29d1,MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing,5ac1829d17c44a1fda918057,N-GCN: Multi-scale Graph Convolution for Semi-supervised Node Classification,"Others considered multiple adjacency powers for feature propagation on graphs, including (Abu-El-Haija et al., 2018) and (Atwood & Towsley, 2016) which combine the powers at the end of the network (right before classification layer), and (Lee et al.",impact-revealing,acknowledge variations in graph feature propagation methods
1810,,35151617a659e286d7f1f54ee674b10546c66e5b,Evidence-B(i)ased Policy Deliberation: A Motivated Reasoning Framework With Applications to the Sex Education Debates,,,"###…students were found to treat theory and data as interdependent, rely on theory (or explanation) to decide which data to take seriously and which data to search for, and rely on data to reject, refine, and elaborate theory (Koslowski, 1996, Koslowski & Maqueda, 1993; Koslowski & Thompson, 2002).###…psychology work on scientific reasoning and evidence appraisal, Koslowski has emphasized the importance of the network of evidentially-relevant background information to thinking in general and to scientific explanation in particular (Koslowski, 1996; Koslowski & Thompson 2002).###…these and other analyses of the nature of evidence and evidence use (e.g., Evans, 1989, Chinn and Brewer, 2001) is that “neither theory nor data alone is sufficient to achieve scientific success; each must be evaluated in the context of, and constrained by, the other” (Koslowski, 1996, p. 252).###Also from a cognitive-developmental perspective, Koslowski (1996) has argued for the importance of prior beliefs and background information in appraising evidence in causal and scientific reasoning.###More recently, from her cognitivedevelopmental psychology work on scientific reasoning and evidence appraisal, Koslowski has emphasized the importance of the network of evidentially-relevant background information to thinking in general and to scientific explanation in particular (Koslowski, 1996; Koslowski & Thompson 2002).",impact-revealing,highlighting the interdependence of theory and data in scientific reasoning
3405,53e9a584b7602d9702eab396,45ce66a661eea0e5b5780ff8cfaf6b2085dd7a1e,Two level bulk preload branch prediction,53e9b2ccb7602d9703d78cc4,The optimum pipeline depth for a microprocessor,"The design described in this paper includes historybased predictors providing both the direction and target address of branches asynchronously, and most often ahead of instruction fetching and delivery.",other,describing the design of a predictive system
1650,,6e69f4ec8804174ffba283f78eb0d1035cc7b016,Moral Identity Predicts Adherence to COVID‐19 Mitigation Procedures Depending on Political Ideology: A Comparison Between the USA and New Zealand,,,"###MORAL IDENTITY AND RESPONSE TO COVID- 19
Moral identity builds on social identity theory (Tajfel & Turner, 1979) to provide a measure of how much people view “being a moral person” as an important aspect of their self- concept (Aquino & Reed, 2002).",impact-revealing,providing context for moral identity in relation to COVID-19
1421,,792249fdfb563263bb15adf659ff735630dab098,Conservative Objective Models for Effective Offline Model-Based Optimization,,,"###…we note that the RHS of the above equation has the same structure as the second term in the RHS of Equation 14 in Kumar et al. (2020), and furthermore since G kf is positive semi-deﬁnite, it satisﬁes the required conditions for Equation 14 and Theorem D.1 from Kumar et al. (2020) to be applicable.###The neural tangent kernel of the function ˆ f ( x ) be deﬁned as: Under these assumptions, we build on the analysis of conservative Q-learning (Kumar et al., 2020) to prove our theoretical result in Theorem 1, shown below: Proposition 1 (Conservative training lower-bounds the true function) .###This design is inspired by recent work in offline RL (Kumar et al., 2020), where a similar approach is used to learn conservative value functions, though it has never been applied to MBO to the best of our knowledge.###This approach is inspired by recent work in ofﬂine RL (Kumar et al., 2020), where a similar objective is used to learn conservative value functions.###…writing the above in matrix form, we note that the RHS of the above equation has the same structure as the second term in the RHS of Equation 14 in Kumar et al. (2020), and furthermore since G kf is positive semi-deﬁnite, it satisﬁes the required conditions for Equation 14 and Theorem D.1 from…###…approaches in supervised learning (Goodfellow et al., 2014b), and building on recent works in ofﬂine reinfor-cmeent learning (Levine et al., 2020; Kumar et al., 2020), COMs ﬁrst explicitly mine for out-of-distribution inputs with erroneously overestimated values and then penalize the…###Under these assumptions, we build on the analysis of conservative Q-learning (Kumar et al., 2020) to prove our theoretical result in Theorem 1, shown below:###Building on recent advances in offline reinforcement learning (Levine et al., 2020; Kumar et al., 2020), the key idea behind our method is to train the learned objective function model with an additional objective that minimizes its predictions on out-of-distribution inputs, thus ensuring it does not overestimate the objective value.###Thus, exactly following the proof of Theorem D.1 in Kumar et al. (2020) for the linear function approximation case in reinforcement learning, with the following substitutions: , a column of the kernel Gram-matrix for a ﬁxed value of the second argument) and a = x T , s = x 0 , we can show that E x…",impact-revealing,drawing on previous work in offline reinforcement learning to support theoretical results
1015,,2d3d9d9fa3d98c390c63dfc4d4f694f97964cfe2,Do Associations Explain Mental Models of Cause?,,,"###For instance, Brewer (1974) argued that conditioning is not a mechanistic bottom up process driven directly by associations and reinforcement; rather, in humans at least, it is always cognitively penetrable.###At least partly, this approach was inspired by readers of Chomsky’s (1959) review of Skinner’s Verbal Behavior (1957), which argued that the traditional associative approach was impoverished and even circular (see also Fodor & Piattelli-Palmarini, 2010).",impact-revealing,highlighting the influence of Chomsky's critique on subsequent theories of conditioning
1509,,c260a3000f04f7e604348180f98463c2aa3bd371,FERLrTc: 2D+3D facial expression recognition via low-rank tensor completion,,,"###Meanwhile tensor decomposition is widely applied for tensor recovery [11] , data classiﬁcation [12] , and harmonic retrieval [13] .###Comparison with tucker decomposition algorithms in other applications: Our proposed approach (FERLrTC) is compared with IRTD [40] , APG _ NTDC [50] , WTucker [51] and KBR _ TC [11] on BU-3DFE database.",impact-revealing,reporting applications of tensor decomposition
2253,5ed12ca69e795e8ab1c11568,b3b60a15502abca31ae7b20ce16bf50049183c5a,knowledge enhanced personalized search,573698476e3b12023e7108a9,EsdRank: Connecting Query and Documents through External Semi-Structured Data.,"[40] takes the entity as a latent space and learn query-document matching relevance through the latent space.###entities and relations from knowledge graphs, in search systems and effectively improves the text representation and ranking accuracy [14, 19, 21, 28, 40, 42, 43].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1050,,e8dea5aa263c910690ac4c0bae13f6bc5103ff63,MacroBase: Prioritizing Attention in Fast Data,,,"###In designing a specialized engine, we were inspired by several past projects, including Gigascope (specialized for network monitoring) [28], WaveScope (specialized for signal processing) [36], MCDB (specialized for Monte Carlo-based operators) [52], and Bismarck (providing extensible aggregation for gradient-based optimization) [33].",impact-revealing,drawing inspiration from previous specialized projects
2915,5d0b00ed8607575390fedd4e,493d5f344eea1468260946b29a80dc81b2be409c,HopSkipJumpAttack: A Query-Efficient Decision-Based Attack,573695fe6e3b12023e5121fc,DeepFool: a simple and accurate method to fool deep neural networks,"…adversarial training work most effectively. b) Baselines: We compare our algorithm with state-of-the-art attack algorithms that require access to gradients, including C&W Attack [6], DeepFool [4] for minimizing (cid:96) 2 -distance, and FGSM [2], and BIM [7, 41] for minimizing (cid:96) ∞ -distance.",other,comparing the proposed algorithm with state-of-the-art attack algorithms
2195,5c45b4d03a55ac25e7f0f55c,e5badcfa663c30a983da24dd682288141d00fcc3,GraphSAR: A Sparsity-Aware Processing-in-Memory Architecture for Large-Scale Graph Processing on ReRAMs,53e9b75bb7602d9704301923,High precision tuning of state for memristive devices by adaptable variation-tolerant algorithm.,"The emergingmetal-oxide resistive random accessmemory (ReRAM) and ReRAM crossbars show huge potential in energy efficient processing-in-memory operations [15, 16], especially for matrixvector multiplications [17].",other,highlighting the potential of ReRAM technology in energy-efficient computing
3141,5cede104da562983788e4508,6303bac53abd725c3b458190a6abe389a4a1e72d,deep high-resolution representation learning for human pose estimation,59ae3bf12bbe271c4c71bbb7,Residual Conv-Deconv Grid Network for Semantic Segmentation,"There are related multi-scale networks for classification and segmentation [5, 8, 74, 81, 30, 76, 55, 56, 24, 83, 55, 52, 18].###The grid network [18], a combination of many weight-shared U-Nets, consists of two separate fusion processes across multi-resolution representations: on the first stage, information is only sent from high resolution to low resolution; on the second stage, information is only sent from low resolution to high resolution, and thus less competitive.",other,describing a specific network architecture for classification and segmentation
4038,53e99e61b7602d97027252c8,e3369553f399fb63b9f24261247afe64fb6c3f65,Virtual Program Counter (VPC) Prediction: Very Low Cost Indirect Branch Prediction Using Conditional Branch Prediction Hardware,53e9b221b7602d9703cbd8b8,Optimizing indirect branch prediction accuracy in virtual machine interpreters.,"We found that this difference is due to the object-oriented nature of the Java programs, which contain a large number of virtual functions, and the behavior of the Java Virtual Machine, which uses a large number of indirect branches in its interpretation and dynamic translation phases [15].###2) The Java virtual machine itself uses a significant number of indirect jumps with many targets in its interpretation routines, as shown in previous work on virtual machines [15].###Finally, Ertl and Gregg [15] proposed code replication and superinstructions to improve indirect branch prediction accuracy on virtual machine interpreters.",other,highlighting differences in Java programs and their impact on performance
3425,5dce78623a55ac93ec834bf7,1a55ab1849eccc03801852e1cf445e15d17f8020,Limago: An FPGA-Based Open-Source 100 GbE TCP/IP Stack,5aed14d117c44a443815874b,Serving DNNs in Real Time at Datacenter Scale with Project Brainwave.,"For instance, it can be used as a customizable smartNIC to offload network virtualization functionality [5], application-level functionality such as RDMA packet processing to support key-value stores [6], or for distributed machine learning algorithms [7].",other,highlighting the versatility and applications of a customizable smartNIC
3147,5bdc315017c44a1f58a05ce0,4930de1aff4b1948157a15ac9cdb02364bee97bb,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,57d063b9ac4436735428eae2,Relation Classification Via Multi-Level Attention Cnns,Zhou et al. (2016) and Wang et al. (2016) proposed to use attention mechanisms over RNN and CNN architectures for this task.,other,reporting existing findings on attention mechanisms in RNN and CNN architectures
3798,5f4f6ec291e0111f07b30a2b,3259c9ab1714a4cfdf6439cca6bdc5f78d78fda3,NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size,53e99859b7602d970209567a,Random Search For Hyper-Parameter Optimization,"(2) On CIFAR-100, REA is similar to BOHB, which outperforms REINFORCE; and RANDOM is the worst among them.###, random search (RANDOM) [47], random search with parameter sharing (RSPS) [27].###Among them, RANDOM, REA, REINFORCE, and BOHB are multi-trial based methods.###In Figure 6, we show the rankings of RANDOM, REA, and REINFORCE is (REA ≥ REINFORCE ≥ RANDOM).###Observations on the size search space Ss. (1) REA significantly outperforms the other methods on all datasets in the size search space Ss. (2) On CIFAR-100 and ImageNet16-120, results of BOHB, REINFORCE, and RANDOM are similar.###(3) On CIFAR-10, REINFORCE is better than BOHB and RANDOM.###Specifically, we evaluate some typical NAS algorithms: (I) Random Search algorithms, e.g., random search (RANDOM) [47], random search with parameter sharing (RSPS) [27].",other,reporting performance comparisons of various methods on CIFAR-100
1891,,f0e0806003ee509f1f4feeabf209a8e93dddf5d2,A revised application of cognitive presence automatic classifiers for MOOCs: a new set of indicators revealed?,,,"###…the development of social climate and interpersonal relationships between the participants in the learning community (Rourke et al., 1999); (3) Teaching presence, reflects the instructional activities that facilitate and intervene in the construction of critical discourse (Garrison et al., 1999).###The Community of Inquiry (CoI) framework (Garrison et al., 1999) has been the most broadly used and validated for analysing educa - tional experience in online discussions.###The CoI framework proposed by Garrison et al. (1999) has been most broadly cited for analysing learning in asynchronous online discussion forums in the past two decades.",impact-revealing,highlighting the significance and widespread use of the CoI framework in educational analysis
2161,,736187060aea4fc97b4f98b373c2e7cfa37b0897,An approach to automating the verification of compact parallel coordination programs. I,,,"###z Although the temporal logic approach [18,  20 , 23] may help in studying various properties of the considered programs for fixed moderate values of N, this approach does not address the case in which N is a parameter.",impact-revealing,highlighting the limitation of the temporal logic approach in addressing parameterized cases
3613,5cede0eeda562983788cd285,e4d99f390901df5caac0b587ff685f9cde100342,end-to-end speech translation with knowledge distillation,5a9cb65d17c44a376ffb826e,End-to-End Automatic Speech Translation of Audiobooks,"LibriSpeech Method WER(↓) BLEU(↑) Bérard [10] greedy 19.###They either pretrain ASR task on high-resource data [11], or use multi-task learning to train ST model with ASR or MT model simultaneously [9, 10].###Contrary to [10] which uses characters as output units, we consider subword units can also obtain improvements.###1 BLEU scores improvement in beam search compared to [10].###As shown in Table 2, all four settings surpass the results in [10].###Following [10], We only use the 100 hours clean train set for training, with 2 hours development set and 4 hours test set, which corresponds to 47,271, 1071 and 2048 utterances respectively.###Therefore, this model has become a new trend in speech translation research studies [6, 7, 8, 9, 10, 11].###decoder which improves performance [10].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3166,5bdc315017c44a1f58a05a1d,f9fb0ad85f903e3ecf5d4bafdf7c74f647e75f58,LEMNA: Explaining Deep Learning based Security Applications,53e9a2dcb7602d9702be45cf,Control Flow Integrity for COTS Binaries.,"Binary code reverse-engineering, which transfers binary code to assembly code, is a crucial step in (1) examining and detecting malware [51], (2) hardening the security of software [75], and (3) generating security patches [56].",other,highlighting the importance of binary code reverse-engineering in malware detection and software security
2708,5f03f3b611dc830562232090,aa2c0bd8345c7698f13ab7cc7e0fe9eaa8e4f108,Cracking Tabular Presentation Diversity for Automatic Cross-Checking over Numerical Facts,5736977f6e3b12023e6658ca,Table Cell Search For Question Answering,"Based on the extracted tables, there are many understanding tasks, such as linking text to table cells [6], table cell search for a given query [15], ad hoc search over tables [18], transforming complex tables to the form that can be in a database [14].",other,providing context for understanding tasks related to tables
1247,,c171de81b51d37e6eb975330442db8ffdc433a64,University ’ s repository of research publications and other research outputs Winnowing ontologies based on application use Conference or Workshop Item,,,"###So even though the approach suggested in [20] is good for a general structure-based scaling down of ontologies, it is not suitable for usage-driven ontology winnowing.###Stuckenschmidt and Klein [20] proposed the use of classical clustering algorithms to partition ontologies based on how their class hierarchies are structured.",impact-revealing,highlighting limitations of a proposed approach for ontology winnowing
2167,,16772bfc1d6dcf78426df4518809cdd2e6929e7e,Bounded Fairness,,,"###Temporal logic, with modal operators 2 and 3, has been commonly used as a tool to specify and analyze such fairness properties, in large measure due to the pioneering work of Pnueli and Manna (see [21, 24, 25]).",impact-revealing,highlighting the significance of pioneering work in temporal logic for fairness properties
3556,5f02f17c91e011ee5e0258c8,3da4626411d83c19c9919bb41dba94fff88da90e,Scaling Graph Neural Networks with Approximate PageRank,53e9afb4b7602d9703a0458c,Local Computation Of Pagerank Contributions,"Forward search [4, 23] and backward search [3] can be viewed as deterministic variants of the random walk sampling method.###Luckily, given the broad applicability of PageRank, many such algorithms have been developed [18, 4, 3, 23, 34, 45, 46, 48, 19].",other,providing context on search algorithms
1160,,be677578824940adfe0a910337b9e37b56ea40bc,Contractibility for open global constraints,,,"###This is true of configuration problems and scheduling problems that involve processdependent activities (Mittal and Falkenhainer 1990; Barták 2003).###This is true of con-ﬁguration problems and scheduling problems that involve process-dependent activities (Mittal and Falkenhainer 1990; Bart´ak 2003).###Work on conditional CSPs, initiated in (Mittal and Falkenhainer 1990), addresses contingent variables by explicitly embedding the contingent nature within a CSP, but that work does not address the addition of variables to constraints.",impact-revealing,acknowledge existing work on configuration and scheduling problems
1130,,da3a188c227d817b90203ab5294685d8424ad1e2,Effective Real Image Editing with Accelerated Iterative Diffusion Inversion,,,"###When using deterministic DDIM inversion, image editing through the DDIM sampling process must also be set deterministic in order to keep the fidelity of unedited areas.###In particular, denoising diffusion implicit models (DDIM) [41] are widely used for their speed and flexibility in deterministic and stochastic generations.###In particular, denoising diffusion implicit models (DDIM) [7] are widely used for their speed and flexibility in deterministic and stochastic generations.###Other than HS-SCLIP, we perform comprehensive comparisons in both reconstruction and editing quality with all related text-based real image editing techniques which use DDIM inversion, including PTP [19], NTI [32] and EDICT [45].###In later works, a simple DDIM inversion process is adopted base on a linear ODE solver: To allow for better reconstruction accuracy, here ϵ ˜ t is instead approximated using t +1 as follows:###As the goal of our proposed iterative inversion is accurate reconstruction, here we use a DDIM sampler where the sampling noise is σ t = 0 for all t to achieve a deterministic sampling process.###For image editing methods based on deterministic DDIM inversion methods, the DDIM sampling process is commonly set as deterministic to better control editing effects.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3863,5bdc315017c44a1f58a05e13,1717255b6aea01fe956cef998abbc3c399b5d7cf,AMC: AutoML for Model Compression and Acceleration on Mobile Devices,53e9a169b7602d9702a582ce,Evolving neural networks through augmenting topologies,"Many works on searching models with reinforcement learning and genetic algorithms [46, 42, 5, 37] greatly improve the performance of neural networks.",other,highlighting the impact of reinforcement learning and genetic algorithms on neural network performance
60,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,57a4e91aac44365e35c979dd,"Data Programming: Creating Large Training Sets, Quickly.","Despite their low cost, such supervision usually can be obtained from different sources, and it has been shown that multi-source weak supervision has the potential to perform similar to gold annotations (Ratner et al., 2016).",impact-revealing,highlighting the potential of multi-source weak supervision
3910,5ecc763e9e795e81e9307559,270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,mpnet: masked and permuted pre-training for language understanding,599c7978601a182cd2641b24,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,"The General Language Understanding Evaluation (GLUE) [20] is a collection of 9 natural language understanding tasks, which include two single-sentence tasks (CoLA [21], SST-2 [22]), three similarity and paraphrase tasks (MRPC [23], STS-B [24], QQP), four inference tasks (MNLI [25], QNLI [26], RTE [27], WNLI [28]).###…(Warstadt et al., 2018), SST-2 (Socher et al., 2013)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017), QQP), four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006), WNLI (Levesque et al., 2012)).",other,providing context on the GLUE benchmark and its tasks
2065,,cd8fabcdf0289489727904047e5fbcf3df34500a,Campylobacter Species Isolated from Chickens in Egypt: Molecular Epidemiology and Antimicrobial Resistance,,,"###On the other hand, fluoroquinolone (FQ) (Allos, 2001), tetracycline and gentamicin antibiotics are also frequently used as alternative drugs in cases of Campylobacter infection (Blaser, 2008).",impact-revealing,acknowledging alternative treatments for Campylobacter infection
581,5dee1b4b3a55ac3d409adcbb,a84689ef8eaf38344eb3de24850ec0720c815605,synchronous transformers for end-to-end speech recognition,5a73cb5d17c44a0b303573f1,Local Monotonic Attention Mechanism for End-to-End Speech And Language Processing.,[7] propose a local monotonic attention mechanism that forces the model to predict a central position at every decoding step and calculate soft attention weights only around the central position.,impact-revealing,reporting prior findings on local monotonic attention mechanism
3609,5550414745ce0a409eb39ec8,6010ebf22c6cc07e93c5335fa1d128be8c6b190b,understanding big data analytics workloads on modern processors,53e9ab69b7602d9703507ebe,Data warehousing and analytics infrastructure at facebook.,"Typical big data analytics workloads include business intelligence, machine learning, bio-informatics, and ad hoc analysis [39, 9].",other,providing context on typical big data analytics workloads
3364,5f8ebbb99fced0a24b4e1966,d299c78121f39c3a4cd09e0994e47ec8cd0c20d6,Diversifying Search Results using Self-Attention Network,573698486e3b12023e7110e7,Search Result Diversification Based on Hierarchical Intents.,"These results are released by Hu et at.###Nowadays both unsupervised and supervised explicit approaches are proposed e.g. xQuAD [7], PM2 [8], HxQuAD/HPM2 [9] and DSSA [14].###To generate di-versified results, these methods either explicitly model subtopic coverage of the results [6–9, Tosimplify the problem and accelerate the online ranking, existing methods usually formalize the diverse ranking process as the greedy document sequential selection.###All those diversification approaches in our experiments are using the search results of Lemur as initial ranking sequences. xQuAD [7], PM2 [8], HxQuAD and HPM2 [9] .###Most of the traditional approaches to search result diversification are unsupervised and they are based on handcrafted features and functions [5–9].###As those previous works do [9] we treat all those subtopics with uniform weights.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3922,599c797a601a182cd2641df7,63a010c69f00e65c946a68b546bbd42cbed03564,MagNet: A Two-Pronged Defense against Adversarial Examples,573696076e3b12023e51a63f,The Limitations of Deep Learning in Adversarial Settings,"More specifically, researchers showed that it was possible to generate adversarial examples to fool classifiers [34, 5, 24, 19].###However, recent research showed that an attacker could generate adversarial examples to fool classifiers [34, 5, 24, 19].",other,highlighting the potential for generating adversarial examples to deceive classifiers
844,5c96086e3cb210d2716c4a08,bb76749fab841ccbe20ace74111a518f65d9870b,make the most out of last level cache in intel processors,53e9bb2fb7602d970476a406,"An adaptive, non-uniform cache structure for wire-delay dominated on-chip caches","Several works have proposed hardware-based strategies – mainly by introducing modifications to the CPU architecture – such as data migration, data placement, and data replication [5, 6, 10, 23, 35, 65, 77].###This paper presents the results of our study of the non-uniform cache architecture (NUCA) [35] characteristics of LLC in Intel processors where the LLC is divided into multiple slices interconnected via a bi-directional ring bus [84], thus accessing some slices is more expensive in terms of CPU…",impact-revealing,reporting on hardware-based strategies in CPU architecture
1331,,14bfdcded32df15199b20b31b04eb90b779a92c2,Faculty Members' Understanding of Teaching Efficacy Criteria and It Relation to Their Characteristics.,,,"###Finally, Boston (2002), Rolheiser and Ross (2000) and others have emphasized the importance of training and professional development for teachers to help them better understand and implement effective practices that are the important elements of assessment.",impact-revealing,highlighting the significance of teacher training and professional development in effective assessment practices
2715,5dde4b463a55ac4c42972afc,43a8b2fd651c3783723f4265d7641f9601a5a6f4,One Man’s Trash Is Another Man’s Treasure: Resisting Adversarial Examples by Adversarial Examples,5d04e8eeda56295d08dc31d9,Resisting Adversarial Attacks by k-Winners-Take-All.,"To this end, there exist a long line of works that apply random transformation to input images [50, 14], or employ stochastic activation functions [10] and nondifferentiable operators in the model [49, 7, 39, 33].###line of works aims to obfuscate the network model’s gradient with respect to its input [50, 14, 49, 7, 39, 33], motivated by the fact that the gradient information is essential for crafting adversarial examples: the gradient indicates how to perturb the input to alter the network’s decision.",other,acknowledge existing methods for input transformation and adversarial example crafting
372,5e09cab43a55ac662f721ac6,5c109db04998e623b794b269494f44b7e5006af1,Category-Level Articulated Object Pose Estimation,5c5ce4fd17c44a400fc388b0,Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation,"Our A-NCSH representation is inspired by and closely related to Normalized Object Coordinate Space (NOCS) [26], which we will briefly review first.###In [26], the Umeyama algorithm [24] is adopted within a RANSAC [8] framework to robustly estimate the 6D pose and size of a single rigid object.###The object pose and size can then be defined as the rigid transformation plus scaling from the NOCS coordinate to the camera space observations.###Such understanding is beyond the scope of typical 6D pose estimation algorithms, which have been designed for rigid objects [28, 23, 22, 26].###NOCS is
defined as a 3D space contained within a unit cube, i.e., {x, y, z} ∈ [0, 1], and was introduced in [26] to estimate the category-level 6D pose and size of rigid objects.###Whereas the work by [26] focuses on pose and size estimation for rigid objects, the work presented here extends the NOCS concept to accommodate articulated objects at both part and object level.###At the same time, these objects are pre-scaled and pre-centered so that their tight bounding boxes all have a diagonal distance of 1 and are centered in the NOCS.###[26] extended the object coordinate based approach to perform categorylevel pose estimation.###NPCS. NPCS is defined similarly to NOCS [26] but for single parts instead of whole objects.###Instead of the global pose and size, we care more about the states of rigid parts and joints, which are all ignored in NOCS.###We use the same protocol as is in [26] to pre-align and pre-scale the parts, which not only allows us to define the 6D pose and size of each part but also provides a natural way to obtain the amodal 3D bounding box for each part.###NOCS provides a common reference frame for each category with a canonical global pose and size, enabling pose estimation even for unseen object instances.###The key idea behind the intracategory generalization is to regress the coordinates within a Normalized Object Coordinate Space (NOCS), where the sizes are normalized and the orientations are aligned for objects in a given category.###Compared with NOCS, NAOCS not only normalizes the global pose and size of the articulated objects, but also normalizes their joint parameters and states.###, {x, y, z} ∈ [0, 1], and was introduced in [26] to estimate the category-level 6D pose and size of rigid objects.###NPCS is defined similarly to NOCS [26] but for single parts instead of whole objects.###However, NOCS is not well-suited for articulated objects.###Since for each part we have its NPCS coordinates and camera space point coordinates in correspondence, we could follow [26] to estimate its 6D pose and size.###Similar to [26], we also use RANSAC for outlier removal.",impact-revealing,describing the extension of a method for articulated objects based on prior work
1564,,6e2ba2b21a46042aadce127cfb0eb59aaadcc004,Method for Localizing and Differentiating Bacteria within Biofilms Grown on Indium Tin Oxide: Spatial Distribution of Exoelectrogenic Bacteria within Intact ITO Biofilms via FISH,,,###Recent studies have demonstrated the importance of the structure of EPS and the ability for microorganisms to transfer electrons extracellularly (Xiao et al. 2017).,impact-revealing,highlighting the significance of recent findings on EPS structure and microbial electron transfer
1233,,4f70b4388b3476afb47f31752ec3a504509f36cc,Optimization of ATP synthase function in mitochondria and chloroplasts via the adenylate kinase equilibrium,,,"###Plants carrying out C4 metabolism (e.g., maize, sugarcane) have chloroplastic ATP synthase both in the mesophyll and bundle sheath cells (Majeran et al., 2008).###However, these pathways are important mainly in preventing overreduction of chloroplast ETC and their capacity is insufficient for fine-tuning of redox and energy balance in the whole cell.",impact-revealing,highlighting the importance of C4 metabolism in plants and its role in chloroplast function
179,58437722ac44360f1082f13a,d2e4587744a89bad95fea69e08842cad6c8ff0dd,Temporal ensembling for semi-supervised learning,57a4e91dac44365e35c98218,Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning.,"The recently introduced transform/stability loss of Sajjadi et al. (2016b) is based on the same principle as our work, and the Π -model can be seen as a special case of it.###We test the Π -model and temporal ensembling in two image classiﬁcation tasks, CIFAR-10 and SVHN, and report the mean and standard deviation of 10 runs using different random seeds.###In purely supervised training the de facto standard way of augmenting the CIFAR-10 dataset includes horizontal ﬂips and random translations, while SVHN is limited to random translations.###The same is probably true for SVHN as well, but there the best published results rely on extra data that we chose not to use.###Sajjadi et al. (2016b) recently introduced a new loss function for semi-supervised learning, so called transform/stability loss, which is founded on the same principle as our work.###In addition, they employ a mutual exclusivity loss term (Sajjadi et al., 2016a) that we do not use.###In SVHN Sajjadi et al. (2016b) provide results without augmentation, with the caveat that they use fractional max pooling, which is a very augmentation-like technique due to the random, local stretching it introduces inside the network.###The street view house numbers (SVHN) dataset consists of 32 × 32 pixel RGB images of real-world house numbers, and the task is to classify the centermost digit.###Given that in a separate experiment our network matched the best published result for non-augmented SVHN when extra data is used (1.69% from Lee et al. (2015)), this gap is quite surprising, and leads us to conclude that fractional max pooling leads to a powerful augmentation of the dataset, well beyond what simple translations can achieve.###A principled comparison with Sajjadi et al. (2016b) is difﬁcult due to several reasons.###In SVHN we chose to use only the Table 2 compares our method to the previous state-of-the-art.",impact-revealing,comparing methods and results in semi-supervised learning
3128,5c2348ceda562935fc1d57a4,007d08aee3b88489fe0377849f70688de66adae8,Bandit Learning with Implicit Feedback,53e9b2f4b7602d9703db25ec,Efficient multiple-click models in web search.,"To enable learning from multiple clicks in the same result ranking list, they adopted the dependent click model [10] to infer user satisfaction after a sequence of clicks [14], and later further extended it to broader types of click models [27].",other,describing the extension of click models for user satisfaction inference
630,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,558be7a384ae6766fdefebd6,The Power of Forgetting: Improving the Last-Good-Reply Policy in Monte Carlo Go,"8) to inform simulations, modified by Baier and Drake [17] to include the forgetting of bad moves.###Baier and Drake [17] propose an extension to LGR-1 and LGR-2 called Last Good Reply with Forgetting (LGRF).###The basic MCTS process is conceptually very simple, as shown in Figure 1 (from [17]).###The basic MCTS process [17].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3614,5f058d15dfae54570ec57ea1,fb93ca1e004cbdcb93c8ffc57357189fa4eb6770,Resnest: Split-attention networks,58d82fced649053542fd71f3,PolyNet: A Pursuit of Structural Diversity in Very Deep Networks,"e of 224 when comparing our ResNeSt with ResNet variants, and a crop size of 256 when comparing with other approaches. Regularization. Very deep neural networks tend to overt even for large datasets [68]. To prevent this, dropout regularization randomly masks out some neurons during training (but not during inference) to form an implicit network ensemble [29, 49, 68]. A dropout layer with the dropout",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1067,,a438cdcd2107c3cad31dffaae21fd7fc4dafb6ef,Face image analysis by unsupervised learning,,,"###A number of researchers have argued that local filters are superior to global ones for face image analysis, and expression analysis in particular (Penev and Atick, 1996; Padgett and Cottrell, 1997; Gray et al., 1997; Zhang et al., 1997; Lee and Seung, 1999).###PCA decorrelates the input through an axis rotation.###Methods based on principal component analysis have successfully classified static graylevel images of facial expressions (Padgett and Cottrell, 1997).###, 1991), and recognizing facial expressions (Cottrell and Metcalfe, 1991; Padgett and Cottrell, 1997; Bartlett et al., 1996).###Padgett & Cottrell (Padgett and Cottrell, 1997) found that “eigenfeatures”, consisting of the principal components of image subregions containing the mouth and eyes, were more effective than global PCA (full-face eigenfaces) for facial expression recognition.###Both ICA representations were superior to the PCA representation for recognizing faces across sessions and changes in expression.###A combined classifier that took input from both ICA representations outperformed PCA for recognizing images under all conditions tested.###Principal component analysis (PCA) finds an orthonormal set of axes point-
ing in the directions of maximum covariance in the data.###Representations based on principal component analysis have been applied successfully to recognizing facial identity (Cottrell and Fleming, 1990; Turk and Pentland, 1991), facial expressions (Cottrell and Metcalfe, 1991; Bartlett et al., 1996; Padgett and Cottrell, 1997), and to classifying the gender of the face (Golomb et al.###In order to piece apart local and global filter properties from data-driven versus pre-defined properties, Section 5 also examines local implementations of PCA (Padgett and Cottrell, 1997) in which the kernels were derived from the statistics of small image patches.###The PCA representation for each observation is
Blue-yellow axis.###Padgett & Cottrell (Padgett and Cottrell, 1997) found that an “eigenfeature” representation of face images, based in the principal components of image regions containing individual facial features such as an eye or a mouth, outperformed the full eigenface representation for classifying facial expressions.###PCA provides a set of axes for encoding the input in fewer dimensions with minimum loss of information, in the squared error sense.###Chapter 2 reviews unsupervised learning and information theory, including Hebbian learning, PCA, mimimum entropy coding, and ICA.###, 1998), holistic spatial pattern analysis using techniques based on principal component analysis (Cottrell and Metcalfe, 1991; Padgett and Cottrell, 1997; Lanitis et al., 1997), graylevel pattern analysis using local spatial filters (Padgett and Cottrell, 1997; Zhang et al.###These included analysis of facial motion through estimation of optical flow; holistic spatial analysis based on second-order image statistics such as principal component analysis, local feature analysis, and linear discriminant analysis; and representations based on the outputs of local filters, such as a Gabor wavelet representations and local PCA.###Representations such as ""eigenfaces"" (Turk and Pentland, 1991) and ""holons"" (Cottrell and Metcalfe, 1991), are based on principal component analysis (PCA), which encodes the correlational structure of the input, but does not address high-order statistical dependencies.###PCA can also be accomplished through Hebbian learning, as described in the next section.###PCA models the data as a multivariate Gaussian where the covariance matrix is restricted to be diagonal.###Chapter 5 showed that PCA, which encodes second-order dependencies through unsupervised learning, gave better recognition performance than a set of hand-engineered feature measurements.###Principal component analysis has been applied successfully to recognizing both facial identity (Cottrell and Fleming, 1990; Turk and Pentland, 1991), and facial expressions (Cottrell and Metcalfe, 1991; Bartlett et al., 1996; Padgett and Cottrell, 1997).###, 1997), graylevel pattern analysis using local spatial filters (Padgett and Cottrell, 1997; Zhang et al., 1998), and methods for relating face images to physical models of the facial skin and musculature (Mase, 1991; Terzopoulus and Waters, 1993; Li et al.###Independent component analysis (ICA) (Comon, 1994) is a generalization of PCA which learns the high-order dependencies in the input in addition to the correlations.",impact-revealing,acknowledging the effectiveness of PCA in facial expression recognition and related tasks
2460,5aed14e217c44a4438159a0f,8f684080d2b81d3178d681d6917cb077c082a9e1,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,59ae3be32bbe271c4c71ba2e,Enhanced Deep Residual Networks for Single Image Super-Resolution,[16] experimentally demonstrate that training with MSE loss is not a good choice.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
360,5fd0a7f691e01147f1d1e367,d4e9bf1d5fab52698a3aa48c65b098ea327deea6,CTRLsum: Towards Generic Controllable Text Summarization,5dbab2523a55acea3c05b02b,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language  Generation, Translation, and Comprehension","The BART baseline uses prompts (P) while CTRLsum uses both keywords (K) and prompts (P).###The distribution p(y|x, z) in CTRLSUM is our fine-tuned version of the pretrained BARTLARGE model (Lewis et al., 2020).###Also, BART fails to generate unimportant-entity-related summaries with poor scores.###We observe that BART with prompt alone over-generates a full summary with low precision scores especially when a concise summary is desired in patent purpose summarization.###We train CTRLsum with another two architectures in addition to BART: (1) convolutional seq2seq (Gehring et al., 2017) with the same hyperparameters as in (Fan et al., 2018), and (2) transformer seq2seq with the same hyperparameters as the base model in (Vaswani et al., 2017).###Interestingly, however, BART fine-tuned on a summarization task – without seeing any question-answer pairs – is able to improve the F1 scores by 24.4 and 25.9 points on NewsQA and SQuAD respectively.###Both BART and GPT2 use prompt alone to decode.###(1) The ability of CTRL SUM to deal with arbitrary input keywords (especially when the keywords are not present in the source) is not sufficiently explored and largely limited by the underlying pre-trained model weights (i.e. BART in our experiments).###Our optimization scheme and hyperparameters follow the BART fine-tuning instructions in fairseq examples.###We use pretrained BART (Lewis et al., 2020) as the underlying architecture and perform experiments on three datasets: CNN/Dailymail news articles (Hermann et al., 2015), arXiv scientific papers (Cohan et al., 2018), and BIGPATENT patent documents (Sharma et al., 2019).###Compared with BART which uses the prompt alone, CTRL-SUM achieves superior performance in most cases through using the guiding text as both the keywords and prompt.###Question-Guided Summarization: In Table 6, the GPT2-Large has 774M parameters while the BART architecture (including CTRL SUM ) has 406M parameters.###Results: BART is pretrained with a denoising task to denoise the source, and unsurprisingly obtains poor results in the zero-shot setting, as shown in Table 6.###It also performs comparably to BART on BIGPATENT in terms of BERTScore, though with an inferior ROUGE-2 score.###The con-trol accuracy for important entity control and purpose control are comparable between BART and CTRL SUM without significant difference ( p -value > 0.05), while CTRL SUM shows better control relevance overall by focusing on the desired information.###The distribution p ( y | x , z ) in CTRL SUM is our fine-tuned version of the pretrained BART LARGE model (Lewis et al., 2020 Evaluation: As emphasized in §2.2, we focus our evaluation on the five well-defined tasks.###Notably, our approach also achieves comparable or superior performance to the strong BART baseline on all datasets in a standard, unconstrained setting (§4.6).###On CNNDM and arXiv datasets CTRL SUM (Automatic Key-words) decently outperforms the strong BART and PEGASUS baselines.###There is a performance gap between BART-based models and PEGASUS on BIGPATENT, possibly due to the inherent difference between BART and PEGASUS.###10 In addition to the BART baseline, we also include the performance from GPT2 language model (Radford et al., 2019) (without fine-tuning) as a reference point.",impact-revealing,comparing performance of summarization models and discussing their limitations
631,5cede104da562983788e3653,05dc5fb3a3bdefdf181aafcc42cd80ff6b7704e7,TuckER: Tensor Factorization for Knowledge Graph Completion,53e9a488b7602d9702da7685,A Three-Way Model for Collective Learning on Multi-Relational Data,"A large number of approaches to link prediction so far have been linear, based on various meth-ods of factorizing the third-order binary tensor (Nickel et al., 2011; Yang et al., 2015; Trouillon et al., 2016; Kazemi and Poole, 2018).###Several linear models for link prediction have previously been proposed: RESCAL (Nickel et al., 2011) HypER (Balaˇzevi´c et al., 2019) is a simpliﬁed convolutional model, that uses a hypernetwork to generate 1D convolutional ﬁlters for each relation, extracting relation-speciﬁc features from…###Several previous tensor factorization models can be viewed as a special case of TuckER: RESCAL (Nickel et al., 2011) Following the notation introduced in Section 3.2, the RESCAL scoring function (see Table 1) has the form: (4) This corresponds to Equation 1 with I = K = n e , P = R = d e , Q = J =…###RESCAL (Nickel et al., 2011) Following the notation introduced in Section 3.###Finally, we show that several previous state-of-the-art linear models, RESCAL (Nickel et al., 2011), DistMult (Yang et al.###Finally, we show that several previous state-of-the-art linear models, RESCAL (Nickel et al., 2011), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016) and SimplE (Kazemi and Poole, 2018), are special cases of TuckER.###RESCAL (Nickel et al., 2011) es Wreo Wr ∈ Re 2 O(nede + nrd(2)r) DistMult (Yang et al.###RESCAL An early linear model, RESCAL (Nickel et al., 2011), optimizes a scoring function containing a bilinear product between vector embeddings for each subject and object entity and a full rank matrix for each relation.###A large number of approaches to link prediction so far have been linear, based on various methods of factorizing the third-order binary tensor (Nickel et al., 2011; Yang et al., 2015; Trouillon et al., 2016; Kazemi & Poole, 2018).",impact-revealing,reporting existing approaches to link prediction
1976,,8941832553f2591755d1c979f261bf0245fcb8f0,"Persistence over time, overlapping distribution and molecular indications of interspecific hybridization in wild potato populations of Northwest Argentina",,,"###Closely related potato species do not present important genomic differentiation (Matsubayashi 1991) and, thus, natural fertile hybrids can be formed (Rabinowitz et al. 1990;  Camadro et al. 2004 ).###The conclusion of Ugent (1970) that ‘‘hybridization and subsequent gene flow within and between ploidy levels often result in exceedingly complicated patterns of variation’’ is reinforced by these observations, giving support to the idea that the biological species concept is not appropriate for the tuberbearing Solanum ( Camadro et al. 2004 ).###Sexual polyploidization coupled with haploidization, as in Dichantium (de Wet 1968), can provide the opportunity for gene flow and introgression within and between ploidy levels ( Camadro et al. 2004 ).###The incomplete internal barriers to hybridization that can be acting among many species ( Camadro et al. 2004 ) would allow the formation of interspecific hybrids, introgression and gene flow as suggested by Clausen et al. (2005) in Palca de Aparzo or demonstrated for Solanum xrechei (Clausen and Spooner 1998).",impact-revealing,highlighting the implications of hybridization and gene flow in closely related potato species
3175,5c234870da562935fc1d4db3,0b6a531754e67379518a946f3a3cf685f59358cc,CritICs Critiquing Criticality in Mobile Apps,53e9a255b7602d9702b5fa56,Profile Guided Selection Of Arm And Thumb Instructions,"• Compress: This is a state-of-the-art thumb compression technique, implementing the Fine-Grained Thumb Conversion heuristic from [78], that ﬁrst converts a whole function to Thumb, then replaces frequently occurring “slower thumb instructions” back to 32 bit ARM instructions. over the baseline.###Even smartly employing the Thumb format (Compress), as in [78], only yields a 8% speedup.",other,describing a state-of-the-art compression technique and its performance
1881,,20bfbdfc5186abf5df1ecc630fc16749808f4f62,Longitudinal link between trait motivation and risk-taking behaviors via neural risk processing,,,"###Prior research has emphasized the importance of the motivational system in risky decision-making processes (Franken and Muris, 2006; Luna et al., 2013; Kim-Spoon et al., 2016a; Uro�sevi�c et al., 2014; van Leeuwen et al., 2011), yet the mechanisms through which individual differences in motivation may influence adolescents’ risk-taking behaviors are not well documented.###negatively associated substance use in college students (Franken and Muris, 2006), and another suggested that low avoidance was linked to a progression into regular substance use in adolescents (van Leeuwen et al.###Prior research has emphasized the importance of the motivational system in risky decision-making processes (Ernst et al., 2006; Franken and Muris, 2006; Kim-Spoon et al., 2016a; Uro�sevi�c et al., 2014; van Leeuwen et al., 2011), yet the developmental pathways from approach",impact-revealing,highlighting the significance of prior research on motivation in risky decision-making
1081,,ee0feed44fa161664123686509995914d5c4085a,RAFIV: A Method for Cognitive Usability Analysis,,,"###Although RAFIV shares some common elements with GOMS (Card, Moran, & Newell, 1983; John & Kieras, 1996a) (the original three-stage model upon which RAFIV is based was the result of a GOMS analysis) there are several differences.###That is, GOMS does not address where or why an error might occur (John & Kieras, 1996b).",impact-revealing,highlighting differences between RAFIV and GOMS models
3228,5c04967517c44a2c74709354,04c131293bf64c67972baa0053e85a510c4aa725,Intervention Harvesting for Context-Dependent Examination-Bias Estimation,53e99bffb7602d97024ac0ef,Yahoo! Learning to Rank Challenge Overview.,"We empirically evaluate the effectiveness and robustness of our method by conducting semi-synthetic experiments on the Yahoo Learning-To-Rank Challenge corpus (set 1) [Chapelle and Chang, 2011].###We evaluate the fidelity of the CPBM model and the effectiveness of the estimator in real-world experiments on the ArXiv full-text search engine and in semi-synthetic experiments on the Yahoo Learning-to-Rank Challenge dataset [7].###We empirically evaluate the effectiveness and robustness of our method through real-world experiments on the ArXiv Full-Text Search 1 and through semi-synthetic experiments on the Yahoo Learning-To-Rank Challenge corpus (set 1) [7].",other,reporting the evaluation methods used for effectiveness and robustness
2362,53e99fe3b7602d97028bddfb,ecf5fd423c117ffb87730d75a473bc05beaae2b8,self-optimizing memory controllers: a reinforcement learning approach,5390981d20f70186a0e04860,A study of performance impact of memory controller features in multi-processor server environment,"Most memory controller proposals [17, 25, 30, 36, 37, 38, 48, 49], including ours, aim at increasing overall system performance by delivering higher DRAM throughput and/or lower access latency.###Natarajan et al. [30] examine the impact of diﬀerent policy decisions in the memory controller in a multiprocessor environment and ﬁnd that better policies that more eﬃciently utilize DRAM bandwidth could provide the same performance as doubling the DRAM bandwidth.",other,highlighting the impact of memory controller proposals on system performance
2123,,176243b513d916fa9127b38384db79b47dd2778b,Network Models In Evacuation Planning,,,"###The cell transmission model (CTM), a popular macroscopic traffic flow model introduced by Daganzo (1994, 1995) is widely used in transportation applications related to traffic flow management.###Duanmu et al. (2010) does not consider any patient-specific attributes or requirements.###The cell transmission model (CTM) presented by Daganzo (1994, 1995) predicts the traffic behaviour by evaluating flow and density at a finite number of intermediate points on the network.###Duanmu et al. (2010) focuses on the routing of hospital vehicles during a hurricane evacuation where the ambulances and general traffic compete for space in the regional traffic flow network. The ambulance trip times are estimated using a simulation model based on various hospital evacuation start times and multiple strategies that minimize the transportation time for patients are produced. Duanmu et al. (2010) does not consider any patient-specific attributes or requirements.###Duanmu et al. (2010) focuses on the routing of hospital vehicles during a hurricane evacuation where the ambulances and general traffic compete for space in the regional traffic flow network.###Daganzo (1995) used this model to study networks where the maximum number of arcs (links) entering/leaving is 3.###Duanmu et al. (2010) does not consider any patient-specific attributes or requirements. Golmohammadi and Shimshak (2011) estimate the evacuation time for the hospital building evacuation using a predictive model that takes patient population and available resources as input and calculates the total evacuation time.###Duanmu et al. (2010) focuses on the routing of hospital vehicles during a hurricane evacuation where the ambulances and general traffic compete for space in the regional traffic flow network. The ambulance trip times are estimated using a simulation model based on various hospital evacuation start times and multiple strategies that minimize the transportation time for patients are produced. Duanmu et al. (2010) does not consider any patient-specific attributes or requirements. Golmohammadi and Shimshak (2011) estimate the evacuation time for the hospital building evacuation using a predictive model that takes patient population and available resources as input and calculates the total evacuation time.###Daganzo (1995) then derived the flow equations for the three type of links.",impact-revealing,acknowledge the use and application of the cell transmission model in traffic flow management
2290,555048eb45ce0a409eb72996,791b65c65f8ae7e16c1ee9203cdc3ee59ffeb99f,Relation Classification via Convolutional Deep Neural Network.,53e9a255b7602d9702b5bb55,Word representations: a simple and general method for semi-supervised learning,"However, there are many trained word embeddings that are freely available (Turian et al., 2010).###In NLP, such methods are primarily based on learning a distributed representation for each word, which is also called a word embeddings (Turian et al., 2010).###Our experiments directly utilize the trained embeddings provided by Turian et al.(2010).",other,reporting on the availability and use of trained word embeddings
1190,,c0ce763dcc79eef4408f9f97637ffb37ce7cd5c4,Ship feature recognition methods for deep learning in complex marine environments,,,"###Reference [24] proposed a novel method based on pyramids and an SSD, called the function single shot detector.",impact-revealing,reporting a novel method in prior work
2625,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,599c7ac8601a182cd26e050b,Attack of the killer microseconds.,"For example, microsecondscale overheads that arise from accesses to Flash [40], emerging memory technologies like 3D XPoint by Intel and Micron [41–43], or 40-100 Gb/s Infiniband and Ethernet network interactions [44] can significantly degrade the request latency of microsecond-scale microservices [45–48] like Cache1 or Cache2.",other,highlighting the impact of various technologies on request latency in microservices
2081,,66e236c2a67d97e27e2848f05d97e0134d758f80,Ëëëòòð Èöó×××òò¸îóðº ¾¸¾¼¼¾¸ôôº ¾¼¼¹¾¾¾,,,"###This implies independence5 of vectors yt for di erent time frame t (given the model parameters y), as assumed in all types of HMMs including our NS-HMM.6 However, the key di erence between the NS-HMM and the conventional HMM is the di erent parameter sets y, one de ning smoothly-varying trajectories and the other does not.###In Section 2 we rst address the issue of speech feature selection, which is tightly associated with use of the NS-HMM as the speech model, and then provide a detailed description of the MMSE formulation for speech enhancement in which NS-HMMs are employed.###(28), we generalize a result of [13] from stationary-state HMMs to NS-HMMs.###By using diagonal
covariance matrices for the NS-HMMs, the problem of high computation cost for the inversion of the covariance matrix for calculation of the output likelihoods is also avoided.###For the MMSE enhancement, the noise HMMs we have implemented contained 3 states and 3 mixture components.9
A global measure of signal-to-noise ratio (SNR) was used as the objective evaluation criterion,
which is calculated by
SNR = 10 log
PK n=1 y
2(n)PK n=1 [y(n) ŷ(n)]2 ; (32)
where K is the frame-length, and y(n) and ŷ(n) are the n-th components of the time-domain clean speech and of the time-domain enhanced speech signals, respectively.###It is an extension of the framework with use of stationary-state HMMs published in [13].###NS-HMMs with 4 states and 4 mixture components and of orders 0, 1, and 2 were trained with the utterance.###For the standard HMM-based enhancement systems, the models used for clean speech and noise are both AR-HMMs of di erent AR orders.###The algorithm is motivated by and is an extension of the segmental k-means algorithm developed in the past for training conventional HMMs [18].###Then, the likelihood for each pre-trained noise HMM is calculated and compared with likelihoods for the other noise HMMs and the model associated with the highest likelihood determines the selected noise model.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3562,5ecb57199e795ec6f2ba59cc,512f34906ddaefe885af2e5eec9b2b3b50ffd377,deep entity matching with pre-trained language models,5e5f7c3291e011df604ec1c3,Neural Natural Language Inference Models Enhanced with External Knowledge,"External knowledge is known to be effective in improving neural network models in NLP tasks [5, 40].",other,highlighting the effectiveness of external knowledge in improving neural network models
2837,573698486e3b12023e711478,c2fd72cb2a77941e655b5d949d0d59b01e173c3b,grarep: learning graph representations with global structural information,5550456245ce0a409eb55cee,Glove: Global Vectors for Word Representation.,"While these methods may yield good performances on some tasks, they can poorly capture useful information since they use separate local context windows, instead of global co-occurrence counts [19].",other,highlighting limitations of existing methods in capturing useful information
1690,,8a8abeb8db166d505ab1a1f7938bf4782e1d37ea,Accepting the disliked. The practice and promotion of tolerance,,,"###In the latter field, over the past sixty years knowledge has accumulated specifically around intergroup processes and prejudice reduction, the idea being that social categorization processes enhance in-group favouritism and out-group derogation (Tajfel, 1982; Tajfel & Turner, 1979; Turner, 1987).###As soon as people have the perception that they belong to a group, even if this group is arbitrarily created in a laboratory setting and does not have any previous relevance or meaning, they tend to favour the ‘own’ group over the ‘other’ (Tajfel & Turner, 1979, 1986).###The understanding that the emergence of prejudice was a fundamental and universal human process was reinforced by research on the ‘minimal group paradigm’ (Duckitt, 2001; Tajfel & Turner, 1979).###Out-group discrimination is a fundamental, universal human process, inextricably bound up with social categorization and social identification processes that occur naturally in social communities (Tajfel & Turner, 1979; Turner, 1987).###Social psychological theory on intergroup relations departs from the notion that social identification and social categorization processes define the way people evaluate others, or ‘out-group’ members (Tajfel & Turner, 1979; Turner, 1987).",impact-revealing,highlighting the foundational theories and research on intergroup processes and prejudice reduction
4003,5ede0553e06a4c1b26a841e6,9a772646ef9ed9c917f45fa592d5f89f7d5f8542,bayesian graph neural networks with adaptive connection sampling,5550443b45ce0a409eb4c39d,Stochastic Backpropagation and Approximate Inference in Deep Generative Models.,"There exist various methods that approximate BNN inference, such as Laplace approximation (MacKay, 1992), sampling-based and stochastic variational inference (Paisley et al., 2012; Rezende et al., 2014; Hajiramezanali et al., 2020; Dadaneh et al., 2020a), Markov chain Monte Carlo (MCMC) (Neal, 2012), and stochastic gradient MCMC (Ma et al.###Although the KL term is not a function of the random masks, the commonly adopted reparameterization techniques (Rezende et al., 2014; Kingma & Welling, 2013) are not directly applicable here for computing the expectation in the ﬁrst term since the drop masks are binary.###Although the KL term is not a function of the random masks, the commonly adopted reparameterization techniques (Rezende et al., 2014; Kingma & Welling, 2013) are not directly applicable here for computing the expectation in the first term since the drop masks are binary.###…approximate BNN inference, such as Laplace approximation (MacKay, 1992), sampling-based and stochastic variational inference (Paisley et al., 2012; Rezende et al., 2014; Hajiramezanali et al., 2020; Dadaneh et al., 2020a), Markov chain Monte Carlo (MCMC) (Neal, 2012), and stochastic gradient MCMC…",other,reporting various methods for approximating BNN inference
268,5f0277e911dc830562231dab,6deae79dec438eaaa524bca3b82c6b8d93553b20,A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data,5b67b45517c44aac1c8607e7,Stable Prediction across Unknown Environments.,"Figure 2 illustrates the main idea of stable feature distillation, which consists of a deep global balancing regression (DGBR) algorithm [13], a teacher network and a student network.###The current stable feature approach [13] needs much time and computing resources.",impact-revealing,describing the components of stable feature distillation
3636,5d1eb9d5da562961f0b0fa03,037aeb767ab431eeebc74a0b85df0d2f5641c652,Pre-Training With Whole Word Masking for Chinese BERT,5e8d92de9fced0a24b636bf7,Natural Questions: a Benchmark for Question Answering Research.,"As we traverse several popular machine reading comprehension benchmarks, such as SQuAD [3], CoQA [4], QuAC [5], NaturalQuestions [6], RACE [7], we can see that most of the top-performing models are based on BERT and its variants [8], [9], [10], demonstrating that the pre-trained language models…",other,highlighting the dominance of BERT-based models in machine reading comprehension benchmarks
824,5bbacbad17c44aecc4eb007e,fdb87d305c59e9ab82a96198e34e46581b838c4e,one-shot relational learning for knowledge graphs,53e9a7ddb7602d970311b83d,Translating Embeddings for Modeling Multi-relational Data.,"Although the entity embeddings from KG embedding models (Bordes et al., 2013; Yang et al., 2014) already have relational information encoded, previous work (Neelakantan et al., 2015; Lin et al., 2015a; Xiong et al., 2017) showed that explicitly modeling the structural patterns, such as paths, is…###In our experiments, we consider the following embedding-based methods: RESCAL (Nickel et al., 2011), TransE (Bordes et al., 2013), Dist-Mult (Yang et al., 2014) and ComplEx (Trouillon et al., 2016).###To automatically complete KGs, extensive research efforts (Nickel et al., 2011; Bordes et al., 2013 et al., 2014; Trouillon et al., 2016; Lao and Co-hen, 2010; Neelakantan et al., 2015; Xiong et al., 2017; Das et al., 2017; Chen et al., 2018) have been made to build relational learning models that…###For TransE, we use the code released by Lin et al. (2015b).###As TransE and DistMult use 1-###Bordes et al. (2013) proposed to model relationships in the 1-###For the other models, we have tried the code released by Trouillon et al. (2016) but it gives much worse results than TransE on our datasets.###Although the entity embeddings from KG embedding models (Bordes et al., 2013; Yang et al., 2014) already have relational information encoded, previous work (Neelakantan et al.###, 2011), TransE (Bordes et al., 2013), DistMult (Yang et al.",impact-revealing,acknowledge existing research on knowledge graph embeddings
511,57d063f6ac44367354296741,908f7931de8768786d9ef7d64f5a8156860709dd,Dynamic Pricing and Traffic Engineering for Timely Inter-Datacenter Transfers,53e9ae69b7602d9703886b6c,Incentive compatibility and dynamics of congestion control,"Incentive issues in routing have also been explored on the supply side, where individual routing nodes may act strategically [13, 15].",impact-revealing,acknowledging prior research on incentive issues in routing
3240,5edf5dd891e011bc656deb7d,14b65a86c82e38fce0eb3506e0d4084ad5cdb583,deberta- decoding-enhanced bert with disentangled attention,5aed14d617c44a4438159341,Self-Attention with Relative Position Representations.,"For an input sequence of length N , it requires a space complexity of OpN2dq [19, 20, 21] to store the relative position embedding for each token.###Existing approaches [19, 21] to relative position encoding use a separate embedding matrix to compute the relative position bias in computing attention weights.###For an input sequence of length N , it requires a space complexity of OpN(2)dq [19, 20, 21] to store the relative position embedding for each token.###It has been shown that relative position representations are more effective for natural language understanding and generation tasks [20, 21].",other,highlighting the effectiveness of relative position representations in NLP tasks
397,5b3d98b017c44a510f7ffede,bf230a74d62b98d6c16c06161f89f89626f12f12,intelligent video surveillance for real-time detection of suicide attempts,5e7345d691e0119dc5414c7a,Automated video surveillance for preventing suicide attempts,"Lately, we presented an intelligent video-based system for automated detection of suicide by hanging attempts (Bouachir and Noumeir, 2016).",impact-revealing,reporting prior findings on intelligent video-based systems for suicide detection
2372,5a9cb66717c44a376ffb89eb,651adaa058f821a890f2c5d1053d69eb481a8352,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,58d82fcbd649053542fd64c5,Adversarial Machine Learning at Scale.,"As benchmarking against iterative optimization-based attacks (e.g., Kurakin et al. (2016a); Madry et al. (2018); Carlini & Wagner (2017c)) has become standard practice in evaluating defenses, new defenses have arisen that appear to be robust against these powerful optimization-based attacks.###However, we note that (1) adversarial retraining has been shown to be difficult at ImageNet scale (Kurakin et al., 2016b), and (2) training exclusively on `∞ adversarial examples provides only limited robustness to adversarial examples under other distortion metrics (Sharma & Chen, 2017).",other,highlighting the challenges and limitations of adversarial retraining and robustness in defense evaluation
2617,5a260c0c17c44a4ba8a1e113,c751ab01aedc2888a7fe6e8b4f77ab1afa94072f,protein interface prediction using graph convolutional networks.,599c797d601a182cd2643e8a,Modeling Relational Data with Graph Convolutional Networks.,"[19] describe Relational Graph Convolutional Networks (RGCNs), which consider graphs with a large number of binary edge types, where a unique neighborhood is defined by each edge type.",other,reporting prior findings on relational graph convolutional networks
2995,5736973b6e3b12023e62b0a8,97e3bb4af723f43927317e9b9f2d794a9e398e8e,PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks,53e9ab48b7602d97034dc28f,Recurrent Neural Network Based Language Model,"Result : word embeddings (cid:126)w . while iter ≤ T do • sample an edge from E ww and draw K negative edges, and update the word embeddings; • sample an edge from E wd and draw K negative edges, and update the word and document embeddings; • sample an edge from E wl and draw K negative edges, and…",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1349,,4edfc098c9222f2f720dde04a03fe8adfd792089,M1 macrophage subtypes activation and adipocyte dysfunction worsen during prolonged consumption of a fructose-rich diet.,,,"###A recent report has described the presence of CD206 marker in M1 ATMs splitting the M1 population into M1a for CD11c CD206 cells and M1b for double positive CD11c CD206 cells [20].###Even though this marker is commonly used for M2 macrophages determination, M1b macrophages have been found to express this receptor [20].",impact-revealing,highlighting recent findings on macrophage markers
290,5e5e18ad93d709897ce2654c,19605cad33f79d3070b7c4b24aa49653ab7c90da,Inductive Matrix Completion Based on Graph Neural Networks,53e9b0c2b7602d9703b390a4,Speedup Matrix Completion with Side Information: Application to Multi-Label Learning.,"To make matrix completion inductive, Inductive Matrix Completion (IMC) has been proposed, which leverages content (side information) of users and items (Jain & Dhillon, 2013; Xu et al., 2013).",impact-revealing,highlighting the proposal of a method for matrix completion
2464,58d82fcbd649053542fd67e0,6b183d2297cb493a57dbc875689ab2430d870043,task-guided and path-augmented heterogeneous network embedding for author identification,573698486e3b12023e711440,Semantic Path based Personalized Recommendation on Weighted Heterogeneous Information Networks.,"Due to the fast emerging of such data, the problem of mining heterogeneous network has gained a lot of attention in the past few years [21, 19].###Examples include bibliographic networks [20, 22], movie recommendation networks [32] and many online social networks containing information of heterogeneous types [19].###To study such networks with multiple types of nodes and/or links, meta paths are proposed and studied [21, 23, 24, 19].###Many work has been devoted to mining heterogeneous networks in the past few years [21, 23, 24, 19].",other,highlighting the growing interest in mining heterogeneous networks
2193,5f76f20a91e011f31b98056c,645bd6eadc247989abc5e0b0aa0be79ec8b11ea6,CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,5cede12ada5629837890ab9f,Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science.,"2 The full data statement is in Appendix A (Bender and Friedman, 2018).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1763,,40e8b193fc5d8056a05735d760c30caefdbd8064,3DICT: A Reliable and QoS Capable Mobile Process-In-Memory Architecture for Lookup-based CNNs in 3D XPoint ReRAMs,,,"###But state-of-the-art mobile CNN accelerators [1, 4, 12, 18, 23, 28] cannot gracefully exploit the tradeoff between CNN test accuracy, latency and energy consumption.###The FODLAMmodel has been correlated and validated by real accelerator chips such as Eyeriss [4].###Eyeriss [4] complex CNNs 278mW [4] 1.###The ASIC-based mobile accelerator, Eyeriss [4], can achieve only 13 FPSwhen processing full precisionAlexNet tests.",impact-revealing,highlighting limitations in state-of-the-art mobile CNN accelerators
1506,,d734c855581cfbe3be89f24cc402c70850156562,The Hippo-YAP pathway in various cardiovascular diseases: Focusing on the inflammatory response,,,"###Mechanical stress, such as when cells are grown on stiff surfaces or exposed to fluid shear stress, triggers YAP and TAZ nuclear translocation (52, 53).###Then, LATS1/2 further phosphorylates the downstream transcriptional coactivators YAP and TAZ (transcriptional coactivator with a PDZ binding motif).###Thus, YAP and TAZ activation– inactivation is a dynamic process involving multiple signaling pathways.###The expression of YAP and TAZ was increased in macrophages undergoing proinflammatory or reparative phenotype changes (91), and the expression of endogenous MST1 in the cardiac macrophages of wild-type mice was decreased in the first 3 days after MI (92), suggesting the potential role of the Hippo pathway in the cardiac inflammatory response after MI. Genetic deletion of Yap/Taz in macrophages impairs the proinflammatory macrophage phenotype and promotes a reparat ive macrophage phenotype, which is accompanied by improved post-MI ventricular remodeling and heart function after MI.###The main antiatherosclerotic drugs are statins, which are cholesterol-lowering compounds and are commonly used as first-line treatments for patients with CVDs. Intriguingly, the anti-inflammatory and anti-plaque effects of statins are now being reinterpreted as being mediated, at least in part, by their capacity to inhibit YAP and TAZ (54, 148, 160).###When the Hippo-YAP pathway is at the “ON” status (red), phosphorylated MST1/2 activates the phosphorylation of LATS1/2, which in turn phosphorylate and promote the degradation of the YAP and TAZ.###Intriguingly, the anti-inflammatory and anti-plaque effects of statins are now being reinterpreted as being mediated, at least in part, by their capacity to inhibit YAP and TAZ (54, 148, 160).",impact-revealing,highlighting the role of YAP and TAZ in cardiac inflammatory response and the effects of statins
1571,,5956938472b5c458604525f1a4fb8b05ce70158d,MD-10-2019-1464_proof 1664..1683,,,"###In the analysis of this heuristic, we build on the adaptive toolbox perspective (Gigerenzer, 2001), according to which individuals intuitively evaluate whether and to what extent a given heuristic fits a specific environment, i.e. its ecological rationality (Gigerenzer, 2008).###We believe that our paper can be considered one of the first attempts to bridge dualprocess theories (Sloman, 1996) and the adaptive toolbox perspective (Gigerenzer, 2008) into an integrative framework that offers a nuanced perspective on the use of representativeness in the decision-making process.###The full terms of this licence may be seen at http://creativecommons.org/licences/by/4.0/legalcode
Management Decision Vol. 59 No. 7, 2021 pp. 1664-1683 Emerald Publishing Limited 0025-1747 DOI 10.1108/MD-10-2019-1464
(e.g. Gigerenzer, 2008).###44): heuristics make guesses on the environment based on past experience or limited search, thus leading to choose the option that exceeds an aspiration level without attempting any exhaustive, optimization-oriented analysis (Gigerenzer, 2008).###When heuristics reduce the chances of overfitting of past information, they are said to be robust (Gigerenzer, 2008): robustness may be spurred by either deliberately ignoring some pieces of information or by cognitive limitations such as forgetting.###As a consequence, a large and heterogeneous sample of past experiences may increase search costs and, even if information would cost nothing, in conditions of uncertainty, cognitive processes should still ignore a portion of that information (Gigerenzer, 2008).###…optimal cost-benefit tradeoff” (Gigerenzer, 2001, p. 44): heuristics make guesses on the environment based on past experience or limited search, thus leading to choose the option that exceeds an aspiration level without attempting any exhaustive, optimization-oriented analysis (Gigerenzer, 2008).###Building on the perspective that the human mind is equipped with an adaptive toolbox of heuristics that are composed of building blocks (Gigerenzer, 2008), we explore the foundations of representativeness.###In fact, according to the “fast and frugal” perspective, the human mind is equipped with an adaptive toolbox of heuristics, composed of building blocks, that can be adjusted and adapted to fit a given situation (Gigerenzer, 2008).###Information derived from experience includes both information that is relevant for the current decision problem and information that is irrelevant, i.e. noise (Gigerenzer, 2008).###Gigerenzer (2008) suggested that the quantity of noise is a function of the difficulty to predict a criterion: the greater the uncertainty, the greater the portion of past information that is potentially irrelevant.###In general terms, Gigerenzer (2008) suggests that heuristics are composed of three building blocks, i.e. search rules, stopping rules and decision rules, which fulfill three main functions, namely giving search directions, stopping the search and making a decision, respectively.",impact-revealing,highlighting the integration of dual-process theories and adaptive toolbox perspective in decision-making
2110,,3d5c960f894789fe62f93becb55abb644b973a32,Information-based optimal subdata selection for big data logistic regression,,,"###Combining the methods of subsampling [6] and bootstrapping [7, 8], [9] proposed a novel approach called bags of little bootstraps (BLB) to achieve computational eﬃciency.",impact-revealing,reporting a novel approach for computational efficiency
1123,,c290754d5a28dc5b2c34afbf8cdba0e015f6d1f5,Learning A Physical-aware Diffusion Model Based on Transformer for Underwater Image Enhancement,,,"###Diffusion Probabilistic Models (DPMs) [18, 41] have been widely adopted for conditional image generation [10, 26, 47, 60, 61].###Recently, there has been a surge of interest in image synthesis [26, 27, 38, 40, 60] and restoration tasks [10, 47, 54, 61], with a notable focus on diffusion-based techniques like DDPM [18] and DDIM [41].",impact-revealing,highlighting the growing interest and application of diffusion probabilistic models in image generation and restoration
2276,5f7ee07491e011a5faf0feb2,c54fb59319288d0d17ad09e6b7dfee6d042f83a8,Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer,5550432045ce0a409eb46029,"DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia.","1 3 10 1 3 10 1 3 10 1 (EL) DBpedia (Lehmann et al., 2015).###English DBpedia (Lehmann et al., 2015) only records its genre as Monogatari (story), whereas Japanese DBpedia identiﬁes more genres, including Love Story , Royal Family Related Story , Monogatari and Literature-Novel .###This has been de facto achieved in a number of inﬂuential knowledge bases, including DBpedia (Lehmann et al., 2015), Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) and YAGO (Re-bele et al., 2016).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
806,5c04967517c44a2c7470927f,b9015d4f1e591eba7ea21c3566c919f80f7c2afe,attentive long short-term preference modeling for personalized product search,5a260c3517c44a4ba8a25227,NeuPL: Attention-based Semantic Matching and Pair-Linking for Entity Disambiguation.,"Attention mechanisms have shown its efficiency in various tasks such as image captioning [6, 66], visual question answering [65, 67], machine translation [2] and information retrieval [5, 34, 47, 64].",impact-revealing,highlighting the efficiency of attention mechanisms in various tasks
3760,5e79da4491e0115bb1157b77,8e74106415626a21bb909ff489ef310625c769e1,Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles,5ceddfb7da5629837879485f,Scientific Paper Recommendation: A Survey.,"Despite their success in NLP, Transformers have gained little attention in the recommender system community so far and are not even mentioned in a recently published survey [5].",other,highlighting the lack of attention to Transformers in recommender systems
782,5e5e190893d709897ce48240,76b8d5f2ef97d71167aa78309918bf3f7d633c96,Author Name Disambiguation on Heterogeneous Information Network with Adversarial Representation Learning,59939d49ffdae9cf10039e6f,IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models,"IRGAN (Wang et al. 2017) unifies generative model and discriminative model in information retrieval, where the discriminative model provides guidance to the generative model, and the generative model generates difficult examples for the discriminative model.",impact-revealing,describing the IRGAN model and its components in information retrieval
3806,573698456e3b12023e70ee1b,524664475292ad6cdbdda3992fe5dc8f036b6ce5,Deep learning for emotion recognition on small datasets using transfer learning,5550488e45ce0a409eb6f5c4,Emotion Recognition in the Wild with Feature Fusion and Multiple Kernel Learning.,In [3] HOG-TOP and audio features were fused using multiple kernel learning.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
23,53e9a710b7602d970304482e,941f318e41147773ae69d9da4f8de9b8dbea70f4,Learning semantic representations using convolutional neural networks for web search,53e99beab7602d970249335e,Natural Language Processing (almost) from Scratch,"In this study, based on a convolutional neural network [1], we present a new Convolutional Deep Structured Semantic Models (C-DSSM).",impact-revealing,introducing a new model based on convolutional neural networks
3909,5b3d98cc17c44a510f80212a,cb91c2f8d3cac0b655a39be318b603334eb18987,learning to optimize tensor programs,573696046e3b12023e517cb1,XGBoost: A Scalable Tree Boosting System,"We use XGBoost [7], which has proven to be a strong feature-based model in past problems.",other,reporting prior findings on the effectiveness of XGBoost
382,58d82fcbd649053542fd5d36,81db3f78f346eecf2f378070712feade6d45d6b1,MOLIERE: Automatic Biomedical Hypothesis Generation System,53e9983db7602d9702065035,Latent dirichlet allocation,"LatentDirichletAllocation [8] is the most common topic modeling process and PLDA+ is a scalable implementation of this algorithm [20, 28].###We use natural language processing methods, such as Latent Dirichlet Allocation (LDA) [8] and topical phrase mining [16], along with other data mining techniques to KDD 2017 Applied Data Science Paper KDD’17, August 13–17, 2017, Halifax, NS, Canada",impact-revealing,reporting common topic modeling processes and their implementations
3830,5eccb534e06a4c1b26a8358b,8a8a5f327ead63fa56d72e8e75a647e3e6154bc8,Residual Feature Aggregation Network for Image Super-Resolution,573696046e3b12023e517e10,Accurate Image Super-Resolution Using Very Deep Convolutional Networks,"designed deeper VDSR [13] and DRCN [14] with 20 layers based on residual learning.###9 shows the comparisons about model size and performance with 11 stae-of-the-art SR methods:SRCNN [4], FSRCNN [5], VDSR [13], LapSRN [15], MemNet [25], NLRN [19], SRMD [36], DBPN [6], RDN [40], RCAN [38] and SAN [3].###We compare our RFANet with 10 state-of-the-art methods: SPMSR [22], SRCNN [4], FSRCNN [5], VDSR [13], IRCNN [35], SRMD [36], RDN [40], SRFBN [17], RCAN [38], and SAN [3].###Many image SR methods have been proposed to tackle this inverse problem, including early interpolationbased [37], reconstruction-based [34], and recent learning based methods [27, 28, 22, 4, 12, 13, 36, 3].###further increased the depth to 20 in VDSR [13] and DRCN [14] by introducing residual learning to ease the training difficulty.###ness of our RFANet, we compare RFANet with 12 stateof-the-art image SR methods: SRCNN [4], FSRCNN [5], VDSR [13], LapSRN [15], MemNet [25], EDSR [18], SRMD [36], NLRN [19], DBPN [6], RDN [40], RCAN [38] and SAN [3].",other,comparing performance of various super-resolution methods
752,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,557c15f5f6672d70710edb0e,Deep-dive analysis of the data analytics workload in CloudSuite,"[63] perform a mi-###We provide insight into the root causes of relatively low IPC using the Top-down Microarchitecture Analysis Method (TMAM) [63] to categorize processor pipelines’ execution stalls, as reported in Fig.###ies [21, 22, 63, 96, 97].",impact-revealing,providing context for analyzing processor pipelines
2516,5e5e189993d709897ce1e202,2bf7c350a8280e7c593d46a60127f99b21517121,on the variance of the adaptive learning rate and beyond,5d3ed2653a55ac61d998598b,RoBERTa: A Robustly Optimized BERT Pretraining Approach,"Our experiments are based on the default Transformers (Vaswani et al., 2017) implementation from the fairseq package (Ott et al., 2019).###This further demonstrates that the convergence problem can be alleviated by reducing the variance of the adaptive learning rate, and also explains why tuning (cid:15) is important in practice (Liu et al., 2019).",other,reporting experimental setup and findings related to Transformers
3786,53e9b8fcb7602d97044e29ac,48536fdbbc79ddf163901c7e63bb70b6f64802e0,RDIP: Return-address-stack Directed Instruction Prefetching,53e9afd3b7602d9703a24a4b,Dead-Block Prediction & Dead-Block Correlating Prefetchers,"It is important to note that execution-path based correlation has been shown to be effective for branch prediction [23], dead-block prediction [17] and last touch prediction [16], but none of these leverage the RAS to make their respective predictions.",other,highlighting the effectiveness of execution-path based correlation in various predictions
2852,5fae6daad4150a363cec035c,413117826eecb3fd1491e0665e4b644a521d3bc3,Spanet: Spatial Pyramid Attention Network for Enhanced Image Recognition,599c7958601a182cd2632c6c,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.,"Thus, we modify SPANetB by adding a point-wise convolutional layer [19] at the beginning of the attention path if C′ 6= C.###MobileNetV2 is typically designed for lightweight models like [19,27,28].",other,describing modifications to a model architecture
1666,,d1635ebb1e4f28ea4b96587ab51bf5a3d9582fbe,Exploring How Employee Sense of Brand Community Affects Their Attitudes and Behavior,,,"###This belief to performance link is further reinforced by an employee’s self-enhancement needs (e.g., employees seek to have positive feelings about what they do; Tajfel et al., 1979).",impact-revealing,highlighting the connection between performance and self-enhancement needs
2766,5c8c52bc4895d9cbc6ddad8d,76e4d56d712d64ec2f77fd5b2fcb504888c07eab,Island loss for learning discriminative features in facial expression recognition,5550460545ce0a409eb5b67c,Facial Expression Recognition via a Boosted Deep Belief Network,"Benefiting from the advance in feature learning, features can be learned either unsupervised by sparse coding [25], [59], [35], [29] or supervised by deep learning [36], [44], [24], [5], [8], [30], [21], [15], [22], [58], [9], [51], [3], [49],",other,acknowledge advancements in feature learning methods
122,5f58a1b491e011e46ee73247,435bc42450259a22cfba92b40217b8d26f4a7ed5,Adversarial Attack on Large Scale Graph,5b67b47917c44aac1c863824,Adversarial Attack on Graph Structured Data.,"So far, much of the current work on attacking graph neural networks has focused on the node classiﬁcation task [12], [13], [14], [15].###Since a number of existing works [12], [13], [15], [26] use vanilla GCN [8] as a surrogate model to conduct attacks, thus we ﬁrst introduce GCN and further draw attention on SGC [18] — a simpliﬁed variant of GCN. Refer to this work [8], GCN is recursively deﬁned as where ˜ A = A + I N is the…###Since we focus on the targeted attack in node classiﬁcation task, here we brieﬂy introduce other proposed state-of-the-art targeted attack methods: Nettack [12] and GradArgmax [13].###Inspired by Simpliﬁed Graph Convolutional Network (SGC) [18] and gradient-based attack methods [13], [15], we propose a novel Simpliﬁed Gradient-based Attack (SGA) framework for effective and efﬁcient adversarial attacks.###• GradArgmax [13].###Focusing on the targeted attack, Dai et al. [13] propose GradArgmax, which extracts gradients of the surrogate model and deletes edges with the largest magnitude of the gradient to generate adversarial examples.###In this scenario, Dai et al. [13] study the adversarial attack on graph structure data and propose a gradient-based method, namely GradArgmax, which modiﬁes links based on gradients information of a surrogate model so as to fool the classiﬁers.",impact-revealing,highlighting the focus on adversarial attacks in graph neural networks
3599,5cede0fada562983788d93ae,3a4d79c5c2646051753ff0d98f57c08604299aa2,"TensorFlow Eager: A Multi-Stage, Python-Embedded DSL for Machine Learning",53e9b2efb7602d9703daa174,OptiML: An Implicitly Parallel Domain-Specific Language for Machine Learning.,"…a thorough treatment of multi-stage programming that is more formal than ours (DeVito et al., 2013); as another example, OptiML is a Scala-embedded DSL for machine learning with support for staging and code generation but without support for automatic differentiation (Sujeeth et al., 2011).",other,acknowledging existing work in multi-stage programming and DSL for machine learning
2631,5ee8986891e011e66831c3bc,965652c0e426c5b42d7218d7429025be7ac542bf,DeeperGCN: All You Need to Train Deeper GCNs,5bdc31b417c44a1f58a0b9f9,On the Properties of the Softmax Function with Application in Game Theory and Reinforcement Learning.,"Energy-Based Learning [LeCun et al., 2006], Knowledge Distillation [Hinton et al., 2015] and Rein-forcement Learning [Gao and Pavel, 2017].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1369,,f347a107626925a4fd7feb45dfeb9dd65ee76745,The second and third optic ganglia of the worker bee,,,"###These early publications were extended by extensive and most careful examinations o f the optic ganglia o f the fly (Braitenberg 1967; 1970; Braitenberg and Strausfeld 1973; Strausfeld 1970a; 1971a, b; 1976a, b; Strausfeld 1970; Strausfeld and Braitenberg 1970) using material stained with modified…",impact-revealing,acknowledging foundational research in optic ganglia studies
1578,,7f0f859a2a975613aa7dfa0aa1edf49c05903459,Antarctic ice-sheet balance velocities from merged point and vector data,,,"###In a related approach, Costa-Cabral and Burges (1994) treat each cell as an initial condition wherein the flux entering the cell is simply the surface accumulation rate (advected fluxes are not included in the flux of the cell chosen for the start of the calculation).###The amount of mass received by cell B (Figure 1b) is determined by the careful downslope tracing of flow lines as described in the original Costa-Cabral and Burges (1994) approach.###We use the Costa-Cabral and Burges (1994) fitting plane algorithm to derive flow direction from the DEM.###11 We modify the downslope algorithm (Costa-Cabral and Burges, 1994) by focusing on the influence matrix aspect of their model but abandoning explicit calculation of flow lines.###We favor the Costa-Cabral and Burges (1994) partition scheme wherein partitioning is proportional to areas defined by segmenting the cell into two regions separated by the flow direction vector.###Our analysis builds on previous research (Budd and others, 1971; Budd and others, 1982; Budd and Smith, 1985; Budd and Warner, 1996; Bamber and others, 2000a, 2000b; Huybrechts and others, 2000) by adapting and modifying an algorithmic approach developed in the hydrology community (Costa-Cabral and Burges, 1994).",impact-revealing,describing the adaptation and modification of a hydrology algorithm
3811,5eede0b791e0116a23aafe75,150f95f9c73820e0a0fa1546140e9f2bdfd25954,temporal graph networks for deep learning on dynamic graphs,5d3ed25a275ded87f97dea92,Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks,"Reddit and Wikipedia are bipartite interaction graphs.###This version of the embedding method was used in JODIE (36).###In the Reddit dataset, users and sub-reddits are nodes, and an interaction occurs when a user writes a post to the sub-reddit.###We test on the following datasets: bipartite dynamic interaction graphs from Wikipedia and Reddit (36) with nodes representing users and items (subreddits and pages, respectively) and edges interactions among these, and non-bipartite Twitter graph (6) with nodes representing users and edges retweets.###For example, Jodie [36] uses the time projection embedding module emb( i, t ) = (1+∆ t w ) ◦ s i ( t ) .###This version of the embedding method was used in JODIE [36].###Our strong baselines are state-of-the-art approaches for continuous time dynamic graphs (CTDNE [47], Jodie [36], and TGAT [66]) as well as state-of-the-art models for static graphs (GAE [34], VGAE [34], DeepWalk [51], Node2Vec [23], GAT [61] and GraphSAGE [27]).###Due to the efﬁcient parallel processing and the need for only one graph attention layer (see section 5.3 for the ablation study on the number of layers), our model is up to 3 × faster than Jodie and about 19 × faster than TGAT to complete a single epoch (see Figure 3a), while requiring a similar number of epochs to converge.###For Jodie [36], we implement our own version in PyTorch, as a speciﬁc case of our framework with the temporal embedding module, and the t-batch training algorithm.###Moreover, only few of these approaches support the inductive setting of generalizing to new nodes not seen during training (46; 3; 59; 36).###Our strong baselines are state-of-the-art approaches for continuous time dynamic graphs (CTDNE (47), Jodie (36), and TGAT (65)) as well as state-of-the-art models for static graphs (GAE (34), VGAE (34), DeepWalk (51), Node2Vec (23), GAT (61) and GraphSAGE (27)).###For the Wikipedia and Reddit datasets, we use Adam optimizer with a learning rate of 0 .###We use three datasets in our experiments: Wikipedia, Reddit [36], and Twitter.",other,acknowledge the use of various datasets and methods in dynamic interaction graph analysis
1794,,1526d50067a80d0a5290c5f10f43c418a9593f99,“Heterogeneous couplings”: Operationalizing network perspectives to study science‐society interactions through social media metrics,,,"###We also build on notions of heterogeneous information networks (Shi, Li, Zhang, Sun, & Yu, 2017), which are essentially conformed by “ multiple types of objects as well as multiple types of links, indicating different sorts of interactions among these objects ” (Sun & Han, 2013).",impact-revealing,providing context on heterogeneous information networks
492,58437785ac44360f108432a7,92527ace7f75188b5ec209ff7d59f431343075e4,Video-based emotion recognition using CNN-RNN and C3D hybrid networks,58d83020d649053542fe37ca,Fusing Aligned and Non-aligned Face Information for Automatic Affect Recognition in the Wild: A Deep Learning Approach,The reason may be that the training data is too limited.,impact-revealing,providing context for potential limitations in training data
472,5550414c45ce0a409eb39fa8,081651b38ff7533550a3adfc1c00da333a8fe86c,How transferable are features in deep neural networks?,53e9b017b7602d9703a6e637,Deep learning of representations for unsupervised and transfer learning,"We are interested in the answers to these questions because, to the extent that features within a network are general, we will be able to use them for transfer learning (Caruana, 1995; Bengio et al., 2011; Bengio, 2011).",impact-revealing,highlighting the importance of general features for transfer learning
4013,5dbebb7447c8f766462c2328,b0d941cfa0a3c43703e5222221addb1ad0f9e68d,Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks,53e9bd59b7602d97049f3661,A New Model For Learning In Graph Domains,"Initially, GNN is applied to the simple situation on directed graphs [4, 26].",other,providing context for the application of GNN
3013,5c04967517c44a2c74709321,54c4642d017830e1faddbb49f0377228d2b01493,HAQ: Hardware-Aware Automated Quantization With Mixed Precision,5d9edbde47c8f7664602a4f4,Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks,"Besides industry, recently academia also works on the bit-level flexible hardware design: BISMO [27] proposed the bit-serial multiplier to support multiplications of 1 to 8 bits; BitFusion [26] supports multiplications of 2, 4, 8 and 16 bits in a spatial manner.###In order to demonstrate the effectiveness of our framework on different hardware architectures, we further compare our framework with PACT [3] under the latency constraints on the BitFusion [26] architecture (Table 4).###(HW1: BitFusion [26], HW2: BISMO [27] edge accelerator, HW3: BISMO cloud accelerator, batch = 16).###[26] is a state-of-the-art spatial ASIC design for neural network accelerator.",other,highlighting advancements in bit-level flexible hardware design
592,53e99804b7602d970201668d,5bda0d60efb98013537d9edd9edfaf59fe809e7b,fetching instruction streams,53e9ba64b7602d9704681e41,A trace cache microarchitecture and evaluation,"We compare our stream fetch architecture with three other state-of-the-art fetch architectures: the FTB architecture [30] using a perceptron branch predictor [18], the Alpha EV8 architecture using a 2bcgskew predictor [34], and the trace cache architecture using a trace predictor [32] and selective trace storage [29].###In Section 2 we discuss previous related work, including state of the art fetch architectures like the FTB proposed by Reinman, Austin and Calder [30] and the trace cache architecture as proposed by Rotenberg, Bennett and Smith in [32].###trace cache mechanism as proposed by Rotenberg, Benett and Smith in [32].###The trace cache [9, 23, 31, 32] is one such high fetch width mechanism, recently implemented in the Pentium4 processor[14] 1.",impact-revealing,comparing different fetch architectures and discussing related work
2189,5aed14e217c44a4438159a0f,8f684080d2b81d3178d681d6917cb077c082a9e1,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,5550472145ce0a409eb64ae3,Learning A Deep Convolutional Network For Image Super-Resolution,"[3, 4] ﬁrst exploit a three-layer convolutional neural network, named SRCNN, to jointly optimize the feature extraction, non-linear mapping and image reconstruction stages in an end-to-end manner.###[3, 4] first exploit a three-layer convolutional neural network, named SRCNN, to jointly optimize the feature extraction, non-linear mapping and image reconstruction stages in an end-to-end manner.###We compare the proposed method with other SR meth-ods, including bicubic, SRCNN [3, 4], VDSR [12], DRCN [13], LapSRN [15], DRRN [22] and MemNet [23].###We compare the proposed method with other SR methods, including bicubic, SRCNN [3, 4], VDSR [12], DRCN [13], LapSRN [15], DRRN [22] and MemNet [23].",other,acknowledge prior work on convolutional neural networks for image reconstruction
2172,,ebf577aa0cd69fb07cdb9238fa2131c9108f917d,"Plateforme ouverte, évolutive, sécurisée et orientée utilisateur pour l'e-commerce. (Open, scalable, secure and user-centric platform for e-commerce)",,,"###) Cloud computing embraces cyber infrastructure and builds upon decades of research in virtualization, distributed computing, grid computing, utility computing, and more recently networking, web and software services » [Vouk 2004]",impact-revealing,providing context on the evolution of cloud computing
1131,,a544c7ae92184a0f26b9d6bb3d6a82ef75e71b33,Score-Based Generative Models for PET Image Reconstruction,,,"###The latter can be achieved by Euler-Maruyama schemes or predictor-corrector methods (Song et al., 2021c).###Denoising Di ff usion Implicit Models (DDIMs) (Song et al., 2021a) were introduced to allow faster sampling, and build upon a result by Tweedie (Efron, 2011) to approximate the expectation E [ x 0 | x t ] via the score model s θ ( x t , t ) as where the positive scalars γ t and ν 2 t are the coe…###SGMs have emerged as a state-of-the-art method for modelling, and sampling from, high-dimensional image distributions (Song et al., 2021c).###We choose η t k = ηβ t k with a hyperparameter η ∈ [0 , 1], controlling the amount of stochasticity in the sampling, and (Song et al., 2021a).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1961,,b7b1e72eb838c9cfb76bdf132c3891b8dba4ab36,Efficient Remote Photoplethysmography with Temporal Derivative Modules and Time-Shift Invariant Loss,,,"###Nevertheless, since most of the existing databases employed finger pulse oximeter to record PPG signals [2, 21, 32, 46], they did not consider the physiological offset or PTT between the facial and finger blood flow [19].",impact-revealing,highlighting limitations in existing databases for PPG signal recording
468,599c795d601a182cd2635171,7743150b3fe21b5cc2ebe9e8a67f54031311f7ae,Smart Mining for Deep Metric Learning,573696086e3b12023e51c1cc,Deep Metric Learning via Lifted Structured Feature Embedding,"∗The first two authors contributed equally to this work the presence of an extremely large number of classes (more than 10(5) classes) and low number of samples per class (in [10(1), 10(2)]), where the implementation of traditional classifiers becomes challenging [19, 13].###The development of deep metric learning models for the estimation of effective feature embedding [2, 4, 9, 11, 15, 16, 17, 13, 22, 25, 27, 26] is at the core of many recently proposed computer vision methods [3, 14, 19, 24, 28].###We set the embedding size to 64 [19] and the learning rate for the randomly initialized fully connected layer is multiplied by 10 to achieve faster convergence similar to [19].###For all our experiments, we initialize the network with pre-trained GoogLeNet [21] weights and randomly initialize the final fully connected layer similar to [19].###our proposed method, and (5) ), we use the same training and test set split described in [19] across all datasets.###[19] to design of a new loss function that integrates all positive and negative samples to form a lifted structured embedding.###For the experiments, we follow the protocol used in previous papers [18, 19, 13], which uses unseen classes from the CUB- 200-2011 [23] and Cars196 [8] datasets in order to assess the clustering quality and k nearest neighbour retrieval [7].###, the adaptive controller) is compared with the following state-of-the-art deep metric learning approaches: (1) triplet learning with semihard negative mining [14] (with and without FANNG [5]), (2) lifted structured embedding [19], (3) N-pairs metric loss [18], (4) clustering [13], and (5) triplet combined with global loss [9].",impact-revealing,highlighting challenges in traditional classifiers due to class imbalance and proposing deep metric learning as a solution
1415,,5b37ecab1039b50102fac9e11dd02b0158ef742c,How to Leverage Unlabeled Data in Offline Reinforcement Learning,,,"###On the AntMaze domain, following prior works (Kumar et al., 2020; Yu et al., 2021a), we use the Lagrange version of CQL, where the coefficient β is automatically tuned against a pre-specified constraint value on the CQL loss equal to τ = 10.###Following (Kalashnikov et al., 2021; Yu et al., 2021a), we use sparse rewards for each task.###For example, one can choose to reweight unlabeled data with the CDS scheme of Yu et al. (2021a), which preferentially upweights transitions based on their conservative Q-values.###Prior works (Kumar et al., 2020; Yu et al., 2021a) have shown that the optimal policy π∗ that optimizes Equation 2 attains a high probability safe-policy improvement guarantee, i.###Prior works (Kumar et al., 2020; Yu et al., 2021a) have shown that the optimal policy π ∗ that optimizes Equation 2 attains a high probability safe-policy improvement guarantee, i.e., J ( π ∗ ) ≥ J ( π β ) − ζ , where ζ is: The ﬁrst term in Equation 3 corresponds to the decrease in performance due…###Note that conservative Q-values refer to the Q-value for a given policy corresponding to a modiﬁed reward function r ( s , a ) − απ ( a | s ) · ( π ( a | s ) /π β ( a | s ) − 1) , computed on the empirical MDP.###To prove our theoretical results, following prior work (Kumar et al., 2020; Yu et al., 2021a) we assume that the empirical rewards and dynamics concentrate towards their mean.###Following (Yu et al., 2021a), we modify the datasets introduced by Fu et al. (2020) by equally dividing the large dataset into different parts for different tasks, where each task corresponds to a different goal position.###, 2021; Fujimoto & Gu, 2021), conservative value functions (Kumar et al., 2020), and model-based training with conservative penalties (Yu et al.###Formally, CDS is given by: where s j , a j , s j denote the transition from D j , r i denotes the reward of s j , a j , s j relabeled for task i , π denotes the task-conditioned policy π ( ·|· , i ) , ∆ π ( s j , a j ) is the condition that shares data only if the expected conservative Q-value of the relabeled transition exceeds the top k -percentile of the conservative Q-values of the original data.###To compute this difference, we follow the following steps Following Kumar et al. (2020) (Theorem 3.6), we can bound the second term ∆ 2 using: To upper bound ∆ 1 , we utilize the reward upper bound from Equation 5: Combining the results so far, we obtain, for any policy π : Lower bounding (cid:98)…###Following (Yu et al., 2021a), we use the antmaze-medium-play and antmaze-large-play datasets from D4RL (Fu et al., 2020) and partitioning the datasets into multi-task datasets in an undirected way deﬁned in (Yu et al., 2021a).###Our theoretical result builds on techniques for showing safe policy improvement bounds (Laroche et al., 2019; Kumar et al., 2020; Yu et al., 2021a).###, 2019) or DCQL from conservative Q-values (Kumar et al., 2020)) between the learned policy π and the effective behavior policy π β .###, 2019) or DCQL (Kumar et al., 2020)) between the learned policy π and the behavior policy πβ computed in expectation over the marginal state-action distribution induced by the policy in the empirical MDP induced by the dataset:###For our analysis, we will abstract conservative offline RL algorithms into a generic constrained policy optimization problem (Kumar et al., 2020):###…) denotes a divergence measure (e.g., KL-divergence (Jaques et al., 2019; Wu et al., 2019), ﬁsher divergence (Kostrikov et al., 2021), MMD distance (Kumar et al., 2019) or D CQL from conservative Q-values (Kumar et al., 2020)) between the learned policy π and the effective behavior policy π eﬀ β .###Then we can use the derivation in proof of Theorem 3.6 in Kumar et al. (2020) or Equation 21 from Achiam et al. (2017), to bound this difference as .###Therefore, the policy optimization objective in Sharing All can be written as follows: where π eﬀ β ( a | s , i ) is the effective behavior policy for task i denoted as π eﬀ β ( a | s , i ) denotes the average return of policy π in the empirical MDP induced by the effective dataset, and D ( π, π eﬀ β ) denotes a divergence measure (e.g., KL-divergence (Jaques et al., 2019; Wu et al., 2019), ﬁsher divergence (Kostrikov et al., 2021), MMD distance (Kumar et al., 2019) or D CQL from conservative Q-values (Kumar et al., 2020)) between the learned policy π and the effective behavior policy π eﬀ β .###Sharing data across different tasks has been found to be effective in multi-task (Eysenbach et al., 2020; Kalashnikov et al., 2021; Yu et al., 2021a) and meta-RL (Dorfman et al., 2021; Mitchell et al., 2021) and it improves performance signiﬁcantly in multi-task ofﬂine RL. Prior works share data based on learned Q-values (Eysen-bach et al., 2020; Li et al., 2020; Yu et al., 2021a), domain knowledge (Kalashnikov et al., 2021) and distance to goals in goal-conditioned settings (Andrychowicz et al., 2017; Liu et al., 2019; Sun et al., 2019; Lin et al., 2019; Chebotar et al., 2021), and the learned distance with robust inference in the ofﬂine meta-RL setting (Li et al., 2019).###…and D ( π, π β ) denotes a divergence measure (e.g., KL-divergence (Jaques et al., 2019; Wu et al., 2019), MMD distance (Kumar et al., 2019) or D CQL (Kumar et al., 2020)) between the learned policy π and the behavior policy π β computed in expectation over the marginal state-action…###Similar to prior work (Kumar et al., 2020; Yu et al., 2021a), we also make a coverage assumption, i.###…et al., 2020; Wu et al., 2019; Kumar et al., 2019; Zhou et al., 2020; Ghasemipour et al., 2021; Fujimoto & Gu, 2021), conservative value functions (Kumar et al., 2020), and model-based training with conservative penalties (Yu et al., 2020c; Kidambi et al., 2020; Swazinna et al., 2020; Lee et al.,…###On the AntMaze domain, following prior works (Kumar et al., 2020; Yu et al., 2021a), we use the Lagrange version of CQL, where the coefﬁcient β is automatically tuned against a pre-speciﬁed constraint value on the CQL loss equal to τ = 10 .###On the hopper domain, when the unlabeled data is random, we use the version of CQL that does not maximize the term E s , a ∼D L ∪D U ˆ Q ( s , a ) to prevent overestimating Q-values on low-quality random data and use β = 1 .###For our analysis, we will abstract conservative ofﬂine RL algorithms into a generic constrained policy optimization problem (Kumar et al., 2020): (2) J D ( π ) denotes the average return of policy π in the empirical MDP induced by the transitions in the dataset, and D ( π, π β ) denotes a…###To compute this difference, we follow the following steps Following Kumar et al. (2020) (Theorem 3.6), we can bound the second term ∆ 2 using: To upper bound ∆ 1 , we utilize the reward upper bound from Equation 5: Combining the results so far, we obtain, for any policy π : Lower bounding (cid:98) J ( π ) − J ( π ) .###Similar to prior work (Kumar et al., 2020; Yu et al., 2021a), we also make a coverage assumption, i.e., we assume that each state-action pair is observed in the dataset D , but the rewards and transition dynamics are stochastic, so, the occurrence of each state-action pair does not trivially imply…",impact-revealing,acknowledge existing methods and approaches in reinforcement learning
1644,,896ce047e71e53ddb78d8bce0fb7f2f967153dab,Exploring the Impact of Social Identity on the Bullying of Construction Industry Apprentices,,,"###Social identity theory (SIT) has been widely used by researchers [20–23] to emphasise important psychological group processes and suggests that people incorporate important group memberships into their self-concepts.###These results were supported by the ﬁrst tenet of SIT, where individuals will join groups if membership is likely to maintain or improve their self-concept [20].",impact-revealing,highlighting the significance of social identity theory in understanding group processes
899,53e9a775b7602d97030b16b6,bcbef8e1945b432a474089796300f2d20d011d6a,Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers,53e9a689b7602d9702fbd82d,An Analysis of the Performance Impact of Wrong-Path Memory References on Out-of-Order and Runahead Execution Processors,All the mentioned effects are modeled correctly and bandwidth limitations are enforced in our model as described in [16].,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3553,5ac1827b17c44a1fda915855,f96a5a9cfa1dbb01df6df749e093c0bad20240a8,Control Flow Checking at Virtual Edges.,53e9997db7602d97021bb942,Detecting LDoS Attacks based on Abnormal Network Traffic.,"D istributed computing and worldwide business transactions over open networks, such as the Internet, increasingly demand for secure communication and secure operation due to rising online fraud and software attacks [1].",other,highlighting the need for secure communication in distributed computing and online transactions
2352,5cede0e8da562983788c741f,b56e5fb4f367a8d54614f1047bd4f9a2d58b9973,Multi-Interest Network with Dynamic Routing for Recommendation at Tmall,5843774bac44360f108397c4,Deep Neural Networks For Youtube Recommendations,"Thus, we use the sampled softmax technique [7] to make the ob-jective function trackable and choose the Adam optimizer [16] for training MIND.###• YouTube DNN [7] As mentioned above, YouTube DNN is one of the most successful deep learning method used for industrial recommendation systems.###For example, the deep neural network proposed for YouTube video recommendation (YouTube DNN) [7] represents each user by one fixed-length vector transformed from the past behaviors of users, which can be a bottleneck for modeling diverse interests, as its dimensionality must be large in order to…###Besides the industrial applications proposed by [7, 31], various types of deep models have gained significant attention.",other,acknowledging the significance of YouTube DNN in recommendation systems
3121,5fe4094e9e795e14f30e634a,94497472eecb7530a2b75c564548c540ebd61e9b,Learning to Pre-train Graph Neural Networks,599c7974601a182cd263f01c,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,"Furthermore, our strategy is a form of meta-learning, in particular, model agnostic meta-learning (MAML) (Finn, Abbeel, and Levine 2017).###Speciﬁcally, MAML learns a prior that can be quickly adapted to new tasks by one or a few gradient updates, so that the prior, after being adapted to the so-called support set of each task, can achieve optimal performance on the so-called query set of the task.###We ﬁrst present a self-supervised base GNN model for learning graph structures in the MAML setting, followed by our dual node-and graph-level adaptations designed to simulate ﬁne-tuning during the pre-training process.###The proposed learning to pre-train can be deemed a form of meta-learning (Finn, Abbeel, and Levine 2017), also known as learning to learn.###Finally, some optimization-based methods directly adjust the optimization algorithm to enable quick adaptation with just a few examples (Finn, Abbeel, and Levine 2017; Yao et al. 2019; Lee et al. 2019; Lu, Fang, and Shi 2020).###In our case, the output of our pre-training θ 0 is the prior knowledge that can quickly adapt to new downstream tasks, while D tr T G and D te T G correspond to the support and query sets in MAML, respectively.###Speciﬁcally, our approach can be formulated as a form of MAML.",other,describing a meta-learning approach and its application in the proposed method
469,599c795d601a182cd2635171,7743150b3fe21b5cc2ebe9e8a67f54031311f7ae,Smart Mining for Deep Metric Learning,58d83051d649053542fe9c5b,Improved Deep Metric Learning with Multi-class N-pair Loss Objective.,"For the experiments, we follow the protocol used in previous papers [18, 19, 13], which uses unseen classes from the CUB- 200-2011 [23] and Cars196 [8] datasets in order to assess the clustering quality and k nearest neighbour retrieval [7].###, the adaptive controller) is compared with the following state-of-the-art deep metric learning approaches: (1) triplet learning with semihard negative mining [14] (with and without FANNG [5]), (2) lifted structured embedding [19], (3) N-pairs metric loss [18], (4) clustering [13], and (5) triplet combined with global loss [9].",impact-revealing,reporting experimental protocol and comparisons
2706,5d3ed25a275ded87f97deb36,37f7eaf57b88ef03efd4196f2f57ae0b4c657a79,Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommendation,5843774bac44360f108397c4,Deep Neural Networks For Youtube Recommendations,"Recommender systems are widely used in Internet applications to meet user’s personalized interests and alleviate the information overload [4, 29, 32].",other,acknowledging the role of recommender systems in addressing information overload
1971,,f007864669e111dd1bc817a35f9b8dcc9166dd36,"Clinical Outcomes, Costs, and Cost-effectiveness of Strategies for People Experiencing Sheltered Homelessness During the COVID-19 Pandemic",,,"###analysis does not account for other potential benefits of temporary housing on physical or 377 mental health.(35) Ultimately, broader policies around supportive housing measures for people 378",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1599,,2cc0c6fe59589853d654e03ae397cf19962c2ccc,Managing the Low-Carbon Transition - From Model Results to Policies,,,"###Moreover, implementation of agricultural GHG mitigation is limited by institutional, social, educational and economic constraints (Smith et al. 2007a) – in particular due to the rather diffuse nature of the sector.###Agriculture accounts for approximately 14% of total global anthropogenic GHG emissions (5.1 to 6.1 Gt CO 2 -eq p.a. in 2005, Smith et al. 2007b).",impact-revealing,highlighting the constraints in implementing agricultural GHG mitigation
1797,,e7228f05383330a78d7ae667a5390e7c96fef152,Exploring ethical issues associated with using online surveys in educational research,,,"###While earlier research (E.A. Buchanan & Hvizdak, 2009) suggested that some IRBs did not have a good understanding of the issues involved in online surveys or adequate processes in place to review this type of research, on the basis of a review of policy from 52 IRBs, Baker (2012) concluded that IRB policy now demonstrates sufficient understanding of these issues.###E.A. Buchanan and Hvizdak (2009) reported that three quarters of the ethics review committees they surveyed did not have a designated reviewer to examine proposals for online research, and a third did not consider evaluation of privacy and security policies of commercial online survey providers to…###While the legal status of IP addresses as personally identifiable information varies across countries (E.A. Buchanan & Zimmer, 2012), they should be treated in
online survey research as potential identifiers.###E.A. Buchanan and Hvizdak (2009) reported that three quarters of the ethics review committees they surveyed did not have a designated reviewer to examine proposals for online research, and a third did not consider evaluation of privacy and security policies of commercial online survey providers to be part of their remit.###While earlier research (E.A. Buchanan & Hvizdak, 2009) suggested that some IRBs did not have a good understanding of the issues involved in online surveys or adequate processes in place to review this type of research, on the basis of a review of policy from 52 IRBs, Baker (2012) concluded that IRB…###Despite online survey research being the most frequently reviewed type of internet research (E. A. Buchanan & Hvizdak, 2009), not all ethics review boards may be fully cognisant of the range of ethical issues associated with online surveys generally, or as applied to educational research…",impact-revealing,highlighting the evolution of IRB understanding and policies regarding online surveys
1346,,b779cfc49c9fa3871f32a75c9982d4251270c88f,Image quality assessment based on edge,,,"###Later, IFC was extended to the VIF (Visual Information Fidelity) by divisive normalization and considering the perceptual noise model [10].###So the structure based SSIM [4-8] and the information based IFC [9, 10] indexes can be used well for IQA task.###This work was motivated by natural scene statistics (NSS) model and quantified the information shared between the distorted and reference images, to form the so called information fidelity criterion (IFC) [9].",impact-revealing,describing the extension of IFC to VIF and its application in IQA tasks
1461,,9529a15aea838ab2c448399eaf40017b8a11ce05,Moral Judgment as Categorization (MJAC),,,"###…below) that is inconsistent with modern understandings of categorization (Barsalou, 2003; Harman et al., 2010; McCloskey & Glucksberg, 1978; Mervis & Rosch, 1981; Oden, 1977; Rosch, 1975; Rosch & Mervis, 1975; Stich, 1993), and this is problematic for explaining key phenomena in the moral domain.",impact-revealing,highlighting inconsistencies in categorization theories
1416,,c271b4d25bc184bc94622cef6c9aba80e8e2cce3,DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization,,,"###For our experiments on D4RL, we utilize the Gym-MuJoCo-v0 environments for evaluating BRAC, since BRAC performed somewhat reasonably on these domains [18], whereas we use the harder AntMaze and Franka Kitchen domains for evaluating CQL, since these domains are challenging for CQL [27].###[27] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.###Conservative Q-learning (Kumar et al., 2020b) The deep Q-network utilized by us is a ReLU network with four hidden layers of size (256 , 256 , 256 , 256) for the D4RL experiments, while for Atari we utilize the standard convolutional neural network from Agarwal et al. (2020); Kumar et al. (2021)…###Conservative Q-learning [27] is an offline RL algorithm that learns a conservative value function such that the estimated performance of the policy under this learned value function lowerbounds its true value.###[27]), the objective for training CQL is given by:###Since our goal is to study the effect of implicit regularization in TD-learning and not distributional shift, we build on top of existing offline RL methods in our experiments: CQL [27], which penalizes erroneous Q-values during training, REM [2], which utilizes an ensemble of Q-functions, and BRAC [49], which applies a policy constraint.###DR3 is inspired by the theoretical derivation of the implicit regularizer, it alleviates coadaptation and can be easily combined with modern offline RL methods, such as REM [2], CQL [27], and BRAC [49].###…on D4RL, we utilize the Gym-MuJoCo-v0 environments for evaluating BRAC, since BRAC performed somewhat reasonably on these domains (Fu et al., 2020), whereas we use the harder AntMaze and Franka Kitchen domains for evaluating CQL, since these domains are challenging for CQL (Kumar et al., 2020b).###…Wang et al., 2021a;b; Farahmand et al., 2010; De Farias, 2002), understanding instabilities in deep RL (Achiam et al., 2019; Bengio et al., 2020; Kumar et al., 2020a; Van Hasselt et al., 2018) and deriving weighted TD updates that enjoy convergence guarantees (Maei et al., 2009; Mahmood et al.,…###DR3 is inspired by the theoretical derivation of the implicit regularizer, it alleviates co-adaptation and can be easily combined with modern ofﬂine RL methods, such as REM (Agarwal et al., 2020), CQL (Kumar et al., 2020b), and BRAC (Wu et al., 2019).###…effect of implicit regularization in TD-learning and not distributional shift, we build on top of existing ofﬂine RL methods in our experiments: CQL (Kumar et al., 2020b), which penalizes erroneous Q-values during training, REM (Agarwal et al., 2020), which utilizes an ensemble of Q-functions, and…",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2507,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,57d063c7ac44367354290c3b,"Understanding Latency Variation in Modern DRAM Chips: Experimental Characterization, Analysis, and Optimization.","Many works tune individual server knobs, such as selective voltage boosting [98–100], exploiting multicore heterogeneity [101–103], trading memory latency/bandwidth [104–107], or reducing front-end stalls [70, 96, 108].",other,acknowledge various tuning approaches in server optimization
1969,,4e1ea95310abad8cfa8e3f48d54ab81cf4733e1b,Novel model for iteration step selection in image denoising using total variation technique,,,"###The commonly used image smoothing ﬁltering and Gaussian ﬁltering are isotropic algo-rithm, e.g. mean ﬁltering and median diffusion, 3,4 which does not consider the characteristics of images and therefore could destroy the edge of the image in the process of image denoising.",impact-revealing,highlighting limitations of common image filtering methods
3164,5db6c73a3a55acec0731cd68,73a5605ce482bd639078ebbb19baac7b903017e2,A Unified MRC Framework for Named Entity Recognition,5a9cb66717c44a376ffb8ac1,Deep contextualized word representations.,", 2018) and ELMo (Peters et al., 2018a) further enhance the performance of NER, yielding state-ofthe-art performances.###Recently, nested NER models are enriched with pre-trained contextual embeddings such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018b).###Recent large-scale language model pretraining methods such as BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018a) further enhanced the performance of NER, yielding state-of-the-art performances.",other,highlighting advancements in named entity recognition through pre-trained models
1897,,42d94c6cf0eb204498ed2dccc371a7ecdb591729,DIGITAL LEARNING IN FOREIGN LANGUAGE TEACHER TRAINING IN HIGHER EDUCATION: A CASE STUDY,,,"###Building understanding, which is the second indicator, refers instead to the practices implemented to foster content knowledge acquisition (Garrison et al. , 2000, p. 101), such as “creating an effective group consciousness for the purpose of sharing meaning, identifying areas of agreement and…###The Community of Inquiry model (Garrison et al. , 2000; Garrison and Arbaugh, 2007; Vaughan et al. , 2013) was adopted to design emergency remote teaching (Bozkurt et al. , 2020; Bozkurt and Sharma, 2020; Hodges et al. , 2020) for a graduate foreign language teacher training course, focusing on…###…content knowledge acquisition (Garrison et al. , 2000, p. 101), such as “creating an effective group consciousness for the purpose of sharing meaning, identifying areas of agreement and disagreement, and generally seeking to reach consensus and understanding” (Garrison et al. , 2000, p. 101).###(Garrison et al. , 2000, pp. 98-99) Students are more likely to engage in collaborative critical thinking when social presence has been developed successfully in online learning environments (Garrison et al. , 2000; Garrison and Arbaugh, 2007; Vaughan et al. , 2013).###Cognitive presence, deeply connected to critical thinking and metacognition, consists in “the extent to which the participants in any particular configuration of a community of inquiry are able to construct meaning through sustained communication” (Garrison et al. , 2000, p. 89).###Social presence is defined as “the ability of participants in a community of inquiry to project themselves socially and emotionally, as ‘real’ people (i.e., their full personality), through the medium of communication being used” (Garrison et al. , 2000, p. 89).###…within the Community of Inquiry theoretical framework, the instructor provided students with timely formative assessment in oral, written, and audio modes both in and out of class consistently (Garrison et al. , 2000; Garrison and Arbaugh, 2007; Vaughan et al. , 2013; Conrad and Openo, 2018).###…of the effectiveness of the teaching, social, and cognitive presence, namely the core components of the Community of Inquiry (CoI) framework (Garrison et al. , 2000; Garrison and Arbaugh, 2007; Vaughan et al. , 2013), implemented in a foreign language teacher trainer course transitioned…###…it is “[t]he teacher’s […] responsibility […] to facilitate reflection and discourse by presenting content, questions and proactively guiding and summarizing the discussion as well as confirming understanding through various means of assessment and feedback” (Garrison et al. , 2000, pp. 101-102).###…contributions (Garrison et al. , 2000, p. 100); and group cohesion, enhanced through strategies targeted at making students feel as members of a learning community, which is instrumental in fostering information sharing and collaborative critical thinking (Garrison et al. , 2000, p. 101).###In this light, when transitioning the course online due to the pandemic, the instructor redesigned the course in keeping with the socio-constructivist theoretical tenets of the CoI framework (Garrison et al. , 2000; Garrison and Arbaugh, 2007; Vaughan et al. , 2013; Selwyn, 2016).###In this respect, instructors and students have to bear in mind that “in a true community of inquiry, the tone of the messages is questioning but engaging, expressive but responsive, skeptical but respectful, and challenging but supportive” (Garrison et al. , 2000, p. 96).###Instructional management, namely the first indicator, refers to the design and implementation of the curriculum, activities, and assessment (Garrison et al. , 2000, p. 101).###…through “[both] the design of the educational experience[,] [which] […] includes the selection, organization, and primary presentation of course content, as well as the design and development of learning activities and assessment[,] […] [and] facilitation” (Garrison et al. , 2000, p. 89).###In a community of enquiry where collaborative tasks aim to foster critical thinking (Garrison et al. , 2000, p. 96), the implementation of relationship-building strategies suitable for promoting an emotionally safe place for students’ interactions is of paramount importance.###…showing mutual awareness, i.e. respect for peer-generated contributions, and recognition, i.e. acknowledgment of peer-generated contributions (Garrison et al. , 2000, p. 100); and group cohesion, enhanced through strategies targeted at making students feel as members of a learning community,…",impact-revealing,acknowledge the Community of Inquiry framework and its application in online teaching
1078,,735e338525e623069266fb7540503aeb6e596d91,Novel Virtual User Models of Mild Cognitive Impairment for Simulating Dementia,,,"###Biswas et al. [21] developed a simulator to help with the evaluation of assistive interfaces using the CPM GOMS [10] model to simulate the optimal behavior and a new model based on Markov processes for suboptimal behavior.###A simplified view of these cognitive architectures is known as the GOMS model [10].###[21] developed a simulator to help with the evaluation of assistive interfaces using the CPM GOMS [10] model to simulate the optimal behavior and a new model based on Markov processes for suboptimal behavior.###However these cognitive architectures, known as the GOMS (goals, operators, methods, and selection rules) model, are mainly suitable for modeling the optimal (skilled) behaviour of users.###However, the system simulates expert performance with GOMSmodeling support while the ACT-R system helps to simulate novice users.###The CogTool system [22] builds on the existing architecture of GOMS models and ACT-R system in order to provide quantitative prediction on interaction.",impact-revealing,reporting on the development of a simulator for evaluating assistive interfaces
1291,,726710765524b63ea3a84f39a153411d9d089b01,Observation and identification of metabolites emerging during postmortem decomposition of brain tissue by means of in situ 1H‐magnetic resonance spectroscopy,,,"###The basis set called “BS1” includes the metabolites initially proposed by Provencher (16), extended by a series of metabolites that are known to occur under clinical conditions comparable with this study, e.###05; z-test, one-tail; calculated from group deviation if n 1 and from Cramer-Rao minimal bounds (16) for n 1).###1 illustrates changes that occur from in vivo to the early postmortem phase and an enormous increase in MR-visible metabolites at 15 days postmortem, such that the vertical scaling had to be reduced by a factor of Table 1 Basis Sets of Metabolites Used for LC Model Fitting (16) of In and Ex Vivo Brain Spectra###The eventual goal of the present investigation is an estimation of PMI up to 3 weeks, based on changes in the concentration of brain metabolites determined by (1)H-MRS in situ and quantitative analysis of the spectra by the program LC Model (16).###A set of 14 separate metabolites was originally proposed by Provencher (16) to quantify brain spectra using LC Model (Table 1).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3459,5ecbc7639fced0a24b502b60,326974bd97dea3278515bcb3551a59e304d6d755,Improving Low-Resource Speech Recognition Based on Improved NN-HMM Structures,55d06634696322190568b85f,Deep learning,"As we all know, deep-learning [7] relies on a large amount of data, so the performance of the ASR system will be unsatis-factory in a low-resource environment.",other,highlighting the limitations of ASR systems in low-resource environments
1539,,d92667b57eb889a38701b98877c549a61f1b33ad,Neural network approach to Locating Cryptography in object code,,,###This work was inspired by two pieces of previous work: findcrypt[2]/findcrypt2[1] and the compromise of the Mifare smartcards[3].###The crack of the Mifare smartcards[3] involved reverse engineering the hardware by examining the distribution of logic gates.,impact-revealing,highlighting inspiration from previous works in cryptography
3181,5ac1829d17c44a1fda917eab,86aeec4d48d949190b3a0c2bf32c101fc23f13a3,Crepe: A Convolutional Representation for Pitch Estimation,53e9a1f3b7602d9702af8560,A sawtooth waveform inspired pitch estimator for speech and music.," implementation provided in mir eval [27] to compute the evaluation metrics. We compare CREPE against the current state of the art in monophonic pitch tracking, represented by the pYIN [13] and SWIPE [12] algorithms. To examine the noise robustness of each algorithm, we also evaluate their pitch tracking performance on degraded versions of MDB-stem-synth, using the Audio Degradation Toolbox (ADT) [28]###rmalized cross-correlation function (NCCF) as proposed by RAPT [9] and PRAAT [10], and the cumulative mean normalized difference function as proposed by YIN [11]. More recent approaches include SWIPE [12], which performs template matching with the spectrum of a sawtooth waveform, and pYIN [13], a probabilistic variant of YIN that uses a Hidden Markov Model (HMM) to decode the most probable sequence of",other,comparing the performance of different pitch tracking algorithms
3454,5c04967517c44a2c74708b7e,c18663fea10c8a303d045fd2c1f33cacf9b73ca3,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,53e9bd04b7602d970498d01a,Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation,"One common method is to recompute the forward pass activations during backpropagation [21, 8], which signiﬁcantly reduces memory required to cache activations.",other,describing a common method for memory optimization in backpropagation
1298,,1be9ba15996101cdef4221f25cc5c4d5d0a0c5b0,BMP signaling is required for cell cleavage in preimplantation-mouse embryos.,,,"###…in various BMP ligands, intracellular transducers, and receptors have underscored the importance of BMP signaling during gastrulation; e.g. Mishina et al. (1995), Macías-Silva et al. (1998), Solloway and Robertson (1999), Yi Se et al. (2009), Arnold et al. (2006), but has thus far failed…###Mice deficient in BMP type I receptor (BMPr1a/ALK3) or type II receptor (BMPrII/BMPR2), however, were smaller and had fewer cells (Beppu et al. 2000; Mishina et al. 1995), supporting a potential involvement of BMP signaling in cell proliferation.",impact-revealing,highlighting the significance of BMP signaling in development
1992,,ecaa55d1061c3f7183dcfecbe341b77b15e8de3e,Ecotones: Marginal or Central Areas of Transition?,,,"###Understanding the mechanisms, both biological and anthropogenic, that account for changes in environmental variables and that translate into altered species richness and species turnover (β-diversity) patterns in space and time has been a cornerstone in eco logical, evolutionary, and conservation research for many years ( Pimm et al., 1995 ).###…and anthropogenic, that account for changes in environmental variables and that translate into altered species richness and species turnover (β-diversity) patterns in space and time has been a cornerstone in ecological, evolutionary, and conservation research for many years (Pimm et al., 1995).",impact-revealing,highlighting the foundational importance of understanding environmental changes in ecological research
1801,,5e636ed4ec0f87c741601771d2521276b10f8458,Scaling Context-Sensitive Points-To Analysis,,,"###Similar to the work on demand-driven points-to analysis for Java [122, 121], they also formulate the computation of (alias) queries as a CFL-reachability problem.###However, in Java, a variable allocated on the stack cannot be address-taken.###[122] propose demand-driven points-to analysis.###Sridharan and Bodik [121] build upon their previous work [122] to propose a demand-driven, client-driven refinement-based context-sensitive points-to analysis for Java.###Sridharan and Bodik [121] build upon their previous work [122] to propose a demand-driven, client-driven refinement-based context-sensitive points-to analysis.###[122] propose demand-driven points-to analysis for Java.###Unlike in Java, stack variables can act as pointees in C.###We mainly focus on C/C++ kinds of programs, but the techniques are extensible to other general purpose imperative languages like Java.###As an example, Whaley and Lam [129] proposed a cloningbased context-sensitive pointer analysis using BDD which successfully analyses 687 KB of Java bytecode but requires around 20 minutes to complete.###Therefore, the only pointees possible in Java are the heap allocated variables [130].",impact-revealing,acknowledge existing work on demand-driven points-to analysis
2027,,42f6b00306f612464323dd7634151f3666a1646e,An in-network service system based on smart routers and edge devices,,,"###However, D2D delivery is not suitable for all environments [4].",impact-revealing,acknowledging limitations of D2D delivery
3008,5736982b6e3b12023e6fd332,d0b8c5464b582e1a582a4affb270e8b02f8414a2,a scalable architecture for ordered parallelism,5390bb1d20f70186a0f3cc47,"Warp speed: executing time warp on 1,966,080 cores","Moreover, state-of-the-art PDES engines have overheads of tens of thousands of cycles per event [6], making them impractical for fine-grain tasks.",other,highlighting the limitations of state-of-the-art PDES engines
3359,5f75aa6a9fced0a24b64599c,10f61ec5c6c822325a91bd7d718a81b93e9628ca,Opportunistic Early Pipeline Re-steering for Data-dependent Branches,5c20b1fcda5629702063afe6,Performance Improvement by Prioritizing the Issue of the Instructions in Unconfident Branch Slices.,"For unconfident predictions, [10] proposes to prioritize the issue of the instructions on the backslice of a branch to reduce the branch misprediction penalty.###Even though the target of [10], [17] and our work is to reduce the branch misprediction penalty, our approach is distinctive in two ways: (1) they do not examine the data-dependent branches as the focus of design, and (2) if the load execution is delayed in the pipeline, 3D-Branch Overrider can…###We leverage from prior proposals such as [10, 12] to build a simple and efficient module to capture the dependence chain of instructions which can then be prioritized for execution.",other,highlighting the distinctiveness of the proposed approach compared to prior work
3963,5f576c1591e011f4c3d5dd7e,c36571ac50808c75fa8a5d37f1041af22e89e6ee,GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training,5b8c9f4a17c44af36f8b713c,"JUST INTERPOLATE: KERNEL ""RIDGELESS"" REGRESSION CAN GENERALIZE","[22] Tengyuan Liang and Alexander Rakhlin.###Second, the majority of modern deep learning models are shown to be able to interpolate the data [5, 22, 41].",other,reporting findings on deep learning model capabilities
3665,5ce3afafced107d4c65f7c4c,d9f6ada77448664b71128bb19df15765336974a6,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,5bbacb9e17c44aecc4eaffec,Modeling Empathy and Distress in Reaction to News Stories,"Similarly, on the Empathetic Reactions dataset, BERT outperforms our human baseline, where BERT’s predictions have a Pearson correlation of 0.45 on empathy and 0.55 on distress, compared to 0.45 and 0.35 for our human baseline.###It was challenging to train annotators to do well on Quora Insincere Questions 10 , Empa-thetic Reactions (Buechel et al., 2018), and a recast version of Ultra-Fine Entity Typing (Choi et al., 2018b, see Appendix C.3 for details), leading to low human performance.###We collected data to produce conservative estimates for human performance on several tasks that we did not ultimately include in our benchmark, including GAP (Webster et al., 2018), PAWS (Zhang et al., 2019), Quora Insincere Questions, 8 Ultraﬁne Entity Typing (Choi et al., 2018b), and Empathetic Reactions datasets (Buechel et al., 2018).###…estimates for human performance on several tasks that we did not ultimately include in our benchmark, including GAP (Webster et al., 2018), PAWS (Zhang et al., 2019), Quora Insincere Questions, 8 Ultraﬁne Entity Typing (Choi et al., 2018b), and Empathetic Reactions datasets (Buechel et al., 2018).###com/c/quora-insincere-questions-classification/data), Empathetic Reactions (Buechel et al., 2018), and a recast version of Ultra-Fine Entity Typing (Choi et al.",other,highlighting the performance comparison between BERT and human baseline on empathy and distress tasks
3709,5efb0d5691e011063336d39c,0cee58946a13a5c2845647b4af8b9d2bf52a8b6b,BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision,5a73cb5d17c44a0b303573b1,"Low-Resource Named Entity Recognition with Cross-lingual, Character-Level Neural Conditional Random Fields.","For examples, Cotterell and Duh (2017); Feng et al. (2018) consider NER for a low resource target language.",other,reporting prior findings on NER in low resource languages
897,53e9a42bb7602d9702d44c0b,2d6f191fd9b08d2a53498f0ed9b4f1a411d83cdf,Temporal Streams In Commercial Server Applications,53e9a914b7602d9703267a80,Identifying hierarchical structure in sequences: a linear-time algorithm,"Like similar studies of repetition in L1 data accesses [7] and program paths [16], we use the SEQUITUR hierarchical data compression algorithm [10] to identify repetitive sub-sequences within the miss traces.",impact-revealing,acknowledge existing methods for identifying repetitive sub-sequences
130,5ce3afafced107d4c65f7c4c,d9f6ada77448664b71128bb19df15765336974a6,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,5b1643998fbcbf6e5a9bc3b6,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.,"In this context, the GLUE benchmark (organized by some of the same authors as this work, short for General Language Understanding Evaluation; Wang et al., 2019) has become a prominent evaluation framework and leaderboard for research towards general-purpose language understanding technologies.###In this context, the GLUE benchmark (Wang et al., 2019a) has become a prominent evaluation framework for research towards general-purpose language understanding technologies.###More information on the tasks included in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher et al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al. (2018, MNLI), and Rajpurkar et al. (2016, the original data source for…###Software Tools To facilitate using SuperGLUE, we release jiant (Wang et al., 2019b), 4 a modular software toolkit, built with PyTorch (Paszke et al., 2017), components from AllenNLP (Gardner et al., 2017), and the transformers package.",impact-revealing,highlighting the significance of the GLUE benchmark in language understanding research
3082,5ac1827b17c44a1fda915855,f96a5a9cfa1dbb01df6df749e093c0bad20240a8,Control Flow Checking at Virtual Edges.,53e9a52cb7602d9702e4de84,Dynamic integrity measurement and attestation: towards defense against return-oriented programming attacks.,"In this context, the integrity of system software and applications is a fundamental requirement and necessary consequence in order to ensure trust in the computing infrastructure [3].",other,providing context on the importance of system software integrity
3105,5c2348ceda562935fc1d57a4,007d08aee3b88489fe0377849f70688de66adae8,Bandit Learning with Implicit Feedback,53e99f94b7602d970286c418,Thompson Sampling for Contextual Bandits with Linear Payoffs,"In a standard Thompson sampling 145 [2], one is required to sample from the true posterior of model parameters.###Even worse, the two popular bandit learning paradigms, upper confidence 114 bound principle [1] and Thompson sampling [2], both demand an accurate estimation of bandit 115 parameters and its uncertainty.",other,highlighting challenges in accurate estimation of bandit parameters
1895,,03ed90278786830afebc726c33515566dc5fdba5,"Guest Editor’s Introduction: Facilitating Interaction, Collaboration, Community, and Problem-Solving Capabilities in Blended and Fully Online Technical Communication Programs: An Introduction to the Special Issue",,,"###Qualitatively, teachers can use the LMS to view students’ forum postings in context and/or look for evidence of cognitive and social presence, as I outlined earlier (Garrison & Cleveland-Innes, 2005; Garrison et al., 2000).###The Community of Inquiry (COI) framework, as developed by Garrison et al. (2000), emphasizes the need for teaching, cognitive, and social presence.###Social presence indicators are evident when learners feel free to engage in risk-free expression, are actively collaborating with one another, and are encouraging others to do the same (Garrison et al., 2000).",impact-revealing,highlighting the importance of the Community of Inquiry framework in educational settings
1524,,a4a377eb80ed815b7b7a9a92ee85d05e29f8b377,Microaggression and everyday resistance in narratives of refugee resettlement.,,,"###Instead, they regularly faced what Sue et al. (2007) term microinsults: small verbal and nonverbal acts that appeared to be done with no intention to hurt, but that clearly conveyed negative assumptions about the target based on their appearance, accent, or race.###Some participants considered media and public discourses that portray refugees as vulnerable to be forms of environmental microaggression, including social and cultural cues that reveal discriminatory assumptions (Sue et al. 2007).###The framework of microaggression described by Sue et al. (2007) is particularly suitable for understanding contemporary forms of racism and discrimination, which are often subtle and understated (Neville, Spanierman, and Lewis 2012; Clark 2014; Fleras 2016).###Finally, microinvalidations are comments or cues that erase or invalidate the feelings and experiences of marginalized groups (Sue 2010: 37; Sue et al. (2007) which is the framework of microaggression that we use to interpret the current results.###To this end, we draw on the psychological concept of microaggression developed by Chester Pierce (Pierce 1974) and elaborated into a framework by Derald Sue and colleagues (Sue et al. 2007; Sue 2010).###While Sue et al. (2007) framework of microaggression can help investigate the subtle forms of discrimination that marginalized groups experience, it does not address the strategies that members of such groups use to acknowledge, resist, and subvert the power of microaggression.###In recent years, Sue et al. (2007) have further elaborated on our understanding of microaggression through their work in the area of social psychology.###Sue et al. (2007) define microaggression as:
[T]he brief and commonplace daily verbal, behavioural, and environmental indignities, whether intentional or unintentional, that communicate hostile, derogatory, or negative racial, gender, sexual-orientation, and religious slights and insults to the…###(p. 273)
Sue et al. (2007) outline three forms of microaggression: the microinsult, microassault, and microinvalidation (Sue 2010).",impact-revealing,providing context and framework for understanding microaggressions
380,5d9edc1647c8f76646032985,10a4db59e81d26b2e0e896d3186ef81b4458b93f,Named Entity Recognition with Bidirectional LSTM-CNNs,56d81390dabfae2eee625461,Torch7: A Matlab-like Environment for Machine Learning,"Collobert et al. (2011b) also applied CNNs to semantic role labeling, and variants of the architecture have been applied to parsing and other tasks requiring tree structures (Blunsom et al., 2014).###As capitalization information is erased during lookup of the word embedding, we evaluate Collobert’s method of using a separate lookup table to add a capitalization feature with the following options: allCaps, upperInitial, lowercase, mixedCaps, noinfo (Collobert et al., 2011b).###We train our network to maximize the sentencelevel log-likelihood from Collobert et al. (2011b).17###5, we found that this more sophisticated method outperforms the method presented by Collobert et al. (2011b), which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches, and marks tokens with YES/ NO.###…of the word2vec (Mikolov et al., 2013) program to train word embeddings on Wikipedia and Reuters RCV1 datasets as well.6
Following Collobert et al. (2011b), all words are lower-cased before passing through the lookup table
2http://ml.nec-labs.com/senna/…###Unfortunately there are many limitations to the model proposed by Collobert et al. (2011b). First, it uses a simple feed-forward neural network, which restricts the use of context to a fixed sized window around each word – an approach that discards useful long-distance relations between words.###Compared to the SENNA lexicon, our DBpedia lexicon is noisier but has broader coverage, which explains why when applying it using the same method as Collobert et al. (2011b), it performs worse on CoNLL-2003 but better on OntoNotes – a dataset containing many more obscure named entities.###As we will see in Section 4.5, we found that this more sophisticated method outperforms the method presented by Collobert et al. (2011b), which treats partial and exact matches equally, allows prefix but not suffix matches, allows very short partial matches, and marks tokens with YES/ NO.###We re-implemented the FFNN model of Collobert et al. (2011b) as a baseline for comparison.###While Collobert et al. (2011b) used Wikipedia text from 2007, we used Wikipedia text from 2011.###The authors would like to thank Collobert et al. (2011b) for releasing SENNA with its word vectors and lexicon, the torch7 framework contributors, and Andrey Karpathy for the reference LSTM implementation.###Our neural network is inspired by the work of Collobert et al. (2011b), where lookup tables transform discrete features such as words and characters into continuous vector representations, which are then concatenated and fed into a neural network.###…by normalizing the above score over all possible tag-sequences [j]T1 using a softmax:
logP ([y]T1 | [x]T1 , θ′)
= S([x]T1 , [y] T 1 , θ ′)− log ∑ ∀[j]T1 eS([x] T 1 ,[j] T 1 ,θ ′)
This objective function and its gradients can be efficiently computed by dynamic programming (Collobert et al., 2011b).###6While Collobert et al. (2011b) used Wikipedia text from 2007, we used Wikipedia text from 2011.
to convert to their corresponding embeddings.###We train our network to maximize the sentencelevel log-likelihood from Collobert et al. (2011b).17
First, we define a tag-transition matrix A where Ai,j represents the score of jumping from tag i to tag j in successive tokens, and A0,i as the score for starting with tag i.###However, we suspect that the method of Collobert et al. (2011b) is not noise resistant and therefore unsuitable for our lexicon because it fails to distinguish exact and partial matches34 and does not set a minimum length for partial matching.35 Instead, when we apply our superior partial matching…###Our best model uses the publicly available 50dimensional word embeddings released by Collobert et al. (2011b)2, which were trained on Wikipedia and the Reuters RCV-1 corpus.###Unfortunately there are many limitations to the model proposed by Collobert et al. (2011b).###Following Durrett and Klein (2014), we applied our model to the portion of the dataset with gold-standard named entity annotations; the New Testaments portion was excluded for lacking gold-standard annotations.###However, Collobert et al. (2011b) proposed an effective neural network model that requires little feature engineering and instead learns important features from word embeddings trained on large quantities of unlabelled text – an approach made possible by recent
advancements in unsupervised learning…###Our best model uses the publicly available 50- dimensional word embeddings released by Collobert et al. (2011b)2, which were trained on Wikipedia and the Reuters RCV-1 corpus.###Furthermore, as lexicons are crucial to NER performance, we propose a new lexicon encoding scheme and matching algorithm that can make use of partial matches, and we compare it to the simpler approach of Collobert et al. (2011b).###In addition, since Collobert et al. (2011b) released their lexicon with their SENNA system, we also applied their lexicon to our model for comparison and investigated using both lexicons simultaneously as distinct features.###Following the speech-recognition framework outlined by Graves et al. (2013), we employed
a stacked1 bi-directional recurrent neural network with long short-term memory units to transform word features into named entity tag scores.###Following Collobert et al. (2011b), all words are lower-cased before passing through the lookup table###We implement the neural network using the torch7 library (Collobert et al., 2011a).###Furthermore, as lexicons are crucial to NER performance, we propose a new lexicon encoding scheme and matching algorithm that can make use of partial matches, and we compare it to the simpler approach of Collobert et al. (2011b). Extensive evaluation shows that our proposed method establishes a new state of the art on both the CoNLL-2003 NER shared task and the OntoNotes 5.###Recently, Santos et al. (2015) presented their CharWNN network, which augments the neural network of Collobert et al. (2011b) with character level CNNs, and they reported improved performance on Spanish and Portuguese NER.###Much later, with the advent of neural word embeddings, Collobert et al. (2011b) presented SENNA, which employs a deep FFNN and word embeddings to achieve near state of the art results on POS tagging, chunking, NER, and SRL.###In addition, as we hypothesized that word embeddings trained on in-domain text may perform better, we also used the publicly available GloVe (Pennington et al., 2014) program and an in-house re-implementation5 of the word2vec (Mikolov et al., 2013) program to train word embeddings on Wikipedia and Reuters RCV1 datasets as well.6
Following Collobert et al. (2011b), all words are lower-cased before passing through the lookup table
2http://ml.nec-labs.com/senna/ 3http://nlp.stanford.edu/projects/glove/ 4https://code.google.com/p/word2vec/ 5We used our in-house reimplementation to train word vectors because it uses distributed processing to train much quicker than the publicly-released implementation of word2vec and its performance on the word analogy task was higher than reported by Mikolov et al. (2013).###…Google’s embeddings were trained in a case-sensitive manner, and embeddings for many common punctuations and sym-
27Wilcoxon rank sum test, p < 0.001 28To make direct comparison to Collobert et al. (2011b), we do not exclude the CoNLL-2003 NER task test data from the word vector training data.",impact-revealing,acknowledge limitations and variations in existing models and methods
3537,5c0f87a5da562944ac95a190,4d157bec76125b1121a999ab2b79730540ab7aad,"Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases",53e9981db7602d9702037df2,Similarity Based Docking,"Therefore, they do not require searching for low energy conformations of compounds contrary to conventional methods, which reduces the computational cost and makes them faster than traditional docking methods [48].###In addition to these traditional methods, there are also similarity-based docking approaches such as HomDock [48], eSimDock [49] and fkcombu [50] that use structural similarities of compounds to predict their protein-bound states by aligning them on the experimentally determined 3D structure of a reference compound that is in complex with a target protein or evolutionarily related structures of that target protein [49].###In addition to these traditional methods, there are also similarity-based docking approaches such as HomDock [48], eSimDock [49] and fkcombu [50] that use structural similarities of compounds to predict their protein-bound states by aligning them on the experimentally determined 3D structure of a reference compound that is in complex with a target protein or evolutionarily related
structures of that target protein [49].",other,highlighting advantages of similarity-based docking approaches over traditional methods
2390,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,5d9ed25547c8f76646f67c8d,Alpacatag: An Active Learning-Based Crowd Annotation Framework For Sequence Tagging,"Our method is also incorporated as a feature for controlling the quality of crowd-annotation in annotation frameworks such as AlpacaTag (Lin et al., 2019) and LEAN-LIFE (Lee et al., 2020).###Crowd-sourcing has been demonstrated to be an effective way of fulﬁlling the label consumption of neural models (Guan et al., 2017; Lin et al., 2019).",other,reporting the incorporation of a method in crowd-annotation frameworks
3929,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,558c469de4b0cfb70a1cb4d0,Video classification based on low-level feature fusion model,"[44] for video classification, Singh et al.###[44] used PCA for video classification, Chetty and Wagner [28] utilized SVD for biometric person authentication, and Potamianos et al.###[44] extracted low-level (color or texture) descriptors from a TREC video and applied a SVM classifier to recognize the pre-defined concepts (e.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
4037,5c04967517c44a2c74708b7e,c18663fea10c8a303d045fd2c1f33cacf9b73ca3,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,5a9cb65d17c44a376ffb820b,Regularized Evolution for Image Classifier Architecture Search,"Deep neural networks have advanced many machine learning tasks, including speech recognition [11], visual recognition [56, 44], and language processing [17].###Figure 1: Strong correlation between top-1 accuracy on ImageNet ILSVRC 2012 validation dataset and model size for representative state-of-the-art image classification models in history [48, 49, 23, 52, 24, 56, 44].###We followed the same hyperparameters and input pre-processings as ImageNet models described in [44] to train AmoebaNet-B (6, 512).",other,highlighting the advancements brought by deep neural networks in various machine learning tasks
3532,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",53e9bc0ab7602d970486fa36,Structure-based learning in wireless networks via sparse approximation,"example, several papers have explored the use of GSP techniques to improve the efficiency of value function estimation in a reinforcement learning scenario [24], [25].",other,highlighting the application of GSP techniques in reinforcement learning
1916,,7d803f5bbfc5a4e6c3e5e636f0eba0f1ef8099cd,Analyzing cognitive presence in online courses using an artificial neural network,,,"###We know that it is possible to learn in electronic environments, Henri (1992) and Garrison, Anderson, and Archer (2000) provide for us a model for analyzing discussion content, we know that content analysis has informed educational practice, we know that computational power can support content…###To enable a comparison between this study and that of Garrison, Anderson, and
Archer (2000), Cohen’s (1960) kappa values are calculated among pairs of raters.###Garrison, Anderson, and Archer (2000) created perhaps the most thorough modification and operationalization of Henri’s model by breaking it into three components (cognitive presence, social presence, and teaching presence) and by expanding each component into deeper subcomponents.###The coders were first trained to code online discussion
messages using a rubric based on that developed by Garrison, Anderson, and Archer (2000).###For this analysis, the rubric from Garrison, Anderson, and Archer (2000) was
modified with examples taken from eCore™ courses in history and political science (see
Appendix A).###Human Content Analysis
This work builds on a series of content analyses described by Garrison, Anderson,
and Archer (2000, 2001) who analyzed online discussions based on a community of inquiry model which splits community-based learning into three overlapping areas: social presence, cognitive…###Recall from Chapter 3 that the human coders have been trained to use the cognitive presence coding rubric outlined by Garrison, Anderson, and Archer (2000).",impact-revealing,acknowledging foundational models for analyzing discussion content in educational environments
1471,,2e388801817884c4af1d81338610669524646ea2,Perceptual differentiation as a source of category effects in object processing: Evidence from naming and object decision,,,"###Category exemplars were chosen from Rosch's (1975) norms and were selected from the whole range of typicality within each category. Drawings of objects were selected from the standardized set of Snodgrass and Vanderwart (1980), and added to these were further drawings by a trained artist (the first author).###The 24 items from each category were divided into two equal lists (A and B) of 12, pairwise matched (across means and ranges), in terms of name frequency (NF) and prototypicality (P), using the Francis and Kucera (1982) and Rosch (1975) norms, respectively.###Category exemplars were chosen from Rosch's (1975) norms and were selected from the whole range of typicality within each category. Drawings of objects were selected from the standardized set of Snodgrass and Vanderwart (1980), and added to these were further drawings by a trained artist (the first author). The creation of these additional drawings was constrained by the criteria of Snodgrass and Vanderwart (1980), namely: Objects whose up-down orientation could vary (e.###Category exemplars were chosen from Rosch's (1975) norms and were selected from the whole range of typicality within each category.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1237,,bc0f67b3c7238a30c0e3122223ced890cdd1b365,Developments in cancer vaccines for hepatocellular carcinoma,,,###New and specific tumor-associated antigens (TAAs) and/ or tumor-associated epitopes (TAEs) can be identified by integration of multiple high-throughput “omics” technologies (reviewed in [42]) and validated by immunoinformatics algorithms [43–47].,impact-revealing,highlighting the integration of technologies for identifying tumor-associated antigens
2815,5db6c73a3a55acec0731cd68,73a5605ce482bd639078ebbb19baac7b903017e2,A Unified MRC Framework for Named Entity Recognition,53e99d0bb7602d97025bde89,Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data,"The other is the outside-in model, in which the ﬁrst CRF identiﬁes out-ermost entities, and then successive CRFs would identify increasingly nested entities.###Traditional sequence labeling models use CRFs (Lafferty et al., 2001; Sutton et al., 2007) as a backbone for NER.###Lample et al. (2016) explored neural structures for NER, in which the bidirectional LSTMs are combined with CRFs with features based on character-based word representations and unsupervised word representations.",other,describing traditional and neural approaches for named entity recognition
2140,,72b96a45fa9f1ab656312cbb4a0f7a64e8063717,GridMM: Grid Memory Map for Vision-and-Language Navigation,,,"###VLN [4, 55, 25, 53, 43, 14, 13] has received significant attention in recent years with the continual improvement.###Among them, a recurrent unit is usually utilized to encode historical observations and actions within a fixed-size state vector [4, 19, 51, 55, 27].###In previous works [4, 19, 51, 55, 27], recurrent states are most commonly used as historical information for VLN, which encode historical observations and actions within a fixed-size state vector.",impact-revealing,highlighting the significant attention and advancements in VLN research
723,5fef1dfc91e0113b265a0220,85e7d63f75c0916bd350a229e040c5fbb1472e7a,making pre-trained language models better few-shot learners,5db1765a3a55ac101c887e97,Exploring the Limits of Transfer Learning with a Unified Text-to-Text  Transformer,"To address this challenging problem, we propose to use T5 (Raffel et al., 2020),###We address this issue by introducing automatic prompt generation, including a pruned brute-force search to identify the best working label words, and a novel decoding objective to automatically generate templates using the generative T5 model (Raffel et al., 2020)—all of which only require the few-shot training data.###…by introducing automatic prompt generation, including a pruned brute-force search to identify the best working label words, and a novel decoding objective to automatically generate templates using the generative T5 model (Raffel et al., 2020)—all of which only require the few-shot training data.###To address this challenging problem, we propose to use T5 (Raffel et al., 2020),
a large pre-trained text-to-text Transformer.",impact-revealing,proposing a novel approach to address a challenging problem using T5
1016,,3e89e769ae9ccdbd9b5c704c49cd70275aaaf1d2,eScholarship International,,,"###At least partly, this approach was inspired by readers of Chomsky’s (1959) review of Skinner’s Verbal Behavior (1957), which argued that the traditional associative approach was impoverished and even circular (see also Fodor & Piattelli-Palmarini, 2010).",impact-revealing,highlighting the influence of Chomsky's critique on the development of the approach
173,57a4e921ac44365e35c99004,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,man is to computer programmer as woman is to homemaker? debiasing word embeddings,53e9acc4b7602d97036a1037,Efficient Estimation of Word Representations in Vector Space,"Unless otherwise stated, the embedding we refer to is the aforementioned w2vNEWS embedding, a d = 300-dimensional word2vec [19, 20] embedding, which has proven to be immensely useful since it is high quality, publicly available, and easy to incorporate into any application.###The primary embedding studied in this paper is the popular publicly-available word2vec [19, 20] 300 dimensional embedding trained on a corpus of Google News texts consisting of 3 million English words, which we refer to here as the w2vNEWS.",impact-revealing,providing context and significance of the w2vNEWS embedding
2793,5eede0b791e0116a23aafe75,150f95f9c73820e0a0fa1546140e9f2bdfd25954,temporal graph networks for deep learning on dynamic graphs,5bdc31c217c44a1f58a0c914,Graph Neural Networks for IceCube Signal Classification,"Graphs are ubiquitously used as models for systems of relations and interactions in many fields [5, 52, 42, 10, 16, 20, 49, 53], in particular, social sciences [68, 43] and biology [76, 62, 18].",other,highlighting the widespread application of graphs in various fields
2371,5e5e190893d709897ce48240,76b8d5f2ef97d71167aa78309918bf3f7d633c96,Author Name Disambiguation on Heterogeneous Information Network with Adversarial Representation Learning,53e9b73ab7602d97042d1332,Entity disambiguation in anonymized graphs using graph kernels.,"To address the problem, some methods focus on the relation information from the network (Zhang and Al Hasan 2017; Hermansson et al. 2013) in a generative way.",other,acknowledge methods addressing relation information
2590,5dc5488edf1a9c0c41511e7e,59ce117f1c290075ee7bb67b8928344b37f788cc,the impact of cache inclusion policies on cache management techniques,57d063e0ac443673542947ae,Back to the Future: Leveraging Belady's Algorithm for Improved Cache Replacement.,Exclusive RRIP [21] X SDBP [25] X SHiP [36] X GIPPR [22] X MDPP [33] X EAF [28] X Perceptron [34] X KPCR [26] X Hawkeye [18] X Bypass and Insertion [10] X CHAR [5] X X ExDRRIP [20] X Table 1: State-of-the-art RPs and their inclusion policy.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2364,5ce3afafced107d4c65f7c4c,d9f6ada77448664b71128bb19df15765336974a6,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,5736986b6e3b12023e72fb73,Semi-supervised Sequence Learning,"Much work prior to GLUE has demonstrated that training neural models with large amounts of available supervision can produce representations that effectively transfer to a broad range of NLP tasks (Collobert and Weston, 2008; Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau and Kiela, 2018; McCann et al., 2017; Peters et al., 2018).###…neural models with large amounts of available supervision can produce representations that effectively transfer to a broad range of NLP tasks (Collobert and Weston, 2008; Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau and Kiela, 2018; McCann et al., 2017; Peters et al., 2018).",other,highlighting the effectiveness of training neural models with large supervision for NLP tasks
3808,5ac1829d17c44a1fda9180a3,632d25ef7914ce962d258920460a9405b8c4553a,Sequence-Based Multi-Lingual Low Resource Speech Recognition,573697f96e3b12023e6d2f31,EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding,"[20] Y. Miao, M. Gowayyed, and F. Metze, “EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding,” in Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on.###Multi-lingual training on
2The code to train the multi-lingual model will be released as part of EESEN [20].
the “MLing” set (the four languages shown in Table 2) improves WER by 1.7% (absolute) on average, while keeping the LSTM layers shared across all languages.###2The code to train the multi-lingual model will be released as part of EESEN [20].###Like in [20] we use this loss along with stacked Bidirectional LSTM layers to encode the acoustic information and make frame-wise predictions.###Connectionist Temporal Classification (CTC, [19]) lends itself to low-resource multi-lingual experiments, because systems built on CTC tend to be significantly easier to train than those that have been trained using hidden Markov models [20, 21].",other,reporting findings on multilingual training improvements
2126,,00ea98e409deb0b338e58d546735c02ae76bbfec,Disturbance feedback for handling uncertainty in Air Traffic Flow Management,,,"###The baseline Eulerian-Lagrangian, or flow based, model implemented is a slight reformulation of the model presented in Sun and Bayen [6] which was inspired by techniques of modelling road traffic [16].",impact-revealing,acknowledge prior work and its influence on the current model
3424,5f1ff7ea91e011d50a621ab3,39b45b0d60807296db97d9eb37b1fb99d244e2ed,MACU-Net for Semantic Segmentation of,5ecbc5829fced0a24b4e836e,Unet 3+: A Full-Scale Connected Unet For Medical Image Segmentation,"Thus, full-scale skip connections are designed in U-Net 3+ [13] to alleviate this limitation, while leading to huge computational complexity.###To test the effectiveness of MACU-Net, we compare the performance of the proposed method with U-Net [11], FGC [19], U-Net++ [12], U-NetPPL [20], WRAU-Net [21], CE-Net [22] and U-Net 3+ [13].###To evaluate the effectiveness of MACU-Net, the U-Net [11], FGC [19], U-Net++ [12], U-NetPPL [20], WRAU-Net [21], CE-Net [22] and U-Net 3+ [13] methods were used as benchmark comparators.",other,comparing the performance of different U-Net architectures
3658,5bdc31b817c44a1f58a0c039,abfa95058fa50c55a0b923a6c35830f470c125ad,Adaptive sampling towards fast graph representation learning,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"We evaluate the performance of our method on four popular benchmarks for node classification, including Cora, Citeseer, Pubmed [11] and Reddit [3].###This includes examples of social networks [3], protein interfaces [4], and 3D meshes [5].###More recently, two kinds of sampling-based methods including GraphSAGE [3] and FastGCN [20] were developed for fast representation learning on graphs.###We evaluate the performance of our methods on the following benchmarks: (1) categorizing academic papers in the citation network datasets–Cora, Citeseer and Pubmed [11]; (2) predicting which community different posts belong to in Reddit [3].###For the Reddit dataset, the hidden dimensions are selected to be 256 as suggested by [3].###The codes of GraphSAGE [3] and FastGCNN [20] provided by the authors are implemented inconsistently; here we re-implement them based on our framework to make the comparisons more fair.###We contrast our approach with GraphSAGE [3] and FastGCN [20] regarding the following aspects:",other,reporting performance evaluation on benchmarks for node classification
910,5de7997c9e795e77580692f9,c919ae4366f5cc4901b854cc259101ccc13e6f3f,Constrained Reinforcement Learning Has Zero Duality Gap,53e9acf7b7602d97036d8927,An Online Actor-Critic Algorithm with Function Approximation for Constrained Markov Decision Processes.,"In particular, the proofs in [7, 11] rely on the fact that this different time-scale is such that allows to consider the multiplier as constant.###This idea can be applied in the context of reinforcement learning as well, where a policy gradient —or actor critic as in [7, 11] —update is followed by an update of the multipliers along the direction of the constraint violation.###Primal-dual algorithms [7, 11], allow us to choose dynamically the multipliers by find the best policy for the current set of parameters and then taking steps along the gradient of the Lagrangian with respect to the multipliers.###An alternative, is to embed all conflicting requirements in a constrained RL problem and to use a primal-dual algorithm as in [7, 11] that chooses the parameters automatically.###[7, 11], in fact converge to the optimal solution under mild assumptions.###Regardless of these limitations, the primal dual algorithm considered here and those proposed in [7, 11] provide a manner to solve constrained policy optimization problems without the need to perform an exhaustive search over the weights that we assign to each reward function, as it is the case in [4, 19, 20].",impact-revealing,discussing the application of primal-dual algorithms in reinforcement learning
2482,5b1643ba8fbcbf6e5a9bc5b5,8c1b00128e74f1cd92aede3959690615695d5101,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,599c7953601a182cd263079b,Reading Wikipedia To Answer Open-Domain Questions,"To measure the speedup of our model against the RNN models, we also test the corresponding model architecture with each encoder block replaced with a stack of bidirectional Published 12 LeaderBoard 13 Single Model EM / F1 EM / F1 LR Baseline (Rajpurkar et al., 2016) 40.4 / 51.0 40.4 / 51.0 Dynamic Chunk Reader (Yu et al., 2016) 62.5 / 71.0 62.5 / 71.0 Match-LSTM with Ans-Ptr (Wang & Jiang, 2016) 64.7 / 73.7 64.7 / 73.7 Multi-Perspective Matching (Wang et al., 2016) 65.5 / 75.1 70.4 / 78.8 Dynamic Coattention Networks (Xiong et al., 2016) 66.2 / 75.9 66.2 / 75.9 FastQA (Weissenborn et al., 2017) 68.4 / 77.1 68.4 / 77.1 BiDAF (Seo et al., 2016) 68.0 / 77.3 68.0 / 77.3 SEDT (Liu et al., 2017a) 68.1 / 77.5 68.5 / 78.0 RaSoR (Lee et al., 2016) 70.8 / 78.7 69.6 / 77.7 FastQAExt (Weissenborn et al., 2017) 70.8 / 78.9 70.8 / 78.9 ReasoNet (Shen et al., 2017b) 69.1 / 78.9 70.6 / 79.4 Document Reader (Chen et al., 2017) 70.0 / 79.0 70.7 / 79.4 Ruminating Reader (Gong & Bowman, 2017) 70.6 / 79.5 70.6 / 79.5 jNet (Zhang et al., 2017) 70 LSTMs as is used in most existing models.###Speciﬁcally, each (embedding and model) encoder block is replaced with a 1, 2, or 3 layer Bidirectional LSTMs respectively, as such layer numbers fall into the usual range of the reading comprehension models (Chen et al., 2017).###…have been proposed to tackle these challenges, including BiDAF (Seo et al., 2016), r-net (Wang et al., 2017), DCN (Xiong et al., 2016), ReasoNet (Shen et al., 2017b), Document Reader (Chen et al., 2017), Interactive AoA Reader (Cui et al., 2017) and Reinforced Mnemonic Reader (Hu et al., 2017).###This module is standard in almost every previous reading comprehension models such as Weissenborn et al. (2017) and Chen et al. (2017).###A great number of end-to-end neural network models have been proposed to tackle these challenges, including BiDAF (Seo et al., 2016), r-net (Wang et al., 2017), DCN (Xiong et al., 2016), ReasoNet (Shen et al., 2017b), Document Reader (Chen et al., 2017), Interactive AoA Reader (Cui et al., 2017) and Reinforced Mnemonic Reader (Hu et al., 2017).###…78.7 69.6 / 77.7 FastQAExt (Weissenborn et al., 2017) 70.8 / 78.9 70.8 / 78.9 ReasoNet (Shen et al., 2017b) 69.1 / 78.9 70.6 / 79.4 Document Reader (Chen et al., 2017) 70.0 / 79.0 70.7 / 79.4 Ruminating Reader (Gong & Bowman, 2017) 70.6 / 79.5 70.6 / 79.5 jNet (Zhang et al., 2017) 70 LSTMs as is…###According to the observations from our experiments and previous works, such as (Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Chen et al., 2017), the validation score is well correlated with the test score.",other,reporting model performance comparisons
580,5cede102da562983788e2310,5dab371fecc43904c0b785a50136d20cee43a99a,Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation,58d82fced649053542fd72f2,Neural machine translation with reconstruction,"This model is similar to the architecture first described by Tu et al. (2017). It combines two encoder-decoder models in a cascade-like fashion, with the decoder of the first stage and the encoder of the second stage being shared (Fig.###com/neulab/xnmt Weiss et al. (2017) report improvements from deeper decoders, but we encountered stability issues and therefore restricted the decoder to a single layer.###Multi-task training for direct speech translation models has previously been used by Weiss et al. (2017); Bérard et al.###As our second contribution, we apply a twostage model (Tu et al., 2017; Kano et al., 2017) as an alternative solution to our problem, hoping that such models may overcome the data efficiency shortcoming of the direct model.###We do not employ the two-phase beam search of Tu et al. (2017) because of its prohibitive memory requirements.",impact-revealing,describing the architecture and improvements of a model
3551,55465e4c0cf2939c2feea7c8,ff917ed73fa5491928c44e8f8850f0d311b0400c,Branch prediction and the performance of interpreters — Don't trust folklore,53e9a09eb7602d9702983d2b,Context Threading: A Flexible And Efficient Dispatch Technique For Virtual Machine Interpreters,"To be more precise, several versions of threading have been proposed: token threading (illustrated in Figure 2), direct threading [1], inline threading [24], or context threading [2].",other,describing various threading methods
3639,5f0d8b6891e011047aff993b,1c53d27c742fb4658fa03085c7c2ca014a122385,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,5736960c6e3b12023e51fb74,TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems.,"More powerful supercomputers [1], [2] and advanced libraries [3], [4], [5], [6], [7] enable the training of more complex models on bigger data sets using advanced processing units (incl.",other,highlighting advancements in computational resources for model training
690,53e9afd3b7602d9703a26d76,a17a0793186e1efaed8604783606a17484bb161f,Coverage directed test generation for functional verification using Bayesian networks,56d8137ddabfae2eee61deea,A Genetic Approach To Automatic Bias Generation For Biased Random Instruction Generation,"For example, in [2], a genetic algorithm is used to select and modify testcases to increase coverage.",impact-revealing,reporting prior findings on genetic algorithms for testcase selection
1129,,9e4d7b3504c0900d596a432278d0deca6243a0db,DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model,,,"###The advancements in 2D diffusion models (Ho et al., 2020; Song et al., 2020a; Rombach et al., 2022a) have greatly simplified the image content creation process and revolutionized 2D design workflows.###The original x 0 distribution addressed in 2D DMs is the (single) image distribution in a dataset.###Other approaches in this category build on optimization using a differentiable 3D scene representation along with the priors encoded in 2D DMs (Poole et al., 2022; Lin et al., 2023a; Wang et al., 2022; 2023).###This is equivalent to x 0 prediction in 2D DMs (Song et al., 2020a); one can solve for x t − 1 from the input x t and prediction x 0 to enable progressive denoising during inference.###We utilize DDIM (Song et al., 2020a) algorithm to improve the inference speed.###3D-aware Diffusion Models (DMs) .###The quality and diversity of their results, however, is far from that achieved by 2D DMs.###Several strategies have been proposed to extend DMs to the 3D domain.###DMs have emerged as foundation models for visual computing, offering unprecedented quality, fine-grained control, and versatility for 2D image generation (Ho et al., 2020; Song et al., 2020b; Rombach et al., 2022a; Po et al., 2023).###Both solve the 3D generation problem using 2D DMs with 3D-aware denoisers.###In particular, the two-stage 3D DMs, Shap-E (3D encoder + latent diffusion) and Point-E (point diffusion + points-to-SDF regression), lead to lower-quality 3D assets, often with incomplete shapes and blurry textures; this suggests the inherent difficulties in denoising 3D points or pretrained 3D latent spaces, a problem our model avoids.###Similar to other DMs, our model can generate various instances from the same input image with different random seeds as shown in Fig.###In this case, the denoiser essentially learns to fill in the missing pixels within the noisy unseen views using cues extracted from the first input view, similar to the task of image inpainting which has been shown to be addressable by 2D DMs (Rombach et al., 2022a).",impact-revealing,highlighting the impact of advancements in 2D diffusion models on image content creation
1216,,e4ce0eab36ce9c75bf4ccf0d744ed01f37369f27,Multiscale modeling of layer formation in epidermis,,,"###To investigate how embryonic epidermis achieves appropriate stratification, a four-stage cell lineage model (Eq 1) similar to [18, 19] was used to explore the transcriptional mechanisms governing growth and differentiation incorporating the Ovol transcription factors, and the model is constrained by the experimental data.###We first consider a non-spatial cell lineage model [11, 18, 19] consisting of three different cell types with four stages: basal stem cells, proliferative intermediate spinous cells, mature nonproliferative spinous cells, and granular cells.",impact-revealing,describing the methodology for investigating epidermal stratification
346,5efb0d5691e011063336d39c,0cee58946a13a5c2845647b4af8b9d2bf52a8b6b,BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision,5d1eb9d4da562961f0b0e960,XLNet: Generalized Autoregressive Pretraining for Language Understanding.,"These language models have achieved state-of-the-art performance in many popular NLP benchmarks with appropriate fine-tuning (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019; Lan et al., 2020b; Raffel et al., 2019), which demonstrates their strong ability in modeling the text data.###, 2019), XLnet (Yang et al., 2019)) which are particularly attractive to this task due to the following merits: First, they are very large neural networks trained with huge amounts of unlabeled data in a completely unsupervised manner, which can be cheaply obtained; Second, due to their massive sizes (usually having hundreds of millions or 1https://www.###These language models have achieved state-of-the-art performance in many popular NLP benchmarks with appropriate ﬁne-tuning (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019; Lan et al., 2020b; Ra ﬀ el et al., 2019), which demonstrates their strong ability in modeling the text data.###To address the challenges in learning from distant supervision, our approach leverages the power of pre-trained language models (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLnet (Yang et al., 2019)) which are particularly attractive to this task due to the following merits: First , they are very large neural networks trained with huge amounts of unlabeled data in a completely unsupervised manner , which can be cheaply obtained; Second , due to their massive sizes (usually having hundreds of millions or billions of parameters), they have strong expressive power to capture general semantics and syntactic information e ﬀ ectively.###…supervision, our approach leverages the power of pre-trained language models (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLnet (Yang et al., 2019)) which are particularly attractive to this task due to the following merits: First , they are very large neural networks trained…",impact-revealing,highlighting the effectiveness and advantages of pre-trained language models in NLP tasks
3974,5d1eb9ddda562961f0b17476,c0aaee2337e5af680e5dca1bfc349a737dfec573,Fixing the train-test resolution discrepancy,5a73cbc317c44a0b3035ec55,Progressive Neural Architecture Search,"We also conduct some experiments using the PNASNet-5-Large [24] architecture that exhibits good performance on ImageNet with a reasonable training time and number of parameters (86.###For larger experiments, we use PNASNet-5-Large [20], learned using “neural architecture search” as a succession of interconnected cells.###For PNASNet-5-Large we use the pretrained version from Cadene’s GitHub repository [1].###PNASNet-5 (N = 4, F = 216) [24] 331 331 86.###For larger experiments, we use PNASNet-5-Large [24], learned using “neural architecture search” as a succession of interconnected cells.###We also conduct some experiments using the PNASNet-5-Large [20] architecture that exhibits good performance on ImageNet with a reasonable training time and number of parameters (86.1M).###In Table 1 we report the result on the PNASNet-5-Large.",other,reporting on the use of a specific architecture for experiments
621,5e15adca3a55ac47ab5b0729,97ebd482a78e6e6c1ba51da5e1b2f8e7640cc8b5,hyperbolic graph convolutional neural networks,599c7972601a182cd263e4b8,Poincaré Embeddings for Learning Hierarchical Representations.,"For instance, Poincaré embeddings [29] capture the hyperbolic properties of real graphs by learning shallow embeddings with hyperbolic distance metric and Riemannian optimization.###For shallow methods, we consider Euclidean embeddings (EUC) and Poincaré embeddings (HYP) [29].###Shallow embedding methods have also been developed in hyperbolic geometry [29, 30] for reconstructing trees [35] and graphs [5, 13, 22], or embedding text###For link prediction, we use the Fermi-Dirac decoder [23, 29], a generalization of sigmoid, to compute probability scores for edges:",impact-revealing,providing context on embedding methods in hyperbolic geometry
2996,5c2c7a9217c44a4e7cf3189c,9ae43e25b04f5c35173b0bf490612015bd86c08f,Face-Focused Cross-Stream Network for Deception Detection in Videos,5bbacb9e17c44aecc4eafda9,Deep Bimodal Regression of Apparent Personality Traits from Short Video Sequences.,"This topic covers a wide range of research problems such as deception detection [36, 37], emotion recognition in videos [54, 18], personality computing [49, 56], and action recognition [6, 26, 28, 35, 40, 47, 59].###For example, it is often important to recognize the deceptive behaviors [36, 37], emotions [54, 18], or personality traits [49, 56] of the subject of a video in real-world scenarios.###With the recent rapid development of human-centric AI, human-centric video analysis [48, 49, 54, 30, 32, 61, 27] has also begun to draw much attention from the computer vision community.",other,highlighting the broad range of research problems in human-centric video analysis
2736,53e9ad3bb7602d970372109a,8686368908956c506f6e3bbcaa2810adfda14914,NoC-sprinting: Interconnect for fine-grained sprinting in the dark silicon era,53e9aa0fb7602d970337917c,A detailed and flexible cycle-accurate Network-on-Chip simulator,0 [10] to test NoC-sprinting under different traffic scenarios.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
719,5f3268fb91e011bc1612aeab,dee8650c0a65588a09eb86751c600fb67a030bbc,Speech Driven Talking Face Generation From a Single Image and an Emotion Condition,5aed14d617c44a4438158d5d,Generating Talking Face Landmarks from Speech.,"dof-hearing population. Consequently, researchers developed systems that can automatically generate talking faces from speech in order to provide the visual cues when they are not available [5], [6], [7], [8], [9], [10], [11], [12]. These systems can increase the accessibility of abundantly available audioonly resources for the hearing impaired population. They can also ﬁnd wide applications in enter###ther. However, this system works only for a single speaker. Another two-stage system is proposed by Chen et al. [5]. The system ﬁrst predicts 68 face landmarks from speech using an LSTM-based network [7], and then predicts a few talking face images from the condition image and the face landmarks. They employ a discriminator network to improve image quality. In another work, Egor et al. [19] proposed ",impact-revealing,highlighting the development of systems for generating talking faces to aid the hearing impaired
2986,5f4f6ec291e0111f07b30a2b,3259c9ab1714a4cfdf6439cca6bdc5f78d78fda3,NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size,5eccb534e06a4c1b26a83a32,Can Weight Sharing Outperform Random Architecture Search? An Investigation With TuNAS,"TAS and FBNetV2 optimize the architecture parameters in a differentiable way, and TuNAS uses REINFORCE.###Observations on the topology search space Ss. TAS, FBNetV2, and TuNAS utilise a set of architecture parameters to enable a learnable distribution of the number of channels (#channels), while they have different mechanisms to optimise these architecture parameters.###(IV) Differentiable algorithms. e.g., first order DARTS (DARTS-V1) [8], second order DARTS (DARTS-V2), GDAS [7], SETN [17], TAS [21], FBNet-V2 [44], TuNAS [49].###As shown in Figure 7b, TAS can quickly find much better model than
both of FBNetV2 and TuNAS3.###In contrast, TuNAS or FBNetV2 can only evaluate one candidate during each search step.###(3) TuNAS samples masks based on the learnable distribution [49].###Notably, DARTS, GDAS, SETN are specifically designed for the topology search space St. TAS, FBNet-V2, and TuNAS can be used on the size search space Ss.###, first order DARTS (DARTS-V1) [8], second order DARTS (DARTS-V2), GDAS [7], SETN [17], TAS [21], FBNet-V2 [44], TuNAS [49].###Since the original hyper-parameters of FBNetV2, TAS, and TuNAS are chosen based on a different setting than our benchmark, they may be sub-optimal for the small-scale datasets used in NATS-Bench.",other,describing optimization methods for architecture parameters
4009,5ebe685391e0117693a52241,99314a532a3358cb86064fc8917ed2c283227539,NAT: Noise-Aware Training for Robust Neural Sequence Labeling,57a4e91aac44365e35c976e2,Improving the Robustness of Deep Neural Networks via Stability Training,"Zheng et al. (2016) pointed out the output instability issues of deep neural networks.###Inspired by recent research in computer vision (Zheng et al., 2016), Neural Machine Translation (NMT; Cheng et al., 2018), and ASR (Sperber et al., 2017), we propose two Noise-Aware Training (NAT) objectives that improve the accuracy of sequence labeling performed on noisy input without reducing…###• We implement a stability training method (Zheng et al., 2016), adapted to the sequence labeling scenario, which explicitly addresses the noisy input data problem by encouraging the model to produce a noise-invariant latent representation ( § 3.4).###Zheng et al. (2016) presented a general method to stabilize model predictions against small input distortions.",other,highlighting the significance of addressing output instability in deep neural networks
862,5dc5488edf1a9c0c41511e82,d33e7907ae6e8a0c8396822df09806661de85710,scaling the capacity of memory systems: evolution and key approaches,58d82fbdd649053542fd3421,"The 2 PetaFLOP, 3 Petabyte, 9 TB/s, 90 kW Cabinet: A System Architecture for Exascale and Big Data.","A notable proposed scalable system architecture is described in [34] in which, NAND flash is used as main memory technology and DRAM as a cache for flash, allowing a substantial increase in main memory",impact-revealing,reporting a notable system architecture proposal
2750,5eede0b091e0116a23aafc15,9a75cb455b4e70c66f3b72e6bb1498d8cab72fb2,Big Self-Supervised Models are Strong Semi-Supervised Learners,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"The effectiveness of big models have been demonstrated on supervised learning [60–63], ﬁne-tuning supervised models on a few examples [64], and unsupervised learning on language [9, 65, 10, 66].###…pretraining followed by supervised ﬁne-tuning on a few labeled examples has been extensively used in natural language processing [6, 5, 7–9], but has only shown promising results in computer vision very recently [19, 20, Table 3: ImageNet accuracy of models trained under semi-supervised…",other,highlighting the effectiveness of big models in various learning scenarios
3042,573697f96e3b12023e6d2f31,97acdfb3d247f8250d865ef8a9169f06e40f138b,EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding,573696fe6e3b12023e5f982b,On Speaker Adaptation Of Long Short-Term Memory Recurrent Neural Networks,"[13, 14, 15] as the acoustic models, and the Long Short-Term Memory (LSTM) units [16, 17, 18, 19] as the RNN building blocks.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1751,,91d8911bbfa02655205847bdee38cf67610e6471,HPV infections in head and neck cancers (HNSCC) – clinical course and efficiency of therapy,,,"###Nevertheless, the authors did not consider patient HPV status and its influence on the studied results [23].",impact-revealing,highlighting a limitation in the authors' study
3517,5cede0e8da562983788c741f,b56e5fb4f367a8d54614f1047bd4f9a2d58b9973,Multi-Interest Network with Dynamic Routing for Recommendation at Tmall,59ae3c262bbe271c4c71ef98,Deep Matrix Factorization Models for Recommender Systems.,"rial applications proposed by [7, 31], various types of deep models have gained significant attention. Neural Collaborative Filtering (NCF) [11], DeepFM [9] and Deep Matrix Factorization Models (DMF) [27] construct a neural network composed of several MLPs to model the interaction between users and items. [23] presents a novel solution to top-N sequential recommendation by providing an united and flex",other,acknowledge existing deep learning models in recommendation systems
2717,573698426e3b12023e70bf59,1917bfe805b46fe3a45903e803b27bd41719cf3c,Symbiotic job scheduling on the IBM POWER8,53e9ab38b7602d97034c7a03,The Benefit Of Smt In The Multi-Core Era: Flexibility Towards Degrees Of Thread-Level Parallelism,"Recently, Eyerman and Eeckhout [7] show that a multicore processor consisting of SMT cores has an additional benefit other than increasing throughput.",other,highlighting a significant finding in multicore processor performance
1408,,e9565e0242aed311888bf4dcc6fef06faf6fc61a,Confidence-Conditioned Value Functions for Offline Reinforcement Learning,,,"###Next, we evaluate our algorithm against prior methods on Atari games (Bellemare et al., 2013) with offline datasets of varying size and quality, previously considered by Agarwal et al. (2020); Kumar et al. (2020).###Our objective, similar to conservative Q-learning (CQL) (Kumar et al., 2020), uses regularization to learn Q-values for all levels of pessimism and optimism, instead of anti-exploration bonuses that may be difficult to accurately compute in complex environments (Rezaeifar et al., 2021).###One common approach to handle distribution shift in offline RL is to optimize a a conservative lower-bound estimate of the expected return, or Q-values (Kumar et al., 2020; Kostrikov et al., 2021; Yu et al., 2020).###Specifically, we have the following iterative update as an alternative to equation 4: where like in Kumar et al. (2020), R is some regularizer (typically the entropy of π ).###We select 5 representative Atari games, similarly considered in Kumar et al. (2020).###We compare to prior offline RL methods, REM (Agarwal et al., 2020) and CQL (Kumar et al., 2020), and ablations of our method where we either replace confidence-conditioning with a simple ensemble, which we dub adaptive ensemble value-learning (AEVL), or behave according to a fixed confidence…###Conservative approaches traditionally rely on estimating the epistemic uncertainty, either explicitly via exploration bonuses (Rezaeifar et al., 2021) or implicitly using regularization on the learned Q-values (Kumar et al., 2020).###On the other hand, conservative methods learn a lower-bound, or conservative, estimate of return and optimize the policy against it (Kumar et al., 2020; Kostrikov et al., 2021; Kidambi et al., 2020; Yu et al., 2020; 2021).###Another relevant method is conservative Q-learning (CQL) (Kumar et al., 2020), which proposes a regularizer to the standard objective to learn pessimistic Q-values: Here, π is some policy that approximately maximizes the current Q-function iterate, and R is some regularizer.###However, akin to Kumar et al. (2020), we can show a lower-bound on the values for all states.###Here, we propose a new objective that is inspired by how CQL achieves pessimistic value functions (Kumar et al., 2020).",impact-revealing,reporting prior findings and methods in offline reinforcement learning
2058,,92a20d6b6253049417854e019827e09ed01c3acb,Lifestyle and Metabolic Syndrome: Contribution of the Endocannabinoidome,,,###This evidence is reinforced by the recent finding that some commensal bacteria produce endocannabinoid-like compounds able to activate the same receptors as their host cell counterparts [17].,impact-revealing,highlighting significant findings on the role of commensal bacteria
3179,5eccb534e06a4c1b26a83514,a9682a89b2fef793507c365a577f1521745db96c,Boosting the Transferability of Adversarial Samples via Attention,599c7cdf601a182cd27e33f3,Practical Black-Box Attacks against Machine Learning.,"One is query-based [24, 2, 10], and the other one is transfer-based [41, 39, 8, 21, 23].###Alternatively, attackers can approximate the loss gradient of the target model through training a local replica [24] or finite difference techniques [2].",other,describing different attack methods in model evaluation
1764,,117fd3a77f887f827e7f3521964b51eb788d33c5,Interstellar: Using Halide's Scheduling Language to Analyze DNN Accelerators,,,"###Eyeriss highlighted the importance of on-chip dataflow on energy efficiency and proposed a row-stationary heuristic [8, 9].",impact-revealing,highlighting the significance of on-chip dataflow for energy efficiency
1405,,04da578636e7eacaeb7fb3b3303ba8bdbc8ebd22,Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning,,,"###For this reason, SAC is more commonly used as a default benchmark and starting point for a lot of algorithms and applications in the RL community [5, 19, 21].",impact-revealing,highlighting the significance of SAC as a benchmark in reinforcement learning
2567,5bbacb9e17c44aecc4eaff2b,728938faa28719e22a9de5ce87717180196da01b,Semi-supervised Training for Improving Data Efficiency in End-to-end Speech Synthesis,5b67b4b417c44aac1c8672a0,ClariNet: Parallel Wave Generation in End-to-End Text-to-Speech.,"We are now able to produce natural prosody with high audio fidelity using a much simplified voice building pipeline [1, 2, 3].",other,highlighting advancements in natural prosody production
876,5ea6adfa91e011a546871d52,7066df8fd89cca546d1ef3d66679cb15eba48d50,flat: chinese ner using flat-lattice transformer,599c7987601a182cd2648373,Attention Is All You Need.,"However, RNN and CNN are hard to model long-distance dependencies (Vaswani et al., 2017), which may be useful in NER, such as coreference (Stanislawek et al.###However, RNN and CNN are hard to model long-distance dependencies (Vaswani et al., 2017), which may be useful in NER, such as coreference (Stanislawek et al., 2019).###Transformer (Vaswani et al., 2017) adopts fully-connected self-attention to model the long-distance dependencies in a sequence.###…relative position encoding of spans is a simple non-linear transformation of the four distances: where W r is a learnable parameter, ⊕ denotes the concatenation operator, and p d is calculated as in Vaswani et al. (2017), where d is d ij and k denotes the index of dimension of position encoding.###Transformer (Vaswani et al., 2017) adopts fully-connected selfattention to model the long-distance dependencies in a sequence.",impact-revealing,highlighting the advantages of transformer models in handling long-distance dependencies
2243,5e09a76bdf1a9c0c41677a7b,71bd6b3a2bfa54ee4ea8499be0c4ff478fd735f6,POLAR++: Active One-Shot Personalized Article Recommendation,53e9ae49b7602d97038629c6,Output Divergence Criterion for Active Learning in Collaborative Settings,"More advanced methods are based on uncertainty reduction [48] or error reduction [49], [50].",other,acknowledge existing advanced methods
2887,58d82fcbd649053542fd6178,515a21e90117941150923e559729c59f5fdade1c,the concrete distribution: a continuous relaxation of discrete random variables,5c755233f56def97985dfb79,A Poisson process model for Monte Carlo,"Follows directly from (a) and the Gumbel-Max trick (Maddison, 2016).###See (Hazan et al., 2016) for a collection of related work, and particularly the chapter (Maddison, 2016) for a proof and generalization of this trick.###To motivate the construction of concrete random variables, we review a method for sampling from discrete distributions called the Gumbel-Max trick [19, 27, 26].###So, the correctness of (9) also reduces to a well known result regarding the argmin of exponential random variables; see [26] for a proof and general treatment of this trick.",other,providing context for the Gumbel-Max trick and related work
1875,,a90332b5a45574f1ab3847cff86039e4aa3e8cb8,Perinatal Outcomes of Prenatal Probiotic and Prebiotic Administration: An Integrative Review,,,"###PURPOSE An integrative review was chosen as the most robust approach to allow the inclusion of the diverse methods used in the body of scientific literature.(13) The purpose of this integrative review was to identify, critique, and synthesize the perinatal evidence on prenatal probiotics###tions raised concerns about overrepresenting primary data sources.(13) Table 1 is organized by study design, according to identified research collectives (lettered A-H), when applicable.###Although most maternal and neonatal outcomes were statistically nonsignificant, a comprehensive presentation is provided in adherence to the principles of the integrative review process.(13) Furthermore, some nonsignificant findings have clinical relevance for perinatal health professionals as the findings may suggest the safety of the intervention and###The goal is to review the evidence as it specifically applies to perinatal practice.(13)",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1153,,9b93dd409df3fcf3d9d0690317da35f37b1b7c72,Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising,,,"###…non-Markovian process into the de-noising diffusion probabilistic model (DDPM)[8] to formulate the denoising diffusion implicit model (DDIM)[13], we proposed a novel method, Implicit Image-to-Image Schrödinger Bridge (I 3 SB), which uses non-Markovian process during the inference of I 2 SB.",impact-revealing,introducing a novel method in image processing
3183,5c04967517c44a2c7470927f,b9015d4f1e591eba7ea21c3566c919f80f7c2afe,attentive long short-term preference modeling for personalized product search,53e9ae28b7602d9703839fb1,Temporal recommendation on graphs via long- and short-term preference fusion.,"It is affected by incidentally transient events, such as new product release, season change and special personal occasions like birthday [63], which can be inferred from the user’s recently purchased products.###It is well recognized that there are two types of user preferences [3, 63]: long-term ones and short-term ones.",other,providing context on user preferences and their influences
3485,5aed147c17c44a4438153a60,5245d411b9dd97ffafe07320981d1282e8f32764,"dCat: dynamic cache management for efficient, performance-sensitive infrastructure-as-a-service",53e9ba00b7602d97045fe5aa,Ibench: Quantifying Interference For Datacenter Applications,"Researchers have pointed out the importance of resource isolation in order to have performance isolation [12, 13] for both responsiveness of latency-critical applications as well as to reduce the dominating tail-latency impact for applications built with microservices model.",other,highlighting the significance of resource isolation for performance in microservices
1554,,272d17f2ddbb564966fb75e7166e4c149cf81503,The Impact of Case Management on Reducing Readmission for Patients Diagnosed With Heart Failure and Diabetes,,,"###Although Jencks et al. (2009) found that readmissions were associated with increases in length of stay, gaps in care, fragmented follow-up care, and poor patient outcomes, a more recent study found that the readmissions for patients diagnosed with HF were reduced by 1%–8%, when the LOS of the prior index admission was extended by 1 day (Carey & Lin, 2014).###The association of readmission and hyperglycemia is well supported (Iribarren et al., 2001).",impact-revealing,highlighting contrasting findings in readmission studies
2375,5eda19c991e01187f5d6d814,befc296197edcf5431e6042ee58f48d5dc2cf970,Convergence and Stability of Graph Convolutional Networks on Large Random Graphs,573696026e3b12023e5160cd,Molecular graph convolutions: moving beyond fingerprints,"In the past few years, they have been successfully applied to, for instance, node clustering [10], semi-supervised learning [25], or graph regression [22, 19], and remain one of the most popular variant of Graph Neural Networks (GNN).",other,highlighting the successful applications and popularity of Graph Neural Networks
3286,5dcbd5da3a55ac789b0dbc7f,f0efc23ecb6d4fb9745d555450b2c4a97e8ac4d5,Robust Design of Deep Neural Networks Against Adversarial Attacks Based on Lyapunov Theory,59ae3be32bbe271c4c71b9a9,Efficient Defenses Against Adversarial Attacks,Works such as [39] have empirically shown that bounding a layer’s response to the input generally improves robustness.,other,reporting prior findings on model robustness
2368,5a9cb65d17c44a376ffb83f3,e06357ac23811054acb23e2ea30d087f5beaef90,an interpretable reasoning network for multi-relation question answering,5550410f45ce0a409eb384f8,Sequence to Sequence Learning with Neural Networks.,"Seq2Seq (Sutskever et al., 2014) is a simplified seq2seq semantic parsing model, which adopts an LSTM to encode the input question sequence and another LSTM to decode the answer path.",other,describing a specific model architecture in semantic parsing
3721,5a260c8117c44a4ba8a30adf,ecf6c42d84351f34e1625a6a2e4cc6526da45c74,representation learning on graphs: methods and applications,599c7948601a182cd262bb1f,Neural Embeddings of Graphs in Hyperbolic Space.,"…algorithm to learn embeddings using random walks that “skip” or “hop” over multiple nodes at each step, resulting in a proximity measure similar to GraRep [9], while Chamberlan et al. [11] modify the inner-product decoder of node2vec to use a hyperbolic, rather than Euclidean, distance measure.",other,describing variations in algorithms for learning embeddings
671,5ce2d184ced107d4c6438f01,cd26a0ae3c5a65d807b8fa7134f3a44cfd0392bd,exploiting edge features for graph neural networks,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,"Those graph convolutional networks (GCNs) [11][18] combine graph node features and graph topological structural information to make predictions.###Indeed, the essential difference between GCN [18] and GAT [27] is whether we use the attention coefficients (i.###or symmetric normalization as in GCN [18]:###[18] approximate the polynomials using a re-normalized first-order adjacency matrix to obtain comparable results on graph node classification tasks.###The three citation network datasets are also used in [32] [18] [27].###The baseline methods we used are GCN [18] and GAT [27].###Following the experiment settings of [18][27], we use two layers of EGNN in all of our experiments for fair comparison.",impact-revealing,providing context on graph convolutional networks and their differences
2285,5cb06564ced107d4c6006f1c,19351711295bddc627f761d59a1ef58ab2fa7e2c,Identifying SDC-Causing Instructions Based on Random Forests Algorithm,53e9b77db7602d9704323f58,SmartInjector: Exploiting intelligent fault injection for SDC rate analysis,"We extract features of instructions according to our analysis and prior work [12, 13, 16, 17 and 18].###SmartInjector [12] proposes an intelligent fault injection framework to identify the SDC-prone instructions.",other,acknowledge prior work and its relevance to feature extraction
1499,,3a8f8b3a2ab49bbbf8bb7d7c586b25d5f0262b16,Cyber Vaccine for Deepfake Immunity,,,"###…and neutraliser, we use the U-Net as the backbone architecture and apply a state-of-the-art version by OpenAI with residual connection and multi-head attention mechanisms [67], which has been widely used as a diffusion probabilistic model for various computer vision tasks [68], [69], [70].",impact-revealing,describing the architecture and its applications
2116,,fbc670fb76cff49231c111f2db7be14b681d1aac,A Test of Two Models of Value Creation in Virtual Communities,,,"###with the exception of one study (see [43]), previous researchers mostly have speculated about the various trust processes, building robust conceptualizations without empirical testing.###A risk avoider who believes that a firm’s behavior is motivated by rational goals would want to process information about the firm in question and assess trust based on one or more trust processes (see [43]).###Further, Kim’s study was set in an online shopping context, with a focus on how consumer trust in a firm is fostered in an online retail environment. thus, we hypothesize as follows: Hypothesis 4: In a firm-sponsored virtual community, the relationships between the sponsor effort variables and customer trust in a sponsoring firm are stronger than the relationships between customer endorsement and customer trust (i.e., H1, H2, H3) in a sponsoring firm (Model 2 only). the Influence of a Sponsor’s Efforts on MGI (Model 2 Only) when a community sponsor makes efforts to provide access to quality content, foster member embeddedness, and encourage interaction among members, members benefit because these efforts help them to fulfill intrinsic needs, such as the need for information, belongingness, and social interaction [21].###However, in a study relying on the cultural dimensions of individualism and collectivism [34], Kim [43] found that the transference process of trust is more powerful in collective cultures than in individualistic cultures.###the transference process of trust formation operates when an individual has insufficient direct experience on which to base a decision to trust a firm [23, 43, 78].###Our study was conducted with a u.S.-based (individualistic) sample. while Kim’s findings do not suggest that the transference trust process is insignificant in individualistic cultures, they at least indicate that trust formed via the transference process would be weaker than trust formed via perceptions based on direct experience/interactions among our respondents.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
68,5f896fa591e01149071e45df,1d16d4cdc3fcce26e2c2097d13896ec09683eee3,Self-training for Few-shot Transfer Across Extreme Task Differences,5e4672c93a55ac14f595d8b5,A Simple Framework for Contrastive Learning of Visual Representations,"This induced notion of similarity is in contrast to current self-supervised techniques which often function by considering each image as its own class and dissimilar from every other image in the dataset (Wu et al., 2018; Chen et al., 2020).###We also compare to an additional baseline, SimCLR that uses the novel domain unlabeled data D u to train a representation using SimCLR(Chen et al., 2020), and then uses the resulting representation to learn linear classiﬁers for few-shot tasks.###A more recent (and better performing) line of self-supervised learning is contrastive learning (Wu et al., 2018; Misra & Maaten, 2020; He et al., 2020; Chen et al., 2020) which aims to learn representations by considering each image together with its augmentations as a separate class.###We use a state-of-the-art self-supervised loss function based on contrastive learning: SimCLR (Chen et al., 2020).",impact-revealing,highlighting differences between self-supervised techniques and contrastive learning
2691,5c04967517c44a2c74708b7e,c18663fea10c8a303d045fd2c1f33cacf9b73ca3,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,53e9b844b7602d970440513c,Imagenet: A Large-Scale Hierarchical Image Database,"One of the most challenging and popular machine learning tasks is to solve the ImageNet visual recognition challenge [16], where researchers compete to create the most accurate model that classiﬁes given im-Figure 1: Strong correlation between top-1 accuracy on ImageNet 2012 validation dataset and…",other,highlighting the challenge and popularity of the ImageNet visual recognition task
293,555048d345ce0a409eb71be1,eba36ac75bf22edf9a1bfd33244d459c75b98305,Recurrent convolutional neural networks for text classification,53e99beab7602d970249335e,Natural Language Processing (almost) from Scratch,"…(Bengio, Courville, and Vincent 2013) have led to new ideas for solving the data sparsity problem, and many neural models for learning word representations have been proposed (Bengio et al. 2003; Mnih and Hinton 2007; Mikolov 2012; Collobert et al. 2011; Huang et al. 2012; Mikolov et al. 2013).###We choose one set of commonly used hyper-parameters following previous studies (Collobert et al. 2011; Turian, Ratinov, and Bengio 2010).###Collobert et al. (2011) introduce convolutional neural network for semantic role labeling.###However, previous studies on CNNs tends to use simple convolutional kernels such as a ﬁxed window (Collobert et al. 2011; Kalchbrenner and Blunsom 2013).",impact-revealing,acknowledging the evolution of neural models for word representations and their hyper-parameter choices
2857,5e7232fe93d709897cfa3461,e39ec42bc0b393fd2d21e4fe9ae55d361e2a752b,A New Method of Fuzzy Support Vector Machine Algorithm for Intrusion Detection,599c7958601a182cd2632c6c,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.,"In recent years, neural networks have made remarkable achievements in computer vision [36–38] and natural language processing [39–41].",other,highlighting the achievements of neural networks in various fields
3769,5992a2ed5ba2006b76482df8,423ad8249c214e1346f337426f2da56deccf10a8,graph edge partitioning via neighborhood heuristic,573695516e3b12023e47c12e,A scalable distributed graph partitioner,"In this paper, we proposed a new graph edge partitioner Neighbor Expansion (NE) that outperforms other state-of-the-art ones including METIS [8] and Sheep [13] in terms of replication factors.###This does lead to the state-of-the-art replication factors on a great number of graphs [13], but it is not applicable to large graphs.###Among existing partitioners, METIS gives the lowest replication factor which is consistent with literature [3, 13].###Sheep [13] partitions the graph in a divide and conquer manner, which uses more graph structure than Oblivious and HDRF, but it works well only for tree-like graphs.###We compare our NE algorithm with six existing edge partitioners, including METIS [8], RAND [6], DBH [17], Oblivious [6], HDRF [15], and Sheep [13].###This important finding attracts great interests in edge partitioning recently [3, 6, 13, 15, 17].###Using only a single thread, NE and Sheep [13] have similar running time.###This result also echoes the fact reported by [13].",other,highlighting the performance of the proposed graph edge partitioner compared to existing methods
3383,5ea2b8bf91e01167f5a89d89,993377a3fc8334558463b82053904e3d684f29c0,SIGN: Scalable Inception Graph Neural Networks,5bdc31b417c44a1f58a0ba6c,How Powerful are Graph Neural Networks?,"On the smaller transductive datasets, we compare to the well established methods GCN Kipf and Welling (2017), GAT Velickovic et al. (2018), JK Xu et al. (2018), GIN Xu et al. (2019), ARMA Bianchi et al. (2019b), and the current state-of-the-art DIGL Klicpera et al. (2019).###Second, on grids complex features can be constructed compositionally from simple ones by using multiple layers, whereas in message passing-type graph neural networks such compositionality is not achievable due to their inability to disambiguate graph substructure [61, 40, 41], hence depth does not increase expressivity.",other,comparing performance of various methods on transductive datasets
4047,573696116e3b12023e52463f,fafcaf5ca3fab8dc4fad15c2391c0fdb4a7dc005,group equivariant convolutional networks,53e9a162b7602d9702a54583,Learning Invariant Representations with Local Transformations.,"This includes work on transforming autoencoders (Hinton et al., 2011), equivariant Boltzmann machines (Kivinen & Williams, 2011; Sohn & Lee, 2012), equivariant descriptors (Schmidt & Roth, 2012), and equivariant ﬁltering (Skibbe, 2013).###Larochelle et al. (2007) 10.38 ± 0.27 Sohn & Lee (2012) 4.2 Schmidt & Roth (2012) 3.98 Z2CNN 5.03 ± 0.0020 P4CNNRotationPooling 3.21 ± 0.0012 P4CNN 2.28 ± 0.0004 Table1.",other,acknowledge various approaches in the field
1950,,ba0a47e96188db1d4aba679a8c54a4e2a3ed0cfe,Efficient Coflow Scheduling of Multi-Stage Jobs with Isolation Guarantee,,,"###Not until all parallel flows have finished transmission will the MapReduce shuffle phase complete, thus the slowest flow in a coflow critically affects the completion time of reducer tasks.###In many data-parallel frameworks like MapReduce/Hadoop, the job and coflow properties, such as source, destination, amount of data transferred of each flow, coflow dependencies are known as a priori (e.g., after the mapper phase in MapReduce) [6], [7], [15].###Varys [6] uses the smallest-effective-bottleneck-first (SEBF) heuristic to sort coflows and the minimum-allocation-for-desired- duration (MADD) heuristic to preferentially allocate least bandwidth to coflows for minimizing average CCT and deadline missing rate.###For example, a MapReduce job distributes mapper tasks and reducer tasks on two sets of machines (not necessarily be disjoint) determined by a master process.###We take as an example the minimum-allocation-fordesired-duration (MADD) algorithm, which is a common approach for sequential coflow bandwidth allocation [4], [6].###, minimizing average coflow-completion-time [6]–[10].###These traces are widely used as a benchmark for coflow scheduling analysis [6], [7], [14], [23].###Index Terms—datacenter network, coflow scheduler, multistage jobs
I. INTRODUCTION
Data-parallel frameworks, such as MapReduce [1], Hadoop [2] and Spark [3], are widely deployed in modern datacenters.###In the MapReduce case, flows in one shuffle phase are termed a coflow.###Workload: We take the realistic coflow traces [22] synthesized from real-world MapReduce workloads collected from a 3000- machine and 150-rack Facebook cluster.###, after the mapper phase in MapReduce) [6], [7], [15].",impact-revealing,describing the impact of coflow scheduling on data-parallel frameworks
1302,,98078cf3abe2a8ae2d6738a564531d8a7dee42a9,Chemistry of Volatile Organic Compounds in the Los Angeles Basin: Formation of Oxygenated Compounds and Determination of Emission Ratios,,,"###Statistical analyses like principal component analysis, positive matrix factorization, chemical mass balance, and others are widely used for source attribution of VOCs (Cai et al., 2010; Guo et al., 2004; Jorquera & Rappenglück, 2004; Leuchner & Rappenglück, 2010; Song et al., 2007).",impact-revealing,reporting widely used statistical analyses for source attribution
239,5a73cbcc17c44a0b3035f1d3,14058c2ebe9905fe5c3acf8b1bfcd5390fe32a28,Deception Detection in Videos,56d86073dabfae2eee7807ea,Deception Detection using Real-life Trial Data,GT-MicroExpression (P ´ erez-Rosas et al. 2015) alone is better than high-level micro-expression features (which is the con-ﬁdence score of the micro-expression classiﬁer).,impact-revealing,highlighting the superiority of GT-MicroExpression over other features
1378,,91e9af722a6e33a9753068f45093782edeaee275,Valid population inference for information-based imaging: From the second-level t-test to prevalence inference,,,"###cies and perform a test at each voxel in these maps, i.e. we have to adjust for multiple comparisons. To do so, we need to specify a spatially extended version of the prevalence null. Again following Friston et al. (1999a), our spatially extended null hypothesis is: — there is an effect with prevalence g g0 in a small area, — and no effect everywhere else. The justiﬁcation for this is that in experiments investigati###ion inference — and, for this data set, statistical power. However, the result of Fig. 4b also calls into question whether the assumption that an effect is constrained to a ‘small area’ (adopted from Friston et al., 1999a) is generally adequate. 5 Discussion In this paper we have shown that the t-test on accuracies commonly used in MVPA studies is not able to provide population inference because the true single-subje###t or below a threshold g0. The rejection of this null hypothesis therefore allows to infer that g &gt; g0. The approaches of Rouder et al. (2007), Rosenblatt et al. (2014), Stephan et al. (2009), and Friston et al. (1999a) are all candidates to be adapted for information-based imaging, to provide population inference with respect to the prevalence of an information effect. In the following we demonstrate this in deta###rior distribution over the space of different model frequencies. In a speciﬁc application this discrete distribution may describe the distinction between a zero effect and a generic non-zero effect.9 Friston et al. (1999a) build upon their previous idea of a conjunction test (Price and Friston, 1997; Worsley and Friston, 2000) and introduce the minimum-statistic approach. Their analysis proceeds in two steps: ﬁrst, t###n to support MVPA. 14 4 Permutation-based information prevalence inference using the minimum statistic In this part we recapitulate the minimum-statistic approach to prevalence inference developed by Friston et al. (1999a), adapt it to be based on permutation statistics, and detail the resulting algorithm. Applied to information-like measures this method allows us to achieve information prevalence inference, i.e. inf",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3486,5c2c7a9217c44a4e7cf314f8,b5e6fb219ca76f0175bd9f28e1460280fa11d5e1,making classification competitive for deep metric learning,5550414145ce0a409eb39bea,Deep metric learning using Triplet network.,"Standard deep neural network metric learning approaches learn image representations through the local relationships between images in the form of pairs [3] [1] or triplets [8] [19].###The metric learning losses, such as contrastive loss[3] and triplet loss[8], are formulated to minimize intra-class distances and maximize inter-class distances.",other,describing standard approaches in deep neural network metric learning
1388,,be68456b13de804d76b55efc98a6858706fa18b9,A Layered Analysis of Consensus,,,"###The benefit of working in a submodel or a set of runs with a simpler structure than that of the original model is well known; some recent examples are [4, 10, 11, 9, 27, 34].###The other, more global, type of proof is closer to the topological approach of works such as [10, 26, 23, 34] and, in particular, [24].###The permutation layering is inspired by the immediate snapshot wait-free model of [10, 34], although we define it both for the message-passing and for the shared-memory models, both 1-resilient.###, [25, 10, 26, 34]) focus on the local final states of processes.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3432,5736974d6e3b12023e638aca,5b9c2b3f85920bc0e160b484ffa7a5f0a9d8f22a,Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction,5550456345ce0a409eb55d33,Typed Tensor Decomposition of Knowledge Bases for Relation Extraction.,"These were soon followed by TransE (Bor-des et al., 2013), Neural Tensor Networks (Socher et al., 2013), and many variants on all of these algorithms (Chang et al., 2014; Garc´ıa-Dur´an et al., 2014; Wang et al., 2014).",other,acknowledge the evolution of algorithms in the field
635,5f993ec291e011a3fbe2fb5c,f1e5e65941617604923225cc4bf464e370fcae67,Combining Label Propagation and Simple Models Out-performs Graph Neural Networks,5ce2d032ced107d4c635260c,Predict then Propagate: Graph Neural Networks meet Personalized PageRank,"For comparable GNN models to our framework (in terms of simplicity or style), we use GCN, SGC, and APPNP.###This type of prediction smoothing is similar in spirit to APPNP (Klicpera et al., 2018), which we compare against later.###For wikiCS, we use APPNP as reported by Mernyei & Cangea (2020).###Not only is this signiﬁcantly more computationally expensive, it also prevents APPNP from incorporating label information at inference.###The Approximate Personalized Propagation of Neural Predictions (APPNP) framework is most relevant to our work, as they also smooth base predictions (Klicpera et al., 2018).###However, APPNP is trained end-to-end, propagates on ﬁnal-layer representations instead of softmaxes, does not use labels, and is motivated differently.###Compared to APPNP, our framework produces more accurate predictions, is faster to train, and more easily scales to large datasets.",impact-revealing,comparing and contrasting GNN models with a focus on APPNP
961,,b3f69e2bb115b5c706c00cb4802524b290ed9b5e,A systematic review of observational studies exploring the relationship between health and non-weight-centric eating behaviours,,,"###Intuitive eating was measured by the Intuitive Eating Scale (IES) validated by Tylka et al., 2006 (Tylka, 2006) and Hawks et al., 2005 (Hawks, Merrill, & Madanat, 2004) (n = 21), and the Intuitive Eating Scale-2 (IES-2) including versions validated in English (Tylka et al., 2013), Turkish (Bas et al., 2017), French (Camilleri et al., 2015), Portuguese, (Duarte, Gouveia, & Mendes, 2016; da Silva et al., 2020) and German (Ruzanska et al., 2017; Van Dyck et al., 2016) (n = 27).###…and Hawks et al., 2005 (Hawks, Merrill, & Madanat, 2004) (n = 21), and the Intuitive Eating Scale-2 (IES-2) including versions validated in English (Tylka et al., 2013), Turkish (Bas et al., 2017), French (Camilleri et al., 2015), Portuguese, (Duarte, Gouveia, & Mendes, 2016; da Silva et al.,…###It was a requirement that studies include a validated measure (Ginty, 2020) of eating behaviour (eg. the Intuitive Eating Scale) (Tylka et al., 2013) to allow for comparison between studies and ensure validity of the eating behaviours explored.",impact-revealing,reporting the use of validated measures for intuitive eating
491,58437785ac44360f108432a7,92527ace7f75188b5ec209ff7d59f431343075e4,Video-based emotion recognition using CNN-RNN and C3D hybrid networks,573698456e3b12023e70eea1,Recurrent Neural Networks for Emotion Recognition in Video,", ) n z z z via the following equations [5, 8]:###Nevertheless, many researchers have tried to identify emotions in videos based on computer vision technologies [3, 5, 8, 23].###results in the history of EmotiW challenges [5, 27].###3 Hybrid CNN-RNN and C3D Networks Previous work shows that either CNN-RNN or C3D model alone can achieve good performance in action recognition [5, 6, 8].###Previous winners usually focus on facial graph analysis [3, 23] or designing specific CNN-RNN networks [5].###Unlike C3D networks, a few works are given for video-based emotion recognition using CNN or RNN structures in recent papers [4, 5, 19].###A particular type of recurrent neural networks, the Long ShortTerm Memory (LSTM) recurrent neural network is widely adopted [4, 5, 8].",impact-revealing,acknowledge existing research in video emotion recognition
810,5d245bb6da56295a28fcd54f,25fd9e491c748995e94719f52d896a41299b5b75,geometric scattering for graph data analysis,53e99813b7602d970202a31c,Deep Scattering Spectrum,"…Euclidean case, while the stability of scattering transforms to deformations can be established analytically (Mallat, 2012), their capacity is typically examined by empirical evidence when applied to machine learning tasks (e.g., Bruna & Mallat, 2011; Sifre & Mallat, 2012; Andén & Mallat, 2014).###Indeed, scattering features have been shown effective in several audio (e.g., Bruna & Mallat, 2013a; Andén & Mallat, 2014; Lostanlen & Mallat, 2015; Andén et al., 2018) and image (e.g., Bruna & Mallat, 2013b; Sifre & Mallat, 2014; Oyallon & Mallat, 2015; Angles & Mallat, 2018) processing…",impact-revealing,highlighting the effectiveness of scattering features in various domains
3390,59ae3be32bbe271c4c71b8e0,2b166bb181428aa3e6837d2081500208af7c70ed,Weak Memory Models: Balancing Definitional Simplicity and Implementation Flexibility,573698776e3b12023e737e32,"Modelling The Armv8 Architecture, Operationally: Concurrency And Isa","in operational definitions: memory models of x86, ARM and POWER have all been formalized operationally [2], [7], [8], [15], [25], and researchers are even seeking operational definitions for high-level languages like C++ [26].###TSO Simple; I2E [2] Simple [3] Multi-copy atomic No Only St-Ld reordering Yes RMO Doesn’t exist Simple; needs fix [4] Multi-copy atomic No All four Yes Alpha Doesn’t exist Medium [5] Multi-copy atomic No All four No RC Doesn’t exist Medium [6] Unclear No All four Yes ARM and POWER Complex; non I2E [7], [8] Complex [9], [10] Non-atomic Yes All four Yes###implemented using the ARMv8 flowing model, which is a general abstraction of non-atomic memory systems [8].",other,acknowledge existing research on operational definitions in memory models
2020,,640c2656c6ec896cdf5e41704015eea431dc4668,Efficient multi-scale imaging of subsurface resistivity with uncertainty quantification using ensemble Kalman inversion,,,"###Since our examples also estimate the length scales, we call our approach “multi-scale”, or “hierarchical EKI (Chada et al. 2018)”.###However, the uptake has been slow and it has not been used in practical ERT problems (although they have been included in theoretical work (e.g. Aghasi et al. 2011; Chada et al. 2018; Chung et al. 2005)).###This approach builds on previous EKI with levelset parameterization work (Chada et al. 2018; Iglesias 2016; Iglesias et al. 2016, 2018) and is modified for ERT.",impact-revealing,highlighting the slow uptake and theoretical basis of a multi-scale approach in ERT problems
1842,,4b9ecb2171a65caa00d704f8aba2b7a998aae8fe,"Language, Semantics, and Methods for Cryptographic Protocols",,,"###The language was inspired by Paulson’s inductive approach to crypto-protocol analysis [Pau99,  Pau98 ].###This Petri-net semantics has a strong relation to both Paulson’s inductive set of rules [ Pau98 ] and strand spaces [THG98c].###Says A B M, means that agent A sends the message M to B .E ventGets A M instead means that A received the message M. The receiver does not know who is the sender of the message, unless the message itself contains enough information to authenticate it. The protocol traces are built up inductively by a set of rules as shown in Figure 1. The presented inductive denition, diers slightly from the one presented in Paulson [ Pau98 ], taking ...###In contrast to the Spi calculus [AG97] and other approaches [THG98c,  Pau98 ], we are no longer bound to the strong encryption assumption: fMgk = fNgk0 ) M = N ^ k = k 0###First introduced in Dolev and Yao [DY83], their model underlies a variety of approaches, e.g., [Low96, MMS97, THG98c, Sch96, Ros98b,  Pau98 ].###This fact is perhaps not surprising, since our language is inspired by Paulson’s inductive approach [Pau,  Pau98 , Pau99].###A precise description of how this set is obtained can be found in [ Pau98 , Pau99].###The inductive method is couched in terms of rules and this has some consequence such as \stuttering"" in the traces generated (see [ Pau98 , Pau99]), although they appear harmless in establishing safety properties.###This denition is used for example in Paulson’s inductive method [ Pau98 ] and in the strand-space approach [THG98c] but originates from Dolev and Yao [DY83].",impact-revealing,providing context on the inductive approach to crypto-protocol analysis
3250,55a6bae665ce054aad73115b,340f48901f72278f6bf78a04ee5b01df208cc508,Human-level control through deep reinforcement learning,53e9a832b7602d970317989a,General Game Playing: Overview of the AAAI Competition,"We set out to create a single algorithm that would be able to develop a wide range of competencies on a varied range of challengingtasks—a central goal of general artificial intelligence 13 that has eluded previous efforts 8,14,15 .",other,highlighting the ambitious goal of developing a versatile AI algorithm
1717,,bc26470f8ba99467d08476636771f97ed915860f,"Sharing a Personal Trainer: Personal and Social Benefits of Individualized, Small-Group Training",,,###We wondered to what extent members were motivated by goals to support others (7) or being more concerned with their own image (20).###We adapted a measure of interpersonal goals (7) for the exercise setting.,impact-revealing,describing the adaptation of a measure for interpersonal goals
952,,501a2f2d28ece691980cde7b6d9741ae3c4aa8c4,"Top-down, bottom-up, and horizontal models: the direction of causality in multidimensional, hierarchical self-concept models.",,,"###self-concept (Byrne, 1996a, 1996b;  Cross & Markus, 1994;  Harter, 1996; Hattie, 1992; Marsh, 1990a, 1993b; Marsh & Craven, 1997), but critical theoretical questions remain unanswered about the hierarchical aspect of this model.###A positive self-concept is widely recognized as both an important outcome and a way to facilitate other desirable outcomes in social, counseling, developmental, sport/exercise, health, education, business, and a variety of other settings (Bandura, 1986;  Cross & Markus, 1994;  Harter, 1986, 1996; Marsh, 1990a, 1993a).",impact-revealing,highlighting the importance of self-concept in various settings
53,5c0495ae17c44a2c747018e6,dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4,Mobilenetv2: Inverted Residuals And Linear Bottlenecks,599c7958601a182cd2632c6c,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.,"Trade-off hyper parameters As in [27] we tailor our architecture to different performance points, by using the input image resolution and width multiplier as tunable hyper parameters, that can be adjusted depending on desired accuracy/performance trade-offs.###MobileNetV2 uses k = 3 (3× 3 depthwise separable convolutions) so the computational cost is 8 to 9 times smaller than that of standard convolutions at only a small reduction in accuracy [27].###We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [27].###Depthwise Separable Convolutions are a key building block for many efficient neural network architectures [27, 28, 20] and we use them in the present work as well.###One minor implementation difference, with [27] is that for multipliers less than one, we apply width multiplier to all layers except the very last convolutional layer.###Following MobileNetV1[27] setup we use initial learning rate of 0.###This has been successfully exploited by MobileNetV1 [27] to effectively trade off between computation and accuracy via a width multiplier parameter, and has been incorporated into efficient model designs of other networks as well [20].###Our network design is based on MobileNetV1 [27].",impact-revealing,acknowledge the use of efficient neural network architectures and their trade-offs
819,5dbebb7447c8f766462c21c0,3d9baf7e87ec43f0ad486e2077824a346a58118e,Emotion-aware Chat Machine: Automatic Emotional Response Generation for Human-like Emotional Interaction,599c7968601a182cd263a565,Emotional chatting machine: emotional conversation generation with internal and external memory,", NLPCC 20132 and NLPCC 20143 emotion classification datasets by following [46], which contain 29, 417 manually annotated data in total, and the best performance(accuracy) of 0.###1Here we follow the work [46], where the emotion categories are {Angry, Disgust, Happy, Like, Sad, Other}.###From Table 2, we can observe that: (i) ECM performs worse than Seq2seq, the reason might be the emotion selection process is based on two-stage process, i.e., post emotion detection process and response emotion selection process, which would significantly reduce the diversity and quality of emotion response generation due to the errors of emotion classification and the transition pattern modeling procedure.###From Table 3, we can observe that: Seq2seqemb provides the worst performance as expected, this is because the generation process apparently interrupted would significantly reduce the accuracy and quality of generating responses and thus generate some hard-to-perceive sentences; ECM achieves a relatively better result, as it is good at modeling the emotion dynamics when decoding (i.e., internal memory) and assigning different generation probabilities to emotion/generic words for explicitly modeling emotion expressions (i.e., external memory); Seq2seq-emb results in a remarkable improvement over Seq2seq-emb in terms of semantic score, but it performs poorly when comparing sentiment score, which demonstrates the effectiveness of emotion injection, however the explicit two-stage procedure might reduce the smoothness of generated responses with low semantic score.###The results of EACM and ECM with (like, other) are similar and correct.###[46] successfully build an emotional chat machine (ECM) that is capable of generating emotional responses according to a pre-defined emotion category, and several similar efforts are also made by [11, 25], such as [48] proposed by Zhou et al.###However, responses given by ECM with other emotions are improper in semantics.###[46] develop an Emotional Chat Machine (ECM) model using three different mechanisms (i.###However, all of ECM with different emotions seems improper for response (especially for ECM with (Happy,other)), which reflects directly using a designated emotion for generation might be a unreasonable way for modeling the emotion interaction pattern.###In literature, Zhou et al. [46] successfully build an emotional chat machine (ECM) that is capable of generating emotional responses according to a pre-defined emotion category, and several similar efforts are also made by [11, 25], such as [48] proposed by Zhou et al. that utilizes emojis to control the emotional response generation process within conditional variational autoencoder (CAVE) framework.###In particular, we follow the work [46] to train an emotion classifier for assigning emotional labels to the sentences in the dataset.###The parameters of imemory and ememory in ECM are the same as the settings in [46].###Thereby, we manually designate a most frequent response emotion to ECM for fairness comparison.###Intuitively, the generated responses from ECM and Seq2seq-emb can be viewed as the indication of the performance of simply incorporating the EIPs for modeling the emotional interactions among the conversation pairs.###In particular, unlike [46] using solely one label for classification, we consider both of the emotion labels and thus regard it as a multi-label classification task.###For example, EACM outperforms ECM by 16.9%, 1.72% and 25.81% in terms of semantic score, sentiment score and response quality, respectively.###The emotion in the case is (Angry, other), however the responses provided by ECM with (Angry, other) is obviously incorrect in semantics, which demonstrate that simply using post’s emotion is inappropriate for response generation.###For ECM, the percentage of (0-0) degrades while the percentage of (1-0) increases as opposed to Seq2seq, which suggests that the effectivenss of EIP, i.e., themost frequent response emotions have low probability to result in emotional conflicts, In addition, the percentage of (1-1) degrades while the percentage of (1-0) increases, which reflects that directly using emotion classifier to model emotion interaction process is insufficient.###ECM [46], as mentioned, ECM model is improper to directly be as the baseline since it cannot automatically select an appropriate emotion label to the respond.###Third, it is also problematic to design a unified model that can generate plausible emotional sentence without sacrificing grammatical fluency and semantic coherence [46].###Zhou et al. [46] develop an Emotional Chat Machine (ECM) model using three different mechanisms (i.e., emotion embedding, internal memory and external memory) to generate responses according to the designated emotion category.###Seq2seq-emb [11, 46], Seq2seq with emotion embedding (Seq2seqemb) is also adopted in the same manner.",impact-revealing,acknowledge the development and limitations of the Emotional Chat Machine (ECM) model
219,5dcbd5da3a55ac789b0dbc7f,f0efc23ecb6d4fb9745d555450b2c4a97e8ac4d5,Robust Design of Deep Neural Networks Against Adversarial Attacks Based on Lyapunov Theory,599c7954601a182cd2631079,Parseval Networks: Improving Robustness to Adversarial Examples.,"Previous works [11, 28, 37, 8] have used a penalty on the spec-###[8] utilizes Lipschitz properties of the DNN to improve robustness against adversarial attacks.###Unlike [8], our approach does not require a predetermined set of hyper-parameters to prove robustness.###All the previous works on this subject [11, 28, 37, 8], keep constant across layers.###The closest to our work are the results given in [11, 28, 37, 8].",impact-revealing,acknowledge prior works and their approaches
2964,5eda19c991e01187f5d6d814,befc296197edcf5431e6042ee58f48d5dc2cf970,Convergence and Stability of Graph Convolutional Networks on Large Random Graphs,53e99800b7602d970201082e,Group Invariant Scattering,"Under this light, a relevant notion is that of stability: since GCNs are trained then tested on different (large) graphs, how much does a change in the graph structure affect its predictions? In the context of signals defined on Euclidean domains, including images or audio, convolutional representations such as scattering transforms or certain CNN architectures have been shown to be stable to spatial deformations [34, 5, 40].###Graph Convolutional Networks (GCNs [8, 14, 25]) are deep architectures deﬁned on graphs inspired by classical Convolutional Neural Networks (CNNs [27]).###When P is proportional to the Lebesgue measure, since NP (τ) is controlled by ‖∇τ‖∞, the GCN is invariant to translations and stable to deformations, similar to Euclidean domains [34].###The study of stability to deformations has been pioneered by Mallat [34] in the context of the scattering transform for signals on Euclidean domains such as images or audio signals [8, 2], and was later extended to more generic CNN architectures [5, 40].###In contrast, we have shown that combining them with random models of large graphs allows us to define intuitive notions of deformations and stability in the continuous world like the Euclidean case [34, 5, 40], with direct applications in community-based social networks or shape analysis on point clouds.###In contrast, our continuous setup allows us to define more intuitive geometric perturbations based on deformations of random graph models and to obtain deformation stability bounds that are similar to those on Euclidean domains [34].###Similar to CNNs [34, 5], studying GCNs in the continuous world allows us to define intuitive notions of model deformations and characterize their stability.###Once again we focus on invariant c-GCNs with pooling, similar to classical scattering transform [34].###d‖∇τ‖∞, recovering the more standard quantity of Mallat [34].###The study of stability to deformations has been pioneered by Mallat [32] in the context of the scattering transform for signals on Euclidean domains such as images or audio signals [7, 2], and was later extended to more generic CNN architectures [4, 38].###Mallat [34] studied the stability to small deformations of the wavelet-based scattering transform, which was extended to more generic learned convolutional network, e.###Mallat [32] studied the stability to small deformations of the wavelet-based scattering transform, which was extended to more generic learned convolutional network, e.g., [4, 38], and tries to establish bounds of the following form for a signal representation Φ( ): where − ) is the deformed signal and N ( τ ) quantiﬁes the size of the deformation, typically through norms of its jacobian ∇ τ , such as k∇ τ k ∞ = sup x k∇ τ ( x ) . the deformation k As we have seen in the introduction, it is not clear how to extend notion of on discrete graphs [16, 18].###Similar to CNNs [32, 4], studying GCNs in the continuous world allows us to deﬁne intuitive notions of model deformations and characterize their stability.###In particular, when P is proportional to the Lebesgue measure and k∇ τ k ∞ < 1, we have q τ ( x ) = det( I − ∇ τ ( x )) − 1 ; then, for small enough k∇ τ k ∞ , we obtain N P ( τ ) . d k∇ τ k ∞ , recovering the more standard quantity of Mallat [32]. we also 2 d if In this case, have the bound In the rest of the section, we will assume for simplicity that the considered GCNs Φ have zero bias at each layer.",other,highlighting the study of stability in Graph Convolutional Networks (GCNs) and their comparison to CNNs
604,556b086b240114513674708f,6776ff919597ca4feccd413208dedc401f6e655d,A Top-Down method for performance analysis and counters architecture,53e9a5b7b7602d9702ee3918,A performance counter architecture for computing accurate CPI components,"Neither at-retirement tagging is required as in IBM POWER5 [6], nor complex structures with latency counters as in Accurate CPI Stacks proposals [1][8][9].###In [1][4][5] there is no consideration of (fetch) bandwidth issues, and short-latency bottlenecks like L1 Bound.###This accurate classification distinguishes our method from previous approaches in [1][5][6].###A main contributor to this, is the fact that these performance events were historically defined in an ad-doc bottom-up fashion, where PMU designers attempted to cover key issues via “dedicated miss events” [1].###Eyerman et al. in [1][9] use a simulation-based interval analysis model in order to propose a counter architecture for building accurate CPI stacks.###Such scenarios of L1 hits and near caches’ misses, are not handled by some approaches [1][5].",impact-revealing,highlighting the distinctions and improvements of the proposed method compared to previous approaches
2330,5ec49a639fced0a24b4de82e,724d182e2b0217ce10ff05e3ad0b2f548fb39b9f,Iterative Edit-Based Unsupervised Sentence Simplification,5b1642d68fbcbf6e5a9b7e6f,"Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer.",Li et al. (2018) proposed edit-based style transfer without parallel supervision.,other,reporting prior findings on edit-based style transfer
1556,,991a15fb45f8c3609736db61c5b1288f265cb46c,Heart failure events with rosiglitazone in type 2 diabetes: data from the RECORD clinical trial,,,"###A history of previous cardiovascular disease was not predictive of HF. Duration of HF hospitalization and rate of HF re-hospitalization were similar in the two groups.###People with diabetes are prone to develop coronary artery disease and diastolic dysfunction, and poor glycaemic control is associated with increased incidence of HF.18,19 Thus, TZD-induced fluid retention is therefore of particular concern in this population at risk of developing HF: in clinical trials which excluded people with a history of HF, TZDs induced a small increase in HF episodes,20 whereas in the PROactive study pioglitazone was associated with a 6% incidence of HF hospitalizations over a follow-up of almost 3 years in people with type 2 diabetes and macrovascular disease, half of these with a previous myocardial infarction.###Change in current HF medication was defined by an increase in dose or IV medication or introduction of a new class of medication specific for the treatment of HF.###The study had an open-label design, and its aim was to assess the noninferiority of rosiglitazone combination to metformin/sulfonylurea for cardiovascular outcomes, the primary endpoint being the time to first cardiovascular hospitalization or cardiovascular death including HF. Data on glucose control and ambulatory blood pressure have been published.10,11, The study was conducted in 364 centres in 25 countries in Europe and Australasia.###The mean age of the population was 60 years, and we cannot extend our conclusions to the elderly or the very elderly, a population at high risk of HF. Owing to the limited number of HF events and of the nonrandomized nature of the decision to continue or discontinue rosiglitazone after an event, we cannot draw any conclusion on the risk of HF in patients experiencing an HF event and continuing rosiglitazone.###Overall, our findings support the guidelines issued for the management of patients treated with TZDs in the presence of HF.###People with diabetes are prone to develop coronary artery disease and diastolic dysfunction, and poor glycaemic control is associated with increased incidence of HF.(18,19) Thus, TZD-induced fluid retention is therefore of particular concern in this population at risk of developing HF: in clinical trials which excluded people with a history of HF, TZDs induced a small increase in HF episodes,(20) whereas in the PROactive study pioglitazone was associated with a 6% incidence of HF hospitalizations over a follow-up of almost 3 years in people with type 2 diabetes and macrovascular disease, half of these with a previous myocardial infarction.###Remme WJ, Swedberg K. Guidelines for the diagnosis and treatment of CHF.###The (unadjusted) incidence of HF (fatal and non-fatal) was analysed by fitting a Cox proportional hazards regression stratified for background medication and using time from randomization to the first event of HF.###The prospective adjudication of all hospitalizations or deaths by an endpoint committee blinded to medications and using pre-specified criteria based on the European Society of Cardiology guidelines limited the risk of misdiagnosis of HF in our study.25 In particular, the diagnosis of HF systematically required the documentation of cardiac
dysfunction and of HF medication changes in addition to the presence of signs and symptoms of HF.###Furthermore, the duration of follow-up in PROactive was shorter than in RECORD (34.5 vs. 66 months), a factor which may have played a role in the long-term assessment of events and outcome of a chronic condition such as HF. Finally, it is noteworthy that HF was not a protocol-defined centrally adjudicated event in PROactive.###Fluid retention and heart failure with thiazolidinediones The use of TZDs has been limited by the knowledge that these agents can cause fluid retention and lead to the development of HF.12 This has led to marketing contraindications in patients with HF and to guidelines cautioning the use of these compounds in some patients with pre-existing cardiovascular disease.4
We report here that rosiglitazone added to either metformin or sulfonylurea was associated with a two-fold increase in the risk of HF hospitalizations or death when compared with standard dual therapy.",impact-revealing,highlighting the risks associated with TZD use in patients with diabetes and heart failure
2492,5db929b747c8f766461fa94f,2a6d160b529272964ce1a6707adf52f3d6ba4861,Diffusion Improves Graph Learning,53e9bcb3b7602d97049320cb,Stochastic blockmodels and community structure in networks.,"The degreecorrected stochastic block model (DCSBM) [30], spectral clustering (using Lsym) [55], DeepWalk###The degreecorrected stochastic block model (DCSBM) [30], spectral clustering (using Lsym) [55], DeepWalk
[61], and Deep Graph Infomax (DGI) [73] are unsupervised models.###This enables us to use GDC with models that only support unweighted edges such as the degree-corrected stochastic block model (DCSBM).###Kloumann et al. [35] and Ragain [64] showed that PPR is optimal in recovering the SBM and DCSBM clusters in the space of landing probabilities under the mean field assumption.",other,acknowledge various unsupervised models and their applications
2984,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Speciﬁ-cally, the base model takes Transformer as the encoder for CRF, which has shown its effectiveness in many NLP tasks (Vaswani et al., 2017; Devlin et al., 2019).",other,providing context on the effectiveness of Transformer in NLP tasks
317,5d04e90ada56295d08ddb6b8,c3229debfda1b015c88404cf98f1074237d80809,Pre-training of Graph Augmented Transformers for Medication Recommendation,5bdc31b817c44a1f58a0c14d,GAMENet: graph augmented memory networks for recommending medication combination,"GAMENet [Shang et al., 2019] is the method to recommend accuracy and safe medication based on memory neural networks and graph convolutional networks by leveraging EHR data and Drug-Drug Interaction (DDI) data source.###However, GAMENet has a different motivation which results in using graph neural networks on drug-drug-interaction graphs instead of medical ontology.###For example, a large number of patients who only have one hospital visit were discarded from training in [ Shang et al. , 2019 ] .###Another work worth mentioning is GAMENet [Shang et al., 2019], which also used graph neural network to assist the medication recommendation task.###Medication Recommendation can be categorized into instance-based and longitudinal recommendation methods [Shang et al., 2019].###Speciﬁcally, even adding the extra information of DDI knowledge and procedure codes, GAMENet still performs worse than G-BERT .###GNNs have already been demonstrated useful on EHR modeling [Choi et al. , 2017; Shang et al. , 2019].###A number of deep learning models were proposed to assist doctors in making medication recommendation [Xiao et al., 2018a; Shang et al., 2019; Baytas et al., 2017; Choi et al., 2018; Ma et al., 2018].###Our ﬁnal model G-BERT is also better than the attention based model, RETAIN, and the recently published state-of-the-art model, GAMENet.###Another work worth mentioning is GAMENet [Shang et al. , 2019], which also used graph neural network to assist the medication recommendation task.###GNNs have already been demonstrated useful on EHR modeling [Choi et al., 2017; Shang et al., 2019].###A number of deep learning models were proposed to assist doctors in making medication recommendation [Xiao et al. , 2018a; Shang et al. , 2019; Baytas et al. , 2017; Choi et al. , 2018; Ma et al. , 2018].###For example, a large number of patients who only have one hospital visit were discarded from training in [Shang et al., 2019].",impact-revealing,discussing the limitations and comparisons of GAMENet in medication recommendation
3953,5a4aef9e17c44a2190f7a8e4,faa98e73eeee551c40923c896817ab640925ce20,Deep Image Prior,53e9aeb1b7602d97038d6516,Low-Complexity Single-Image Super-Resolution Based On Nonnegative Neighbor Embedding,We evaluate super-resolution ability of our approach using Set5 [2] and Set14 [32] datasets.,other,reporting evaluation datasets for super-resolution ability
256,5c8fd41a4895d9cbc66534e9,950aae7979a2faa874b7481179c064d3ad151ea8,Objects Segmentation From High-Resolution Aerial,57a4e91dac44365e35c987bb,Fully Convolutional Networks for Semantic Segmentation.,"To overcome this problem, skip connection methods are used [18], [25].###A key idea of FCN is changing of the CNN model from classification to dense prediction by reinterpretation of fully connected layers of the classifier as a fully convolution layer [25].",impact-revealing,providing context for the use of skip connection methods in CNNs
2316,573695fd6e3b12023e510ff5,06c06885fd53b2cbd407704cf14f658842ed48e5,deeply-recursive convolutional network for image super-resolution,53e9b55db7602d97040973cd,Super-resolution through neighbor embedding,"This is achieved with various techniques: neighbor embedding [4, 19], sparse coding [31, 32, 28, 29], convolutional neural network (CNN) [5] and random forest [23].",other,reporting various techniques used for a specific task
851,5e5e18e493d709897ce3a0f2,7a064df1aeada7e69e5173f7d4c8606f4470365b,albert: a lite bert for self-supervised learning of language representations,599c7987601a182cd2648373,Attention Is All You Need.,"The idea of sharing parameters across layers has been previously explored with the Transformer architecture (Vaswani et al., 2017), but this prior work has focused on training for standard encoderdecoder tasks rather than the pretraining/finetuning setting.###The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder (Vaswani et al., 2017) with GELU nonlinearities (Hendrycks & Gimpel, 2016).",impact-revealing,acknowledging prior work on parameter sharing in Transformer architecture
248,5db6c73a3a55acec0731cd68,73a5605ce482bd639078ebbb19baac7b903017e2,A Unified MRC Framework for Named Entity Recognition,5cf48a2cda56291d5828dda3,Entity-Relation Extraction as Multi-turn Question Answering,"Additionally, Li et al. (2019) utilized a template-based procedure for constructing queries to extract semantic relations between entities and their queries lack diversity.###Different from this work, Li et al. (2019) focused on relation extraction rather than NER.###…time and the intensiveness in developing hand-crafted features, etc. Inspired by the current trend of formalizing NLP problems as question answering tasks (Levy et al., 2017; McCann et al., 2018; Li et al., 2019), we propose a new framework that is capable of handling both ﬂat and nested NER.###Different ways have been proposed for question generation, e.g., Li et al. (2019) utilized a template-based procedure for constructing queries to extract semantic relations between entities.###, Li et al. (2019) utilized a template-based procedure for constructing queries to extract semantic relations between entities, as is referred as Rule-based Template Filling.###Inspired by the current trend of formalizing NLP problems as question answering tasks (Levy et al., 2017; McCann et al., 2018; Li et al., 2019), we propose a new framework for NER that is caar X iv :1 91 0.###Our work is signiﬁcantly inspired by Li et al. (2019), which formalized the task of entity-relation extraction as a multi-turn question answering task.",impact-revealing,highlighting the influence of prior work on the proposed framework for NER
2503,5fdb2e1691e0118a02c4f566,16913a534b1630d33770b392767bb316f4fdb11e,Clinical Temporal Relation Extraction with Probabilistic Soft Logic Regularization and Global Inference,5f0688719e795e4b2f9a1674,A Comprehensive Typing System for Information Extraction from Clinical Narratives,"…to automatically and precisely curate the clinical case reports into structured knowledge, i.e. extract important clinical named entities and relationships from the narratives (Aronson and Lang 2010; Savova et al. 2010; Soysal et al. 2018; Caufield et al. 2019; Alfattni, Peek, and Nenadic 2020).###extract important clinical named entities and relationships from the narratives (Aronson and Lang 2010; Savova et al. 2010; Soysal et al. 2018; Caufield et al. 2019; Alfattni, Peek, and Nenadic 2020).",other,highlighting the goal of curating clinical case reports through entity extraction
1926,,9d29c8901df6250d687ee0167801bc0321eefd06,Digital watermarking methods for data security and authentication,,,"###Other biometric attributes, which are commonly used for authentication include iris, hand geometry, face, and fingerprints (See [136] and [137]).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3708,5d8b3b1d3a55acc418bda58c,40c6fadf7b08fbcd5aedfc8bebf99ccbdb52d945,Portuguese Named Entity Recognition using BERT-CRF,5b8c9f0f17c44af36f8b237f,A Survey on Recent Advances in Named Entity Recognition from Deep Learning models,"(2011), neural network NER systems have become popular due to the minimal feature engineering requirements, which contributes to a higher domain independence (Yadav and Bethard, 2018).",other,highlighting the advantages of neural network NER systems
1048,,0a9f0b436b610d4cef1a022ab6083ac2278dc7e2,Data preprocessing for anomaly based network intrusion detection: A review,,,"###Standard preprocessing steps include dataset creation, data cleaning, integration, feature construction to derive new higher-level features, feature selection to choose the optimal subset of relevant features, reduction, and discretisation (Kotsiantis et al., 2006).###The focus is motivated by the fact that data preprocessing takes a signiﬁcant amount of e ﬀ ort, and directly impacts on the accuracy and capability of the downstream algorithm (Lee and Stolfo, 2000; Kotsiantis et al., 2006).###The focus is motivated by the fact that data preprocessing takes a significant amount of effort, and directly impacts on the accuracy and capability of the downstream algorithm (Lee and Stolfo, 2000; Kotsiantis et al., 2006).",impact-revealing,highlighting the importance of data preprocessing in algorithm performance
2689,5ec49a639fced0a24b4de82e,724d182e2b0217ce10ff05e3ad0b2f548fb39b9f,Iterative Edit-Based Unsupervised Sentence Simplification,5c04966a17c44a2c7470858e,CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling,"A probabilistic language model (LM) is often used as an estimate of sentence ﬂuency (Miao et al., 2019).###Miao et al. (2019) used Metropolis–Hastings sampling for constrained sentence generation.",other,reporting prior findings on probabilistic language models
3514,5f8fffb591e01125c27ddec9,67f473caaa52a97e65bb1bcb9029a580c4f8d10f,FLAG: Adversarial Data Augmentation for Graph Neural Networks,5d9ed28247c8f76646f6e046,Encoding Social Information With Graph Convolutional Networks For Political Perspective Detection In News Media,"…Network (GCN) (Kipf and Welling, 2016) and its variants have been applied to a wide range of tasks, including visual recognition (Shen et al., 2018), meta-learning (Garcia and Bruna, 2017), social analysis (Qiu et al., 2018; Li and Goldwasser, 2019), and recommender systems (Ying et al., 2018).",other,acknowledge applications of GCN in various tasks
2171,,308c32eb37ac7083bd0f8b76fc1c8c193efeb716,Challenges and Opportunities with Cloud Computing,,,"###[10] The paper has brief discussion on the ―Cloud‖ computing which is build on decades of research in virtualization, distributed computing, utility computing, and, more recently, networking, web and software services.",impact-revealing,providing context on the evolution of cloud computing
3652,5dc9327d3a55acc1042498de,2cf3bd0cc1382f35384e259d99e4f9744eeaed28,Blockwise Self-Attention for Long Document Understanding,599c7987601a182cd2647a85,Triviaqa: A Large Scale Distantly Supervised Challenge Dataset For Reading Comprehension,"We evaluate BlockB ERT on several question answering tasks, including SQuAD 1.1/2.0 (Rajpurkar et al., 2018) and ﬁve other tasks from the MrQA shared task 6 — HotpotQA (Yang et al., 2018), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019).###This means that for SearchQA and TriviaQA, a BERT model with sequence length N = 512 can only capture half of the context.###For example, SQuAD, NaturalQA, and HotpotQA consist of mostly short paragraphs (shorter than 512), while paragraphs in SearchQA (average length 1,004) and TriviaQA (average length 934) have around 1,000 tokens.###…several question answering tasks, including SQuAD 1.1/2.0 (Rajpurkar et al., 2018) and ﬁve other tasks from the MrQA shared task 6 — HotpotQA (Yang et al., 2018), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019).###, 2017), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al.",other,evaluating performance on various question answering tasks
1685,,ccbee7b8c85a479370e6b895fb0a43f5c78e956f,"Review of the book 'The mind club: Who thinks, what feels and why it matters' by D.M. Wegner & K. Gray",,,"###As a social psychologist reading the book, it is a shame that authors did not consider
social identity theory (Tajfel & Turner, 1979; Turner, Hogg, Oakes, Reicher, & Wetherell, 1987), which could provide a lot more insight into the chapter on groups.",impact-revealing,Critique of authors' oversight in considering social identity theory
3047,5fbe5cf091e011e6e11b3cf5,15a84047e5145891d0c7ee054c00a00f2f5d38a1,Boosting Contrastive Self-Supervised Learning with False Negative Cancellation,5efdaf7b91e01191d3d28242,Debiased Contrastive Learning,"Most existing methods focus on mining hard negatives [39, 25], or most recently, increasing positive samples to counter-balance the negatives [12].###, those that are close to the anchor) [39, 25], and using more positive samples to counter-balance the effects of undesirable negatives [12].",other,acknowledge existing methods in mining hard negatives
1812,,38981d1fd2b5e8ba77ce4717bcc5d7f7d777435d,Plasmapheresis vs . immunoglobulin in autoimmune neurologic diseases : a meta-analysis Word counts for :,,,"###For the pediatric autoimmune neuropsychiatric disorders associated with streptococcal infection (PANDAS) the global assessment scale applied at the beginning and a month after the treatment was taken into account.###Results: 725 articles were found and 27 met the criteria for a population studied of 4717
cases: 14 articles were about Guillain Barré syndrome, 10 of Myasthenia Gravis, one of
Sydenham Chorea, one of Chronic inflammatory demyelinating polyneuropathy, and one
of PANDAS.###The evidence is insufficient with PE for MG (preoperative and crisis), Sydenham Chorea and PANDAS (4, 37).###Of the studies included (Appendix e-1), 14 corresponded to SGB (2545 cases, age: 4–85 years), 10 to MG (2112 cases, age: 18–84 years), one to CIDP (19 patients, age: 22–52 years), one to PANDAS (29
Ortiz-Salas 5
patients, age: 5.8–13 years) and one to Sydenham Chorea (12 patients, age: 5–14 years).###The evidence is insufficient for the use of IVIG in children, as well as in Sydenham Chorea and PANDAS but it is probably effective in MG.###This is one of the reasons why plasma exchange, plasmapheresis (PE) or the application of intravenous immunoglobin (IVIG), have shown to be effective in most of these pathologies as has been demonstrated in different trials (3, 4).",impact-revealing,reporting findings on pediatric autoimmune neuropsychiatric disorders and treatment effectiveness
1303,,21c38cd817a87ad8b75d019c0b70c3a5e168d828,"Chemical Characteristics of Fine Particles (PM1) from Xi'an, China",,,"###PMF has been widely used for receptor modeling, and it is most often applied when source profiles are unknown (such as Hwang and Hopke 2007; Lee and Hopke 2006; Lestari et al. 2009; Miller et al. 2002; Shrivastava et al. 2007; Song et al. 2007; Uchimiya et al. 2008).",impact-revealing,reporting prior findings on the application of PMF in receptor modeling
3595,5f8cf5159e795ea21aee7f07,0ee0801ba010a441403f9ed666ef9bf006b3aa07,Adaptive Universal Generalized PageRank Graph Neural Network,5e4e5ac53a55ac305df4b706,Outcome Correlation in Graph Neural Network Regression,"Neighborhood aggregation is problematic and significantly more difficult for heterophilic graphs (Jia & Benson, 2020).",other,highlighting the challenges of neighborhood aggregation in heterophilic graphs
2598,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,5736960c6e3b12023e51f0a0,End-To-End Sequence Labeling Via Bi-Directional Lstm-Cnns-Crf,"Many recent sequence labeling frameworks (Ma and Hovy, 2016b; Misawa et al., 2017) share a very basic structure: a bidirectional LSTM network followed by a CRF tagging layer (i.e. BLSTM-CRF).###Recent research efforts in neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) (Ma and Hovy, 2016a) or bidirectional long short-term memory (BLSTMs) (Lam-ple et al., 2016) can largely eliminate human-crafted features.",other,highlighting advancements in sequence labeling frameworks and neural network models
698,5a9cb66717c44a376ffb87ea,fe9b8aac9fa3bfd9724db5a881a578e471e612d7,efficient neural architecture search via parameter sharing,58d82fc8d649053542fd59b8,Neural Architecture Search with Reinforcement Learning.,"The decision of what previous nodes to connect to allows the model to form skip connections (He et al., 2016a; Zoph & Le, 2017).###We also do not tune our hyper-parameters extensively like Melis et al. (2017), nor do we train multiple architectures and select the best one based on their validation perplexities like Zoph & Le (2017).###It is possible to improve our experimental results by training all the sampled models from scratch and selecting the model with the highest performance on a separated validation set, as done by other works (Zoph & Le, 2017; Zoph et al., 2018; Liu et al., 2017; 2018).###This design of our search space for RNN cells is different from the search space for RNN cells in Zoph & Le (2017), where the authors ﬁx the topology of their architectures as a binary tree and only learn the operations at each node of the tree.###Second, we suspect this cell is a local optimum, similar to the observations made by Zoph & Le (2017).###47% , achieved by the second best NAS model (Zoph & Le, 2017).###Neural architecture search (NAS) has been successfully applied to design model architectures for image classiﬁcation and language models (Zoph & Le, 2017; Zoph et al., 2018; Cai et al., 2018; Liu et al., 2017; 2018).###Importantly, ENAS cell outperforms NAS (Zoph & Le, 2017) by more than 6 perplexity points, whilst the search process of ENAS, in terms of GPU hours, is more than 1000 x faster.###4 (Zoph & Le, 2017) and which is a new state-of-the-art among Penn Treebank’s approaches that do not utilize post-training processing.###Therefore, ENAS is not at any advantage, compared to Zoph & Le (2017); Yang et al. (2018); Melis et al. (2017), and its improved performance is only due to the cell’s architecture.",impact-revealing,discussing the design and performance of neural architecture search methods
1071,,04aae16018fe97bb69fcc1c8a2d8e3d088175bc5,Image Clustering via Deep Embedded Dimensionality Reduction and Probability-Based Triplet Loss,,,"###2) Clustering Metrics: For fairness comparison in different clustering algorithms, we adopt two evaluation metrics, clustering accuracy (ACC) and normalized mutual information (NMI) [38], which are widely used for clustering in unsupervised learning.",impact-revealing,reporting evaluation metrics for clustering comparison
2962,5a9cb66717c44a376ffb89eb,651adaa058f821a890f2c5d1053d69eb481a8352,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,5ca600ae6558b90bfa4d76e9,Towards Deep Learning Models Resistant to Adversarial Attacks,"Much of what we describe below has been discussed in prior work (Carlini & Wagner, 2017a; Madry et al., 2018); we repeat these points here and offer our own perspective for completeness.###This is signiﬁcantly weaker than the original Madry et al. (2018) model that does not use thermometer encoding.###At the time of writing this paper, four of the defenses we study made complete source code available (Madry et al., 2018; Ma et al., 2018; Guo et al., 2018; Xie et al., 2018).###The speciﬁc choice of optimizer is far less important than choosing to use iterative optimization-based methods (Madry et al., 2018).###Using this attack, the authors perform the adversarial training of Madry et al. (2018) on thermometer encoded networks.###We study the adversarial training approach of Madry et al. (2018) which for a given (cid:15) -ball solves To approximately solve this formulation, the authors solve the inner maximization problem by generating adversarial examples using projected gradient descent.###We ﬁnd that combining adversarial training (Madry et al., 2018) with PixelDefend provides no additional robustness over just using the adversarially trained classiﬁer.###As benchmarking against iterative optimization-based attacks (e.g., Kurakin et al. (2016a); Madry et al. (2018); Carlini & Wagner (2017c)) has become standard practice in evaluating defenses, new defenses have arisen that appear to be robust against these powerful optimization-based attacks.",other,acknowledging prior work and providing context for the discussion
2003,,537e5b7a38e9590a0925462b91e80033b45765ed,Features of the Responses of the Protective Systems of the Brain in Adult Rats to Stressors and Lipopolysaccharide,,,"###Chronic stressors such as forced swimming [23, 24] and restriction to the animals’ mobility [25, 26], as well as peripheral administration of LPS [21], are widely used for induction of depression-like behavior in studies on rodents.###Louis, USA), using a dose for rats indicated in the literature [21] of 0.",impact-revealing,acknowledge common methods for inducing depression-like behavior in rodent studies
2073,,2122856630ec02f8a3c31e80c63b8bbeb6714143,LoRa Meets IP: A Container-Based Architecture to Virtualize LoRaWAN End Nodes,,,"###…(JSON): JavaScript Object Notation (JSON) is a text-based, language-independent data interchange format based on JavaScript object syntax and on a set of formatting rules for the serialization and representation of structured data, commonly used for transmitting data in Web applications [25].",impact-revealing,providing context about a data interchange format
1822,,12a7fb4e33cb4a719b848c1013d2979650e79dc3,A goal function approach to remodeling of arteries uncovers mechanisms for growth instability,,,"###Subsequent work builds on this idea of cost-optimization in vascular systems (Klarbring et al. 2003; Kassab 2006; Lindström et al. 2014) and includes global optimization methods (Klarbring et al. 2003; Kassab 2006).###Blood vessels consist of different constituents, such as elastin, collagen and smooth muscle (Boron and Boulpaep 2008, pp. 473–481).",impact-revealing,highlighting the development of cost-optimization methods in vascular systems
2600,5c2c7a9217c44a4e7cf317b4,61946177ea5d44ad18ff09de7929556d7b34cd8b,simd-x: programming and processing of graph algorithms on gpus.,53e9ac82b7602d970365a664,X-Stream: Edge-Centric Graph Processing Using Streaming Partitions,"“Think like a vertex” [37, 75] focuses on tasks on active vertices in a graph, whereas “think like an edge” [49, 48] iterates on edges and simpliﬁes programming.###On the other hand, the edge-centric model is initially introduced by the external-memory graph engine X-stream [49] to improve IO performance.###Recent advance in graph computing falls in algorithm innovation [39, 72], framework developments [37, 14, 33, 28, 30, 75, 77, 18, 53, 51, 49, 19, 42, 48, 61, 6, 66, 68, 52, 73, 62, 43, 74, 70, 69, 3, 64, 40, 17, 8] and accelerator optimizations [63, 29, 38, 25, 71, 47].",other,acknowledge variations in graph computing approaches
2723,5ea2b8bf91e01167f5a89d89,993377a3fc8334558463b82053904e3d684f29c0,SIGN: Scalable Inception Graph Neural Networks,5bdc31b817c44a1f58a0c039,Adaptive Sampling Towards Fast Graph Representation Learning,"Layer-wise sampling [10, 28] avoids over-expansion of neighborhoods to overcome the redundancy of node-wise sampling.###On the inductive datasets, we compare our method to GCN [32], FastGCN [10], Stochastic-GCN [11], AS-GCN [28], GraphSAGE [22], ClusterGCN [12], and GraphSAINT [65], which constitute the current state-of-the-art.###So far, various graph sampling approaches [22, 63, 10, 28, 11, 12, 65, 71] have ∗Equal contribution",other,acknowledge existing graph sampling methods
3199,53e9b42fb7602d9703f2696f,8c34cdd2bab66623d2831004fbd1fa1cdf8a0366,Improving memory scheduling via processor-side load criticality information,53e9a3abb7602d9702cbe7eb,Focusing Processor Policies Via Critical-Path Prediction,"Fields et al. proposed a method for statically determining the critical path of an application using directed graphs, and proposed a token-based hardware mechanism to approximate this in hardware [5].###The generally accepted method of determining the critical path of program execution was proposed by Fields et al. [5].",other,reporting existing findings on critical path determination
848,5fc61cdb91e0118947381abc,c9d736dd9f967844d2391bb13c4cb477576ab373,On Disambiguating Authors: Collaboration Network Reconstruction in a Bottom-up Manner,53e99acab7602d970234a328,Weisfeiler-Lehman Graph Kernels,"Due to the inefficiency of computing paths and cycles with larger networks, IUAD adopts Weisfeiler-Lehman sub-graph kernel [39] to evaluate the similarity between two vertices.###Due to page limitation, more details please refer to [39], [40].",impact-revealing,providing context for the method used in evaluating similarity
46,5b1643ba8fbcbf6e5a9bc5b5,8c1b00128e74f1cd92aede3959690615695d5101,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,58d82fced649053542fd717d,Dynamic Coattention Networks For Question Answering.,"To measure the speedup of our model against the RNN models, we also test the corresponding model architecture with each encoder block replaced with a stack of bidirectional Published 12 LeaderBoard 13 Single Model EM / F1 EM / F1 LR Baseline (Rajpurkar et al., 2016) 40.4 / 51.0 40.4 / 51.0 Dynamic Chunk Reader (Yu et al., 2016) 62.5 / 71.0 62.5 / 71.0 Match-LSTM with Ans-Ptr (Wang & Jiang, 2016) 64.7 / 73.7 64.7 / 73.7 Multi-Perspective Matching (Wang et al., 2016) 65.5 / 75.1 70.4 / 78.8 Dynamic Coattention Networks (Xiong et al., 2016) 66.2 / 75.9 66.2 / 75.9 FastQA (Weissenborn et al., 2017) 68.4 / 77.1 68.4 / 77.1 BiDAF (Seo et al., 2016) 68.0 / 77.3 68.0 / 77.3 SEDT (Liu et al., 2017a) 68.1 / 77.5 68.5 / 78.0 RaSoR (Lee et al., 2016) 70.8 / 78.7 69.6 / 77.7 FastQAExt (Weissenborn et al., 2017) 70.8 / 78.9 70.8 / 78.9 ReasoNet (Shen et al., 2017b) 69.1 / 78.9 70.6 / 79.4 Document Reader (Chen et al., 2017) 70.0 / 79.0 70.7 / 79.4 Ruminating Reader (Gong & Bowman, 2017) 70.6 / 79.5 70.6 / 79.5 jNet (Zhang et al., 2017) 70 LSTMs as is used in most existing models.###…(Wang & Jiang, 2016) 64.7 / 73.7 64.7 / 73.7 Multi-Perspective Matching (Wang et al., 2016) 65.5 / 75.1 70.4 / 78.8 Dynamic Coattention Networks (Xiong et al., 2016) 66.2 / 75.9 66.2 / 75.9 FastQA (Weissenborn et al., 2017) 68.4 / 77.1 68.4 / 77.1 BiDAF (Seo et al., 2016) 68.0 / 77.3 68.0 /…###…end-to-end neural network models have been proposed to tackle these challenges, including BiDAF (Seo et al., 2016), r-net (Wang et al., 2017), DCN (Xiong et al., 2016), ReasoNet (Shen et al., 2017b), Document Reader (Chen et al., 2017), Interactive AoA Reader (Cui et al., 2017) and Reinforced…###Empirically, we ﬁnd that, the DCN attention can provide a little beneﬁt over simply applying context-to-query attention, so we adopt this strategy.###Most high performing models additionally use some form of query-to-context attention, such as BiDaF (Seo et al., 2016) and DCN (Xiong et al., 2016).###Then we learn the interactions between context and question by standard attentions (Xiong et al., 2016; Seo et al., 2016; Bahdanau et al., 2015).###A great number of end-to-end neural network models have been proposed to tackle these challenges, including BiDAF (Seo et al., 2016), r-net (Wang et al., 2017), DCN (Xiong et al., 2016), ReasoNet (Shen et al., 2017b), Document Reader (Chen et al., 2017), Interactive AoA Reader (Cui et al., 2017) and Reinforced Mnemonic Reader (Hu et al., 2017).###According to the observations from our experiments and previous works, such as (Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Chen et al., 2017), the validation score is well correlated with the test score.",impact-revealing,reporting various end-to-end neural network models proposed for addressing challenges in the field
262,5e3a93a93a55ac06c6119df5,cad9e682ddec3b1dd532cb8301737109d9eda7d7,Collaborative Distillation for Top-N Recommendation,5b67b45517c44aac1c860823,Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System.,"Illustration of rank distillation (RD) [11].###Then, we explain the concept of knowledge distillation (KD) [10] and present rank distillation (RD) [11] that applies knowledge distillation to recommender models.###Recently, Tang and Wang [11] proposed a KD model to address the ranking problem, called rank distillation (RD).###• RD [11] and RD-Rank: We set ρ to be 0.###Note that the improvement gap for RD is somewhat different from that in [11].###For Caser, we used the public PyTorch implementation 6 provided in [11].###Tang and Wang [11] proposed ranking distillation (RD) that applies KD for ranking models.###Also, the gain indicates how additional accuracy achieved by the proposed model over that of RD [11].###Since RD [11] is the state-of-the-art KD###• RD [11]: To define the KD loss in equation (4), this utilizes only the top-K items of the soft target by quantizing their values to 1.",impact-revealing,providing context and explanation of rank distillation and knowledge distillation
3581,5f0d85c69fced0a24be4f019,6817b5d48cf8f665b680de07d23b91814d2923b1,Bouquet of Instruction Pointers: Instruction Pointer Classifier-based Spatial Hardware Prefetching,5d04eeba8607575390f83f47,Efficient metadata management for irregular data prefetching,"IP information has been used extensively at the L2 and the LLC [14] [56], [60], [25], [29], [11], [24], [59].###It is well known that spatial prefetchers fail to improve performance for server workloads like CloudSuite [14], [33], [53], [58], [59] and additional prefetchers [12], [24], [52], [58], [59] can be used on top of IPCP to improve the performance.###Usually, temporal prefetchers demand hundreds of KBs. Recently, Managed ISB (MISB) [59] and Triage [58] have optimized the hardware overhead without compromising coverage.###Apart from these spatial prefetchers, there are temporal prefetchers [54], [55], [24], [12], [59], [58] that target irregular but temporal accesses.",other,acknowledge existing research on prefetchers and their performance
3085,5e3940c73a55ace46ed436d2,845b4941d8c016aa5f8967da2f86d38ef6c18fa3,a survey on knowledge graphs representation acquisition and applications,53e9bb8cb7602d97047d52a4,AMIE: association rule mining under incomplete evidence in ontological knowledge bases,"der, and entities Xand Y, there is a rule in the reverse form of logic programming as: (Y;sonOf;X) (X;hasChild;Y) ^(Y;gender;Male) (34) Logical rules can been extracted by rule mining tools like AMIE [77]. The recent RLvLR [78] proposes a scalable rule mining approach with efﬁcient rule searching and pruning, and uses the extracted rules for link prediction. More research attention focuses on injectin",other,highlighting advancements in rule mining approaches
1896,,51940b67991f181d245f186edf1d214763089633,Development and validation of the Online Learning Self-efficacy Scale (OLSS): A structural equation modeling approach,,,"###The CoI framework by Garrison et al. (2000) defines, describes, and measures three elements that support the development of online learning communities: cognitive presence, social presence, and teaching presence.###This theoretical framework highlighted the role of technology in online learning, embraced the social presence element in the Community of Inquiry (CoI) framework (Garrison, Anderson, & Archer, 2000), and emphasized the importance of self-regulation and self-motivation in online learning.###Social presence is defined as “the ability of participants in the community of inquiry to project their personal characteristics into the community, thereby presenting themselves to others as ‘real people’” (Garrison et al., 2000, p. 89).###Teaching presence, as “a means to an end – to support and enhance social and cognitive presence for the purpose of realizing educational outcomes” (Garrison et al., 2000, p. 90), consists of the design and facilitation of the educational experience.",impact-revealing,describing the CoI framework and its significance in online learning
3460,5fd8acf991e0119b22c1f38d,5b9d8bcc46b766b47389c912a8e026f81b91b0d8,Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting,573696056e3b12023e519679,Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),#ERROR!,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3828,5d79a6ff3a55ac5b650357fb,6fb4facc2d16c76047f7cd96af5a691ab16c08c5,Combining Prefetch Control and Cache Partitioning to Improve Multicore Performance,53e9b350b7602d9703e268f6,"Utility-Based Cache Partitioning: A Low-Overhead, High-Performance, Runtime Mechanism To Partition Shared Caches","[14] proposes a utility-based cache partitioning (UCP) mechanism to partition the shared LLC among multiple applications.###Early work [13], [14] proposed theory and michroarchitectural techniques.",other,reporting prior findings on cache partitioning mechanisms
2009,,f2013972121352d274fb51ff74f21dd36f8d4a82,"Understanding Donor-advised Funds: The Behavioral Economics, Macroeconomics, and Public Policies Relating to an Emerging Trend in Philanthropy",,,"###We know that MTurk participants are motivated by making money (Sheehan and Pittman, 2016), but it is unclear how strong the profit motive is, and if it would override any charitable inclinations.",impact-revealing,highlighting uncertainty in participant motivations on MTurk
1510,,9639d245cecba60b5d0c88a0b27e1f76f014d840,Unidirectional Spatial and Spectral Smoothed Tensor Ring Decomposition for Hyperspectral Image Denoising and Destriping,,,"###Given the prevalence of similar patterns within the spatial dimension of HSI, nonlocal self-similarity prior is frequently used for HSI denoising and has achieved advanced performances [16], [17], [18].",impact-revealing,highlighting the effectiveness of nonlocal self-similarity prior in HSI denoising
2207,5f9a9af391e0114d7e7813ed,0d67d3ddca1c4e370eaf1e99ec674f612c39c66c,Graph Contrastive Learning with Adaptive Augmentation,5a260c8117c44a4ba8a30a57,"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec.","s using noise-contrastive estimation (NCE) [12]. These random-walk-based methods are proved to be equivalent to factorizing some forms of graph proximity (e.g., transformation of the adjacent matrix) [35], which overly emphasize on the structural information encoded in these graph proximities and also face severe scaling problem with large-scale datasets. Also, these methods are knowntobeerror-pronewi",other,highlighting limitations in random-walk-based methods
2749,5c04966a17c44a2c74708959,6b98bef930182a848c027dece1bfb58ca706449d,Improving End-to-end Speech Recognition with Pronunciation-assisted Sub-word Modeling,53e9b0e6b7602d9703b60222,Japanese And Korean Voice Search,"Concur-rently, [15] borrow the practice in voice search [16] to segment words into wordpiece which maximizes the language model probability.",other,acknowledging a method borrowed from voice search for word segmentation
1626,,b3c80050efb72248e80fe33b91b84aa123e176d9,You Cannot Fix What You Cannot Find! An Investigation of Fault Localization Bias in Benchmarking Automated Program Repair Systems,,,"###In the APR literature [19], [33], [35], [39], [38], Ochiai [25] is widely used as the ranking metric of SBFL.###…the systems rely on a testing framework such as GZoltar [21], and a spectrum-based fault localization formula [22], [23], [24], such as Ochiai [25]. v Eventually, bug ﬁxing performance is measured by counting the number of bugs for which the system can generate a patch that passes all test…###However, their work is limited to evaluating a single APR tool, GenProg [6] and a single FL technique, Ochiai [25] while our study evaluates and compares 14 different APR systems.###Various studies in the literature have explored the effectiveness of fault localization [59], [25], [60], [61], [46], [24], [62], [42].",impact-revealing,highlighting the limitations of existing APR evaluations and comparing multiple systems
591,53e99804b7602d970201668d,5bda0d60efb98013537d9edd9edfaf59fe809e7b,fetching instruction streams,53e9a627b7602d9702f52f26,A scalable front-end architecture for fast instruction delivery,"Fetch target queue Following the proposal of Reinman, Austin and Calder [30] 
we have decoupled the branch prediction stage from the instruction cache access stage.###In Section 2 
we discuss previous related work, including state of the art fetch architectures like the FTB proposed 
by Reinman, Austin and Calder [30] and the trace cache architecture as proposed by Rotenberg, Bennett 
and Smith in [32].###Further development of this fetch architecture leads to a decoupling of the dynamic branch prediction mechanism and the instruction cache access, as proposed by Reinman, Austin, and Calder [30].###Following the proposal of Reinman, Austin and Calder [30] we have decoupled the branch prediction stage from the instruction cache access stage.###The use of an FTQ is not novel, it was introduced in [30].###In Section 2 we discuss previous related work, including state of the art fetch architectures like the FTB proposed by Reinman, Austin and Calder [30] and the trace cache architecture as proposed by Rotenberg, Bennett and Smith in [32].###We compare our stream fetch architecture with three other state-of-the-art fetch architectures: the FTB architecture [30] using a perceptron branch predictor [18], the Alpha EV8 architecture using a 2bcgskew predictor [34], and the trace cache architecture using a trace predictor [32] and selective trace storage [29].###Another important contribution of [30] is the Fetch Target Buffer (FTB).###Further development of this fetch architecture leads to a decoupling of the 
dynamic branch prediction mechanism and the instruction cache access, as proposed by Reinman, Austin, 
and Calder [30].",impact-revealing,acknowledging prior work on fetch architectures and their contributions
356,5550446645ce0a409eb4d54a,06e122f475a21d92dba137609c40f35690217475,Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification,555048d245ce0a409eb71bd7,Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis,"We employ a novel adaptive multi-compositionality layer in recursive neural network, which is named as AdaRNN (Dong et al., 2014).",impact-revealing,introducing a novel method in recursive neural networks
2563,59ae3c262bbe271c4c71ea21,83e7654d545fbbaaf2328df365a781fb67b841b4,Enhanced LSTM for Natural Language Inference,5550456245ce0a409eb55cee,Glove: Global Vectors for Word Representation.,"We use pre-trained 300-D Glove 840B vectors (Pennington et al., 2014) to initialize our word embeddings.",other,method use for word embedding initialization
326,5736986b6e3b12023e72fc2d,51a55df1f023571a7e07e338ee45a3e3d66ef73e,Character-level convolutional networks for text classification,53e9b85bb7602d970441f6c2,Gradient-Based Learning Applied to Document Recognition,"On the other hand, many researchers have found convolutional networks (ConvNets) [17] [18] are useful in extracting information from raw signals, ranging from computer vision applications to speech recognition and others.",impact-revealing,acknowledging the utility of convolutional networks in various applications
1551,,e0d860f3948faf92ad9552825d2949be669c6fd1,Turnover Destination as a Factor in the Relationship between Employee Performance and Turnover in South Africa,,,"###In order to include such environmental factors in the determinants of performance, it was suggested that an opportunity factor is included as a determinant of performance (Blumberg & Pringle, 1982).###Secondly, the equation assumes that the lower the values of any of the determinants, the lower the level of performance (Blumberg & Pringle, 1982).###…motivation, willingness is indicative of the effect of ‘job satisfaction, personality, attitudes and norms, values, status, anxiety, task characteristics, job involvement, perceived role expectations, self-image, need states and closely related concepts’ on behaviour (Blumberg & Pringle, 1982:563).###Although the abovementioned formulation was largely adopted without any major changes, it has been criticized for being unable to account for additional variance in performance (Campbell & Pritchard, 1976 as cited in Blumberg & Pringle, 1982).###Furthermore, Blumberg and Pringle (1982) suggested that the motivation factor should be broadened to the term willingness to perform.###Blumberg and Pringle (1982) explain
that whilst it is widely accepted that there are numerous variables that impact on task performance, only a few of these variables have been focused on in the research.###Figure 1: Diagram Illustrating the Interactions between the Determinants of Performance (Blumberg and Pringle, 1982)................................................................................................................................. 25 Figure 2: Figure Illustrating the Difference in…###‘Capacity refers to the physiological and cognitive capabilities that enable an individual to perform a task effectively’ (Blumberg & Pringle, 1982:563).###Many researchers have acknowledged the importance of environmental influences on job performance (eg., Blumberg & Pringle 1982; Landy & Farr 1980; Lewin 1961; Wherry & Bartlett, 1982 as cited in Kane, 1993).###…that the literature on the determinants of job performance has been criticised for being narrowly focused and considering too few antecedent variables in the studies (Blumberg & Pringle, 1982; Griffin, Welsh & Moorhead, 1981; Pierce & Dunham, 1976; Staw 1977 as cited in Waldman & Spangler, 1989).###The model proposed by Blumberg and Pringle (1982) has been criticized in that it assumes that any test of the model has taken into consideration all three elements of capacity, willingness and opportunity.###…𝑥 Opportunity)
This formulation is best depicted by the following illustration:
Figure 1: Diagram Illustrating the Interactions between the Determinants of Performance (Blumberg & Pringle, 1982)
Capacity
Willingness
Opportunity
Performance
The above formulation makes two basic assumptions.###As explained by Blumberg and Pringle (1982), if the formulation of the determinants of performance is correct as stated above, then any factor which impacts job performance should fit under the category of either ability or motivation.###Blumberg and Pringle (1982) highlight that the opportunity term should encompass variables that extend further than the individual’s immediate task environment.###In addition to ability, capacity is indicative of ‘an individual’s knowledge, skills, intelligence, age, state of health, level of education, endurance, stamina, energy level, motor skills and similar variables’ (Blumberg & Pringle, 1982:563).###Both practitioners and researchers have sought to establish a relationship that illustrates that motivation results in performance and many theories have emerged (Blumberg & Pringle, 1982).###The ‘psychological and emotional characteristics that influence the degree to which an individual is willing to perform’ reflects the willingness dimension (Blumberg & Pringle, 1982:563).###Blumberg and Pringle (1982) highlight that evidence suggests
that there are certain environmental factors which are beyond the control of the employee; however these factors may still have an impact on the performance of an individual.###Blumberg and Pringle (1982) further suggested including in the determinants of performance, a factor termed the opportunity to perform.###Blumberg and Pringle (1982) proposed a solution to improve the above formulation.###Miner (1980 as cited in Blumberg & Pringle, 1982) suggests that the models proposed only apply to certain individuals in certain situations.",impact-revealing,acknowledging the limitations and criticisms of existing performance determinants models
2151,,209b56614032dabd9b7d7d0737af522f65f87664,Design of an wide-band FIR filter with sharp transition using generalized sampling kernels,,,"###[7] S.###Accordingly, one of widely used adjustable window functions whose impulse responses are of finite length [7] is employed in this paper as a lowpass filter.###Note that the filter length of the raised-cosine filter with a desired roll-off factor can be determined by using the equations for adjustable window functions [7].###More specifically, we employ, as an adjustable window function [7], the raised-cosine filter with a desired roll-off factor R, whose impulse response of finite length is a low-pass filter widely used for pulse shaping in the communication fields.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3857,5f0d85c69fced0a24be4f019,6817b5d48cf8f665b680de07d23b91814d2923b1,Bouquet of Instruction Pointers: Instruction Pointer Classifier-based Spatial Hardware Prefetching,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"We also use a set of Convolutional Neural Networks (CNNs) and a Recurrent Neural Network (RNN) [19], [21], [22], [36], [37], [46] that are commonly used in applications like object recognition and image classiﬁcation.",other,reporting commonly used neural network architectures
2432,5d9ed4a047c8f76646fb6da2,0fd26ed185aaf860f2db491c194884914fc29311,A Neural Multi-digraph Model for Chinese NER with Gazetteers,5bdc315017c44a1f58a05c5e,Learning Named Entity Tagger using Domain-Specific Dictionary.,"Several selection strategies are proposed, such as maximizing the total number of matched tokens in a sentence (Shang et al., 2018), or maximum matching with rules (Sassano, 2014).",other,acknowledge existing selection strategies
3158,5aed14d617c44a4438158e20,9e788f1530af08a1f2140e6016fd4aeaa8b29033,Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform,58d82fcbd649053542fd6361,Controlling Perceptual Factors in Neural Style Transfer,[14] use semantic maps to control perceptual factors in neural style transfer.,other,reporting prior findings on semantic maps in neural style transfer
3874,599c7ea4601a182cd28b81a7,f323036340ad0ec30cafacd6b09a3e28379e24f1,maximizing cache performance under uncertainty,53e9a4ebb7602d9702e094b1,A survey of Web cache replacement strategies,"First, most empirical policies exploit dynamic behavior to select a victim, most commonly by using the recency and frequency heuristics [28].",other,acknowledge existing empirical policies in victim selection
3554,5f44e5bd91e011872f85ed90,9c160a71d3265eedaf7645c39be073c966f10433,A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild,5bdc31b817c44a1f58a0bd3e,LRS3-TED: a large-scale dataset for visual speech recognition,"p region, we found that lip landmarks can be quite inaccurate on generated faces. Thus, there is a need for a metric that is designed specifically for measuring lip-sync errors. LRW [8] LRS2 [1] LRS3 [3] Method LSE-D ↓ LSE-C ↑ FID ↓ LSE-D ↓ LSE-C ↑ FID ↓ LSE-D ↓ LSE-C ↑ FID ↓ Speech2Vid [17] 13.14 1.762 11.15 14.23 1.587 12.32 13.97 1.681 11.91 LipGAN [18] 10.05 3.350 2.833 10.33 3.199 4.861 10.65 3.###ideos. Also, it should not be fine-tuned further on the generated frames like it is done in LipGAN. One such network that has been used to correct lip-sync errors for creating large lip-sync datasets [1, 3] is the SyncNet [9] model. We propose to adapt and train a modified version of SyncNet [9] for our task. 3.3.1 Overview of SyncNet. SyncNet [9] inputs a windowV ofTv consecutive face frames (lower hal### list of pairs of video and a pseudo-randomly chosen audio as a consistent test set. We create three consistent benchmarks test sets, one each using the test set videos of LRS2 [1], LRW [8], and LRS3 [3] respectively. For each video Vs, we take the audio from another randomly-sampled videoVt with the condition that the length of 3github.com/joonson/syncnet_python the speech Vt be less than Vs. We cre",other,highlighting the need for a specific metric to measure lip-sync errors
2521,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,53e99e61b7602d9702725b5d,Multimodal information fusion for video concept detection,"[141] reported two approaches to study the optimal combination of multimodal information for video concept detection, which are gradient-descent-optimization linear fusion (GLF) and the super-kernel nonlinear fusion (NLF).",other,reporting prior findings on multimodal information for video concept detection
891,5b1643998fbcbf6e5a9bc447,e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e,Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates,573696016e3b12023e5154b2,Neural Machine Translation of Rare Words with Subword Units,"3 Subword segmentations with language model 3.1 Byte-Pair-Encoding (BPE) Byte-Pair-Encoding (BPE) (Sennrich et al., 2016; Schuster and Nakajima, 2012) is a subword segmentation algorithm widely used in many NMT systems 1 .###word/character (Wu et al., 2016), BPE (Sennrich et al., 2016) and our unigram model with or without subword regularization.###A common approach for dealing with the open vocabulary issue is to break up rare words into subword units (Schuster and Nakajima, 2012; Chitnis and DeNero, 2015; Sennrich et al., 2016; Wu et al., 2016).",impact-revealing,acknowledge the use of subword segmentation algorithms in NMT systems
2968,573697f96e3b12023e6d2f31,97acdfb3d247f8250d865ef8a9169f06e40f138b,EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding,555044b045ce0a409eb5004e,Towards speaker adaptive training of deep neural network acoustic models.,"We will study new speaker adaptation [35, 36] and adaptive training [37, 38] techniques for the CTC models.",other,suggesting future research directions in speaker adaptation and training techniques
2134,,abff9e836d31e5f9835ffa4bad4ade6e6da73047,Air Traffic Flow Management Under Uncertainty: Interactions Between Network Manager and Airline Operations Centre,,,"###The baseline flow based optimization model implemented is a slight reformulation of the model presented in Sun and Bayen [7] which was inspired by the Lighthill-Whitham-Richards theory [15], [16] and by the Daganzo cell transmission model [17], [18] commonly used in highway traffic.",impact-revealing,acknowledge the inspiration and basis for the optimization model
759,5ce3cd34e1cd8e3f7932b9ee,4f9ba5e89a7d23675ca65473ae85e352a6d2c379,Aging-aware Lifetime Enhancement for Memristor-based Neuromorphic Computing,56d83bb4dabfae2eee6332ea,Mapping weight matrix of a neural network’s layer onto memristor crossbar,"In practice, it is difficult to program the conductances of memristors to exact values and thus the resistances are usually programmed instead [13], [14].",impact-revealing,highlighting practical challenges in programming memristor conductances
812,5992a2ed5ba2006b76482df8,423ad8249c214e1346f337426f2da56deccf10a8,graph edge partitioning via neighborhood heuristic,5550453545ce0a409eb548d8,Balanced graph edge partition,"A p -edge partitioning of G refers to a disjoint partitioning of its edge set E into p subsets E i , such that The replication factor [3] of a partitioning is defined as Definition 1 (MIN-RF ( p , α ) ).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2420,5d04e8d7da56295d08daef06,448b64df68335d3695a37c54770e7d5cd5f6fe68,Context Attentive Document Ranking and Query Suggestion,53e9a965b7602d97032c06ca,A Deep Architecture for Matching Short Texts.,"Encouraged by the recent success of neural network based retrieval solutions [5, 14, 18, 19, 29], various models have been developed to optimize session-based retrieval.",other,highlighting the influence of neural network-based solutions on retrieval models
1140,,e3950d18cc81edbeafd83129a646a62850cad62a,A Cheaper and Better Diffusion Language Model with Soft-Masked Noise,,,"###Our work is inspired by recent advances in diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021; Yang et al., 2022; Ramesh et al., 2022; Rombach et al., 2022) that are introduced as a new generative modeling approach based on iterative denoising and have achieved…###Diffusion Models for Language There has been growing attention in deep generative diffusion models, which is a latent variable generative method based on iterative denoising (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021).",impact-revealing,highlighting the significance of recent advances in diffusion models for generative modeling
197,5dcd263a3a55ac58039516c5,add2f205338d70e10ce5e686df4a690e2851bdfc,Momentum contrast for unsupervised visual representation learning,5cede0e5da562983788c40d8,Unsupervised Embedding Learning via Invariant and Spreading Instance Feature,"Following [63, 2], we take two random “views” of the same image under random data augmentation to form a positive pair.###As the focus of this paper is not on designing a new pretext task, we use a simple one mainly following the instance discrimination task in [61], to which some recent works [63, 2] are related.###The end-to-end update by back-propagation is a natural mechanism ( e.g ., [29, 46, 36, 63, 2, 35], Figure 2a).###In this paper, we follow a simple instance discrimination task [61, 63, 2]: a query matches a key if they are encoded views ( e.g ., different crops) of the same image.###The networks f q and f k can be identical [29, 59, 63], partially shared [46, 36, 2], or different [56]. x q encoder encoder q encoder k q Conceptual comparison of three contrastive loss mechanisms (empirical comparisons are in Figure 3 and Table 3).###The input x q and x k can be images [29, 61, 63], patches [46], or context consisting a set of patches [46].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2405,5aed147c17c44a4438153ea5,665c0dde22c2f8598869d690d59c9b6d84b07c01,domino temporal data prefetcher,53e997c2b7602d9701faf260,Stealth prefetching,"While stride [3], [4], [5] or spatial [30], [31], [32], [33], [34] prefetchers are usually incapable of prefetching dependent misses [22] due to the lack of stride/spatial access patterns among dependent misses, temporal prefetchers can capture such misses, and hence, boost performance through substantially increasing",other,highlighting the advantages of temporal prefetchers over traditional methods
636,5f993ec291e011a3fbe2fb5c,f1e5e65941617604923225cc4bf464e370fcae67,Combining Label Propagation and Simple Models Out-performs Graph Neural Networks,5f03f3b611dc830562232042,Residual Correlation in Graph Neural Network Regression,"Since then, these techniques have been used for learning on relational data from just the labels (i.e., no features) (Koutra et al., 2011; Gleich & Mahoney, 2015; Peel, 2017; Chin et al., 2019) but have largely been ignored in GNNs.###Omitting the graph structure for these base predictions avoids the scalability issues with GNNs.###Our approach here is inspired in part by residual propagation (Jia & Benson, 2020), where a similar concept is used for node regression tasks, as well as generalized least squares and correlated error models more broadly (Shalizi, 2013).###…of Rice University, where classes are dorm residences and features are attributes such as gender, major, and class year, amongst others (Traud et al., 2012), as well as a geographic dataset of US counties where classes are 2016 election outcomes and features are demographic (Jia & Benson, 2020).###…use label propagation as a pre-processing step to weight edges for GNNs, whereas we use label propagation as a post-processing step and avoid GNNs. Jia & Benson (2020) use label propagation with GNNs for regression tasks, and our error correction step adapts some of their ideas for the case of…###Wang & Leskovec (2020) use label propagation as a pre-processing step to weight edges for GNNs, whereas we use label propagation as a post-processing step and avoid GNNs. Jia & Benson (2020) use label propagation with GNNs for regression tasks, and our error correction step adapts some of their ideas for the case of classiﬁcation.###Recent research connects GNNs to label propagation (Wang & Leskovec, 2020; Jia & Benson, 2020) as well as Markov Random ﬁelds (Qu et al., 2019; Gao et al., 2019), and some techniques use ad hoc incorporation of label information in the features (Shi et al., 2020).###…propagation as a pre-processing step to weight edges for GNNs, whereas we use label propagation as a post-processing step and avoid GNNs. Jia & Benson (2020) use label propagation with GNNs for regression tasks, and our error correction step adapts some of their ideas for the case of…###This type of propagation is provably the right approach under a Gaussian assumption in regression problems (Jia & Benson, 2020); however, for the classiﬁcation problems we consider, the smoothed errors ˆ E might not be at the right scale.",impact-revealing,highlighting the evolution and application of label propagation techniques in GNNs
2085,,ed43c8c17d2832e236714b668370c148be9a30ad,PresiShare: opportunistic sharing and presentation of content using public displays and QR codes,,,"###To allow for a more immediate interaction, Ballagas et al. [1] proposed a novel technique that relied on a grid with visual markers to act as a coordinate system for phonecam-based interaction.",impact-revealing,reporting prior findings on a novel interaction technique
3235,5c0f8548da562944ac906a71,5a3da29970d0c3c75ef4cb372b336fc8b10381d7,CNN-Based Real-Time Dense Face Reconstruction with Inverse-Rendered Photo-Realistic Face Images,56d92e92dabfae2eeeebba44,Dynamic 3D Avatar Creation from Hand-held Video Input,"Inspired by [23], we estimate d using the following objective function:###Actually image-based 3D face reconstruction itself is a fundamental problem in computer vision and graphics, and has many applications such as face recognition [5], [54] and face animation [23], [53].###Particularly, like many recent 3D face reconstruction works [18], [23], [48], we assume Lambertian surface reflectance and smoothly varying illumination in our inverse rendering procedure, which may lead to inaccurate fitting for face images with specular reflections or self-shadowing.",other,highlighting the significance of 3D face reconstruction in computer vision and its applications
1674,,7197316fbae893133ac707ea78e2ebd73c1b3fef,A SOCIAL IDENTITY ANALYSIS OF PEOPLE WITH DISABILITIES PERCEIVING DISCRIMINATION AS ILLEGITIMATE,,,"###Drawing on social identity theory (SIT; Tajfel & Turner, 1979) predictions, Study 1 (N = 335
people with disabilities) assesses whether socio-structural beliefs—permeability of group boundaries, cognitive alternatives to the status quo, and perceived pervasiveness of discrimination—predict…###…project, the articulation and development of the social model within DS scholarship provides the necessary paradigm shift for situating disability experience within the social psychological theoretical proposals offered by the social identity approach (Tajfel & Turner, 1979; Turner et al., 1987).###More precisely, the SIA to disability uses social identity and self-categorization theories (Tajfel &
Turner, 1979; Turner et al., 1987) to account for when a PWD will categorize as a member of the disability group and how a PWD will manage the stigma that accompanies disability group membership.###Perceived permeability of group boundaries has long been considered a pivotal factor for
predicting when stigmatized group members would pursue individualistic or collectivist coping outcomes (Ellemers, 1993; Tajfel & Turner, 1979).###Counter to SIT hypotheses, group boundary permeability did not account for significant variance
in illegitimacy of discrimination.###Drawing on social identity theory (SIT; Tajfel & Turner, 1979) predictions, Study 1 (N = 335
people with disabilities) assesses whether socio-structural beliefs—permeability of group boundaries, cognitive alternatives to the status quo, and perceived pervasiveness of discrimination—predict perceptions of illegitimate discrimination.###While it is not a socio-structural belief, group identification is potentially
predictive of perceived illegitimacy of discrimination, because it promotes a view of the world through an intergroup—'us’ vs. ‘them’—lens (Tajfel & Turner, 1979).###Regardless of proposed effect, this area of work could contribute not only to a better understanding of disability as a collective experience, but it can fill an important gap in the SIT literature.###This emphasis on individual-centered approaches to managing disability issues (i.e., remediation or rehabilitation) is emblematic of a social mobility strategy (SIT; Tajfel & Turner, 1979), and is predicated on the idea that one can leave the disability group with enough treatment and effort (Gill, 1997; Gilson, Tusler, & Gill, 1997).###Counter to SIT prediction (Tajfel & Turner, 1979; Jetten et al., 2013), and to Study 1 findings, perceived pervasiveness of discrimination had a null relationship with illegitimacy appraisals in the structural model.###This difference is likely an artifact of differences between the ‘cognitive’ aspects of social identity explored in self-categorization theory (Turner et al., 1987) and the ‘affective’ aspects more relevant to social identity theory (Tajfel & Turner, 1979).###SIT proposes that group members’ socio-structural beliefs are significant predictors of perceiving discrimination as illegitimate.###While social (but not medical) model endorsement did predict increased pervasiveness of discrimination, pervasiveness of discrimination did not predict perceptions of illegitimate discrimination, a finding that runs counter to SIT propositions (Jetten et al., 2013).###Perceiving discrimination as illegitimate
How group members begin to challenge the legitimacy of unequal status relations between groups
is a significant question that SIT can help explain (Tajfel & Turner, 1979; Reicher et al., 2010).###The results generally support social identity theory’s (SIT) contention that a group members’ perceptions of how pervasive group-based discrimination appears to be and their capacity to think of alternative, more equitable intergroup arrangements are significant antecedents to calling into question negative treatment against the group.###This follows a basic SIT proposition that to the extent status relations between groups are stable and secure, both the high and low status group members will generally accept the status quo (Tajfel & Turner, 1979).###The first hypothesis for Study 3, derived from SIT (Tajfel & Turner, 1979), suggests that once again socio-structural beliefs (group boundary permeability, cognitive alternatives to the status quo, and pervasiveness of discrimination) will predict perceptions of discrimination as illegitimate.###In line with SIT propositions, group boundary permeability (β = -.39, p   .001) and cognitive
alternatives to the status quo (β = .20, p = .030) predicted perceived illegitimacy of discrimination (see
Figure 4).###…because of rigid group boundaries (e.g., they cannot physically change groups or adequately approximate high-status group
norms, values, and/or physical presentation), subsequent strategies depend on the relative stability and legitimacy of intergroup status relations (Tajfel & Turner, 1979).###…individual-centered approaches to managing disability issues (i.e., remediation or rehabilitation) is emblematic of a social mobility strategy (SIT; Tajfel & Turner, 1979), and is predicated on the idea that one can leave the disability group with enough treatment and effort (Gill, 1997; Gilson,…###Across three studies, findings provide a conceptual replication of SIT predictions that socio-
structural beliefs predict perceptions of discrimination as illegitimate, adding to the literature the perspectives of people with disabilities.###…(Dirth & Branscombe, 2018), by synthesizing the meta-theoretical contributions of the social identity approach (Reicher, Spears, & Haslam, 2010; Tajfel & Turner, 1979; Turner, Hogg, Oakes, Reicher, Weatherall, 1987) and Disability Studies (Linton, Mello, & O’Neil, 1995), emphasizes the…###One way in which this project approached this objective was by testing the social identity theory (SIT) proposition that socio-structural beliefs are a critical determinant of illegitimacy appraisals for stigmatized group members (Jetten et al., 2013; Tajfel & Turner, 1979).###Relevant to the present inquiry, SIT proposes that in the presence of pervasive stigma and discrimination, group members can attempt to distance themselves from the group in order to protect their personal identity from threat (social mobility), or they can move closer to the group in order to reinterpret the meaning of the devalued characteristic (social creativity), or challenge the outgroup’s claim to a higher status position (social competition; Branscombe, Fernàndez, Gòmez, & Cronin, 2011).###Given that this research is derived from the stigma-management propositions found in SIT, it should not be too surprising that its formulation of social identity is more closely related to the variables of interest.###These factors are rooted in the fundamental SIT assumption that people strive for positive social identity, and it is a group members’ beliefs about the nature of the intergroup social structure, that determine how people will pursue a positive social identity (Ellemers, 1993).###Not only does the medical model provide an expectation that one is capable of recovery (i.e., group boundaries are permeable), it suggests that recovery (social mobility strategy; Tajfel & Turner, 1979; Dirth & Branscombe, 2018) should be the primary motivation of the disabled person.###This is likely because affinity or affective attachment toward the group is more consistent with SIT’s operationalization of social identity (Ellemers et al., 1999), whereas centrality is more consistent with the cognitive operationalization of social identity provided by self-categorization theory (Turner et al., 1987; van Zomeran et al., 2008).###According to SIT, whether a PWD will pursue a collective coping strategy, over an individualistic coping strategy, depends on group members’ perceptions of group boundaries as permeable (Ellemers, 1993; Ellemers, van Knippenburg, & Wilke, 1990).",impact-revealing,highlighting the application of social identity theory in understanding disability experiences
1761,,9ded246119861dd325e7004b2050bba310e08797,Timeloop: A Systematic Approach to DNN Accelerator Evaluation,,,"###NVDLA-derived [28] Eyeriss [8]###This is inspired by how Eyeriss is actually implemented in [8], which is slightly different from the model in [6].###We show that popular dataflows such as output-stationary or weight-stationary [6], [8] are but specific instances of a larger set of constraints that can be imposed on the mapspace, limiting the computation schedules and operand reuse patterns that the architecture can exploit.",impact-revealing,highlighting the differences in implementation and constraints of dataflows
1092,,c2f82fa97676ae31afc5effd6fc326d153ef7252,Multi-class support vector machine application in the field of agriculture and poultry: a review,,,"###To classify fruits using computer vision and multiclass SVM (Zhang and Wu (2012)) Multiclass kernel support vector machine (kSVM) Proposed a novel approach in classifying fruits by combining the color histogram, Unser’s texture and shape feature before classifying it using three different multi-class SVM in the various kernel###Within the sample x classiﬁcation, the largest values of decision function will be applied in Yu et al. (2012) and Zhang and Wu (2012).###Agriculture To classify fruits automatically (Zhang and Wu (2012)) To One-against-one, One-against-all, Direct Acyclic Graph SVM To One-against-one with linear kernel gives the best accuracy result compared to other multiclass algorithm",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
408,5aed14d617c44a4438158e20,9e788f1530af08a1f2140e6016fd4aeaa8b29033,Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform,5a260c8617c44a4ba8a32251,Be Your Own Prada: Fashion Synthesis with Structural Coherence,[52] propose an approach to generate new clothing on a wearer.,impact-revealing,reporting prior findings
400,5a260c8117c44a4ba8a30ec9,5fdd40601a2ccdaa2d2ade27872bd8b3f43b2c1c,MemNet: A Persistent Memory Network for Image Restoration,5a260bfb17c44a4ba8a1c61e,Image Super-Resolution Via Deep Recursive Residual Network,"Tai et al. [34] proposed deep recursive residual network (DRRN) to address the problems of model parameters and accuracy, which recursively learns the residual unit in a multi-path model.###[34] proposed deep recursive residual network (DRRN) to address the problems of model parameters and accuracy, which recursively learns the residual unit in a multi-path model.###Dataset Scale Bicubic SRCNN [8] VDSR [20] DRCN [21] DnCNN [40] LapSRN [23] DRRN [34] MemNet###We follow [34] to do data augmentation.",impact-revealing,reporting prior findings on deep recursive residual network
2143,,d72edb162b19c2e61caa737ff5330fae94c1805d,A new composite approach for COVID-19 detection in X-ray images using deep features,,,"###Various computer science concepts are frequently and recently employed in healthcare studies [1, 2, 3, 4, 5, 6].###Although Polymerase chain reaction (PCR) and serological test methods are frequently used for COVID-19 determinations, there are increasing detection researches using X-Ray and computed tomography images in the literature [8, 9, 2, 10, 11, 12, 13, 4, 14, 15, 16, 17, 18, 19].###They used 8066 normal, 5526 non-COVID-19, and 53 COVID-19 X-Ray images in their experiments, and reported an accuracy of 92.4% . a In another study, COVIDX-Net was proposed by Hemdan et al.[10].",impact-revealing,highlighting the application of computer science concepts in healthcare studies
3318,5ece0f029e795ebde7de1cce,d9c99592667c92d01cded2b7ca25cba4fdf83729,Bipartite Graph Neural Networks for Efficient Node Representation Learning,57aa28de0a3ac518da9896d5,node2vec: Scalable Feature Learning for Networks,"• Node2Vec (Grover and Leskovec, 2016): This approach is an extension of Word2Vec (Mikolov et al.###DeepWalk (Perozzi, Al-Rfou, and Skiena, 2014) and Node2vec (Grover and Leskovec, 2016) are representative random walk-based methods to model homogeneous graphs.###We contrast the performance of our algorithm against several unsupervised graph learning counter-parts: Node2Vec (Grover and Leskovec, 2016), VGAE (Kipf and Welling, 2016), GraphSAGE (Hamilton, Ying, and Leskovec, 2017), and ASGCN (Huang et al.###(Perozzi, Al-Rfou, and Skiena, 2014; Grover and Leskovec, 2016), where graph topology and node relations are embedded as vector space.",other,reporting prior findings on random walk-based methods for graph modeling
3867,5d1b2f5a3a55ac071793c55c,83b56c3c7a61767bd88d85796aa5dbc4976912c3,gpt-based generation for classical chinese poetry,5550456245ce0a409eb55c88,Chinese Poetry Generation with Recurrent Neural Networks.,Recurrent Neural Network (RNN) [11] was recently introduced as it has been proved to be eﬀective in generation tasks such as machine translation and dialog generation.###Recurrent Neural Network (RNN) [11] was recently introduced as it has been proved to be effective in generation tasks such as machine translation and dialog generation.,other,acknowledge the effectiveness of RNN in generation tasks
1284,,b1311124a7dfe1442036347f294a924b21a74e91,The ketogenic diet compensates for AGC1 deficiency and improves myelination,,,"###We therefore retrieved raw data from MR spectroscopy, fitted spectra in the frequency domain, and quantified using LC Model software and LCMgui.(15) This revealed increases in both NAA and creatine, illustrating that the NAA/creatine ratio is insufficient for monitoring of the patient’s progressive brain maturation (Table S2).###We therefore retrieved raw data from MR spectroscopy, fitted spectra in the frequency domain, and quantified using LC Model software and LCMgui.15 This revealed increases in both NAA and creatine, illustrating that the NAA/creatine ratio is insufficient for monitoring of the patient’s progressive brain maturation (Table S2).",impact-revealing,highlighting findings from MR spectroscopy analysis
328,5843777eac44360f108417ec,455afd748e8834ef521e4b67c7c056d3c33429e2,Hierarchical Attention Networks for Document Classification,5550456345ce0a409eb55d24,Modeling Interestingness with Deep Neural Networks.,"Attention serves two beneﬁts: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classiﬁcation decision which can be of value in applications and analysis (Shen et al., 2014; Gao et al., 2014).###Attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis (Shen et al., 2014; Gao et al., 2014).",impact-revealing,highlighting the benefits of attention mechanisms in classification
2306,5f7fdd328de39f0828397e22,21e33bd0ad95ee1f79d8b778e693fd316cbb72d4,Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs,53e9985fb7602d970209ab90,Understanding belief propagation and its generalizations,"Belief propagation (BP) [40] is a classic messagepassing algorithm for graph-based semi-supervised learning, which can be used for graphs exhibiting homophily or heterophily [19] and has fast linearized versions [10, 8].###In belief propagation [40], a message-passing algorithm used for inference on graphical models, the different levels of homophily or affinity between classes are captured via the class compatibility, propagation or coupling matrix, which is typically pre-defined based on domain knowledge.",other,providing context on belief propagation in semi-supervised learning
1727,,6cc182c4e1949e59673bdca2d6da0bbc1b95c60f,Transaction based dynamic partial replication in mobile environments,,,"###The two-tier lazy master replication scheme allows mobile nodes to read and update the database while disconnected from the network [6].###Eager replication [3], which requires read one and write all in one transaction, is not suitable for mobile applications where most nodes are disconnected [6].",impact-revealing,describing a database replication method for mobile nodes
1018,,7fa11081fc1b829665052382c816b5a8ee362a4c,Health-related needs of people with multiple chronic diseases: differences and underlying factors,,,"###Cluster analysis revealed an eight-cluster solution as the optimal number of clusters that could explain the profile structure of the patients’ self-reported problems (EQ-6D).###Table 2 Self-reported problems, happiness, satisfaction, and loneliness of people with multimorbidity versus one chronic disease (N = 1.092)
Multimorbid (n = 561)*
One chronic disease (n = 531)*
p uncorrected p corrected**
n % n %
EQ-6D Mobility: some/extreme problems 258 46.7 158 30.0 p\ .001 p\ .005 EQ-6D Self-care: some/extreme problems 66 12.0 36 6.8 p\ .005 n.s. EQ-6D Usual activities: some/extreme problems 249 44.9 183 34.6 p\ .001 p\ .05 EQ-6D Pain/discomfort: some/extreme problems 373 67.5 290 55.1 p\ .001 p\ .001 EQ-6D Anxiety/depression: some/extreme problems 121 21.9 105 20.0 n.s. n.s. EQ-6D cognition: some/extreme problems 136 24.9 102 19.3 p\ .05 n.s.###The EQ-6D consists of six items assessing patients’ selfreported problems with regard to (1) mobility, (2) self-care, (3) usual activities, (4) pain/discomfort, (5) anxiety/depression (all similar to EQ-5D), and (6) cognitive functioning.###To identify subgroups of multimorbid patients, cluster analysis was performed and differences in EQ-6D scores between clusters were tested with Chisquare tests.###This analysis was performed with the use of the six EQ-6D dimensions being the clustering variables and including the total group of multimorbid patients.###Moreover, a study among almost 10,000 GP patients representative for the general Dutch population showed that 11.5 % reported some or extreme anxiety or depression (this was measured with the EQ-6D as well; [19]).###Health problems among multimorbid people
Full answers on the EQ-6D were missing for 24 participants—therefore these people were excluded from further analyses.###Health problems were assessed by the EQ-6D [18, 19], a multi-dimensional instrument based on the EQ-5D [20].###To assess problems in the social domain (not covered by the EQ-6D), we included the Loneliness Scale developed by de Jong-Gierveld and Kamphuis [21] and validated by van Tilburg and de Leeuw [22].###They completed the EQ-6D, a multi-dimensional questionnaire on health problems (October 2013).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
459,5f9a9af391e0114d7e7813ed,0d67d3ddca1c4e370eaf1e99ec674f612c39c66c,Graph Contrastive Learning with Adaptive Augmentation,5ee3526a91e011cb3bff72d6,Contrastive Multi-View Representation Learning on Graphs,"Following DGI, GMI [32] employs two discriminators to directly measure MI between input and representations of both nodes and edges without data augmentation; MVGRL [16] proposes to learn both node- and graph-level representations by performing node diffusion and contrasting node representations to augmented graph summary representations.###Moreover, to supplement the input graph with more global information, MVGRL [16] proposes to augment the input graph via graph diffusion kernels [24].###We consider representative baseline methods belonging to the following two categories: (1) traditional methods including DeepWalk [34] and node2vec [11] and (2) deep learning methods including Graph Autoencoders (GAE, VGAE) [22], Deep Graph Infomax (DGI) [46], Graphical Mutual Information Maximization (GMI) [32], and Multi-View Graph Representation Learning (MVGRL) [16].###The proposed GCA framework follows the common graph CL paradigm where the model seeks to maximize the agreement of representations between different views [16, 53].###In summary, we provide a brief comparison between the proposed GCA and other state-of-the-art graph contrastive representation learning methods, including DGI [46], GMI [32], andMVGRL [16] in Table 1, where the last two columns denote data augmentation strategies at topology and attribute levels respectively.",impact-revealing,acknowledge existing graph representation learning methods
3246,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",59ae3c3a2bbe271c4c71fe80,Grasp: A Matlab Toolbox For Graph Signal Processing,"Finally, it is worth mentioning that many of the basic GSP tools described here are available in several Matlab/Python toolboxes: GSPBox [215], GraSP [216] and PyGSP [217].###toolboxes: GSPBox [215], GraSP [216] and PyGSP [217].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1813,,801e5dc668035f2f72cf25fb18fd94bf22f623f1,Effectiveness of Continuous Subcutaneous Insulin Infusion on Parental Quality of Life and Glycemic Control Among Children With T1D: Meta‐Analysis,,,"###As there was only one study in three subgroups in which age of children was below 6 years (Opipari- Arrigan et al., 2007), between 9 and 13 years (Weintrob et al., 2003), and between 14 and 18 years (Cohen et al., 2003), respectively, it was insufficient for subgroup analysis.###…described in detail in the seven RCTs (Cohen et al., 2003; Meschi, Beccaria, Vanini, Szulc, & Chiumello, 1982; Nuboer, Borsboom, Zoethout, Koot, & Bruining, 2008; Opipari- Arrigan et al., 2007; Schiaffini, Patera, Bizzarri, Ciampalini, & Cappa, 2007; Skogsberg et al., 2008; Weintrob et al., 2003).###Five studies were included in the meta- analysis (Cohen et al., 2003; Nuboer et al., 2008; Opipari- Arrigan et al., 2007; Skogsberg et al., 2008; Weintrob et al., 2003).###Two RCTs adopted a crossover design (Cohen et al., 2003; Weintrob et al., 2003), and five were of parallel design (Meschi et al., 1982; Nuboer et al., 2008; Opipari- Arrigan et al., 2007; Schiaffini et al., 2007; Skogsberg et al., 2008).###In the four studies with the intervention of 1–6 months (Cohen et al., 2003; Nuboer et al., 2008; Opipari- Arrigan et al., 2007; Weintrob et al., 2003), the MD of HbA1c was - .27% (95% CI = −0.58 to .03, z = 1.79, p = .07, I2 = 0%).###In three studies, all participants
completed the study (Meschi et al., 1982; Schiaffini et al., 2007; Weintrob et al., 2003).###Six studies were included in our meta- analysis (Cohen et al., 2003; Meschi et al., 1982; Nuboer et al., 2008; OpipariArrigan et al., 2007; Skogsberg et al., 2008; Weintrob et al.,
2003).",impact-revealing,highlighting the insufficiency of existing studies for subgroup analysis
3778,57d063e8ac443673542950ad,0509facb30efa81c4730f98bfc28cdbd6178822e,"Image sentiment analysis using latent correlations among visual, textual, and sentiment views",53e9bd3fb7602d97049d070b,Sentribute: image sentiment analysis from a mid-level perspective.,"Conventional methods of image sentiment analysis have aimed to design e ﬀ ective visual features for training sentiment polarity classiﬁers [4–6].###We will introduce additional views or features such as facial expressions [6].###Similarly, attribute features including facial expression were used as mid-level features in [6].",other,acknowledge existing methods in image sentiment analysis
1054,,f60c78e905d92b72651c45c97ce7cb53c4e45c8a,Programmable Host-Network Traffic Management,,,###Elephant flow scheduling: The second application is inspired by how Hedera [3] and Mahout [8] schedule large flows.,impact-revealing,drawing inspiration from existing flow scheduling methods
2396,5e09a76bdf1a9c0c41677a7b,71bd6b3a2bfa54ee4ea8499be0c4ff478fd735f6,POLAR++: Active One-Shot Personalized Article Recommendation,5550401245ce0a409eb3205c,Dropout: a simple way to prevent neural networks from overfitting,"Following [42], we use the distribution of the network parameter with dropout [53] as q ( ω ) .",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1356,,f73eb182180cb897da21ea8d88900e5d396c16c8,Net load forecasting for high renewable energy penetration grids,,,"###It has been widely applied for forecasting various kind of time-series such as finance [69], load [70], stock market [71], etc.",impact-revealing,acknowledging the application of a method in various time-series forecasting domains
1410,,bdb12ecda298e6b0cba82e4837d3dbab8bab52b8,Analyzing and Overcoming Degradation in Warm-Start Reinforcement Learning,,,"###…proposed method to overcome WSRL Degradation is inspired by some of the recent Offline RL algorithms: Batch-Constrained Q-Learning (BCQ) [17], BEAR [18], Behavior-Regularized [19][20][21], Rerouted-Behavior-Improvement [22], Policy-Constraint [23], KL-Control [24], and Critic Regularization [25].",impact-revealing,drawing inspiration from recent offline reinforcement learning algorithms
3276,5843777eac44360f108417ec,455afd748e8834ef521e4b67c7c056d3c33429e2,Hierarchical Attention Networks for Document Classification,5736974d6e3b12023e6386c5,Hierarchical Recurrent Neural Network for Document Modeling,"There are some other works that use hierarchical structure in sequence generation (Li et al., 2015) and language modeling (Lin et al., 2015).",other,acknowledge existing works on hierarchical structure
3602,5cede0e1da562983788bfe5f,988a378f640eb7fb681f977d6cb1e0c830c07b4c,Adversarial Examples Are a Natural Consequence of Test Error in Noise,53e9a194b7602d9702a893ff,Mean squared error: Lot it or leave it? A new look at Signal Fidelity Measures,"In order to visualize worst-case perturbations at varying l 2 distances, we visualize an image that minimizes similarity according to the SSIM metric (Wang & Bovik, 2009).",other,providing context for visualizing perturbations in images
3575,5b67b4b417c44aac1c86789a,abd91aca4d78799492256b406f5abc199d3802e4,Multilingual End-to-End Speech Recognition with A Single Transformer on Low-Resource Languages,5a73cb6317c44a0b30358209,Multilingual Recurrent Neural Networks With Residual Learning For Low-Resource Speech Recognition,"Although multilingual speech recognition has been studied [1, 2, 3, 4, 5] for a long time, these researches are commonly limited to making acoustic model (AM) multilingual, which require language-speciﬁc pronunciation model (PM) and language model (LM).###The baseline systems come from our previous work [5] and all results are summarized in Table 5.###SHL-MLSTM [5] further explores long short-term memory (LSTM) [7] with residual learning as the shared hidden layer instead of DNN and achieves better results than SHL-MDNN.###Multilingual speech recognition has been investigated for many years [1, 2, 3, 4, 5].###A comparison with SHL-MLSTM [5] with residual learning is investigated on CALL-HOME datasets with 6 languages.",other,highlighting the limitations and advancements in multilingual speech recognition
3718,5f0d8b6891e011047aff993b,1c53d27c742fb4658fa03085c7c2ca014a122385,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,59e7741d0cf2df775dd34e44,MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets.,"7. https://github.com/agemagician/ProtTrans/tree/master/Benchmark###MMSeqs2 [51] with highest sensitivity ( -s 7.5 ) removed proteins with >20% PIDE to either the training set or to itself.###Increasing model size improves performance for some NLP applications [12], although the massive data challenges the communication between thousands of nodes and divergence between large batches during training.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1192,,f30908fea6a6c4d0eab0e44ab5010ff903310c48,Fifty years of the psychology of programming,,,"###Psychological research, including work psychology and organisational psychology (Weinberg 1971), offered the potential to gain improved understanding of the
AC CE
PT ED
M AN
US CR
IP T
problems experienced in such projects, and perhaps evaluate alternative approaches to addressing them (Weinberg &…",impact-revealing,highlighting the relevance of psychological research to project understanding
2135,,dbc155ebb46307290e91cc794598465dcc0b5e29,Large-scale modeling and optimization of en route air traffic flow,,,"###It is based on the Daganzo Cell Transmission Model (CTM) [22; 23] in which the traffic flowing into a control volume changes the density of aircraft in that control volume and, hence, changes the outflow of the control volume.###This model is powered by a discretized version of the Lighthill-Whitham-Richards (LWR) partial differential equation (PDE) [45; 63] and inspired by the Daganzo Cell Transmission Model [22; 23].###This model is inspired by the Lighthill-Whitham-Richards theory [45; 63], and by the Daganzo Cell Transmission Model [22; 23], which is commonly used in highway traffic modeling.###Note that this is very close to the approach taken by Daganzo in his definition of the original CTM [22; 23].###Several approaches have been proposed to solve this problem, in particular split coefficients [49], which is inspired by the highway transportation literature [58; 23].###– The terminology CTM(L) is in reference to the seminal Daganzo Cell Transmission Model (CTM), which was commonly used for highway traffic [22; 23].",impact-revealing,providing context on the Daganzo Cell Transmission Model and its applications
449,5db92aec47c8f76646216865,146128184cd585b5a1298d3e75e15fb8fbbbd7a5,Improved low-resource Somali speech recognition by semi-supervised acoustic and language model training,5bbacb3717c44aecc4eabdc4,Semi-Orthogonal Low-Rank Matrix Factorization For Deep Neural Networks,"Index Terms: speech recognition, Somali, semi-supervised, TDNN-F, under-resourced language###In comparison with our previous ASR system [9], the improvement afforded by TDNN-F is clear (rows 1 and 2).###A baseline TDNN-F acoustic model was trained using this multilingual data and semi-supervised training was carried out in three passes.###It has recently been shown that, when semi-orthogonal lowrank matrix factorisation is applied to the parameter matrices of TDNN layers, ASR performance can be improved in lowresource situations [13].###We make use of factorised time-delay neural networks (TDNN-F) for acoustic modelling, since these have recently been shown to be effective in resourcescarce situations.###Hence, the TDNN-F models are faster to train.###This factorisation allows the TDNN-F model to use fewer parameters than hybrid architectures such as TDNN-LSTM and TDNN-BLSTM (bidirectional LSTM).###We make use of TDNN-F acoustic models and experiment with the incorporation of additional but unannotated Somali speech data by semisupervised training, an approach which has been applied successfully in some other low-resource settings [8, 14–16].###Consequently, a TDNN-F acoustic model (10 time-delay layers followed by a rank reduction layer) was trained using the Librispeech recipe for Kaldi (version 5.2.164).###Even though TDNN-F uses only half the number of parameters as CNNTDNN-BLSTM, it is able to offer better performance.###Our TDNN-F was trained using the lattice-free maximum mutual information objective criterion [22].###The recently-introduced factorised time-delay neural networks (TDNN-F) [13] utilise half the number of parameters than the hybrid networks with comparable performance, in particular in a low-resource setting.",impact-revealing,highlighting the effectiveness of TDNN-F models in low-resource settings
793,5736982b6e3b12023e6fd099,684a466028785c39911770b96fb0c814e75b5b6d,DynaMOS: Dynamic schedule migration for heterogeneous cores,53e9b365b7602d9703e48f3c,Discerning the dominant out-of-order performance advantage: is it speculation or dynamism?,These results corroborate with recent work [8] that credit the OoO’s ability to create good static schedules as the main reason for the perfor-,impact-revealing,reporting findings that support the effectiveness of the OoO's scheduling capabilities
3905,5c7a561ff56def9798e6a297,abe4e92d6e4868115aef57a4eba97ec4d48ac124,language-adversarial transfer learning for low-resource speech recognition,5a73cb6317c44a0b303581e3,The Kaldi Openkws System: Improving Low Resource Keyword Search,"The transfer learning methods can be roughly classified into two categories: transferring bottleneck features [17], [23]–[26] and transferring model parameters [10], [11], [27].###Multilingual training is an effective technique to train the source model [14]–[17].",other,describing categories of transfer learning methods
1885,,fbbbe7f36ac5b6f40b73673ce8ecb94017d1310c,Adapting Education: Navigating Hybrid Classrooms in The Post-Pandemic Era,,,"###The Community of Inquiry Model (CoI), initially conceptualized by Garrison et al. (1999), has been instrumental in guiding educators in the design of online classrooms.",impact-revealing,highlighting the significance of the Community of Inquiry Model in online education
3792,5dc5488edf1a9c0c41511e82,d33e7907ae6e8a0c8396822df09806661de85710,scaling the capacity of memory systems: evolution and key approaches,5c756ee1f56def97986167c0,HBM (High Bandwidth Memory) DRAM Technology and Architecture,"On the other hand, new interfaces such as High Bandwidth Memory (HBM) [36] [58] and Hybrid Memory Cube (HMC) [60] [66] offer potential solutions to some of these problems described, but are still not widely available nor provide the overall capacity offered by a DSM system.",other,highlighting potential solutions to memory problems in computing
55,5e3be3c33a55ac29c4ae7e18,6dbdc34000b034b75b8ff70872fc7c35549e273a,Interpretable & Time-Budget-Constrained Contextualization For Re-Ranking,599c7987601a182cd2648373,Attention Is All You Need.,"er of lightweight Trans1 TU Wien, Austria, email: s.hofstaetter@tuwien.ac.at 2 TU Wien, Austria, email: markus.zlabinger@tuwien.ac.at 3 TU Wien, Austria, email: hanbury@ifs.tuwien.ac.at former layers [32] (we evaluate up to three) can effectively contextualize query and document word embeddings. TK’s second contribution is a network structure built for explainability. In contrast to BERT-based approac###e of the contextualization by the end-toend learned parameter. This allows the model to decide the intensity of the contextualization. We calculate the context(t 1:n) with a set of Transformer layers [32]. First, the input sequence is fused with a positional encoding to form p 1:n, followed by a set of lTransformer layers: Transformer l(p 1:n) = MultiHead(FF(p 1:n)) + FF(p 1:n) (2) Here, FF is a two-l###xtends KNRM by adding a CNN layer on top of the word embeddings, enabling word-level n-gram representation learning – a local contextualization, ﬁxed by the n-gram size hyperparameter. Vaswani et al. [32] proposed the Transformer architecture in the context of language translation. Their encoder-decoder is built of Transformer layers, each containing multi-head self attention. These Transformer layers###matured and a distinct trade-off emerged between a neural re-ranking model’s effectiveness and its efﬁciency. While IR-speciﬁc networks are reasonably fast [36, 5, 15], large Transformer based models [32], such as BERT [6], show substantially better effectiveness at the cost of orders of magnitude longer inference time [12, 20, 25]. Given the same amount of limited time, a faster reranking model can i",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3398,5bdc315817c44a1f58a05e88,9ea992f009492888c482d5f4006281eaa8b758e7,"X2Face: A network for controlling face generation by using images, audio, and pose codes",58d82fced649053542fd68c2,Lip Reading Sentences in the Wild,"Examples are controlling the mouth with speech [8, 38], controlling a head with audio and a known emotional state [16], and controlling body movement with music [36].",other,providing examples of control methods in various contexts
2895,5e5e18d493d709897ce3320c,222b9a7b8038120671a1610e857d3edbc7ac5550,Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models,53e9a508b7602d9702e2bcf5,Rectified Linear Units Improve Restricted Boltzmann Machines,"We choose them because Phang et al. (2018) have observed that it was unstable to finetune BERTLARGE on these four tasks.###For each hidden layer, we add layer normalization (Ba et al., 2016) right after the ReLU (Nair & Hinton, 2010) nonlinearity.",other,highlighting the instability of finetuning BERTLARGE on specific tasks
2971,5c2c7a9217c44a4e7cf317b4,61946177ea5d44ad18ff09de7929556d7b34cd8b,simd-x: programming and processing of graph algorithms on gpus.,53e9b38fb7602d9703e710a2,Green-Marl: a DSL for easy and efficient graph analysis,"Furthermore, domain specific programming language systems, such as Galois [54], Green-Marl [23] and Trinity [63], allow programmers to write single-threaded source code while enjoying multi-threaded processing.###Recent advance in graph computing falls in algorithm innovation [51, 87, 15], framework developments [49, 18, 45, 39, 42, 90, 92, 22, 66, 63, 61, 23, 54, 60, 74, 7, 80, 84, 65, 88, 75, 55, 89, 86, 85, 3, 78, 52, 21, 9, 81] and accel-###Furthermore, domain speciﬁc programming language systems, such as Galois [42], Green-Marl [19] and Trinity [51], allow programmers to write single-threaded source code while enjoying multi-threaded processing.",other,acknowledge advancements in domain-specific programming languages
2578,5db92aec47c8f76646216865,146128184cd585b5a1298d3e75e15fb8fbbbd7a5,Improved low-resource Somali speech recognition by semi-supervised acoustic and language model training,5bbacb3717c44aecc4eabd03,Multilingual Neural Network Acoustic Modelling For Asr Of Under-Resourced English-Isizulu Code-Switched Speech,"By leveraging available resources from better-resourced but unrelated languages [8,10,11], a system using a hybrid neural network acoustic model was able to achieve a word error rate (WER) of 53.",other,highlighting the effectiveness of a hybrid neural network model in achieving low word error rates
1206,,8bc7b2005e9c3e4de73e1306061e789346f283fc,"Constraint Nondegeneracy, Strong Regularity, and Nonsingularity in Semidefinite Programming",,,"###Our research in this paper is motivated by [42] on various characterizations of strong regularity, one of the most important concepts in sensitivity and perturbation analysis, introduced by Robinson in his seminal paper [32], for a local optimal solution of the general nonlinear SDP problem.###In [32], Robinson introduced an important concept called strong regularity for a solution of generalized equations.",impact-revealing,highlighting the motivation drawn from prior work on strong regularity
2714,5736982b6e3b12023e6fd332,d0b8c5464b582e1a582a4affb270e8b02f8414a2,a scalable architecture for ordered parallelism,53e99dc5b7602d9702686030,Parallel simulation made easy with OMNeT plus,"Second, they are important in simulating systems whose state evolves over time, such as circuits [47], computers [12,59], networks [37,72], healthcare systems [39], and systems of partial differential equations [32, 44].",other,highlighting the importance of certain systems in various fields
3872,5bdc315017c44a1f58a05a1d,f9fb0ad85f903e3ecf5d4bafdf7c74f647e75f58,LEMNA: Explaining Deep Learning based Security Applications,59ae3c3a2bbe271c4c71fdd3,Neural Nets Can Learn Function Type Signatures From Binaries,"So far, researchers have successfully applied deep neural networks to train classifiers for malware classification [2, 16, 21, 48, 68], binary reverse-engineering [15, 52, 71] and network intrusion detection [24, 62], which all achieved an exceptionally high accuracy.###1[15] presents some case studies using saliency map to explain RNN, but is forced to ignore the feature dependency of RNN, leading to a low explanation fidelity.###The applications of deep learning in binary analysis include identifying function boundaries [52], pinpointing the function type signatures [15] and tracking down similar binary code [71].###As a result, Recurrent Neural Networks (RNN) or Multilayer Perceptron Model (MLP) are more widely used [15, 21, 52, 68].###There are many other security applications such as detecting the “function end” for binary code, pinpointing the function types and detecting vulnerable code [15, 24, 47, 52, 66].###also use RNN to accurately track down the arguments and types of functions in binaries [15].",other,highlighting the successful application of deep neural networks in various security tasks
1834,,1d598793f9fca02674efe542b1d68d934c4fdc97,Symbolic Privacy Analysis through Linkability and Detectability,,,"###Formal methods are widely used as a tool for the analysis of security in communication protocols [2, 5, 12, 14].###Protocols are commonly modelled using process algebras (e.g., [2]); alternative approaches exist, e.g., using induction [14].###Recent work [3] proposes to deﬁne and verify linkability using the inductive method [14].",impact-revealing,acknowledge the use of formal methods in security analysis
2994,5f03f3b611dc83056223206d,75c8466a0c1c3b9fe595efc83671984ef95bd679,XGNN: Towards Model-Level Explanations of Graph Neural Networks,53e99dbeb7602d970267ce45,Network motifs: theory and experimental approaches,"type of patterns is known as network motifs that represent simple building blocks of complex networks (graphs), which widely exist in graphs from biochemistry, neurobiology, ecology, and engineering [1, 2, 23, 30].",other,providing context on network motifs in various fields
3334,5d9edbfc47c8f7664602eba5,dde65325dc7600d02983a76bd54693f0050946a4,integrating both visual and audio cues for enhanced video caption,573698636e3b12023e729288,"Look, Listen and Learn - A Multimodal LSTM for Speaker Identification","(Ren et al. 2016) proposed a multimodal Long Short-Term Memory (LSTM) for speaker identification, which referred to locating a person who has the same identity with the ongoing sound in a certain video.###Inspired by (Ren et al. 2016), we propose to build temporal dependency across visual and audio modalities through sharing weights for video caption, aiming at exploring whether temporal dependency across visual and audio modalities can capture the resonance information among them or not.###Ren et al. (Ren et al. 2016) proposed a multimodal Long Short-Term Memory (LSTM) for speaker identiﬁcation, which referred to locating a person who has the same identity with the ongoing sound in a certain video.",other,drawing inspiration from prior work on multimodal LSTM for speaker identification
1798,,07dded40073658b978ae1090edbff9cb7ace755a,Intraoperative detection of intimal lipid in the radial artery predicts degree of postoperative spasm.,,,"###Widespread acceptance of the RA has been hindered by its tendency to develop post-operative spasm and early failure (2).###Yet the RA is utilized as a conduit in less than 20% of CABG procedures performed in the US due mainly to concerns about the risk of early failure and spasm after grafting (2).###Angiographic “string sign” was defined as diffuse narrowing of the graft to <1 mm, as previously described (2).###If the traditional angiographic criteria of “string sign” or diffuse narrowing to <1 mm were used in our study, we would have noted a similar incidence of RA spasm (13%) as described in previous reports (2).",impact-revealing,highlighting the limitations and concerns regarding the use of the RA in CABG procedures
1606,,a5881560968963d0c845c468a273261fde0b7248,Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing,,,"###Our implementation adapts and builds on top of the open-source package TextAttack (Morris et al., 2020).###, 2020) algorithm to generate adversarial examples via the open-source package Textattack(Morris et al., 2020).###We use the popular TextFooler(Jin et al., 2020) algorithm to generate adversarial examples via the open-source package Textattack.",impact-revealing,reporting the use of an open-source package for generating adversarial examples
441,5d9edc7547c8f766460401fb,f797fd44b9ddd5845611eb7a705ca9464a8819d1,very deep convolutional networks for text classification,5550415645ce0a409eb3a69e,Very Deep Convolutional Networks for Large-Scale Image Recognition.,"The design of our architecture is inspired by recent progress in computer vision, in particular (Simonyan and Zisserman, 2015; He et al., 2016a).",impact-revealing,highlighting the influence of recent advancements in computer vision on architecture design
233,5f7fdd328de39f0828397afd,c841c9704bf35873a051f228a15f67b30d650c2f,Scalable Graph Neural Networks via Bidirectional Propagation,5f02f17c91e011ee5e0258c8,Scaling Graph Neural Networks with Approximate PageRank,"PPRGo [4] calculates approximate the Personalized PageRank (PPR) matrix ∑∞ `=0 α(1 − α)Ã by forward push algorithm [2] and then applies the PPR matrix to the feature matrix X to derive the propagation matrix.###We also use one state-ofthe-art scalable GNN from each of the three categories: LADIES (layer sampling) [40], GraphSAINT (graph sampling) [37], SGC and PPRGo (linear model) [30, 4].###PPRGo [4] uses Personalized PageRank to capture multi-hop neighborhood information and uses a forward push algorithm [2] to accelerate computation.###PPRGo [4] calculates approximate the Personalized PageRank (PPR) matrix ∑∞ `=0 α(1 − α)`Ã` by forward push algorithm [2] and then applies the PPR matrix to the feature matrix X to derive the propagation matrix.###A major drawback of PPRGo is that it takes O( n ε ) space to store the PPR matrix, rendering it infeasible on billion-scale graphs.###For PPRGo, it has a longer running time than other methods because of its expensive feature propagation per epoch.###However, we will mainly focus on two setups in this paper: 1) w` = α(1− α)` for some constant decay factor α ∈ (0, 1), in which case P becomes the Personalized PageRank used in APPNP and PPRGo [16, 17, 4]; 2) w` = 0 for ` = 0, . . . , L− 1 and wL = 1, in which case P degenerates to the L-th transition probability matrix in SGC [30].###However, we will mainly focus on two setups in this paper: 1) w` = α(1− α) for some constant decay factor α ∈ (0, 1), in which case P becomes the Personalized PageRank used in APPNP and PPRGo [16, 17, 4]; 2) w` = 0 for ` = 0, .###We also point out that PPRGo starts to converge and achieves an F1-score of 0.15 in 4500 seconds when the feature dimension is increased to 10000.###We first observe that both GBP and SGC can capture the structural information with random features, while PPRGo and GBP(PPR) fail to converge.",impact-revealing,describing the functionality and limitations of PPRGo in graph-based methods
3606,5aed147c17c44a4438153a60,5245d411b9dd97ffafe07320981d1282e8f32764,"dCat: dynamic cache management for efficient, performance-sensitive infrastructure-as-a-service",53e9b790b7602d9704338380,Pipp: Promotion/Insertion Pseudo-Partitioning Of Multi-Core Shared Caches,"Fine-grained cache partitioning on chip Some researchers have noticed the performance interference caused by shared LLC and tried to provide a series of chip-level cache allocation mechanisms according to the workloads behaviors [22, 23, 27, 36, 37, 41, 42].",other,acknowledge existing research on cache allocation mechanisms
2564,5bdc315017c44a1f58a05e13,1717255b6aea01fe956cef998abbc3c399b5d7cf,AMC: AutoML for Model Compression and Acceleration on Mobile Devices,58d82fc8d649053542fd59b8,Neural Architecture Search with Reinforcement Learning,"During the update, the baseline reward b is subtracted to reduce the variance of gradient estimation, which is an exponential moving average of the previous rewards [56, 6]:",other,providing context for reward adjustment in gradient estimation
3400,5f8d6be69fced0a24bbab01e,a87e4124f7305a97a8efaa574c1b270dccf4a563,DisenHAN: Disentangled Heterogeneous Graph Attention Network for Recommendation,5b67b46417c44aac1c86124b,Aspect-Level Deep Collaborative Filtering via Heterogeneous Information Networks.,[10] extract aspect-level factors from different meta paths and fuse the factors with attention mechanism for recommendation.###• NeuACF [10] (B): This method considers multiple aspects of users and items with a deep neural network for recommendation in HIN.,other,reporting prior findings on aspect-level factors extraction for recommendation
4055,5cede0e5da562983788c40d8,e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b,Unsupervised Embedding Learning via Invariant and Spreading Instance Feature,53e9b98ab7602d970457be97,Stochastic k-Neighborhood Selection for Supervised and Unsupervised Learning.,"2) Estimating Between-image Labels, it usually estimates between-image labels using the clustering technique [3, 9, 26] or kNN-based methods [41], which provide label information.",other,describing methods for estimating between-image labels
1670,,1830387e7fe6a42c76248d70d3b297a272622651,Double Trouble: How Being Outnumbered and Negatively Stereotyped Threatens Career Outcomes of Women in STEM,,,"###This paper builds on social identity theory (SIT; Tajfel and Turner, 1979) to investigate what aspects of masculine work contexts may form career barriers among women STEM graduates.###Following from the social identity approach (Tajfel and Turner, 1979), being one of the only few women at work means being highly dissimilar from most other colleagues.###We took a social identity lens (Tajfel and Turner, 1979) to put forward gender identity threat as an important mechanism to explain how masculine work contexts translate into career barriers for women in STEM.###Following from SIT (Tajfel and Turner, 1979), the more importance or self-relevance women attach to their gender identity (i.e., high gender identification), the more motivated they will be to maintain or protect a positive image of that gender identity (Tajfel and Turner, 1986; Ellemers et al.,…###Following from SIT (Tajfel and Turner, 1979), the more importance or self-relevance women attach to their gender identity (i.e., high gender identification), the more motivated they will be to maintain or protect a positive image of that gender identity (Tajfel and Turner, 1986; Ellemers et al., 1999) and hence, the greater the experience of gender identity threat in a context that signals male-dominance (Schmader, 2002; Major et al., 2003).###The SIT approach posits that in organizational contexts people’s attitudes and behaviors are determined, at least in part, by their group memberships (e.g., being a woman, a professional, a member of a team), and the importance people attach to these groups (Haslam et al., 2014).###Following from SIT (Tajfel and Turner, 1979), the more importance or self-relevance women attach to their gender identity (i.",impact-revealing,investigating gender identity threat in masculine work contexts using social identity theory
1645,,1f8255f5b82c1bce9bf6c6e62b3db1f9fc04bd6e,Gender and sexual diversity: Inclusion in the Namibian education context,,,"###The SIP maintains that recognising differences makes individuals feel belonging to a group, boosts their self-esteem and makes them feel good about themselves (Burke et al., 2007; Tajfel & Turner, 1979).###Further, society perceives any behaviour different from the norm as threatening the social order, leading to social rejection (Tajfel & Turner, 1979).###Literature (Stryker, 1977; Tajfel & Turner, 1979) states that when people look at others as belonging to a particular group, they tend to develop a discriminating attitude towards the people they have positioned in the out-group.###The literature (i.e., Powell & Menendian, 2016; Spivak, 1985; Tajfel & Turner, 1979) maintains that social rejection occurs once the society perceives any behaviour different from the norm as a threat to the social order.###The SIP posits that social identities are formed when people construct sameness and differences, and once people believe these identities to be the norm, they identify with them (Spivak, 1985; Stryker, 1977; Tajfel & Turner, 1979).###In that regard, the SIP demonstrated the importance of social inclusion in increasing the learners’ pride, social self, and sense of social identity (McLeod, 2019; Tajfel & Turner, 1979).###Hence, diverse characteristics from the perceived ideal lead to exclusion (Spivak, 1985; Stryker, 1977; Tajfel & Turner, 1979), which leads to mistreatment and discrimination.###Tajfel and Turner (1979) maintain that discrimination occurs when people believe that individuals can only belong to a particular group.###Consequently, in terms of GSD, as literature (Spivak, 1985; Stryker, 1977; Tajfel & Turner, 1979) maintains, social categorisation of people based on GSD could lead to the exclusion of learners, discrimination, and homophobia in schools.",impact-revealing,discussing the implications of social identity theory on group dynamics and discrimination
2941,5736982b6e3b12023e6fd154,5061f0c3637f22b1776f013f28f1bdc518a5c304,MORC: A manycore-oriented compressed cache,53e99bfeb7602d97024ab4ce,A Fully Associative Software-Managed Cache Design,"Other indirect caches have the same needs [28].###In this respect, a MORC cache is similar to indirect caches [28, 20] except that the LMT is over-provisioned in order to track additional lines resulting from compression.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1010,,f775db217b0b789e918d17a6a9f465205169ce98,Pharmacokinetic and pharmacodynamic interactions between antiepileptics and antidepressants,,,"###5-HT: Serotonin receptor; a: Alpha adrenergic receptor; H: Histamine receptor; m-CPP: M-chlorophenylpiperazine; MT: Melatonin receptors; OCD: Obsessivecompulsive disorder; RCT: Randomized clinical trial; REM: Rapid eye movement; TCA: tricyclic antidepressant; SNRI: Serotonin and noradrenaline reuptake inhibitor; SSRI: Selective serotonin reuptake inhibitor.###Figure 4 summarizes antidepressant mechanisms of action in several disorders including depression, OCD, anxiety, pain, weight loss and other indications.###Some articles implied the possibility of additive- and/or synergistic-type PDDIs between antiepileptics and antidepressants for two unapproved indications: menopausal vasomotor symptoms (Figures 2 and 4) and treatment-resistant OCD.###Antidepressant drugs are also widely used for the treatment of other psychiatric conditions including anxiety disorders, obsessive-compulsive disorder (OCD), eating disorders and various forms of chronic pain such as diabetic neuropathic pain and fibromyalgia [5].###{Fluoxetine, fluvoxamine, paroxetine and sertraline are approved in the USA for OCD.###A recent guideline recommended SSRI augmentation with lamotrigine or topiramate as one of the options after initial SSRI nonresponse in OCD [111].###A RCT indicated that adjunctive therapy with pregabalin in patients with a partial response to antidepressants in generalized anxiety disorder was more effective than placebo [20] and this augmentation strategy is recommended by a recent guideline [111].###OCD guidelines usually recommend SSRIs as first-line agents, followed by augmentation with clomipramine and/or antipsychotics, but there is no information for clinicians on what to do when all three treatments have been tried.###The lack of studies in OCD is an even more problematic case.###There are few RCTs with antiepileptics in OCD, but one [121] suggests that lamotrigine may potentiate the effects of serotonergic reuptake inhibitors in treatment-resistant OCD, possibly due to its glutamatergic effects.",impact-revealing,highlighting the significance of antidepressant mechanisms and their applications in various psychiatric conditions
2488,5c8fd41a4895d9cbc66534e9,950aae7979a2faa874b7481179c064d3ad151ea8,Objects Segmentation From High-Resolution Aerial,53e9af2db7602d9703969c5a,"The Role Of Site Features, User Attributes, And Information Verification Behaviors On The Perceived Credibility Of Web-Based Information","In this paper, we modeled the enhanced semantic segmentation model capable of better segmenting multiclass objects from aerial images by exploiting a state-of-the-art CNN-based algorithm.###A key idea of FCN is changing of the CNN model from classification to dense prediction by reinterpretation of fully connected layers of the classifier as a fully convolution layer [25].###Our CNN-based multiobject segmentation with a large scale of data set made to train algorithms only focused on the urban areas of South Korea.###After they trained CNN by using a large scale of raw OSM data for binary classification, a tiny piece of the manually labeled data were used to tune convolutional filters.###In addition, deep learning techniques such as convolutional neural networks (CNNs) have attracted much attention to segment objects in satellite images [11], [12].###Index Terms— Aerial images, convolutional neural networks (CNNs), object segmentation.###In this section, CNN-based segmentation algorithms are described as follows.###An FCN is a modified CNN to semantically segment images.###There are several algorithms that are applied to object segmentation based on CNNs [22]–[24].###The encoder part of FCN consists of visual geometry group network (VGGNet) [26] that is a famous CNN classification model and the decoder part consists of a deconvolution layer for upsampling.###The defects of OSM data might disturb the training for CNN [30].",other,describing the methodology and significance of CNN-based segmentation in aerial images
2762,5f0277e911dc830562231dea,60fc1eefcc4743fcd96597c2c9be11da688e4ef7,Reinforcement Learning to Rank with Pairwise Policy Gradient,53e9b042b7602d9703aa0d88,Optimizing search engines using clickthrough data.,"Existing learning to rank studies can be categorized into pointwise approaches[8, 23], pairwise approaches [1, 3, 16], and listwise approaches [2, 4, 36].###We compared the proposed PPG to the traditional learning to rank baselines, including RankSVM [16], RankNet [2], ListNet [4], AdaRank [36], and MDPRank [40].",other,acknowledge existing learning to rank studies and compare proposed method
3547,5f03f3b611dc83056223206d,75c8466a0c1c3b9fe595efc83671984ef95bd679,XGNN: Towards Model-Level Explanations of Graph Neural Networks,5a9cb66717c44a376ffb868e,Junction Tree Variational Autoencoder for Molecular Graph Generation.,"r manual analysis. 3.2 Interpreting GNNs via Graph Generation Recent advances in graph generation lead to many successful graph generation models, such as GraphGAN [38], ORGAN [14], Junction Tree VAE [17], DGMG [22], and Graph Convolutional Policy Network (GCPN) [41]. Inspired by these methods, we propose to train a graph generator which generates G∗step by step. For each step, the graph generator gen",other,acknowledge existing graph generation models
1028,,38b4915c87781f6593baa64fee29ff4c016bf142,Evidence for response bias as a source of error variance in applied assessment.,,,"###Many indicators of PIM fail to consider one and sometimes two of these facets (Lanyon, 2004; Paulhus & John, 1998).###However, Paulhus and John (1998) subsequently concluded that no scale is effective at discriminating between the two motivations.###The authors suggested regressing the substantive indicator onto the bias indicator and using the residual as an indicator of job competence corrected for bias (see equation on p. 576; for a similar recommendation, see also the self-criterion residual described by Paulhus & John, 1998).",impact-revealing,highlighting limitations in existing PIM indicators
223,573696026e3b12023e515eec,2c03df8b48bf3fa39054345bafabfeff15bfd11d,Deep residual learning for image recognition,5550417d45ce0a409eb3bc08,Going Deeper With Convolutions,"In [43, 24], a few intermediate layers are directly connected to auxiliary classifiers for addressing vanishing/exploding gradients.###43† GoogLeNet [43] (ILSVRC’14) 7.###33 GoogLeNet [43] 9.###Recent evidence [40, 43] reveals that network depth is of crucial importance, and the leading results [40, 43, 12, 16] on the challenging ImageNet dataset [35] all exploit “very deep” [40] models, with a depth of sixteen [40] to thirty [16].",impact-revealing,highlighting the importance of network depth in achieving leading results on ImageNet
1675,,4013f5f340e2f5cadfd13c0acdc14f32bc7448cb,Direct and Indirect Xenophobic Attacks: Unpacking Portfolios of Identity,,,"###Our approach to answering this question begins with insight from SIT (Tajfel and Turner 1979) and its offshoot Self Categorization Theory (SCT) (Turner et al. 1987).###They emphasize the analytical utility of group identification over group membership and build on the idea that individuals are motivated to uphold a positive self image (Tajfel and Turner 1979).###We think these individuals embrace their non-Mexican national origin identity as a way of preserving their self worth and maintaining a positive distinctiveness to the extent possible, a central prediction in SIT.###SIT theorists argue that human interaction ranges on a spectrum from being purely interpersonal on the one hand to purely intergroup on the other.###SIT predicts that individuals can seek higher status groups as a response to maintain a positive self image when their current group is being devalued as opposed to engaging in pro-group activities to maintain the positive distinctiveness of the group (Tajfel and Turner 1979; Ellemers et al. 1999, 2002).###…answer to these questions knits together theoretical insight from SIT, which emphasizes the motive to see oneself in a positive light (cf. SIT and Tajfel and Turner 1979), with the view shared by a growing number of scholars who underscore the fluidity between distinct social identities that are…###Tajfel and Turner (1979) argued that a motivating principle underlying behavior was a desire for a positive and secure self-concept, which could be achieved by attachment to a group (i.e., a group identity).###Of course, consistent with SIT and SCT, the salience of any given identity within one’s portfolio is contingent on the the environmental and social context where they find themselves (Tajfel and Turner 1979).###Consistent with the broader work in SIT, we observe that weak identifiers, whether Mexican heritage or non-Mexican heritage, do little to preserve the positive distinctiveness of the group, reinforcing the consistent finding that the strength of identity is key to understanding the political response.###Our answer to these questions knits together theoretical insight from SIT, which emphasizes the motive to see oneself in a positive light (cf. SIT and Tajfel and Turner 1979), with the view shared by a growing number of scholars who underscore the fluidity between distinct social identities that are contained in a larger repertoire or portfolio of identities (Carter and Pérez 2016; Huddy and Turner 2014; Chandra 2012).###…that individuals can seek higher status groups as a response to maintain a positive self image when their current group is being devalued as opposed to engaging in pro-group activities to maintain the positive distinctiveness of the group (Tajfel and Turner 1979; Ellemers et al. 1999, 2002).###This measure is consistent with the SIT framework we are drawing on and used in other studies and data sets (Pérez 2015b).###…group will respond to maintain the group’s positive distinctiveness because individuals derive a positive self image from the status of the group (Tajfel and Turner 1979), and because the targeted threat raises the salience of that segment of one’s identity (Roccas and Brewer 2002; Ellemers et…###Recent work by Pérez (2015b) and Valenzuela and Michelson (2016) leverage theoretical insights from Social Identity Theory (SIT) to examine a set of conditions under which social identities become consequential for politics.",impact-revealing,Integrating theoretical insights from Social Identity Theory to explore group identity and behavior
1667,,e08856e0e7da3b08b69bba14cd3d3d4674794bf4,The Structure and Behavioral Effects of Revealed Social Identity Preferences,,,"###The logo and the (14)Using this payoff structure, we build on a commonly used approach in the social psychology literature, going back to the seminal paper by Tajfel et al. (1971). (15)24 individuals took part per laboratory in one session, while 20 individuals took part per laboratory in two other sessions due to no-shows.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3629,5c8fd41a4895d9cbc66534e9,950aae7979a2faa874b7481179c064d3ad151ea8,Objects Segmentation From High-Resolution Aerial,573696bb6e3b12023e5bc1b8,Building Detection In Very High Resolution Multispectral Data With Deep Learning Features,Most works to segment objects such as roads and buildings have been carried out using aerial images in rural areas [14]–[16].,other,acknowledging existing research on object segmentation in aerial images
738,5fc75d8591e0114897921043,b62edbf6e619eeed886c63e51fdff2c3d94f998f,graph convolutions that can finally model local structure,5e5e18a093d709897ce21291,What graph neural networks cannot learn: depth vs width,"Although such network could in principle have a strong discriminative power with enough layers, it was shown that practical networks can have trouble solving even basic structure related tasks, such as detecting small cycles(Loukas, 2019; Chen et al., 2020).###This problem has been investigated by many recent works (Chen et al., 2020; Nikolentzos et al., 2020; Abu-El-Haija et al., 2019; Loukas, 2019; Fey et al., 2020).###Without virtual node, convolution based networks are at most as discriminative as the Weisfeiler-Lehman test(Loukas, 2019).",impact-revealing,highlighting challenges in practical network applications and referencing recent investigations
1223,,92e9cc7cbe9da2b62c34ac377c8016457016b704,Factors Associated With Overweight and Obesity Among Mexican Americans and Central Americans: Results From the 2001 California Health Interview Survey ORIGINAL RESEARCH,,,"###adult population was recently estimated to be overweight or obese ( 1-2 ).###The Hispanic Health and Nutrition Examination, for example, sampled only Mexican Americans, Cuban Americans, and Puerto Ricans, and NHANES III, conducted from 1989 to 1994, sampled only Mexican Americans ( 2 ).###Our study also did not address environmental factors that contribute to weight gain, such as a reliance on fast food outlets and convenience stores with limited dietary choices, and heavy marketing of calorie-dense foods ( 2 ,43,44).###of overweight and obesity (body mass index [BMI] >25kg/m2) increased by nearly 40% between 1976–1980 and 1999–2000 (from 46% to 64.5%), and the prevalence of obesity (BMI >30kg/m2) increased by 110% (from 14.5% to 30.5%) ( 2 ).",impact-revealing,highlighting limitations in the study and acknowledging environmental factors
89,5d3ed25a275ded87f97dea9b,0ddd7c9a1955c76eb831a230ef20a68b662886bb,Mining Algorithm Roadmap in Scientific Publications,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"We follow BERT [7] which recently achieves great success in multiple NLP tasks, to add a special <CLS> token at the start of the paragraph and a special <SEP> token at the end of each sentence in the paragraph.###A convolutional layer Conv with residual connection follows the self-attention layer: We follow BERT [7] which recently achieves great success in multiple NLP tasks, to add a special <CLS> token at the start of the paragraph and a special <SEP> token at the end of each sentence in the paragraph.###We base on recent Transformer architecture [7, 36] to build this module, due to its better performance in encoding long-distance context compared to Long Short Term Memory Networks (LSTMs) [15] and Convolutional neural networks (CNNs).",impact-revealing,describing the method based on BERT and its architecture
1594,,33fc3520cdd8a8a8f694c9fcd12f3aebec519799,A comparison between unfocused and focused transmit strategies in cardiac strain imaging,,,"###Myocardial Elastography (Konofagou et al 2002, Lee et al 2007), a cardiac strain imaging technique developed by our group, is the particular focus of this study; its applications include tracking ischemia progression in canines (Lee et al 2011), lesion monitoring during RF ablation in canines and…###The strain estimation precision of Myocardial Elastography based on cardiac wall segment was investigated.###Previous iterations of Myocardial Elastography have implemented the Lagrangian definition of strain (Lee et al 2007).###…approaches to estimating cardiac strain with ultrasound (Cikes et al 2014), it is well understood that accurate strain estimation is contingent on a frame rate higher than those normally used in clinical B-modes (D’hooge et al 2000, Konofagou et al 2002, Chen et al 2009, Bunting et al 2014).###Myocardial Elastography (Konofagou et al 2002, Lee et al 2007), a cardiac strain imaging technique developed by our group, is the particular focus of this study; its applications include tracking ischemia progression in canines (Lee et al 2011), lesion monitoring during RF ablation in canines and humans (Grondin et al 2015, Bunting et al 2018), and quantitatively differentiating ischemic from normal patients as validated by nuclear imaging and angiography (Grondin et al 2017b).",impact-revealing,highlighting the significance and applications of Myocardial Elastography in cardiac imaging
3152,5e5794b791e011545375102b,38bccac4f05585ec595c7bb7c0e747561dcad886,DNN-Chip Predictor: An Analytical Performance Predictor for DNN Accelerators with Various Dataflows and Hardware Architectures,5a260c1d17c44a4ba8a222c4,PredictiveNet: An energy-efficient convolutional neural network via zero prediction,"As such, there has been intensive research on DNN accelerators in order to take advantage of different hardware platforms, such as FPGAs and ASICs, for improving DNN acceleration efficiency [9, 10, 11, 12, 13, 14].",other,highlighting the focus on improving DNN acceleration efficiency through hardware platforms
1693,,21b99b905791ed8b3385deaa199667ad671cca45,“A letter for Dr. Outgroup”: on the effects of an indicator of competence and chances for altruism toward a member of a stigmatized out-group,,,"###According to SIT (Tajfel and Turner, 1979), encountering highly competent out-group members might be perceived as threat to the in-group’s high status.###…et al., 1971; Mullen et al., 1992), and various theories build on this ingroup preference to explain intergroup conflict from different perspectives such as the Social Identity Theory (SIT; Tajfel and Turner, 1979) or the Realistic Intergroup Conflict perspective (RIC; e.g., Sherif et al., 1961).###A host of classic studies document reliably that individuals tend to treat members of their in-group more favorably than out-group members (Tajfel et al., 1971; Mullen et al., 1992), and various theories build on this ingroup preference to explain intergroup conflict from different perspectives such as the Social Identity Theory (SIT; Tajfel and Turner, 1979) or the Realistic Intergroup Conflict perspective (RIC; e.g., Sherif et al., 1961).",impact-revealing,providing context for theories on intergroup conflict
745,5b67b46b17c44aac1c861fc4,f6d06993e003fa6fec5bf630efded9e4fd90a030,fluency boost learning and inference for neural grammatical error correction,57d063b9ac4436735428eb51,Grammatical Error Correction: Machine Translation And Classifiers,"Recently, many novel approaches (Susanto et al., 2014; Chollampatt et al., 2016b,a; Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2016; Mizumoto and Matsumoto, 2016; Yuan et al., 2016; Hoang et al., 2016; Yannakoudakis et al., 2017) have been proposed for GEC.###• CUUI and VT16: the former system (Ro-zovskaya et al., 2014) uses a classiﬁer-based approach, which is improved by the latter sys-tem (Rozovskaya and Roth, 2016) through combining with an SMT-based approach.###, 2014) uses a classifier-based approach, which is improved by the latter system (Rozovskaya and Roth, 2016) through combining with an SMT-based approach.",impact-revealing,acknowledge recent advancements in grammatical error correction
1711,,5893c96f7ace4a1953d445e6eb15f38cf7e5b6b1,Examining the role of identity in negotiation decision making:the case of Cyprus,,,"###Examining the role of identity
415
The social contextualist perspective in negotiator decision making builds upon social identity theory, which examines interpersonal behavior through the lens of social group memberships (see Tajfel and Turner, 1979, 1985).",impact-revealing,providing context for the social contextualist perspective in negotiator decision making
2041,,47be321bff23f73c71d7e5716cd107ead087c3ae,Optimal Multi-Object Segmentation with Novel Gradient Vector Flow Based Shape Priors,,,"###LOGISMOS has been widely used for multiple surface segmentation with the capability of enforcing mutual surface interactions, while still achieving globally optimal solutions [5, 14, 7, 9, 8].",impact-revealing,acknowledge the effectiveness of LOGISMOS in surface segmentation
476,599c7945601a182cd262a009,6727f574ad8b1c3763be8d58eeaf82c551aa33ef,Generative and Discriminative Text Classification with Recurrent Neural Networks,57a4e91aac44365e35c97ff6,Progressive Neural Networks.,"Discriminative models are known to suffer from catastrophic forgetting when learning sequentially from examples from a single class at a time, and specialized techniques are actively being developed to minimize this problem (Rusu et al., 2016; Kirk-patrick et al., 2017; Fernando et al., 2017).",impact-revealing,highlighting the challenges faced by discriminative models in sequential learning
2248,5a73cbcc17c44a0b3035f3c9,5e2bb96c47ccaa16a4e7192e8fadb3b3e1c3acdc,deep learning: a critical appraisal,5a73cbcc17c44a0b3035f2ba,The NarrativeQA Reading Comprehension Challenge,"A third focus might be on human understanding of narrative, a notion long ago suggested by Roger Schank and Abelson (1977) and due for a refresh (Marcus, 2014; Kočiský et al., 2017).###• A comprehension challenge (Paritosh & Marcus, 2016; Kočiský et al., 2017)] which would require a system to watch an arbitrary video (or read a text, or listen to a podcast) and answer open-ended questions about what is contained therein.",other,highlighting the need for renewed focus on human understanding of narrative
2666,5f0d85c69fced0a24be4f019,6817b5d48cf8f665b680de07d23b91814d2923b1,Bouquet of Instruction Pointers: Instruction Pointer Classifier-based Spatial Hardware Prefetching,5b67b47917c44aac1c8635ca,Division of Labor: A More Effective Approach to Prefetching.,"There are component prefetchers like division of labor (DOL) [35] that target speciﬁc program semantics (like pointer chains, loops, etc.) for prefetching by getting the information of interest from the processor core.###DOL [35] at L1 and L2 fails to outperform the top four prefetchers.###DOL [35] is a recent prefetching framework that uses component prefetchers (similar to IPCP).",other,acknowledge limitations of the DOL prefetching framework
983,,5404cb1a3515ff9dd9ad4a5020adf9872518398c,Influence diffusion detection using the influence style (INFUSE) model,,,"###Guadagno et al. (2008) measured the personality of bloggers to predict blogging based on the five key personality traits: neuroticism , extra-version , agreeableness , openness to experience , and conscientiousness as observed by Costa & McCrae (1992).###Previous studies (Guadagno, Okdie, & Eno, 2008; Yarkoni, 2010) had measured bloggers’ characteristic and personality with regards to their propensity to blog, but they did not consider the bloggers’ ability to influence.",impact-revealing,highlighting the focus on personality traits in blogging and identifying a gap in influence measurement
3012,5f3268fb91e011bc1612aeab,dee8650c0a65588a09eb86751c600fb67a030bbc,Speech Driven Talking Face Generation From a Single Image and an Emotion Condition,5bdc31b817c44a1f58a0bcec,Albumentations: Fast And Flexible Image Augmentations,"During training, we randomly augmented the data using the Albumentations library [35] to improve the generalization capability of our network.",other,describing a method used to enhance model performance
3238,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,5e5e189a93d709897ce1e760,InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization,InfoGraph [55] extended the idea to learning representations of whole graphs instead of just nodes.,other,reporting prior findings on graph representation learning
3523,5736982b6e3b12023e6fd099,684a466028785c39911770b96fb0c814e75b5b6d,DynaMOS: Dynamic schedule migration for heterogeneous cores,53e99bc6b7602d970246fe01,Conservation Cores: Reducing The Energy Of Mature Computations,"These include cores with different sets of microarchitectural parameters [21, 22], specialized hardware for specific codes [23, 24], and heterogeneous ISAs [25].",other,acknowledging variations in hardware architectures
2121,,e5d2b10ca6a612fc969517970350742935ccb8b3,Impact of curbside bus stop locations on mixed traffic dynamics: a bus route perspective,,,"###Table 5 comparesMAPE of the proposed and traditional CTMwith time resolution being 2 and 5min, where the proposedmodel always corresponds to lower MAPE than that from traditional CTM.###Considering that the curb lane with bus stops is different from the adjacent lane, lane-based CTM is employed and extended to capture the impact of dwelling buses on mixed traffic delay and their interaction with general traffic (see Figure 2), such as queuing, merging, diverging, weaving and lane changing.###Since its proposal, various efforts have been made to apply CTM to traffic state forecasting (Szeto et al. 2009), jam simulation (Long et al. 2011; Su, Kurzhanskiy, and Horowitz 2013), lane changing (Laval and Daganzo 2008; Carey, Balijepalli, and Watling 2015) and intersection flow (Flötteröd and Nagel 2005; Qi et al. 2013).###This study, proposed to address this critical issue, develops anextendedLink-Node structured two-lane cell transmission model (CTM) to capture the impact of multiple curbside bus stop locationsonmixed traffic dynamics alongabus routeunder varying traffic demand levels.###In this study, due to bus dwelling and car driving behaviours with and without LC, the CTM model is extended to allow the total number of vehicles in cell i on lane j, nji(t), to be divided into different components as illustrated in Figure 4, given by Equation (2):
nji(t) = β∗bji(t) + cji(t) (2)
where β is car equivalent per bus, bji(t) is the number of buses, and cji(t) is the number of cars in cell i on lane j at time t, given by:
cji(t) = vji(t) + wji(t) (3)
where vji(t) is the number of cars without LC, while wji(t) refers to the number of cars intending to make LC in cell i on lane j at time t, given by:
wji(t) = mwji(t) + dwji(t) (4)
wheremwji(t) is thenumberof carswithMLC in cell ion lane j at time t, which is exogenously initiated by the link traffic demand and lane function at the beginning of lane j and then calculated iteratively with Equation (18); while dwji(t) is the number of cars intending to
makeDLC in cell ion lane j at time t, triggeredby instant imbalancebetween concentrations of the downstream parallel cells, given by:
Δji(t) = max { 0,
i+a∑ u=i+1
nju(t) − nj′u(t) }
(5a)
dwji(t) = { min{vji(t), pi ∗ Δji(t)}, ifC\{CN ∪ CT }, b′j′(i+1)(t) = 0 0, otherwise
(5b)
In Equation (5a), Δji(t) calculates the concentration gap between the immediately downstream apairs of cells, which is annulled if that gap is negative.###Gu et al. (2011, 2013, 2014) have enhanced cell transmission model (CTM) to explore the impact of bus stops on bus and car delay at a single intersection with or without signal control.###Moreover, recent studies have progressed to explore the interactions between buses and cars with the extended cell transmission model (CTM) (Daganzo 1994, 1995).###Performance of the proposed model is compared with the traditional CTM, which is lane-based but does not specifically model the impact of bus stops.###An overview of the logic flow of the extended CTM is given in Figure 7.###This study, proposed to fill the above scientific gap, develops an extended Link-Node structured (Zhang et al. 2015; Su 2014) two-lane cell transmission model (CTM) to capture the impact of multiple curbside bus stop locations on mixed traffic dynamics along a bus route under varying traffic demand levels.###Vehicle movement in the extended CTM is constrained by the sending and receiving capacities of a cell (Daganzo 1994), given by:
sji(t) = min{nji(t)/τ ,Qji(t)} rji(t) = min{δ∗(N* l − nji(t))/τ ,Q′ji(t)}
(1)
Figure 3.###With road divided into homogeneous cells, the potential of CTM is confirmed to capture various traffic dynamics.###Islam et al. (2018) applied CTM to a unified Bus Rapid Transit with transit signal priority for single destination networks.",impact-revealing,highlighting the development of an extended cell transmission model to address traffic dynamics
2284,5aed148b17c44a4438154fae,fa54b47df8641dff1579b5e8e0f18f057de68e73,DRN: A Deep Reinforcement Learning Framework for News Recommendation,57fdf424654a3f2774ecce07,Learning Hidden Features for Contextual Bandits,"Hidden Linear Upper Confidence Bound [42] further allows learned hidden feature to model the reward.###For all compared algorithms, the recommendation list is generated by selecting the items with top-k estimated potential reward (for LinUCB , HLinUCB and our methods) or probability of click (for LR , FM and W&D ) of each item.###Linear Upper Confidence Bound [23] can select an arm (i.e., recommend a piece of news) according to the estimated upper confidence bound of the potential reward.###(An improved version of the original LinUCB – HLinUCB will also be compared.)###• HLinUCB [42] is another state-of-art bandit-based approach in recommendation problem.###) • HLinUCB [42] is another state-of-art bandit-based approach in recommendation problem.###To our surprise, some baseline methods, like HLinUCB , also achieve comparable recommendation diversity, which indicates that UCB can also achieve reasonable exploration result (but this kind of unguided exploration will harm the recommendation accuracy).###Recently, some people try to combine bandit with clustering based collaborative filtering [14], andmatrix factorization [6, 21, 32, 42, 43, 51], in order to model more complex user and item relationship, and utilize the social network relationship in determining the reward function.",other,describing various recommendation algorithms and their performance
613,5eede1bc91e0116a822a4942,156d11ca27740b591fb827e143cc71e9795a9745,Riptide: Fast End-to-End Binarized Neural Networks,5a9cb66717c44a376ffb88c7,TVM: End-to-End Optimization Stack for Deep Learning.,"To compile our described algorithms to efﬁcient machine code, we extend TVM (Chen et al., 2018a) to support bitse-rial operations.###} 3: b 0 = BatchNorm ( c 0 ) 4: q 0 = LinearQuantize ( b 0 ) 5: a 0 = BitPack ( q 0 ) 6: for k = 1 to L do 7: in both TensorFlow (Abadi et al., 2016) and TVM (Chen et al., 2018a).###…et al., 2013) and TVM (Chen et al., 2018a) have arisen that attempt to simplify the process of creating optimized schedules by sep-arating the deﬁnition of compute from the schedule itself, and in some cases supporting automated hyperparameter search to produce good schedules (Chen et al., 2018b).###Recently, projects such as Halide (Ragan-Kelley et al., 2013) and TVM (Chen et al., 2018a) have arisen that attempt to simplify the process of creating optimized schedules by sep-arating the deﬁnition of compute from the schedule itself, and in some cases supporting automated hyperparameter search…###Although these primitives require well chosen hyperparam-eters to maximize performance, we leverage AutoTVM (Chen et al., 2018b) to automatically search and ﬁnd high quality settings.",impact-revealing,describing the process of compiling algorithms to machine code
1354,,a2f5dc508989e2d832697980388d2a25df6ad500,Prolonged treatment with the anabolic–androgenic steroid stanozolol increases antioxidant defences in rat skeletal muscle,,,"###The benefit of AAS use for healthy eugonadal men has been questioned for decades but, in recent years, it has been demonstrated that testosterone and AASs, administered at suprapharmacological doses, can induce hypertrophy of type I and II muscle fibres [4, 21] and are effective in increasing skeletal muscle mass and strength [ 22 ].",impact-revealing,highlighting recent findings on the effects of testosterone and AAS on muscle hypertrophy
1774,,ac7e7cc6e88f435606f95ab08f1324dcb0463a22,Grade 4 Rural Learners' Views and Learning Experiences That Address Social Justice in Postapartheid South Africa,,,"###…with the children facilitated communication, resulting in detailed discussions with the learners about the conditions of learning in rural schools, and bridged the gap between the researchers’ and participants’ worlds be cause understanding was anchored in the photographs (Wells et al., 2013).",impact-revealing,highlighting the role of photographs in facilitating communication and understanding in research
2378,5da1a6d447c8f7664606888c,404d82a8da658f16c46714442155490d79a413f1,temporal prefetching without the off-chip metadata,5b67b47917c44aac1c8635c8,Criticality Aware Tiered Cache Hierarchy: A Fundamental Relook at Multi-Level Cache Hierarchies.,"Second, the marginal utility of the last-level cache (LLC) [34, 40] is typically outweighed by the benefits of an effective prefetcher.",other,highlighting the trade-off between cache utility and prefetcher effectiveness
3376,5e3d353b3a55ac4de4104f40,3024f58826a5bce3378af94f677e8fb90cbb49e0,LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation,5c04967517c44a2c74708e3a,Deep Item-based Collaborative Filtering for Top-N Recommendation.,"hat historical items have different contributions to shape personal interest. Towards this end, attention mechanisms areintroducedtocapturethevaryingcontributions,suchasACF[3], NAIS [17], and DeepICF [43], to automatically learn the importance of each historical item. When revisiting historical interactions as a user-item bipartite graph, the performance improvements can be attributed to the encoding ",other,highlighting the role of attention mechanisms in improving performance
2630,5f1ff7ea91e011d50a621ab3,39b45b0d60807296db97d9eb37b1fb99d244e2ed,MACU-Net for Semantic Segmentation of,5550411745ce0a409eb38760,Deep Learning Face Attributes in the Wild,"hierarchical feature representations in an automatic fashion, influencing the field of computer vision (CV) significantly [9].",other,highlighting the significant influence of hierarchical feature representations in computer vision
1986,,8b339a66404565b6f7b863cbad8b379357a62162,The Effect of Systemic Parameters and Baseline Characteristics in Short-Term Response Analysis with Intravitreal Ranibizumab in Treatment-Naive Patients with Neovascular Age-Related Macular Degeneration,,,"###Also, we analysed only short-term response as it allows for an improved understanding of the visual treatment potential and guidance on clinical management [25].###This classification of treatment response was based on Amoaku et al. [25].",impact-revealing,acknowledging the basis for treatment response classification
2064,,ce5a617d2f263191c32d005036e4133980548a73,Comparison of Antimicrobial Susceptibility of Campylobacter Strains Isolated from Food Samples and Patients with Diarrhea,,,###High resistance to ciprofloxacin in the present study may be due to the fact that fluoroquinolones such as ciprofloxacin are frequently used for treatment of campylobacteriosis because of their broad spectrum of activity against enteric pathogens [32] .,impact-revealing,highlighting the potential reasons for high resistance to ciprofloxacin
2787,5d8dded23a55acd1b54967df,1ecbaf7a2cd3059e07261e72a1195a7c70b3d664,GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning,573695fe6e3b12023e511794,Revisiting Semi-Supervised Learning with Graph Embeddings,", 2014) which can then be used as an input to any classifier, or learns the node embedding and target prediction jointly (Yang et al., 2016).###Another line of work learns node embedding in an unsupervised way (Perozzi et al., 2014) which can then be used as an input to any classiﬁer, or learns the node embedding and target prediction jointly (Yang et al., 2016).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2063,,d0980419e63f404bccf7ffed9d9f2564efa4ab7a,Two-dimensional ferroelectricity induced by octahedral rotation distortion in perovskite oxides,,,"###Recently, the successful growth of the freestanding perovskite oxides down to the monolayer limit has paved the way for the design of some functional properties based on 2D perovskite oxides [18, 19].",impact-revealing,highlighting the significance of recent advancements in 2D perovskite oxides
2778,5c2c7a9217c44a4e7cf317b4,61946177ea5d44ad18ff09de7929556d7b34cd8b,simd-x: programming and processing of graph algorithms on gpus.,53e99b16b7602d97023a9d53,Scalable Gpu Graph Traversal,"Prior work requires signiﬁcant programming effort [38, 29, 63], or runs the risk of poor performance [25].###Unlike prior work [29, 63, 38], SIMD -###This approach [63, 38, 10] ﬁrst loads all the edges of the active vertices to construct an active edge list.###Besides batch ﬁlter [63, 38], there also exist other task management approaches – strided ﬁlter [29, 31] and atomic ﬁlter [34].###…order to use existing systems efﬁciently, a programmer needs to possess an in-depth knowledge of GPU architecture [12, 1], e.g., Gunrock requires explicit management of GPU threads and memory [63], and B40C [38] and Enterprise [29] need thousands of lines of CUDA code for BFS speciﬁc optimizations.###Our JIT task management can largely reduce the memory consumption, thereby accommodate the graphs much larger than prior work [38, 63].###Recent advance in graph computing falls in algorithm innovation [39, 72], framework developments [37, 14, 33, 28, 30, 75, 77, 18, 53, 51, 49, 19, 42, 48, 61, 6, 66, 68, 52, 73, 62, 43, 74, 70, 69, 3, 64, 40, 17, 8] and accelerator optimizations [63, 29, 38, 25, 71, 47].",other,highlighting the limitations of prior work and the advantages of the proposed approach
2361,5e15adca3a55ac47ab5b0729,97ebd482a78e6e6c1ba51da5e1b2f8e7640cc8b5,hyperbolic graph convolutional neural networks,55a6753765ce054aad689fca,CORTECON: a temporal transcriptome analysis of in vitro human cerebral cortex development from human embryonic stem cells.,"Each protein has a label indicating the stem cell growth rate after 19 days [40], which we use for the node classification task.",other,providing context for protein labeling in node classification
923,5cede0f6da562983788d5a5f,31c343d741b31daeab7cce6ddb768767523d185e,Relational Graph Attention Networks.,5a260c8117c44a4ba8a30f54,Graph Attention Networks.,"Similar to Vaswani et al. (2017); Veličković et al. (2017), we also ﬁnd that using multiple heads in the attention mechanism can enhance performance where ⊕ denotes vector concatenation, α are the normalised attention coeﬃcients under relation r computed by either WIRGAT or ARGAT , and g It might…###A recent approach that began with Graph Attention Networks ( GAT s), applied attention mechanisms to graphs, and does not share these limitations (Veličković et al., 2017; Gong and Cheng, 2018; Zhang et al., 2018; Monti et al., 2018; Lee et al., 2018).###Logits Following Veličković et al. (2017); Zhang et al. (2018), we assume the attention coeﬃcient between two nodes is based only on the features of those nodes up to a neighborhood-level normalisation.###The ﬁrst realisation of a we consider is the relational modiﬁ-cation of the logit mechanism of Veličković et al. (2017) where the query and key dimensionality are both D = 1 , and q ( r ) i and k ( r ) i are scalar ﬂattenings of their one-dimensional vector counterparts q ( r ) i , k ( r ) i ∈ R 1 .###Logits Following Veličković et al. (2017); Zhang et al.###A recent approach that began with Graph Attention Networks (GATs), applied attention mechanisms to graphs, and does not share these limitations (Veličković et al., 2017; Gong and Cheng, 2018; Zhang et al., 2018; Monti et al., 2018; Lee et al., 2018).###Additive attention logits The first realisation of a we consider is the relational modification of the logit mechanism of Veličković et al. (2017)###Following the experimental setup of Schlichtkrull et al. (2018) for the transductive tasks, we evaluate our model on the Resource Description Framework ( RDF ) datasets AIFB and MUTAG.###We follow the construction of the GAT layer in Veličković et al. (2017), extending to the relational setting, using ideas from Schlichtkrull et al. (2018).",impact-revealing,highlighting the effectiveness of multiple heads in attention mechanisms
2653,5b1643998fbcbf6e5a9bc32d,2fe2cfd98e232f1396f01881853ed6b3d5e37d65,Taskonomy: Disentangling Task Transfer Learning,53e9a5e2b7602d9702f0bc11,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,"These representations, however, can transmit statistics useful for solving other outputs (tasks), presumably if the tasks are related in some form [80, 17, 56, 44].###compositional modeling [33, 8, 11, 21, 53, 89, 87], homomorphic cryptography [40], lifelong learning [90, 13, 82, 81], functional maps [68], certain aspects of Bayesian inference and Dirichlet processes [52, 88, 87, 86, 35, 37], few-shot learning [78, 23, 22, 67, 83], transfer learning [72, 81, 27, 61, 64, 57], un/semi/selfsupervised learning [20, 6, 15, 100, 17, 80], which are studied across various fields [70, 91, 10].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3054,5fa909a591e011e83f7406b0,ba9f6368370ca07c1a0c9a5684b0908f6d2e0c6f,Sandslash: a two-level framework for efficient graph pattern mining,53e9ac18b7602d97035d9999,The webgraph framework I: compression techniques.,"Uk UK2007 [9] 106M 6,604M 31 0 Gsh Gsh-2015 [10] 988M 51,381M 52 0###We also include widely used large graphs (Lj, Or, Tw4, Fr, Uk), and a very large web-crawl [10] (Gsh).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3060,599c7ea4601a182cd28b8342,103baca878b17a15d148a684c0b0152e78591be1,A Split Cache Hierarchy for Enabling Data-Oriented Optimizations,53e99cf4b7602d97025a841c,Improving Cache Management Policies Using Dynamic Reuse Distances,") • Cache bypassing to improve performance and energy by only installing a cacheline if it is likely to see reuse [12], [13], [14], [15], [16], [17], [18], [19].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1447,,4550801a27fad8dea5d5d7aa9448524040e5b804,Tyrosine-Nitrated Proteins: Proteomic and Bioanalytical Aspects.,,,"###In MnSOD, four tyrosine residues were found to be nitrated (Tyr 2, Tyr 9, Tyr 11, and Tyr 34), and the distribution of the nitrated residues depends on the nitrating agent used.###In our lab, the general strategy based on 2DE fractionation, followed by immunoblotting, ingel digestion, and MS identification, has been widely used for nitroproteome studies of both single nitrated proteins (e.g., human MnSOD [hMnSOD], Fig.###MnSOD nitration via the hemeperoxidase/ NO2 pathway does not lead to significant inactivation of the enzyme (30, 112), and it occurs at tyrosine residues that are more superficial than Tyr 34 such as solventexposed Tyr 9 and 11 (112).###Thus, the association of nitration plus inactivation of MnSOD may serve to shed light on the chemical nature of the nitrating species in vivo (82).###A remarkable example of loss of enzyme activity linked to nitration in vivo is the mitochondrial enzyme manganese superoxide dismutase (MnSOD).###In this model, the authors reported, through the use of proteomic-based methods, that CsA induced nitration of MnSOD-Tyr34 with the concomitant inhibition of the enzyme; the authors were unable to quantitate the amount of nitration of MnSOD, and overexpression of the enzyme was necessary to reach the limit of detection for the Tyr34-containing nitrated peptide (89).###Nitration mechanism Peroxynitrite dependent Regio-specific nitration by transition metals Y34 MnSOD (64, 80, 89, 119)###MnSOD usually circumvents peroxynitrite formation by dismutating superoxide anion radical (O2
-).###However, to have biological significance, a loss-of-function modification requires a large fraction of protein to become nitrated at specific critical tyrosine residues and it is doubtful that many proteins will undergo such an extent of nitration, with one of the few already well-demonstrated examples being the case of MnSOD (16, 63, 64, 84).###Mn-SOD, an essential mitochondrial antioxidant enzyme, is nitrated and inactivated in vivo under a variety of conditions, leading to mitochondrial dysfunction and/or inflammation (30, 31, 63, 80, 119).###Nitrated and inactivated MnSOD is found in acute and chronic inflammatory processes in both animal models and human diseases (63, 89).###The nitration of MnSOD in a critical tyrosine residue (Tyr 34) represents a prime example of an oxidative PTM in vivo that is significant and directly associated with a loss of function (63, 82, 119).###Interestingly, even in the case of the peroxynitrite-dependent nitration of the metal-depleted
form of MnSOD (i.e., apoMnSOD), regio-selectivity at Tyr 34 is lost and overall nitration yield is increased, indicating that the metal center and protein conformation play key roles in defining the target residue and yields of the nitration process (80).###The nitration of MnSOD in a critical tyrosine residue (Tyr 34) represents a prime example of an oxidative PTM in vivo that is significant and directly associated with a loss of function (63, 82, 119). hMnSOD contains a total of nine residues and depending on the nitrating agent and mechanism of nitration, one or more of them could be nitrated with a different probability (30, 112).###This protein is nitrated by peroxynitrite in Tyr 34 by an Mn-catalyzed process, which leads to enzyme inactivation (63, 64, 82, 119).###Transition metal centers (Fe, Mn, Cu) Promote peroxynitrite-dependent nitration Y34 in MnSOD (64, 80, 89, 119) Y430 in prostacyclin synthase (24)###It is possible that with the use of a nanoLC plus superior mass spectrometers for quantitation purposes (i.e., triple quadrupole or hybrid triple quadrupole linear ion trap machines), one may be able to quantitate the amount of nitrated MnSOD under the reported conditions.###Tyr 34 is part of the superoxide radical entrance channel and active site, and its nitration leads to enzyme inactivation (67, 119).",impact-revealing,highlighting the significance of nitration in MnSOD and its impact on enzyme function
1560,,2c8f6b20700ffc370bdeaa14b4eebde947df16c1,Using Robust Queueing to Expose the Impact of Dependence in Single-Server Queues,,,"###The Lindley recursion in (1) here leads directly to the expression for the steady-state waiting time in terms of the partial sums Sk in (2), so it is natural for us to focus on the variances Var(Sk). However, the variances Var(Sk) in our uncertainty set (9) are variances of sums of random variables, which includes covariances of the summands X j when these summands are not required to be independent. As indicated above, our uncertainty sets are motivated by CLTs, but CLTs without the usual independence assumption. The second paragraph of section 2.5 in Mamani et al. (2016) also mentions CLTs for dependent random variables but seems to be suggesting that the conditions are too restrictive to be useful. Unlike Mamani et al. (2016), the CLT and the heavy-traffic theory play a big role here to expose what properties of the model have the greatest impact upon the queue performance; see Section EC.###The waiting time of arrival n satisfies the Lindley (1952) recursion
Wn (Wn−1 +Vn−1 −Un−1)+
≡max{Wn−1 +Vn−1 −Un−1 , 0}, (1)
where Vn−1 is the service time of arrival n − 1, Un−1 is the interarrival time between arrivals n − 1 and n, and ≡ denotes equality by definition.###The Lindley recursion in (1) here leads directly to the expression for the steady-state waiting time in terms of the partial sums Sk in (2), so it is natural for us to focus on the variances Var(Sk). However, the variances Var(Sk) in our uncertainty set (9) are variances of sums of random variables, which includes covariances of the summands X j when these summands are not required to be independent. As indicated above, our uncertainty sets are motivated by CLTs, but CLTs without the usual independence assumption. The second paragraph of section 2.5 in Mamani et al. (2016) also mentions CLTs for dependent random variables but seems to be suggesting that the conditions are too restrictive to be useful.",impact-revealing,providing context for the application of the Lindley recursion in queue performance analysis
3161,5b3d98cc17c44a510f802054,8a564ee07fa930ebc1176019deacdc9951063a99,Collaborative Learning for Deep Neural Networks.,57a4e91aac44365e35c97efc,Deep Multi-task Representation Learning: A Tensor Factorisation Approach.,"Multi-task learning is an approach to learn multiple related tasks simultaneously so that knowledge obtained from each task can be reused by the others [4, 3, 21].",other,providing context on multi-task learning approach
4020,5fdb2e1691e0118a02c4f566,16913a534b1630d33770b392767bb316f4fdb11e,Clinical Temporal Relation Extraction with Probabilistic Soft Logic Regularization and Global Inference,5bdc313a17c44a1f58a04684,Self-training improves Recurrent Neural Networks performance for Temporal Relation Extraction.,"…and Uzuner 2013; Xu et al. 2013; Tang et al. 2013; Lee et al. 2016; Chikka 2016) such as SVMs, MaxEnt and CRFs, and neural network based methods (Lin et al. 2017, 2018; Dligach et al. 2017; Tourille et al. 2017; Lin et al. 2019; Guan et al. 2020; Lin et al. 2020; Galvan-Sosa et al. 2020).",other,acknowledge various machine learning methods used in the field
86,5d3ed25a275ded87f97dea9b,0ddd7c9a1955c76eb831a230ef20a68b662886bb,Mining Algorithm Roadmap in Scientific Publications,5736974d6e3b12023e638acb,Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks,"We base on recent Transformer architecture [7, 36] to build this module, due to its better performance in encoding long-distance context compared to Long Short Term Memory Networks (LSTMs) [15] and Convolu-tional neural networks (CNNs).###To this end, our model consists of a single-sentence module with Piecewise CNN [40], and a cross-sentence module which leverages self-attention to attend to all words capturing the paragraphlevel relation information, and abbreviation-attention to attend to all abbreviations helping describe the relation of the candidate pair.###PCNN_cross: The same PCNN model as PCNN_single where cross-sentence instances are also used.###“CNN” could be an algorithm Convolutional Neural Network but also a television channel (Cable News Network).###PCNN_cross considering cross-sentence instances further improves the performance of the model, which shows the importance of cross-sentence instances in finding comparative relation.###embedding of each token is xi , which is a concatenation of word embedding and positional embedding [40].###PCNN_single: Piecewise CNN model [40], which is one of the stateofartsingle-sentencerelationextractionmethods.###We use PCNN (piecewise convolutional neural networks) [40] as our single-sentence relation extractor, which is a well-performed model for short-context relation extraction.###For example, Table 2 shows algorithms such as CNN (Convolutional Neural Network), datasets such as MNIST (Modified National Institute of Standards and Technology dataset), and metrics such as AUC (Area under curve).###To this end, our model consists of a single-sentence module with Piecewise CNN [40], and a cross-sentence module which leverages self-attention to attend to all words capturing the paragraph-level relation information, and abbreviation-attention to attend to all abbreviations helping describe the relation of the candidate pair.###Evaluated methods can be divided to unsupervised methods including co-occurrence based methods [10], word-similarity based methods [20], and supervised relation extraction methods [40].###The relation extraction model PCNN_single that uses single-sentence works well, but its precision drops rapidly when recall increases.###PCNN_single only uses single-sentence instances for candidate pairs.###network based methods have achieved great success in relation extraction, including CNN-based approaches [40, 41] and LSTMbased approaches [31].###Recent neural network based methods have achieved great success in relation extraction, including CNN-based approaches [40, 41] and LSTM-based approaches [31].###PCNN_single: Piecewise CNN model [40], which is one of the state of art single-sentence relation extractionmethods.###PCNNis a variation of CNN that adopts piecewise max pooling in relation extraction.",impact-revealing,providing context on the use of Transformer architecture for model building
331,5f8fffb591e01125c27ddec9,67f473caaa52a97e65bb1bcb9029a580c4f8d10f,FLAG: Adversarial Data Augmentation for Graph Neural Networks,5ce3acd5ced107d4c65ad719,Adversarial Training for Free,"FLAG leverages “free” adversarial training methods (Shafahi et al., 2019) to conduct eﬃcient adversarial training so that it is highly scalable to large datasets.###We leverage “free” adversarial training (Shafahi et al., 2019) to craft adversarial data augmentations.",impact-revealing,highlighting the efficiency and scalability of adversarial training methods
2005,,c13b8534ef47d5a041fb4a3ffad4bdfd8766e459,The Nucleoshuttling of the ATM Protein: A Unified Model to Describe the Individual Response to High- and Low-Dose of Radiation?,,,"###Unfortunately, there is still no consensus about the prediction of these RI risks from molecular and cellular data [2].###This subpopulation may represent 5–20% of individuals [2].###Particularly, they obey specific dose-, time-, and dose-rate effect functions that are not necessarily linear and can present some thresholds [2,31].###The threshold d ses were reviewe in [2].###subpopulation may represent 5–20% of individuals [2].###First reported by Giezel, Voigt, Albers-Schönberg, and Bouchacourt [6–8], detailed descriptions of radiodermatitis and RI reactions to other irradiated organs have progressively led to the definition of consensual severity scales [2,9], like the Common Terminology Criteria for Adverse Events (CTCAE) [10] and the Radiation Therapy Oncology Group (RTOG) [11] scales.###Telomere length and telomerase activity are frequently cited as the most specific endpoints to describe aging [2,30].###This is notably the case of the investigations about the secondary effects of anti-cancer radiotherapy [1,2], the radiation-induced cancers after repeated mammographic views in young women [3], and the radiation-induced (RI) pathologies observed in nuclear workers [4].###The threshold doses were reviewed in [2].###In addition to this doseand time- dependence, the occurrence and the degree of the three precited RI effects strongly depend on the individual status [2] and on the irradiated tissues/organs [27].",impact-revealing,highlighting the lack of consensus on predicting RI risks and the complexity of individual responses
2748,573695fd6e3b12023e511373,e49ff72d420c8d72e62a9353e3abc053445e59bd,Deep convolutional networks on graph-structured data,5550401245ce0a409eb3205c,Dropout: a simple way to prevent neural networks from overfitting,"For such type of data of dimension N , deep learning strategies are reduced to learning with fully-connected layers, which have O ( N 2 ) parameters, and regularization is carried out via weight decay and dropout [17].",other,describing deep learning strategies for high-dimensional data
1561,,2997b21b31dd0b9c1fa6c1be23cfec811a2bfffe,Stability index for chaotically driven concave maps,,,"###1 is inspired by proofs of a related result in queuing theory, namely the determination of Loynes’ exponent [12] for the stationary distribution of Lindley’s recursion [11], see also [5] and in particular [10, Lemmas 4 and 5].",impact-revealing,drawing inspiration from queuing theory for the current research
2886,558c2b08e4b00c3c48e0a105,368e031ce85bee93ad5bda8c0970cda76c9cf140,the heterogeneous block architecture,53e9b8a8b7602d970447f6c6,Bulksc: Bulk Enforcement Of Sequential Consistency,", [8, 9, 25, 38, 39, 42, 47, 53, 57]) exploited the notion of large atomic code blocks to improve performance, efficiency and design simplicity.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1446,,1e31e33db928155b917a5666d7f6d740fa2ec677,Multi-Level Attention Recognition of EEG Based on Feature Selection,,,"###The time-domain features of EEG have the advantages of being specific, visual and easy to obtain, and therefore have been widely used by researchers [32,33].",impact-revealing,highlighting the advantages of time-domain features of EEG in research
3996,5e15adca3a55ac47ab5b0729,97ebd482a78e6e6c1ba51da5e1b2f8e7640cc8b5,hyperbolic graph convolutional neural networks,53e9a9dfb7602d9703343611,Hierarchical organization in complex networks.,"However, many real-world graphs, such as protein interaction networks and social networks, often exhibit scale-free or hierarchical structure [7, 50] and Euclidean embeddings, used by existing GCNs, have a high distortion when embedding such graphs [6, 32].",other,highlighting the limitations of existing graph convolutional networks in handling real-world graph structures
2341,5e54f1813a55acae32a25da5,76b8d5f2ef97d71167aa78309918bf3f7d633c96,Author Name Disambiguation on Heterogeneous Information Network with Adversarial Representation Learning,5b67b45517c44aac1c86078b,"Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop.","…similarity between each pair of papers using
the carefully designed pairwise features, including author names, titles, institute names etc.
AMiner (Zhang et al. 2018): This model designs a supervised global stage to fine-tune the word2vec result, and designs an unsupervised local stage based on…###The dataset is released by (Zhang et al.
2018), which contains 500 author names for training and 100 author names for testing.###And Zhang et al. (2018) actually transform the academic network into a homogeneous paper network after a complicated feature engineering.###, paper network (Zhang et al. 2018), paper-author network (Zhang and Al Hasan 2017).###Thus constructing the network becomes the critical part of these methods, e.g., paper network (Zhang et al. 2018), paper-author network (Zhang and Al Hasan 2017).###However, either complicated feature engineering or the supervision (Zhang et al. 2018) is required.###AMiner (Zhang et al. 2018): This model designs a supervised global stage to fine-tune the word2vec result, and designs an unsupervised local stage based on the first stage.###In the experiment on AMiner-AND, we use 100 names for testing and compare the result with the results of other models reported in (Zhang et al. 2018).###Zhang et al. (2018) construct paper networks, where the weights of edges are decided by a supervised model based on the sharing information between two papers.###The dataset is released by (Zhang et al. 2018), which contains 500 author names for training and 100 author names for testing.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
333,5aed14e217c44a4438159759,c097be22f1e87a846385047346b73610d91fea4e,GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs,5a260c8117c44a4ba8a30f54,Graph Attention Networks.,"The difference between our aggregator and that in GAT (Veli ˇ ckovi ´ c et al., 2018) is that we have adopted the key-value attention mechanism and the dot product attention while GAT does not compute additional value vectors and uses a fully-connected layer to compute φ ( k ) w .###The difference between our aggregator and that in GAT (Veličković et al., 2018) is that we have adopted the key-value attention mechanism and the dot product attention while GAT does not compute additional value vectors and uses a fullyconnected layer to compute φ w .###GAT did not use neighborhood sampling, L2 regularization, or dropout.###It has later been adopted as a graph aggregator to solve the node classification problem (Veličković et al., 2018).###It has later been adopted as a graph aggregator to solve the node classiﬁcation problem (Veliˇckovi´c et al., 2018).###…problems by graph convolution (Duvenaud et al., 2015; Atwood and Towsley, 2016; Kipf and Welling, 2017; Fout et al., 2017; Hamilton et al., 2017a; Veliˇckovi´c et al., 2018; Li et al., 2018), which generalizes the stan-∗ These two authors contributed equally. dard deﬁnition of convolution over a…###This includes GraphSAGE (Hamilton et al., 2017a), GAT (Veli ˇ ckovi ´ c et al., 2018), and FastGCN (Chen et al., 2018).###The 3-layer GAT model consisted of 4, 4 and 6 heads in the ﬁrst, second and third layer respectively.###, 2017a), GAT (Veličković et al., 2018), and FastGCN (Chen et al.###To illustrate the effectiveness of incorporating graph structures, we also evaluate a two-layer fully-connected (Hamiltonetal.,2017a) (61.2) 95.4 GAT (Veliˇckovi´cetal.,2018) 97.3 ± 0.2 - Fast GCN (Chenetal We train all the aggregator-based models with Adam (Kingma and Ba, 2015) and early stopping on the validation set.###Recent research, however, has pivoted to solving these problems by graph convolution (Duvenaud et al., 2015; Atwood and Towsley, 2016; Kipf and Welling, 2017; Fout et al., 2017; Hamilton et al., 2017a; Veličković et al., 2018; Li et al., 2018), which generalizes the stan-",impact-revealing,comparing differences in aggregation methods in graph attention networks
3306,5bdc315817c44a1f58a05e88,9ea992f009492888c482d5f4006281eaa8b758e7,"X2Face: A network for controlling face generation by using images, audio, and pose codes",5a260bfb17c44a4ba8a1c5e3,Disentangled Representation Learning Gan For Pose-Invariant Face Recognition,") by conditioning the generated image on known ground truth information which may be head pose, expression, or landmarks [44,21,42,5,12,30].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2519,5f06e5e591e0117f54657c19,f6d32ed0eee5fb3f6ac518f3aebc8ceff2aae397,NVAE: A Deep Hierarchical Variational Autoencoder,5a260c8117c44a4ba8a30d9b,"Progressive Growing of GANs for Improved Quality, Stability, and Variation","Datasets: We examine NVAE on the dynamically binarized MNIST [71], CIFAR-10 [72], ImageNet 32 × 32 [73], CelebA HQ [28], and FFHQ 256 × 256 [74].###We examine NVAE on the dynamically binarized MNIST [71], CIFAR-10 [72], ImageNet 32 × 32 [73], CelebA HQ 256 × 256 [28], and FFHQ 256 × 256 [74] datasets.###Figure 1: 256 × 256-pixel samples generated by NVAE, trained on CelebA HQ [28].",other,reporting datasets used for examination
962,,6186ac8a8438dcdf6ab5b76c438d6fdc2873ec62,Development and validation of the Multidimensional Internally Regulated Eating Scale (MIRES),,,"###In general, MIRES, IES-2,
PLOS ONE | https://doi.org/10.1371/journal.pone.0239904 October 8, 2020 11 / 20
and ecSI-2 displayed comparable predictive abilities (S6 Table) and all were better at predicting behavioral and psychological outcomes, compared to physical outcomes.###MIRES accounted for a slightly larger amount of variance in RES, SR, and SE compared to the other scales, IES-2 was better at predicting BES, BMI, MWC, and WCS, and finally ecSI-2 was better at predicting PCS, BAS-2, SWLS, and SISE.###MIRES: Multidimensional Internally Regulated Eating Scale, IES-2: Intuitive Eating Scale-2, ecSI-2: Eating Competence Satter Inventory 2.0, BES: Binge Eating Scale, RES: Restrictive Eating Scale, PCS: Proactive Coping Scale, SR: Satiety Responsiveness, SE: Slowness in Eating, BAS-2: Body Appreciation Scale-2, SWLS: Satisfaction With Life Scale, SISE: Single Item Self-Esteem Scale, BMI: Body Mass Index, MWC: Maximal Weight Change, WCS: Weight Cycling Severity.###High correlations were particularly observed between certain MIRES subscales and conceptually related constructs of IES-2 and ecSI-2.###Existing measures of IRE, such as the Intuitive Eating Scale 2 (IES-2) [11], the Eating Competence Satter Inventory 2 (ecSI-2) [12], the Mindful Eating Questionnaire (MEQ) [13] and the Mindful Eating Scale (MES) [14] have made impactful contributions, but have failed to capture the full complexity of IRE and the inter-connectedness between the characteristics that define the IRE style.###The 21-item IES-2 [11] was used to measure the four constructs of intuitive eating, namely, Unconditional Permission to Eat (UPE), Eating for Physical Rather Than Emotional Reasons (EPR), Reliance on Hunger and Satiety Cues (RHSC), and Body Food Choice Congruence (BFCC).###The incremental validity of MIRES, above and beyond IES-2 and ecSI-2, was supported for most outcome variables measured in this study.###MIRES did not account for a significant increase in explained variance of physical outcomes (BMI [ΔR2 = 0], MWC [[ΔR2 = 0], and WCS [[ΔR2 = 0.002]) above and beyond IES-2, neither for satisfaction with life (ΔR2 = 0) and self-esteem (ΔR2 = 0.005) above and beyond the variance explained for by ecSI-2.###MIRES: Multidimensional Internally Regulated Eating Scale, IES-2: Intuitive Eating Scale-2, ecSI-2: Eating Competence Satter Inventory 2, BES: Binge Eating Scale, RES: Restrictive Eating Scale, PCS: Proactive Coping Scale, SR: Satiety Responsiveness, SE: Slowness in Eating, BAS-2: Body Appreciation Scale-2, SWLS: Satisfaction With Life Scale, SISE: Single Item Self-Esteem Scale, BMI: Body Mass Index, MWC: Maximal Weight Change, WCS: Weight Cycling Severity. a Values obtained with SEM. b Values obtained with hierarchical regression analysis.###Existing measures of IRE, such as the Intuitive Eating Scale 2 (IES-2) [11], the Eating Com-
petence Satter Inventory 2 (ecSI-2) [12], the Mindful Eating Questionnaire (MEQ) [13] and the Mindful Eating Scale (MES) [14] have made impactful contributions, but have failed to capture the full complexity of IRE and the inter-connectedness between the characteristics that define the IRE style.###Similarly, SEH and SES correlated most strongly with the RHSC subscale of IES-2 (0.66 and 0.68, respectively).###Analyses for IES-2 and ecSI-2 were conducted only at the level of total scores.###Bivariate correlations of the MIRES total score, RI, and MIRES subscales with the IES-2 and ecSI-2 total scores were substantial and significant (0.32–0.70) (S5 Table).###Specifically, MIRES accounted for 0.7%-16% additional variance in outcome measures above and beyond IES-2 and ecSI-2.###Specifically, we examined whether MIRES accounted for variance in each outcome measure above and beyond the variance accounted for by IES-2 and ecSI-2, respectively.###The incremental validity of MIRES in relation to IES-2 and ecSI-2 was examined with SEM (for multi-item outcomes) and hierarchical regression analysis (for single-item outcomes).###The criterion validity of MIRES, IES-2, and ecSI-2 was examined with Structural Equation Modelling (SEM) (for outcomes measured with multiple items) and with linear regression (for the single-item outcomes SISE, BMI, MWC, and WCS).###Existing measures of intuitive eating [11, 19], eating competence [12], mindful eating [13, 14], and interoceptive awareness [20] were used for inspiration during item generation.",impact-revealing,highlighting the limitations of existing measures of intuitive eating and their contributions
4060,53e99967b7602d97021ac42b,41721de035c15528a7e35d3ab4d79b053633d763,Feedback-directed memory disambiguation through store distance analysis,53e9ae35b7602d970384a0ee,Cooperative hardware/software caching for next-generation memory systems,"[9] C. Fang, S. Carr, S. Onder, and Z. Wang.###[25] Z. Wang.###Steve Carr ¨ Soner Onder Zhenlin Wang Department 
of Computer Science Michigan Technological University Houghton MI 49931 USA  {carr,soner,zlwang}@mtu.edu 
1.###Even though using bits from the offset field may increase register usage and address computation, Wang [25] has observed only a negligible performance difference (usually none) by reducing the offset from 16 to 12 bits in the Alpha instruction set.###Even though using bits from the offset .eld may increase register 
usage and address computation, Wang [25] has observed only a negligible performance difference (usually 
none) by reducing the offset from 16 to 12 bits in the Alpha instruction set.###[8] C. Fang, S. Carr, S. Onder, and Z. Wang.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1363,,fe9ec5eccab155b5c97f66d32f8726a514c44d2c,Evaluating security and usability of profile based challenge questions authentication in online examinations,,,"###Favourite Questions: Favourite questions have been widely used for credential recovery [20].###The results of a relaxed algorithm were derived from the data collected in the online examination disregarding capitalisation, whitespaces and minor spelling errors using a combination of substring and distance algorithm as described in an earlier study [20].###Some studies have reported usability and security issues related to the use of challenge questions in credential recovery [17,20].###Personal Questions: Personal questions are believed to be more memorable and therefore, widely used for credential recovery [20].###Schechter [20] performed guessing attacks by acquaintances and statistical guessing in the context of credential recovery to evaluate security of challenge questions.###Previous research suggests that challenge questions can be vulnerable to guessing attacks by friends and colleagues [20,25].###[20] argue that the personal information can be found on the social media websites.###[20] implemented an equality algorithm for string-to-string comparison, substring algorithms, and distance algorithms were also used.###Effectiveness is the degree of accuracy and completeness with which the user achieves a specified task in a certain context [20].###Research studies [20,25] suggest that challenge questions can be vulnerable to blind, focused and informed guessing attacks by adversaries, acquaintances, friends and colleagues.",impact-revealing,highlighting vulnerabilities in credential recovery methods
3728,573696116e3b12023e52463f,fafcaf5ca3fab8dc4fad15c2391c0fdb4a7dc005,group equivariant convolutional networks,5550415645ce0a409eb3a690,Fractional Max-Pooling,"Extreme data augmentation and model ensembles can also further improve the numbers (Graham, 2014).",other,acknowledging methods to enhance model performance
337,58437725ac44360f1082f7f7,79da740db9006b2aa3e7b571d038ec895e323121,Accelerating the Super-Resolution Convolutional Neural Network,558c4a2384ae6766fdf2358f,Image Super-Resolution Using Deep Convolutional Networks,"Deeper structures have also been explored in [18] and [19].###Furthermore, all these networks [8,18,19] need to process the bicubic-upscaled LR images.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2264,5c2c7a9217c44a4e7cf314f8,b5e6fb219ca76f0175bd9f28e1460280fa11d5e1,making classification competitive for deep metric learning,5b67b45517c44aac1c86086d,Visual Search at Alibaba.,"Learning image representations, also known as image embeddings, is a core problem of a variety of applications including face recognition [19], ﬁne-grained retrieval [22] [28] [20], clustering [31], and visual search [32] [15] [33] [9].",other,highlighting the significance of learning image representations in various applications
1319,,fb5bc0758e496cd1ea5fe5f906c0e91647acce81,"Comparison of maximum likelihood approach, Diggle–Kenward selection model, pattern mixture model with MAR and MNAR dropout data",,,"###…have received considerable attention, especially the relatively “modern” analytic approaches that assume MAR, that is, Maximum Likelihood (ML) and Multiple Imputation (MI) procedures (e.g., Carpenter, Kenward, and Vansteelandt 2006; Enders 2010; Little and Rubin, 2002; Schafer and Graham 2002).###Therefore, there will be a sizable bias in estimates of parameters and standard errors if the model fails to account for this kind of mechanism, which will lead to worse confidence interval estimates (Schafer and Graham 2002; Little and Rubin 2002).###Although the MAR mechanism is widely assumed and applied and MAR-based approaches represent the current state of art (Schafer and Graham, 2002), certain situations exist where the assumptions of the MAR mechanism are incorrect.###As Little and Rubin (2002) and Schafer and Graham (2002) recommended, the selection model and the pattern mixture model could be employed.###An MNAR occurs, however, when the propensity for missingness does depend on unobserved data (Schafer and Graham 2002).###Methodologists currently regard maximum likelihood as a state-of-art missing data technique (Schafer and Graham 2002) because ML estimates are unbiased and more efficient than the estimates provided by other methods (e.g., listwise, pairwise and single imputation) under an MAR mechanism.",impact-revealing,highlighting the significance of modern analytic approaches to missing data
1220,,313e9f3c7ef4d46095d566e1659cd57ebb7ffd8a,Factors Associated With Overweight and Obesity Among Mexican Americans and Central Americans: Results From the 2001 California Health Interview Survey,,,"###adult population was recently estimated to be overweight or obese (1-2).###Our study also did not address environmental factors that contribute to weight gain, such as a reliance on fast food outlets and convenience stores with limited dietary choices, and heavy marketing of calorie-dense foods (2,43,44).###The Hispanic Health and Nutrition Examination, for example, sampled only Mexican Americans, Cuban Americans, and Puerto Ricans, and NHANES III, conducted from 1989 to 1994, sampled only Mexican Americans (2).",impact-revealing,acknowledging limitations in the study's scope and sample
1436,,db2f2085028ab4f9dc89045ed30ac17c52eb41c2,A General Approach to Uniformly Handle Different String Metrics Based on Heterogeneous Alphabets,,,"###In the former case, they are widely used as training and/or test data in statistical machine translation [41] and in cross-lingual retrieval methods [42]; in the latter case, they are the basis of inter-linguistic analysis and language comparison [43].",impact-revealing,acknowledge the use of datasets in various linguistic applications
1935,,734e14b9b13e9a3a3e36eb9980eadedf10a1c282,The Use of Phonological and Semantic Strategies in Written Word Learning Among Chinese Children With Dyslexia,,,"###Second, the child scored one grade below the mean score of Grade 5 (M = 119.04, SD = 12.10, based on Liu et al., 2017) on the screening task—a character recognition task (H. Li et al., 2012) widely used for screening children with dyslexia in mainland China (e.g., Shu et al., 2003, 2006).",impact-revealing,providing context for a character recognition task in dyslexia screening
2104,,b3e89bf67337130f25f74d20f656335e7fc07c6f,A Novel Secure Key Management Module for Hierarchical Clustering Wireless Sensor Networks,,,"###This project is motivated by secure triple-key management [1] and enormous efforts in key management approach in WSNs such as key management relying on deployment knowledge [2,3],master key predistribution technique, key calculation post-deployment technique and random and extended random key predeployment techniques [4,5].",impact-revealing,highlighting motivation from existing key management approaches
3805,5a9cb60d17c44a376ffb35be,b8bbef8e62b4ef342243666f39b205de9f20eb8c,MSTM: A novel map matching approach for low-sampling-rate trajectories,53e9b91eb7602d97045060d9,Mining interesting locations and travel sequences from GPS trajectories.,"This phenomenon can be illustrated by the conception of stay point proposed in [17].###Usually, we don’t have to pay more attention to the stay points which are easy for map matching [17].",other,providing context for the concept of stay points
2758,5e539eca3a55ac4db70a53f1,c529f5b08675f787cdcc094ee495239592339f82,learning to simulate complex physics with graph networks,5bdc31b417c44a1f58a0b336,Relational Forward Models for Multi-Agent Learning.,"…et al., 2018; Mrowca et al., 2018; Li et al., 2019; Sanchez-Gonzalez et al., 2019), as well as non-physical systems, such as multi-agent dynamics (Tacchetti et al., 2018; Sun et al., 2019), algorithm execution (Velikovi et al., 2020), and other dynamic graph settings (Trivedi et al., 2019;…###…et al., 2018; Mrowca et al., 2018; Li et al., 2019; Sanchez-Gonzalez et al., 2019), as well as non-physical systems, such as multi-agent dynamics (Tacchetti et al., 2018; Sun et al., 2019), algorithm execution (Velikovi et al., 2020), and other dynamic graph settings (Trivedi et al., 2019; 2017;…",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1219,,aef46e761da13b6b8e01258353e239ee8328ed16,Ethnic and Gender Disparities in Adolescent Obesity and Elevated Systolic Blood Pressure in a Rural US Population,,,"###Nonetheless, most data on US children and adolescents have come from studies on large metropolitan areas or national surveys that do not distinguish between urban and rural communities.(1-7) Although African American and Hispanic adolescents have higher incidence and prevalence of overweight and obesity compared with non-Hispanic whites adolescents,(1,3-6) it is unknown whether these ethnic differences exist in rural communities.###A majority of studies report that African American and Hispanic adolescents have a higher incidence and prevalence of overweight and obesity compared to nonHispanic white adolescents.(1,3-6) Our prevalence data for obesity in males supports this claim and is further reinforced by the logistic regressions, which demonstrates that both Hispanic and African American adolescents have greater odds of being obese than non-Hispanic whites.",impact-revealing,highlighting the need for further research on obesity prevalence in rural communities
4051,5fd3404791e01161cf73952c,e988e15d200faf64bb71e155b8354c4e127f7dab,Bipartite Graph Embedding via Mutual Information Maximization,599c7ce9601a182cd27e7834,Neural Collaborative Filtering.,"• Collaborative filtering: NeuMF [15] andNGCF [37].###The reconstruction-based works[15, 32, 34, 37, 40, 42] are closely related with collaborative filtering [28].###Matrix completion [34, 42] and collaborative filtering [15, 37] are also connected with modeling bipartite graphs closely.",other,acknowledge related methods in collaborative filtering
1467,,f887b935251bed7e6cf7fe178fafee53e74f8915,Concept Combination in Weighted Logic,,,"###According to the Prototype Theory [4, 1], concepts are represented by means of prototypes, i.###Still within the Prototype Theory, Hampton proposes an attribute inheritance model 2 , which analyses the case of conjunctive noun-noun combinations (e.g., a Sport which is a Game).###The authors propose an elaboration of the Prototype Theory, interpreting concepts in terms of schema structures.###According to the Prototype Theory [4, 1], concepts are represented by means of prototypes , i.e., sets of features associated with weights representing their relevance for the concept.###The design of the tooth operator is inspired by the Prototype Theory : the concepts in the ∇∇ -definition of 𝐶 may be seen as the features of 𝐶 and their weights may be intended to represent the relevance of such features (for 𝐶 ).###The approach proposed here is plunged in the Prototype Theory paradigm, both in terms of general inspiration and in terms of strategies adopted for the combination of concepts.###We start by considering the case of Pet Fish that has been advocated to show the inadequacy of the Prototype Theory to capture concept combinations.###Vice versa, cognitive theories of concepts focus on typicality effects by sacrificing compositionality: this is the case of, e.g. Prototype Theory.###However, the Prototype Theory seems inadequate to capture compositionality, as paradigmatically illustrated by the Pet Fish example.",impact-revealing,discussing the limitations of Prototype Theory in capturing compositionality
3598,5e09a801df1a9c0c41680233,1438d8c68b4495947fd5de001b87e6ef5a263a3e,Learning to Jointly Generate and Separate Reflections,57a4e921ac44365e35c98eb0,Precomputed Real-Time Texture Synthesis With Markovian Generative Adversarial Networks,"For the discriminator networks, we use 70 × 70 PatchGANs [9, 16] which can be applied to arbitrarily-sized images in a fully convolutional fashion.",other,describing the architecture of discriminator networks
1850,,c4d49213d8be3b7d8007f7c802440fd39dc81050,Pair Programming in Middle School,,,"###In successful collaborations, conversational turns build on each other and the content moves the pair closer to solving the problem (Roschelle & Teasley, 1995; Schegloff, 1991).",impact-revealing,providing context on successful collaboration dynamics
965,,2fcb0a5e3bec5351402c672ad5e296f1fd639ed5,Examination of the Response Styles Theory in a Community Sample of Young Adolescents,,,"###Of the studies that could have examined this question, three did not find a gender difference in depression (Abela et al. 2002; Silk et al. 2003; Ziegert and Kistner 2002), three did not examine whether girls’ higher use of rumination explained their higher depressive symptoms (Grabe et al. 2007;…###Fewer studies have examined the theory in children and adolescents, but those studies have generally found that rumination is related to depressive symptoms in youth (e.g., Abela et al. 2002; Borderick and Korteland 2004; Hart and Thompson 1996; Li et al. 2006; Schwartz and Koenig 1996).###The reliability and validity of the CRSQ, as well as its subscales, have been demonstrated in several studies (Abela et al. 2004;  Abela et al. 2002;  Abela et al. 2007).###... of children and adolescents have documented higher rates of rumination among girls as compared to boys (Grabe et al. 2007; Grant and Compas 1995; Grant et al. 2004 ;H ampel and Peterman 2005; Hart and Thompson 1996 ;L i et al.2006; Muris et al. 2004; Schwartz and Koenig 1996 ;S ilk et al.2003; Ziegert and Kistner 2002), although some have not found a gender difference in rumination, despite sampling from similar age ranges (e.g.,  Abela et al. ...###Of the three studies that examined gender differences in problem-solving, one found no difference (Abela et al. 2007), one found that girls reported more problem-solving than boys (Li et al. 2006), and one offered mixed results (i.e., 3rd grade girls reported more problem-solving than boys, but 7th grade girls reported less problem-solving than boys;  Abela et al. 2002 ).###Response Styles We used the Children’s Response Styles Questionnaire (CRSQ; Abela et al. 2002) to examine participants’ tendency to engage in rumination, distraction, and problem-solving when distressed.###Of the five studies that examined gender differences in distraction, four found no differences (Abela et al. 2002; Abela et al. 2007; Li et al. 2006; Schwartz and Koenig 1996), and one found that girls showed less distraction compared to boys (Hampel and Peterman 2005).###A study of 3rd and 7th grade children found that neither distraction nor problemsolving were related to concurrent or prospective depressive symptoms ( Abela et al. 2002 ).###Of the five studies that examined gender differences in distraction, four found no differences ( Abela et al. 2002;  Abela et al. 2007; Li et al. 2006; Schwartz and Koenig 1996), and one found that girls showed less distraction compared to boys (Hampel and Peterman 2005).###Of the studies that could have examined this question, three did not find a gender difference in depression ( Abela et al. 2002 ; S ilk et al. 2003 ;Z iegert and Kistner2002), three did not examine whether girls’ higher use of rumination explained their higher depressive symptoms (Grabe et al. 2007 ;L i et al. 2006; Muris et al. 2004), two found that rumination did not account for the gender difference in depression scores (Grant and Compas ...###An association between a ruminative response style and depressive symptoms in youth has been documented in a number of studies both concurrently (Abela et al. 2002; Garnefski et al. 2002; Grabe et al. 2007; Grant et al. 2004; Kuyken et al. 2006; Li et al. 2006; Schwartz and Koenig 1996; Ziegert and…###A study of 3rd and 7th grade children found that neither distraction nor problemsolving were related to concurrent or prospective depressive symptoms (Abela et al. 2002).###Response Styles We used the Children’s Response Styles Questionnaire (CRSQ;  Abela et al. 2002 ) to examine participants’ tendency to engage in rumination, distraction, and problem-solving when distressed.###The reliability and validity of the CRSQ, as well as its subscales, have been demonstrated in several studies (Abela et al. 2004; Abela et al. 2002; Abela et al. 2007).###An association between a ruminative response style and depressive symptoms in youth has been documented in a number of studies both concurrently (Abela et al. 2002; Garnefski et al. 2002; Grabe et al. 2007; Grant et al. 2004; Kuyken et al. 2006 ;L i et al.2006; Schwartz and Koenig 1996; Ziegert and Kistner 2002) and prospectively ( Abela et al. 2002;  Abela et al. 2007; Borderick and Korteland 2004; Nolen-Hoeksema et al. 2007; Schwartz and ...###Fewer studies have examined the theory in children and adolescents, but those studies have generally found that rumination is related to depressive symptoms in youth (e.g.,  Abela et al. 2002;  Borderick and Korteland 2004; Hart and Thompson 1996 ;L i et al.2006 ;S chwartz and Koenig 1996).###…1996; Li et al. 2006; Muris et al. 2004; Schwartz and Koenig 1996; Silk et al. 2003; Ziegert and Kistner 2002), although some have not found a gender difference in rumination, despite sampling from similar age ranges (e.g., Abela et al. 2002; Abela et al. 2007; Borderick and Korteland 2004).###An association between a ruminative response style and depressive symptoms in youth has been documented in a number of studies both concurrently ( Abela et al. 2002;  Garnefski et al. 2002; Grabe et al. 2007; Grant et al. 2004; Kuyken et al. 2006 ;L i et al.2006; Schwartz and Koenig 1996; Ziegert and Kistner 2002) and prospectively (Abela et al. 2002; Abela et al. 2007; Borderick and Korteland 2004; Nolen-Hoeksema et al. 2007; Schwartz and ...###…one found
no difference (Abela et al. 2007), one found that girls reported more problem-solving than boys (Li et al. 2006), and one offered mixed results (i.e., 3rd grade girls reported more problem-solving than boys, but 7th grade girls reported less problem-solving than boys; Abela et al. 2002).###…et al. 2002; Grabe et al. 2007; Grant et al. 2004; Kuyken et al. 2006; Li et al. 2006; Schwartz and Koenig 1996; Ziegert and Kistner 2002) and prospectively (Abela et al. 2002; Abela et al. 2007; Borderick and Korteland 2004; Nolen-Hoeksema et al. 2007; Schwartz and Koenig 1996; Silk et al. 2003).",impact-revealing,highlighting the association between rumination and depressive symptoms in youth
3757,5f76f20a91e011f31b98056c,645bd6eadc247989abc5e0b0aa0be79ec8b11ea6,CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,5843777eac44360f1084184b,A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories.,"The prompts are either premise sentences taken from MultiNLI’s fiction genre (Williams et al., 2018) or 2–3 sentence story openings taken from examples in ROCStories (Mostafazadeh et al., 2016).###, 2018) or 2–3 sentence story openings taken from examples in ROCStories (Mostafazadeh et al., 2016).###For each example, a crowdworker wrote standalone sentences inspired by a prompt that was drawn
from either MultiNLI (Williams et al., 2018) or ROCStories (Mostafazadeh et al., 2016).",other,describing the source of prompts used in the study
1570,,a797479eacda332b14daf185f601de8394d4565d,Bridging the Gap between the Normative and the Descriptive: Bounded Epistemic Rationality,,,"###…strategy of reasoning, problem solving or decision making is ecologically rational to the extent that it is adapted to the structure of the task; (ir)rationality of a strategy should not be judged according to a priori normative criteria, but by its degree of fit with the environment [34, 35].###In other words, there is often no trade-off between frugality and accuracy [34, 35].###…an empirical one, and one of the main goals of his research is to explain the characteristics of the environment that are relevant for determining the ecologically rational strategy: degree of uncertainty, number of possible alternatives, the size of a learning sample and so on [34, 35].###Ecological rationality is therefore intertwined with the notion of success and is instrumental in this sense, but still keeps the standard epistemic goals by emphasizing the importance of truth or accuracy [34, 35].###The function of an organism’s adaptive toolbox is defined in evolutionary terms: to reach proximal goals, such as finding food, avoiding predators, finding a mate, etc. Adaptiveness is therefore an important component of Gigerenzer’s understanding of rationality [34, 35].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1568,,fa906a902189afbf04711c4991ccd1c6c3fdb1d3,Leverage and Uncertainty,,,"###Based on experience and skills and reinforced by luck and modelling, adaptive behaviour and heuristics (Gigerenzer 2008) can make uncertainty feel like risk.",impact-revealing,highlighting the role of adaptive behavior and heuristics in decision-making under uncertainty
2125,,ff7cec18e1c3842c0c5b60726ea41574411a803e,Morphology analysis of urban traffic based on multilevel cellular automata networks dynamics,,,"###Please note that the spatial structure of intersection does not consider the faulty hypothesis that vehicles fly over intersections freely [9], [10], [11].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1746,,15e10fbd7b44765c7fc8952043552c023a3b8bfd,Neural correlates of creativity in analogical reasoning.,,,"###The present findings build on a growing and largely convergent body of brain-based analogy research (Bunge et al., 2005; Cho et al., 2009; Geake & Hansen, 2005; Green, Fugelsang, Kraemer, et al., 2006; Green et al., 2010; Hampshire et al., 2011; Volle et al., 2010; Wendelken et al., 2008).###In particular, the emergence of STG from parametric analyses of semantic distance in the present study as well as our recent study of analogy evaluation (Green et al., 2010) indicates this region is a likely candidate for participation in creative reasoning.###This integration/mapping–associated activation was very near the activation we found in the present study, as well as in our previous investigations (Green, Fugelsang, Kraemer, et al., 2006; Green et al., 2010).###In particular, parametric analyses revealed that semantic distance values of analogy stimuli were a significant predictor of activity within an a priori frontopolar region that we have previously implicated in analogical mapping (Green, Fugelsang, Kraemer, et al., 2006; Green et al., 2010).###…(radius ϭ 10 mm) centered at a functional peak in left frontopolar cortex that we have previously implicated (Green, Fugelsang, Kraemer, et al., 2006; Green et al., 2010 Table 1 displays the results of the main parametric semantic distance analysis over the whole brain at the exploratory threshold…###These results are consistent with our recent findings concerning semantic distance in analogical evaluation (Green et al., 2010) and with previous work indicating that specific task demands of complex reasoning, rather than time on task or difficulty per se, account for frontopolar recruitment…###We have previously identified frontopolar cortex as a key mediator of relational integration during analogical reasoning (Green, Fugelsang, Kraemer, et al., 2006) and found that evaluating semantically distant analogical mappings leads to increased frontopolar recruitment (Green et al., 2010).###This article is intended solely for the personal use of the individual user and is not to be disseminated broadly. ric regressor in SPM99.",impact-revealing,highlighting the contribution of previous brain-based analogy research to current findings
2007,,6bceb970588628ad5dcf3af5b293374c69e020f4,Middleware Support for Dynamic Reconfiguration in Sensor Networks,,,###The results in this paper build upon previous work in [7] where we reported on our general approach to supporting distributed reconfiguration in sensor networks.,impact-revealing,building upon previous work in sensor networks
9,59ae3c262bbe271c4c71f4a2,610cff0a09c76c43739be1a6e5b0ed7a1a24ee60,metapath2vec: Scalable Representation Learning for Heterogeneous Networks,53e9b108b7602d9703b85b88,Distributed Representations of Words and Phrases and their Compositionality.,"In particular, by mapping the way that people choose friends and maintain connections as a “social language,” recent advances in natural language processing (NLP) [3] can be naturally applied to network representation learning, most notably the group of NLP models known as word2vec [17, 18].###proposed word2vec to learn the distributed representations of words in a corpus [17, 18].###proposed the word2vec framework—a two-layer neural network—to learn the distributed representations of words in natural language [17, 18].###Usually, given a network G = (V ,E), the objective is to maximize the network probability in terms of local structures [8, 18, 22], that is:###introduced negative sampling [18], in which a relatively small set of words (nodes) are sampled from the corpus (network) for the construction of so_x0089_max.###where Nt (v ) denotes v’s neighborhood with the t th type of nodes and p (ct |v ;θ ) is commonly de_x0080_ned as a so_x0089_max function [3, 7, 18, 24], that is: p (ct |v;θ ) = e Xct ·Xv ∑ u∈V eXu ·Xv , where Xv is the vth row of X, representing the embedding vector for node v .###_x008c_e metapath2vec and metapath2vec++ methods can be parallelized by using the same mechanism as word2vec and node2vec [8, 18].###Second, we extend the skip-gram model [18] to facilitate the modeling of geographically and semantically close nodes.",impact-revealing,highlighting the application of NLP models in network representation learning
3446,5bdc317017c44a1f58a08086,6062d8f5cc50014e5ff0f9aa467e1b044d33c051,Learning Cloud Dynamics to Optimize Spot,5550447745ce0a409eb4e006,"On-demand, Spot, or Both: Dynamic Resource Allocation for Executing Batch Jobs in the Cloud.","To limit the variance in the allocated resources, [21], [22] suggest that users rent resources in both the on-demand market and the spot market.",other,reporting prior findings on resource allocation strategies
3322,53e99822b7602d9702041bcd,5e4c84225338263e74c0a41eac4938f9a18fedb5,spatial memory streaming,53e9bb0fb7602d970474ae72,Performance analysis of the Alpha 21364-based HP GS1280 multiprocessor,"Although modern servers provide copious memory bandwidth [7], the memory system remains underutilized because these dependence chains severely limit available MLP—one study reports an average of just 1.",other,highlighting limitations in memory system utilization
2577,5b67b46b17c44aac1c861fc4,f6d06993e003fa6fec5bf630efded9e4fd90a030,fluency boost learning and inference for neural grammatical error correction,5550410f45ce0a409eb384f8,Sequence to Sequence Learning with Neural Networks.,"Sequence-to-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) for grammatical error correction (GEC) have drawn growing attention (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Schmaltz et al., 2017; Sakaguchi et al., 2017; Chollampatt and Ng, 2018) in recent years.###As neural machine translation (NMT), a typical neural GEC approach uses a Recurrent Neural Network (RNN) based encoder-decoder seq2seq model (Sutskever et al., 2014; Cho et al., 2014) with attention mechanism (Bahdanau et al., 2014) to edit a raw sentence into the grammatically correct sentence it…",other,highlighting the growing interest in seq2seq models for grammatical error correction
166,5b8c9f5317c44af36f8b775c,a6876ea89e677a7cc42dd43f27165ff6fd414de5,UNet++: A Nested U-Net Architecture,5550414745ce0a409eb39ec6,Deeply-Supervised Nets.,"We propose to use deep supervision [6] in UNet++, enabling the model to operate in two modes: 1) accurate mode wherein the outputs from all segmentation branches are averaged; 2) fast mode wherein the ﬁnal segmentation map is selected from only one of the segmentation branches, the choice of which…",impact-revealing,describing the proposed method for deep supervision in UNet++
132,5f0d8b6891e011047aff993b,1c53d27c742fb4658fa03085c7c2ca014a122385,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,5ce3af9aced107d4c65f6b80,Biological Structure And Function Emerge From Scaling Unsupervised Learning To 250 Million Protein Sequences,"Special thanks to Konstantin Schütze for helping with grant writing and providing early results for the structure prediction task.###The leap of NLP through advanced LMs has been successfully generalized toward understanding the language of life through advanced LMs trained on proteins [31], [32], [33], [34], [35], [36], [37], [38], [39].###Largely, we transferred conﬁgurations successfully from NLP to protein sequences [36], [39], [60], with the exception of the number of layers that was increased to optimize memory utilization.###As previously established for another protein LM [39], the t-SNE projections (e.g. ProtT5-XL-U50 SOM Fig.",impact-revealing,acknowledging contributions and advancements in NLP applied to protein sequences
2546,5f1022a091e01168a7d6fc4f,04e234c7e4f300b4e8a16370728d875f9f484b39,learning visual context by comparison,53e9b844b7602d970440513c,Imagenet: A Large-Scale Hierarchical Image Database,"ResNet [14] and DenseNet [16, 29] pretrained on ImageNet [8] have set a strong baseline for these tasks, and other studies have been conducted on top of them to cover various issues of recognition task in the chest X-ray modality.###ResNet [14] and DenseNet [16,29] pretrained on ImageNet [8] have set a strong baseline for these tasks, and other studies have been conducted on top of them to cover various issues of recognition task in the chest X-ray modality.###For all experiments, we initialize the backbone weights with ImageNet-pretrained weights and randomly initialize context-related modules.",other,acknowledge strong baseline models in recognition tasks
3846,5aed14d617c44a4438158e20,9e788f1530af08a1f2140e6016fd4aeaa8b29033,Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform,58437725ac44360f1082fa5a,A Learned Representation For Artistic Style,"lization (CN) applies a learned function of some conditions to replace parameters for feature-wise afﬁne transformation in BN. Some variants of CN have proven highly effective in image style transfer [12,17,15], visual question answering [6] and visual reasoning [35]. Perez et al. [36] develop a feature-wise linear modulation layer (FiLM), to exploit linguistic information for visual reasoning. This layer c",other,acknowledge effectiveness of CN variants in various visual tasks
3818,53e9b0e6b7602d9703b63850,b0bca59d8cf1e0332caa111f4b91756cac61040f,dynamically managed multithreaded reconfigurable architectures for chip multiprocessors,53e9b520b7602d97040533d9,The effect of reconfigurable units in superscalar processors.,"A number of research efforts [5, 13, 20] have investigated the high level integration of a reconfigurable",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2917,5e982cc591e0119e8a952209,b5ef0f91663f0cbd6910dec9a890c138f7ec10e0,Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks,5c04967517c44a2c74708ebd,The Open Images Dataset V4,"Novel Object Captioning (NoCaps) [1] extends the image captioning task, and provides a benchmark with images from the Open Images dataset [17] to test models’ capability of describing novel objects which are not seen in the training corpus.",other,acknowledging the contribution of NoCaps to image captioning
1387,,e29d5e310d4dc9f722116e094518eddbbb7e09c6,Oblivious Collaboration,,,"###The proof of this lower bound builds upon previous impossibility results for set consensus (see [9,18,20]), by showing that given a hypothetical algorithm for AR(n, 2n− 2) in addition to read-write registers one can solve set consensus in a wait-free manner.",impact-revealing,building upon previous results in set consensus
8,5e5f7c4791e011df604ecbed,0ca7d8c3250d43d14fdde46bf6fc299654d861ef,Heterogeneous Graph Transformer,61a724c96750f8421870395d,Attention is all you need,"RTE is inspired by Transformer’s positional encoding method [15, 21], which has been shown successful to capture the sequential dependencies of words in long texts.###Inspired by the architecture design of Transformer [21], we map target node t into a Query vector, and source node s into a Key vector, and calculate their dot product as attention.",impact-revealing,highlighting the inspiration drawn from Transformer architecture for RTE
3328,5cd7fa07ced107d4c65bf34f,371c799bde8b162e7f8fa2b2a0a8cfb29765f89f,Knowledge Graph Convolutional Networks for Recommender Systems,57a4e91dac44365e35c9830c,Learning Convolutional Neural Networks for Graphs,"To handle the neighborhoods with varying size and maintain the weight sharing property of CNN, researchers propose learning a weight matrix for each node degree [6], extracting locally connected regions from graphs [13], or sampling a fixed-size set of neighbors as the support size [7].",other,acknowledge existing methods for handling varying neighborhood sizes in CNN
2814,599c7945601a182cd262a009,6727f574ad8b1c3763be8d58eeaf82c551aa33ef,Generative and Discriminative Text Classification with Recurrent Neural Networks,5550456245ce0a409eb55cee,Glove: Global Vectors for Word Representation.,"In or-Table der to do this, we use pretrained GloVe word embedding vectors (Pennington et al., 2014).",other,tool used for word embedding
2216,5cede0f9da562983788d862a,44d43bfbd23d55b1e7c4c4fd91fe101c0eaf1a06,Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks,5a73cbcc17c44a0b3035f180,Quantifying Translation-Invariance in Convolutional Neural Networks,"In practice, CNNs are not truly translation-invariant [9, 14].",other,highlighting a limitation in CNNs
2601,5c0f8548da562944ac906a71,5a3da29970d0c3c75ef4cb372b336fc8b10381d7,CNN-Based Real-Time Dense Face Reconstruction with Inverse-Rendered Photo-Realistic Face Images,5550437245ce0a409eb47812,Displaced dynamic expression regression for real-time facial tracking and animation,"For example, [8] and [7] learn facial geometry while not recovering facial appearance property, such as albedo.###Most of them use a 3D Morphable Model [18], [22], [53] or a multi-linear face model [7], [8], [9], [45], [48] as a prior.###Recently, several approaches have been proposed for RGB video based facial performance captureing [7], [8], [18], [22], [45], [53].###[8], [9] adopt a learning-based regres-",other,acknowledge existing approaches in facial geometry and performance capture
1704,,5f747b34bcd3696ad911d16167453cc16ea3aa3a,The Impact of Hispanic and White Group Cues on Attitudes Towards the Violation of Generic Norms,,,###SIT suggests that differentiation among individuals within groups is motivated by ingroup identification and the desire for positive ingroup distinctiveness [19].,impact-revealing,providing context for social identity theory
2028,,b0dedf8a0b45b8cec242fd514b12983532a4059f,Socially aware mobile peer-to-peer communications for community multimedia streaming services,,,"###The blue curve corresponding to SMMC is lower than that of AMCV.###AMCV does not consider the similarity of demand and socialization, and the change of network topologies brings the negative effects for resource sharing and delivery performance.###Therefore, SMMC performs better than those of AMCV.###Demand similarity in SMMC and video access probability in AMCV are calculated according to historical playback logs.###In AMCV, the mobile users who request the same video are grouped into a community.###Although two curves have the fall trend, SMMC outperforms AMCV.###We compare the performance of SMMC with our ant-inspired mini-community-based solution for video-on-demand services (AMCV) [12] in terms of startup delay and video quality during a stimulation time period of 1000 s.",impact-revealing,comparing the performance of two methods in video-on-demand services
391,5e85c28491e0114016e821a6,21b11793b960a3e37c0eab7aae6127c28fd38e5c,Code Prediction by Feeding Trees to Transformers,599c7987601a182cd2648373,Attention Is All You Need.,") For more details, please refer to [28] and [20].###In our implementation, we did not use positional encoding [28] or positional embedding [20] to provide extra positional information over elements since our early trials with LeafSeq suggested positional embedding is rather hurting than helping.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2325,5db9298547c8f766461f8b65,ddb2aecf8777007414b1eb341c6c19ec799280d3,Frame attention networks for facial expression recognition in videos,57a4e91dac44365e35c98cb7,Ms-Celeb-1m: A Dataset And Benchmark For Large-Scale Face Recognition,"By default, for feature embedding, we use the ResNet18 which is pre-trained on MS-Celeb-1M [21] face recognition dataset and FER Plus expression dataset [22].",other,reporting the default model used for feature embedding
1630,,6762f59605945c251aeb926eb4275a230d5cdb33,AVATAR: Fixing Semantic Bugs with Fix Patterns of Static Analysis Violations,,,"###In the framework, we leverage the Ochiai [52] ranking metric to actually compute the suspiciousness scores of statements that are likely to be the faulty code locations.###To that end, we attempt to replicate two scenarios of fault localization used in APR assessments: the first scenario assumes that the faulty method name is known [10] and thus focuses on ranking the inner-statements based on Ochiai suspiciousness scores; the second scenario makes no assumption on fault location and thus uses the default setting of AVATAR.###We include in this category other APR systems whose authors do not explicitly describe the actual fault localization configuration, but which still manage to fix bugs that we could not localize with GZoltar/Ochiai.###The usage of GZoltar and Ochiai reduces the comparison biases since both are widely used by APR systems in the literature.###B. Experimental Setup
For evaluation purpose, we apply different fault localization schemes to the experiment of each RQ, while the default setting of AVATAR is to use the GZoltar framework with the Ochiai ranking metric for ordering suspicious statements.###In this experiment, we consider a group of APR systems, namely jGenProg [66], jKali [66], jMutRepair [57], Nopol [15], FixMiner [21] and LSRepair [60], which leverage a similar configuration as AVATAR for fault localization: GZoltar/Ochiai.",impact-revealing,describing the experimental setup and methods for fault localization
3940,5c8dd94c4895d9cbc6a7d918,97f1d08c306040401112ff0564f37e6c6a312522,BiNE: Bipartite Network Embedding,59a03016b161e8ad1a7b6ed2,Neural Factorization Machines for Sparse Predictive Analytics,"ns number of negative samples [1, 2, 4, 6, 8, 10] ws size of window [1, 3, 5, 7, 9] p walk stopping probability [0.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2779,5a260c8117c44a4ba8a30ec9,5fdd40601a2ccdaa2d2ade27872bd8b3f43b2c1c,MemNet: A Persistent Memory Network for Image Restoration,573696086e3b12023e51beec,Highway Networks,"Difference to Highway Network First, we discuss how the memory block accomplishes the gating mechanism and present the difference between MemNet and Highway Network – a very deep CNN model using a gate unit to regulate information flow [32].###The success of AlexNet [22] in ImageNet [31] starts the era of deep learning for vision, and the popular networks, GoogleNet [33], Highway network [32], ResNet [12], reveal that the network depth is of crucial importance.",other,discussing differences between memory block and Highway Network
295,5dc5488edf1a9c0c41511e1e,f033fb9906a9d1b246041d2c233a4d3a84909a36,Red-blue pebbling revisited: near optimal parallel matrix-matrix multiplication,53e9a6dfb7602d97030134b8,I/O complexity: The red-blue pebble game,"We extend the main lemma by Hong and Kung [34], which provides a method to _x0080_nd an I/O lower bound for a given CDAG.###Vh } meets the remaining properties of a valid X -partition S(X ), we use the same reasoning as originally done [34].###• Based on the red-blue pebble game abstraction [34], we provide a new method of deriving I/O lower bounds (Lemma 4), which may be used to generate optimal schedules (§ 4).###Assume that we know the optimal complete calculation of the CDAG, where a calculation is a sequence of allowed moves in the red-blue pebble game [34].###We now use the above de_x0080_nitions and observations to generalize the result ofHong andKung [34].###We extend the S-partition method and the related main lemma by Hong and Kung [34].###Hong and Kung use a speci_x0080_c variant of this partition, denoted as S-partition [34].###1 General I/O Lower Bounds Hong and Kung [34] analyzed the I/O complexity for general CDAGs in their the red-blue pebble game, on which we base our work.###Hong and Kung’s red-blue pebble game [34] models an execution of an algorithm in a two-level memory structure with a small-and-fast as well as large-and-slow memory.###Hong and Kung [34] derived an asymptotic bound Ω ( n3/ √ S ) for the sequential case.###Hong and Kung [34] derived an asymptotic bound###• We reduce memory footprint for communication buffers and guarantee minimal local data reshuffling by using a blocked data
2Throughout this paper we use the original notation from Hong and Kung to denote the memory size S .###We now use the above definitions and observations to generalize the result of Hong and Kung [34].###Red-Blue Pebble Game Hong and Kung’s red-blue pebble game [34] models an execution of an algorithm in a two-level memory structure with a small-and-fast as well as large-and-slow memory.",impact-revealing,extending and building upon previous work on I/O lower bounds
2106,,f0b0a833d0ce9ca127ed275673be416907a12b78,Location-based pairwise key predistribution for wireless sensor networks,,,"###Since skRKP-D [5], which is the improved version of RKP-D [3], and a variant###Compared to RKP-D [3], skRKP-D [5], and GGD [7], only FRP is perfectly resilient to node capture, whereas skRKP-D and GGD require λ-security (Fig.###For skRKP-D, since key spaces used in a single zone group are shared among adjacent zones according to the proportion of ?̂? and ?̂?, n + (4?̂? + 4?̂?)n is limited by λω/τ where 4?̂?+ 4?̂? = 1 in [5].###From [5], we develop the cdf ge(z) as the sum of fe within the direct communication range R of node, which is centered at distance z from its deployment###τ )2 where the ratio of shared key spaces r is 1 for p1s and set as key pool sharing factors of [5] for p1a.###They include skRKP-D and grid-group deployment (GGD) [3], [5], [7].###While previous location-based schemes [3], [5], [7], [9] are based on the Blom scheme [1] and RKP [6], we distinctly start from full pairwise key predistribution (FP) and its random predistribution version called RP.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3834,5b67b45517c44aac1c860823,7f01c6fe27f57ee6191b51efa18b9199baf7b82a,Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System,57d063b4ac4436735428dc44,Discrete Collaborative Filtering,"Discrete hashing techniques [39, 40, 42] and binary coding of model parameters are suggested to speed up the calculation of the relevance score for a given ( q , d ) pair.###Balancing effectiveness and efficiency has been a line of recent research [23, 34, 39, 41, 42].",other,acknowledging recent research on balancing effectiveness and efficiency in relevance scoring
771,5ed7796e91e011e6e91120f0,8ade0f3bbe8d8251c5de9ef39886a13055f463e5,TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding,53e99b36b7602d97023d6339,Political Polarization on Twitter.,"On the other hand, on social network datasets, it is quite intuitive trying to extract information from text data to do ideology-detection [5, 8, 15–17], only a few paid attention to links [9, 13].###Even though some realized the importance of links [9, 13], they failed to provide an embedding.",impact-revealing,highlighting the gap in ideology detection methods on social network datasets
1439,,3a722b7f0c75b4370d5640be1a4d58da323f84c1,Set-membership localization and tracking of autonomous underwater vehicles,,,###The algorithms have been validated by simulations in which uncertainty models have been obtained from field data at sea.,impact-revealing,reporting validation of algorithms through simulations
3123,5ecfae0d9e795eb20a615048,fde4e53ba166567f3b9b977a866020f10a996c02,LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition,61dcef236750f87f4f90d7f8,Characteristics of text-to-speech and other corpora,Collecting high-quality speech data for TTS is typically costly [13].,other,highlighting the cost associated with collecting high-quality speech data for TTS
3466,53e9afa5b7602d97039f09be,140145d91bf2e2d0039d7986a70b656c077b5e43,Static and dynamic co-optimizations for blocks mapping in hybrid caches,53e9b349b7602d9703e21e4f,Design Implications Of Memristor-Based Rram Cross-Point Structures,"The energy 
data of the STT-RAM array are obtained from NVSim [20].###The energy data of the STT-RAM array are obtained from NVSim [20].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1984,,53ec67cba1a919f44fe1df6f43feaf619f9b8781,Providers' Socioecological Perspectives on the Supports for and Challenges to Engagement in Care for Latino Youth Living with HIV: A Qualitative Study,,,"###With the availability of effective treatment for HIV/AIDS, the focus for
HIV has shifted toward understanding how to engage PLWH across the continuum of HIV care (Cheever, 2007).###HIV has shifted toward understanding how to engage PLWH across the continuum of HIV care (Cheever, 2007).###The concept of adherence for PLWH has expanded beyond adherence to ART to include adherence to clinical care, commonly referenced as engagement in care (Cheever, 2007; Mugavero et al., 2009).###Embedded in this continuum of care are the processes of engagement, which include testing, linkage, retention, and reengagement for those who do not follow up (Cheever, 2007).",impact-revealing,highlighting the shift in focus towards engagement in HIV care
2911,5cf48a33da56291d5829579e,10973505dd4d872c13a37322a50453bf5157c552,Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search,5a9cb65d17c44a376ffb845a,Dynamic Optimization of Neural Network Structures Using Probabilistic   Modeling,"More recent studies (Brock et al., 2018; Shirakawa et al., 2018; Pham et al., 2018; Liu et al., 2019; Xie et al., 2019; Cai et al., 2019), on the other hand, optimize the weights and the architecture simultaneously within a single training by treating all possible architectures as subgraphs of a supergraph.###We generalize the work by Shirakawa et al. (2018) to enable arbitrary types of architecture variables including categorical variables, ordinal (such as real or integer) variables, and their mixture.###In the last direction, promising approaches transform a coupled optimization of weights and architectures into optimization of a differentiable objective by means of continuous relaxation (Liu et al., 2019; Xie et al., 2019) or stochastic relaxation (Shirakawa et al., 2018; Pham et al., 2018).###More recent studies (Brock et al., 2018; Shirakawa et al., 2018; Pham et al., 2018; Liu et al., 2019; Xie et al., 2019; Cai et al., 2019), on the other hand, optimize the weights and the architecture simultaneously within a single training by treating all possible architectures as subgraphs of a…###Shirakawa et al. (2018) has introduced it to model connections and types of activation functions in multi-layer perceptrons.###, 2019) or stochastic relaxation (Shirakawa et al., 2018; Pham et al., 2018).",other,acknowledge recent advancements in optimizing neural network architectures
888,5dbab2523a55acea3c05b02b,395de0bd3837fdf4b4b5e5f04835bcc69c279481,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",5f8eab579e795e9e76f6f6a0,Improving language understanding by generative pre training,"Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).###We compare the following approaches: Language Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.###BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).###BART uses the standard sequence-to-sequence Trans-former architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0 , 0 .###GPT (Radford et al., 2018) only models left-ward context, which is problematic for some tasks.###Figure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).",impact-revealing,providing context on model architectures and comparisons
1616,,5a37362c45c3485ae1c9f136556764b80301dbc2,"Introduction: Embodiment and Empathy, Current Debates in Social Cognition",,,"###…of understanding the other’s embodied intersubjective engagement prior to gaining a theoretical understanding of the other as a ‘‘minded’’ being (e.g. Gallese 2001, 2003a, b, 2005; Goldman and Gallese 1998; Goldman 2006; Iacoboni 2009;
Gallagher 2005; 2008a, b, 2011a, b; Gallagher 2012;…###This is particularly the case for ST views that directly build on the empirical hypothesis of mirroring mechanisms in the brain (e.g. Gallese 2001, 2003b, 2005; Iacoboni 2009; Rizzolatti and Sinigaglia 2010).",impact-revealing,highlighting the theoretical implications of embodied intersubjective engagement
3052,5e15adcb3a55ac47ab5b0b8c,aa63ac11aa9dcaa9edd4c88db18bec87e0834328,Graph Transformer Networks,5db9295647c8f766461f44c0,Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification,"Also, as several heterogeneous graph datasets have been recently studied for other network analysis tasks, such as link prediction [36, 41] and graph classiﬁcation [17, 24], applying our GTNs to the other tasks can be interesting future directions.",other,suggesting future research directions for applying GTNs to other tasks
2315,5efb0d5691e011063336d39c,0cee58946a13a5c2845647b4af8b9d2bf52a8b6b,BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision,5550456245ce0a409eb55cee,Glove: Global Vectors for Word Representation.,"For the baselines, we implement LSTM-CNN-CRF with Pytorch 15 and use the pre-trained 100 dimension GloVe Embeddings (Pennington et al., 2014) as the input vector.",other,describing the implementation details of the baseline model
2643,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e9b8eeb7602d97044d1f4f,Monte-Carlo tree search in Ms. Pac-Man.,• Combined heuristics can be more powerful than individual heuristics.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
585,5bbacb9e17c44aecc4eaff2b,728938faa28719e22a9de5ce87717180196da01b,Semi-supervised Training for Improving Data Efficiency in End-to-end Speech Synthesis,53e99bffb7602d97024ac0e6,Generating Sequences With Recurrent Neural Networks.,"We use a baseline Tacotron architecture specified in [8], where we use a GMM attention [9], LSTM-based decoder with zoneout regularization [10] and phoneme inputs derived from normalized text.",impact-revealing,reporting the architecture and methods used in the study
2934,5bdc315017c44a1f58a05e13,1717255b6aea01fe956cef998abbc3c399b5d7cf,AMC: AutoML for Model Compression and Acceleration on Mobile Devices,573696096e3b12023e51cb6b,Continuous control with deep reinforcement learning,"DDPG Agent As illustrated in Figure 1, the agent receives an embedding state s t of layer L t from the environment and then outputs a sparsity ratio as action a t .###Speciﬁcally, our DDPG agent processes the network in a layer-wise manner.###Such large action spaces are difficult to explore efficiently [32].###Our reinforcement learning agent (DDPG) receives the embedding s t from a layer t , and outputs a sparsity ratio a t .###Therefore, instead of searching over a discrete space, we come up with a continuous compression ratio control strategy with a DDPG [32] agent to learn through trial and error: penalizing accuracy loss while encouraging model shrinking and speedup.###We use the deep deterministic policy gradient (DDPG) for continuous control of the compression ratio, which is an oﬀ-policy actor-critic algorithm.###Therefore, instead of searching over a discrete space, we come up with a continuous compression ratio control strategy with a DDPG [32] agent to learn through trials and errors: penalizing accuracy loss while encouraging model shrinking and speedup.",other,describing the DDPG agent's role in continuous control of compression ratio
2270,5ce3af9aced107d4c65f6b25,f2bb7e2f5a1afad5370159c15760c44df93c0438,Very Deep Self-Attention Networks for End-to-End Speech Recognition,5bbacb4c17c44aecc4eac691,Speech-Transformer: A No-Recurrence Sequence-To-Sequence Model For Speech Recognition,"ith unsatisfactory results. [6] found that self-attention in the encoder (acoustic model) was not effective, but combined with an LSTM brought marginal improvement and greater interpretability, while [9] did not ﬁnd any notable improvement using the Transformer in which the encoder combines self-attention with convolution/LSTM compared to other model architectures. In this work, we show that the Tran###ention with LSTMs, while [30] uses self-attention as an alternative in CTC models. A variation of the Transformer has been applied to ASR with additional TDNN layers to downsample the acoustic signal [9]. Though self-attention has provided various beneﬁts such as training speed or model interpretability, previous works have not been able to point out any enhancement in terms of performance. Our work ",other,highlighting the limitations and mixed results of self-attention in acoustic models
135,5eede0b091e0116a23aafc15,9a75cb455b4e70c66f3b72e6bb1498d8cab72fb2,Big Self-Supervised Models are Strong Semi-Supervised Learners,5e4672c93a55ac14f595d8b5,A Simple Framework for Contrastive Learning of Visual Representations,"In SimCLR [1], the MLP projection head g(·) is discarded entirely after pretraining, while only the ResNet encoder f(·) is used during the fine-tuning.###Figure 2: Top-1 accuracy of previous state-of-the-art (SOTA) methods [1, 2] and our method (SimCLRv2) on ImageNet using only 1% or 10% of the labels.###Inspired by the recent successes of learning from unlabeled data [19, 20, 1, 11, 24, 12], the proposed semi-supervised learning framework leverages unlabeled data in both task-agnostic and task-specific###Following the semi-supervised learning setting in [30, 19, 1], we evaluate the proposed method on ImageNet ILSVRC-2012 [21].###3% relative improvement over the previous state-of-the-art [1].###In this work, we propose SimCLRv2, which improves upon SimCLR [1] in three major ways.###Unlike SimCLR [1] and other previous work [27, 20], whose largest model is ResNet-50 (4×), we train models that are deeper but less wide.###We develop an improved variant of a recently proposed contrastive learning framework, SimCLR [1], for unsupervised pretraining of a ResNet architecture [25].###To learn general visual representations effectively with unlabeled images, we adopt and improve SimCLR [1], a recently proposed approach based on contrastive learning.###2 Furthermore, instead of throwing away g(·) entirely after pretraining as in SimCLR [1], we fine-tune from a middle layer (detailed later).",impact-revealing,highlighting improvements in semi-supervised learning frameworks
2963,5c0f8548da562944ac906a71,5a3da29970d0c3c75ef4cb372b336fc8b10381d7,CNN-Based Real-Time Dense Face Reconstruction with Inverse-Rendered Photo-Realistic Face Images,53e997e4b7602d9701fdcb72,Being John Malkovich,"such as facial expression transfer [52], [53] and face replacement [12], [16], [30].",other,acknowledge existing methods in facial expression transfer and face replacement
3881,5ac1829d17c44a1fda917e29,ef2ec69e7c94b4194ba01719ac76d4595e6b4bdf,L2-Nonexpansive Neural Networks,58437722ac44360f1082f4d8,Towards Evaluating The Robustness Of Neural Networks,"Adversarial defense is a well-known difﬁcult problem (Szegedy et al., 2014; Goodfellow et al., 2014; Carlini & Wagner, 2017a; Athalye et al., 2018; Gilmer et al., 2018).###However, as will be shown in Tables 1 and 2, the robustness by adversarial training diminishes when a white-box attacker (Carlini & Wagner, 2017a) is allowed to use more iterations.###…in poor robustness and vulnerability under adversarial attacks which has been reported on a variety of networks including image classiﬁcation (Carlini & Wagner, 2017a; Goodfellow et al., 2014), speech recognition (Kreuk et al., 2018; Alzantot et al., 2018; Carlini & Wagner, 2018), image…###There are many avenues to defense (Carlini & Wagner, 2017b; Meng & Chen, 2017), and here we will focus on defense works that fortify a neural network itself instead of introducing additional components.###The attack code of Carlini & Wagner (2017a) is used.###It’s worth noting that the slow degradation of Model 2’s accuracy is an artifact of the attacker (Carlini & Wagner, 2017a): when gradients are near zero in some parts of the input space, which is true for MNIST Model 2 due to adversarial training, it takes more iterations to make progress.",other,highlighting the challenges and vulnerabilities in adversarial defense
1770,,16c9531de6a2c6be21a8e31da4ac9a60db66b2f4,Decision-making Factors Toward the Adoption of Smart Home Sensors by Older Adults in Singapore: Mixed Methods Study,,,"###Technology acceptance and intention to use and adopt technology by older adults have been measured in many studies [12-15].###To understand older adults’ behaviors in using the internet in China, Pan and Jordan-Marsh [12] expanded the TAM model to include 2 additional variables—subjective norm and facilitating conditions—which highlighted the importance of policy making in alleviating social and cultural obstacles facing older adults.",impact-revealing,highlighting the significance of the expanded TAM model for understanding older adults' internet use
2115,,95b57e3db66745e0d9aed81870e219524a90fc97,Antecedents and consequences of perceived risk in Internet shopping in China and France : a cross-cultural approach,,,"###Antecedents and consequences of perceived risk in Internet shopping in China and France : a cross-cultural
approach Lili Zheng
To cite this version: Lili Zheng.###, China) are more likely to share their opinions and attitudes (Kim, 2008).###First, our research, conducted in China and France, demonstrates the influence of privacy concerns, security protection, and reputation on the Internet, taking into consideration perceived risk.###Thus, in more collectivist cultures, decisions are influenced by the group norm and members‘ opinions (Kim, 2008).###They are somewhat separate from their social context (Kim, 2008).###Hofstede‘s cultural model has been criticized for equating nation with culture (Kim, 2008).###Antecedents and consequences of perceived risk in Internet shopping in China and France : a cross-cultural approach.###11 1.3.3 Clothing purchase on the Internet ....................................................................................................... 11
1.3.3.1 Textile and Apparel E-retailers ....................................................................................................................... 11 1.3.3.2 Perceived Risk Regarding to Post-purchase Stage .......................................................................................... 12
1.4 METHODOLOGY............................................................................................................................................... 17 1.4.1 Information System Study and Post-positive Perspective .................................................................... 17 1.4.2 Cross-culture Study Issues ................................................................................................................... 18 1.5 DISSERTATION STRUCTURE ................................................................................................................................. 20
CHAPTER 2: PERCEIVED RISK OF INTERNET SHOPPING AND CULTURE ............................................................ 25
2.1 OVERVIEW ..................................................................................................................................................... 25 2.2 INTERNET SHOPPING IN FRANCE AND IN CHINA ...................................................................................................... 27
2.2.1 E-commerce in Europe, specifically in France ......................................................................................###Context refers to how individuals and their society seek information (Kim, 2008).###Thus, multiple group SEM analysis has been suggested as a reliable method
viii
for determining measurement equivalence if a grouping variable (i.e., culture in this study) affects a structural equation model across groups (Steenkamp and Baumgartner, 1998; Kim, 2008).###The Frequency that Risk Dimensions are Mentioned by Chinese and French Interviewees for Buying a Clothing Product on the Internet ......................................................................................................................................................... 158 Table 5.3.###Consumers are usually concerned about online retailers‘ guarantee of security, because online shoppers have to divulge confidential information, such as credit card information on the Internet (Kim, 2008).###Patterns of perceived Risk by Consumer Buying Stage for Internet Airline Reservation Services, Traditional Airline Reservation Services and the Internet Risk Premium .............................................................................................................. 15 Figure 1.4.###With such a large Internet shopper population and increasing online consumer spending, better understanding of online shopping risk as perceived by Chinese and French e-shoppers becomes particularly relevant.###Markus and Kitayama (1991) equal the terms independence and interdependence to individualism and collectivism to represent two diverging views of self that are derived from the two contrasting cultures (Kim, 2008).###40 2.4.2 Perceived Risk across Product Categories............................................................................................ 41 2.4.3 Perceived Risk across Purchase Situation ............................................................................................ 46 2.4.4 Perceived Risk Dimensions in Internet Shopping Context .................................................................... 47 2.4.5 Perceived Risk Reliever Strategies in Internet Shopping Context ......................................................... 51 2.4.6 The Measure of Perceived Risk ............................................................................................................ 54
2.4.6.1 Two- Component Model Measurement ......................................................................................................... 54 2.4.6.2 Multi-item Scale Measurement...................................................................................................................... 58 2.4.6.3 The Experimental Methods ............................................................................................................................ 59
xiv
2.5 ANTECEDENTS OF PERCEIVED RISK ...................................................................................................................... 61 2.5.1 The Study of Zhou et al. (2007) ............................................................................................................ 62 2.5.2 The Study of Li and Zhang (2002) ........................................................................................................ 63 2.5.3 The Study of Volle (1995) ..................................................................................................................... 65 2.5.4 The Study of Tiangsoongnern (2007) ................................................................................................... 67 2.6 CONSEQUENCES OF PERCEIVED RISK ....................................................................................................................###, culture in this study) affects a structural equation model across groups (Steenkamp and Baumgartner, 1998; Kim, 2008).###NNT : 2013GRENG011. tel-01124126
Université Joseph Fourier / Université Pierre Mendès France / Université Stendhal / Université de Savoie / Grenoble INP
THÈSE Pour obtenir le grade de DOCTEUR DE L’UNIVERSITÉ DE GRENOBLE Spécialité : SCIENCES DE GESTION (275) Arrêté ministériel : 7 août 2006
Présentée par Lili ZHENG
Thèse dirigée par Marc FAVIER codirigée par Pei HUANG préparée au sein du laboratoire CERAG Centre d’Etudes et de Recherches Appliquées à la Gestion, UMR 5820 dans l'École Doctorale de Sciences de Gestion - EDSG
Les Antécédents et les Conséquences des Risques Perçus dans les Achats sur
Internet en Chine et en France : Une Approche Interculturelle
Thèse soutenue publiquement le 1 er juillet, 2013 devant le jury composé de :
Monsieur Serge AMABILE, MCF/HDR Université Aix-Marseille (Examinateur) Monsieur Marc FAVIER, Pr Université Grenoble (Directeur de recherche) Monsieur Pei HUANG, Pr Université Fudan-Chine (Co-directeur de recherche) Monsieur Jean-Fabrice LEBRATY, Pr Université Lyon 3 (Rapporteur) Monsieur Michel PLAISENT, Pr UQAM Montréal (Rapporteur) Monsieur Pierre VALETTE-FLORENCE, Pr Université Grenoble (Examinateur)
L’université n’entend donner aucune approbation ni improbation aux opinions émises
dans les thèses.###To identify how people from different countries respond to various situations, he used Context (low/high context communications), Space (universalism/particularism) and Time (monochromic/polychromic time) as cultural variables (Kim, 2008; Refaat El Said, 2005).###Tout d'abord, nous avons constaté que les répondants chinois et français perçoivent de faibles niveaux de risques non-personnels et personnels dans leurs achats de vêtements sur Internet, mais les répondants chinois perçoivent un niveau plus élevé de risques non-personnels et personnels que les français.
x
Pour les différents scores de risques perçus entre les deux échantillons, nous avons trouvé que les consommateurs chinois perçoivent à la fois un niveau plus élevé de risques nonpersonnels et de risques personnels que les consommateurs français.###Our research suggest that the multinational Internet business managers should put special emphasis on privacy concerns, security protection, and reputation as viewed from the perspective of the cultural background of their target consumers.
ix###Penetration of Internet shoppers in France, compared to EU27 Average (in###It is important to note that, though widely used and cited, Hofstede‘s work has received criticism in its validity and its limitations at the level of both theory and methodology (Kim, 2008).",impact-revealing,highlighting cultural differences in perceived risk during online shopping
2640,53e99fe3b7602d97028bddfb,ecf5fd423c117ffb87730d75a473bc05beaae2b8,self-optimizing memory controllers: a reinforcement learning approach,53e9aafab7602d970347ab98,Adaptive History-Based Memory Schedulers,"Most memory controller proposals [17, 25, 30, 36, 37, 38, 48, 49], including ours, aim at increasing overall system performance by delivering higher DRAM throughput and/or lower access latency.###Note that, with an integrated memory controller (which is the industry trend as seen in IBM POWER5 [17, 39], Sun Niagara [22], AMD Athlon/Opteron [1], and Intel Nehalem [3]), it is relatively easy to communicate sequence numbers and whether a request is due to a load or store miss from the processor to the controller.###Hur and Lin [17] propose a history-based memory scheduler that adapts to the mix of read and write requests from the processor.",other,acknowledge existing memory controller proposals and their goals
301,5fb24ee191e01186d3f5decc,050fecf6e3e21faeeb7629f179ba839c4dfd1ead,EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints,5ea8009091e0111d387ee87a,Lexically Constrained Neural Machine Translation With Levenshtein Transformer,"Enforcing hard constraints as in Susanto et al. (2020) increases the term usage by +8–10% and improves BLEU by +0.3–0.6 over LevT using soft constraints.###We evaluate EDITOR on the terminology test sets released by Dinu et al. (2019) to test its ability to incorporate terminology constraints and to further compare it with prior work (Dinu et al., 2019; Post and Vilar, 2018; Susanto et al., 2020).###Consistent with previous ﬁndings by Susanto et al. (2020), incorporating soft constraints in LevT improves BLEU by +0.3 on Wiktionary and by +0.4 on IATE.###The LevT base-line in Susanto et al. (2020) achieves higher BLEU than ours on the small Wiktionary and IATE test sets, while it underper-forms our LevT on the full WMT14 test set (26.5 vs. 26.9). improve BLEU.###Furthermore, Results highlight the beneﬁts of soft constraints over hard ones – EDITOR with soft constraints achieves translation quality on par or better than both EDITOR and Levenshtein Transformer with hard constraints (Susanto et al., 2020).###We also explore the decoding technique introduced in Susanto et al. (2020) to support hard constraints .###…almost all the time when they are provided as soft constraints, so there is little beneﬁt to enforcing hard constraints instead: they help close the small gap to reach 100% term usage and do not 14 We use our implementations of Susanto et al. (2020)’s technique for a more controlled comparison.###Our work is closely related to Susanto et al. (2020)’s idea of applying the Levenshtein Trans-former to MT with hard terminology constraints.",impact-revealing,highlighting the impact of hard constraints on translation quality and comparing with prior work
3783,5eede0b091e0116a23aafb82,1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af,Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,58d82fced649053542fd6e59,Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction,"Many self-supervised methods manipulate the input data to extract a supervised signal in the form of a pretext task [1, 14, 30, 33, 35, 41, 44, 47, 48, 54, 55, 65].",other,acknowledge the role of self-supervised methods in extracting supervised signals
249,5d1eb9ecda562961f0b261db,1bfad6fd818bd64db381791efd9252e0313dc100,Certifiable Robustness and Robust Training for Graph Convolutional Networks,5c04966a17c44a2c74708306,Semidefinite relaxations for certifying robustness to adversarial examples.,"For this work, specifically the class of methods based on convex relaxations are of relevance [18, 20].###As an alternative, recent works have considered certifiable robustness [3, 10, 18, 20] providing guarantees that no perturbation w.###[10, 18, 20]), we tackle various additional challenges: Being the first work for graphs, we have to deal with perturbations of multiple instances simultaneously.###For the remaining layers, since the input to them is no longer binary, we adapt the bounds proposed in [18].###, the infinity-norm or L2-norm [3, 18, 20], often e.",impact-revealing,highlighting the relevance of convex relaxation methods in the current work
3970,5f7d893591e011346ad27d16,ff8a988d88ebe5d5ca116340baa34ae00ce011e8,PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation,5bbacb9e17c44aecc4eaff49,Learning Neural Templates for Text Generation,"Consequently, planning modules are designed and added into neural systems to enhance content relevance (Wiseman et al., 2018; Moryossef et al., 2019; Yao et al., 2019; Hua and Wang, 2019).",other,highlighting the development of planning modules in neural systems
3442,5ee8986891e011e66831c452,a9872078cc6dabd2428750543862b45f4a482dfc,Graph Meta Learning via Local Subgraphs,599c7974601a182cd263f01c,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,"Meta learning methods generally fall into three categories, model-based [14, 50, 34], metric-based [37, 38, 47], and optimizationbased [32, 29, 12] techniques.###Meta-GNN t e a t - [63] applies MAML [12] to Simple Graph Convolution (SGC) [52].###This result confirms our analysis of using the prototypical loss to leverage the label inductive bias and MAML to transfer knowledge across graphs and labels.###Across meta-learning models, we observe G-META is consistently better than others such as MAML and ProtoNet.###Meta-GNNt -e at - [63] applies MAML [12] to Simple Graph Convolution (SGC) [52].###It allows direct adaptation to MAML since individual subgraphs can be considered as an individual image in the classic few-shot meta-learning setups.###To transfer the structural knowledge across graphs and labels, we use MAML, an optimization-based meta-learning approach.###MAML [12] switches ProtoNet to MAML as the meta-learner.###Note that baseline ProtoNet and MAML are the ablations of G-META.###MAML and ProtoNet have volatile results across different problems and tasks, whereas G-META is stable.###The goal of Model-Agnostic Meta-Learning (MAML) [12] is to obtain a parameter initialization ✓⇤ that can adapt to unseen tasks quickly, such as Dtest, using gradients information learnt during meta-training.###Note that the baselines ProtoNet and MAML can be considered as an ablation of G-META removing MAML and Prototypical loss respectively.###Finally, it uses prototypical loss for inductive bias and MAML for knowledge transfer across graphs and labels.",other,describing categories of meta-learning methods and their applications
3401,5ea2b8bf91e01167f5a89d89,993377a3fc8334558463b82053904e3d684f29c0,SIGN: Scalable Inception Graph Neural Networks,573695fd6e3b12023e511373,Deep Convolutional Networks on Graph-Structured Data,"If ˆ g is a smooth function, the resulting ﬁlters are localized in the node domain Henaff et al. (2015).",other,reporting prior findings on localized filters
2652,5da1a6d447c8f7664606888c,404d82a8da658f16c46714442155490d79a413f1,temporal prefetching without the off-chip metadata,58437762ac44360f1083d406,"Enabling technologies for memory compression: Metadata, mapping, and prediction","Finally, metadata optimizations have been explored in the context of memory compression [13, 19, 48], but these techniques are orthogonal to our work.",other,acknowledging the distinction between metadata optimizations and current work
2239,53e9af67b7602d97039a85ee,529e8c6e6b5a6cb4f1cf202c47d9d42f5889ec1d,Last-Touch Correlated Data Streaming,53e99b26b7602d97023bf6ae,Pointer cache assisted prefetching,"Many proposals are inherently limited in scope, targeting only specific access patterns, such as strided accesses [1,19], pointer dereference [6,8], or accesses to linked data structures [17].",other,highlighting limitations in existing proposals targeting specific access patterns
681,5db9294247c8f766461f1f4a,79c93274429d6355959f1e4374c2147bb81ea649,lxmert: learning cross-modality encoder representations from transformers,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Since our EMNLP submission, a few other useful preprints have recently been released (in August) on similar cross-modality pre-training directions: ViLBERT (Lu et al., 2019) and VisualBERT (Li et al., 2019).",impact-revealing,acknowledging recent related work in cross-modality pre-training
1991,,f560d6584ef3e619b362a29e3e655c502bb6294d,The susceptibility of species to extinctions in model communities,,,"###Later studies emphasized the importance of ecosystem size to the number of feasible trophic levels in a system (Post, Pace, & Hairston 2000; Post 2002).",impact-revealing,highlighting the significance of ecosystem size in trophic levels
3461,5cede0edda562983788cb3c2,1e43c7084bdcb6b3102afaf301cce10faead2702,BioBERT: a pre-trained biomedical language representation model for biomedical text mining.,53e99d5eb7602d970261ab25,LINNAEUS: A species name identification system for biomedical literature,", 2018) for a fair evaluation; however, the splits of LINAAEUS and Species-800 could not be found from Giorgi and Bader (2018) and may be different.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3494,5d04eeba8607575390f83f4d,9cceaadb580c24d0d5c381fa8e3d4afb32fd88b9,perceptron-based prefetch filtering,5bdc319c17c44a1f58a0a1eb,MTB-Fetch: Multithreading Aware Hardware Prefetching for Chip Multiprocessors.,"Modern prefetching mechanisms are more sophisticated as they look into past memory behavior [10, 11], locality [12–17], controlflow speculation [18, 19], and other other aspects to detect complex memory access patterns.",other,highlighting advancements in modern prefetching mechanisms
471,5f03f3b611dc83056223206d,75c8466a0c1c3b9fe595efc83671984ef95bd679,XGNN: Towards Model-Level Explanations of Graph Neural Networks,5db9293347c8f766461f059b,GNNExplainer: Generating Explanations for Graph Neural Networks,"ing models on graph data become increasingly important but is still less explored. To the best of our knowledge, there is no existing study on interpreting GNNs at the model-level. The existing study [4, 40] only provides example-level explanations for graph models. As a radical departure from existing work, we propose a novel interpretation technique, known as XGNN, for explaining deep graph models at t###the summary statistics of these datasets in Table 1. Since there is no existing work investigating model-level interpretations of GNNs, we have no baseline to compare with. Note that existing studies [4, 40] only focus on interpreting GNNs at example-level while ignoring the model-level explanations. Comparing with them is not expected since these example-level and model-level are two totally different i###anations. Without understanding and verifying the inner working mechanisms, GNNs cannot be fully trusted, which prevents their use in critical applications pertaining to fairness, privacy, and safety [7, 40]. For example, we can train a GNN model to predict the effects of drugs where we treat each drug as a molecular graph. Without exploring the working mechanisms, we do not know what chemical groups in ###ould not exceed its maximum chemical valency. 2.3 Graph Model Interpretations To the best of our knowledge, there are only a few existing studies focusing on the interpretability of deep graph models [4, 40]. The recent GNN interpretation tool GNN Explainer [40] proposes to explain deep graph models at the example-level by learning soft masks. For a given example, it applies soft masks to graph edges and",impact-revealing,highlighting the lack of existing studies on model-level interpretations of GNNs
3153,5a9cb65d17c44a376ffb83f3,e06357ac23811054acb23e2ea30d087f5beaef90,an interpretable reasoning network for multi-relation question answering,5736960e6e3b12023e521449,Learning to Compose Neural Networks for Question Answering,"Neural Module Network (Andreas et al., 2015; Andreas et al., 2016) customized network architectures for different patterns of reasoning, making the reasoning network interpretable.",other,providing context on Neural Module Networks
2062,,2f92d808845e1affbf349b8d6e3257b7002dec21,Optimal timing of first reproduction in parasitic nematodes,,,"###An improved understanding of how selection acts on maturation time is of applied as well as theoretical interest since medical and veterinary intervention programmes are expected to alter selection on parasite life history schedules (Medley, 1994; Read & Skorping, 1995; Buckling et al., 1997; Poulin, 1998; Skorping & Read, 1998).###…selection acts on maturation time is of applied as well as theoretical interest since medical and veterinary intervention programmes are expected to alter selection on parasite life history schedules (Medley, 1994; Read & Skorping, 1995; Buckling et al., 1997; Poulin, 1998; Skorping & Read, 1998).",impact-revealing,highlighting the significance of understanding selection on maturation time in relation to medical and veterinary interventions
1938,,4282acf712b98379b9ac002af2e7fbee719ee453,Two-Character Chinese Compound Word Processing in Chinese Children With and Without Dyslexia: ERP Evidence,,,"###Despite the fact that previous studies (e.g., McBride-Chang et al., 2003; Shu et al., 2006) have underscored the importance of morphological processing across dyslexic and typically developing children, it is still unclear how Chinese children process morphological information in the unfolding time…###This question is important for Chinese dyslexic children, that is, those who have difficulties in reading and spelling despite normal intelligence and adequate formal education, because Chinese children with developmental dyslexia have shown deficits in morphological awareness (Shu et al., 2006).###The awareness and ability in understanding and
manipulating the morphological compound structure has been identified to significantly correlate with children’s language learning and literacy development (e.g., McBride-Chang et al., 2003; Shu et al., 2006).###One plausible explanation for the lack of N400 effect is that dyslexic children’s imperfect tacit knowledge of morphemes and morphological structure (e.g., Shu et al., 2006) leads to less than adequate, abstract, and integrative semantic representations of two-character compound words.###In addition, studies have consistently reported that Chinese children with dyslexia have difficulties in combining separate morphemes into Chinese compound words (e.g., McBride-Chang, Lam, et al., 2011; McBride-Chang, Liu, Wong, Wong, & Shu, 2011; Shu et al., 2006).###For example, they could generate a novel pseudo-multi-morpheme word “mushroom oil” when they were asked to answer the question “When an oil is made of peanut, we call that ‘peanut oil;’ what should we call it if the oil is made of mushrooms?” In addition, studies have consistently reported that Chinese children with dyslexia have difficulties in combining separate morphemes into Chinese compound words (e.g., McBride-Chang, Lam, et al., 2011; McBride-Chang, Liu, Wong, Wong, & Shu, 2011; Shu et al., 2006).###Despite the fact that previous studies (e.g., McBride-Chang et al., 2003; Shu et al., 2006) have underscored the importance of morphological processing across dyslexic and typically developing children, it is still unclear how Chinese children process morphological information in the unfolding time course of word processing.###Overall, the rich morphological nature of Chinese makes morphological awareness a strong correlate of Chinese reading development and impairment (e.g., Shu et al., 2006).###manipulating the morphological compound structure has been identified to significantly correlate with children’s language learning and literacy development (e.g., McBride-Chang et al., 2003; Shu et al., 2006).###Moreover, such morphological awareness weakness persists into adolescence (Chung, Ho, Chan, Tsang, & Lee, 2010), and it has become one of the most salient indicators of Chinese children with development dyslexia (e.g., Shu et al., 2006).",impact-revealing,highlighting the significance of morphological processing in dyslexic children
2001,,717e091577d964b755630a6154dbc0dd38f04687,The Phenomenological Exploration of Constructionism Among Female Undergraduate Communications Media Students When Designing a 2D Game Interface,,,"###(Carmen, interview #1) The participants’ descriptions support the statements made by Brar (2012), Hayes (2005),
and Dickey (2006) that the current method of attempting to classify players by their gender was
72
deeply flawed.###As a result, female characters within games are frequently cited as being portrayed as unrealistic and prone to using oversexualization, often using gender stereotypes as defining characteristics (Hayes, 2005).###preferences (Hayes, 2005; Carr, 2005), gameplay frequency (Lucas & Sherry, 2004), academic application (Besisser, 2006), and gender comparisons (Royse et al.###However, Hayes (2005) and Brar (2012) argue that the majority of preference-based
studies are flawed, with researchers failing to account for other variables that may determine a player’s preferences.###Hayes (2005) goes further by stating that when women are used as a marketing tool, they are more prone to oversexualization in order to attract the male demographic.###However, the majority of studies that have focused on a female population have been limited to determining player preferences (Hayes, 2005; Carr, 2005), gameplay frequency (Lucas & Sherry, 2004), academic application (Besisser, 2006), and gender comparisons (Royse et al., 2007; Brar, 2012).",impact-revealing,highlighting flaws in gender classification methods in gaming studies
2855,5f7ee07491e011a5faf0feb2,c54fb59319288d0d17ad09e6b7dfee6d042f83a8,Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer,57a4e91aac44365e35c97887,Complex Embeddings for Simple Link Prediction,"MRR weighting ( KEnS m ): MRR is a widely-used metric for evaluating the ranking performance of a model (Bordes et al., 2013; Yang et al., 2015; Trouillon et al., 2016), which may also serve as a weight metric for estimating the prediction con-ﬁdence of each language-speciﬁc embedding in ensemble…###Representative models including translational models (Bordes et al., 2013; Wang et al., 2014) and bilinear models (Yang et al., 2015; Trouillon et al., 2016) have achieved satisfactory performance in predicting missing facts.",other,reporting on the use of MRR as a metric and its application in evaluating model performance
2902,5f8d00a29e795ea21aee8001,34d6bc3dc0a4811eb262508379fc74f600671687,a collective approach to scholar name disambiguation,53e99f7fb7602d9702856a05,Efficient Topic-Based Unsupervised Name Disambiguation,", agglomerative clustering [9], [19], [39], affinity propagation [11] and Markov clustering [41], or topic modeling [28], [29] to divide the set of author references into different subsets.###Most existing methods tackle name disambiguation separately [5], [6], [9], [11], [13], [15], [17], [18], [19], [29], [32], [35], [36], [37], [38], [39], [40], [44].###In general, existing work for scholar name disambiguation can be divided into two classes: supervised [4], [15], [17], [18], [35], [38], [40], [44] and unsupervised [7], [9], [11], [19], [28], [29], [32], [34], [37], [39], [41], [42].###[7], [38], web information [19], affiliation [5], and implicit evidence [28], [29].",other,acknowledge existing methods for scholar name disambiguation
968,,cc192999108dd1b3446884babdf26362572a11dd,A Strawson-Lewis defence of social preferences,,,"###Players with the mutual belief that they are motivated by kindness can attain the cooperative outcome in a one-shot PD.
Ingenious experimental designs have demonstrated that both concerns play a role in cooperation, e.g. Cox (2004), Bacharach et al. (2007), Cox et al. (2008), Falk et al. (2008).",impact-revealing,highlighting the role of mutual belief in cooperation outcomes
2275,5e982cc591e0119e8a952209,b5ef0f91663f0cbd6910dec9a890c138f7ec10e0,Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks,5550415645ce0a409eb3a6ac,Deep Visual-Semantic Alignments for Generating Image Descriptions.,"1K Test Set 5K Test Set DVSA [14] - 38.###We did not use ranking losses [14,18], as we found that the binary classification loss works better, similarly as reported in [27].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1008,,c055827720c9699e84e1451fb6330c7980d87a44,"Psychological Trauma : Theory , Research , Practice , and Policy Continuous Exposure to Life Threats Among Different Age Groups in Different Types of Communities",,,"###With regard to traumatic events, Kawachi and Berkman (2001) have highlighted the importance of the availability of social connections as a buffer against the impact of disasters.",impact-revealing,highlighting the significance of social connections in mitigating disaster impact
1802,,abbf135440f7103e969a697a9d2a26d978dee4bf,Relooper: refactoring for loop parallelism in Java,,,"###To check the uniqueness invariant, the analysis builds upon a context-sensitive, ﬂow-insensitive, demand-driven pointer analysis [2] implemented in WALA [3].",impact-revealing,reporting method used for analysis
2022,,2c4c224d709702be030bcad75c18d85c79bdd186,Differential Vectors Empower Snow Ablation Optimizer,,,"###In the past two decades, meta-heuristic algorithms (MHAs) are inspired by nature and animal behavior to solve various kinds of engineering issues, especially in finding the optimal solution is difficult or impossible, like in the part of NP-hard problems [1]–[6].",impact-revealing,highlighting the significance of meta-heuristic algorithms in solving complex engineering problems
863,5ec49a639fced0a24b4de922,0d965ed237a3b4592ecefdb618c29f63adedff76,Towards Debiasing Sentence Representations,5cede0e9da562983788c83b6,Identifying and Reducing Gender Bias in Word-Level Language Models,Bordia and Bowman (2019) only study word-level language models and also requires re-training.,impact-revealing,acknowledging limitations in prior studies on language models
587,5c04966a17c44a2c74708959,6b98bef930182a848c027dece1bfb58ca706449d,Improving End-to-end Speech Recognition with Pronunciation-assisted Sub-word Modeling,53e99867b7602d97020a27c2,"A Simple, Fast, and Effective Reparameterization of IBM Model 2.","We call this method pronunciation-assisted sub-word modeling (PASM), which adopts fast align [7] to align a pronunciation lexicon ﬁle and use the result to ﬁgure out common correspondence between sub-word units and phonetic units.",impact-revealing,introducing a new method for sub-word modeling
3259,5736982b6e3b12023e6fd332,d0b8c5464b582e1a582a4affb270e8b02f8414a2,a scalable architecture for ordered parallelism,53e9a472b7602d9702d9376a,Toward Efficient And Robust Software Speculative Parallelization On Multiprocessors,"For example, tracking all possible dependences on a 1024-task window using bit-vectors, as proposed in prior work [13, 58], would require 1024× 1023 1Mbit of state.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
465,5d9c5e4d3a55ac916a95fbd8,038b2d276e3c04e4a6ef0c04ace98e9621edf5bc,Negative Sampling in Variational Autoencoders,5c2c7a9217c44a4e7cf31307,Deep Anomaly Detection with Outlier Exposure.,"Our investigations are mostly inspired by and related to recent work on the evaluation of generative models on OOD data [1], [2], [6].###The ominous observation is presented also by [6], but they concentrate on improving the OOD data detection with Outlier Exposure.###Reference [6] demonstrates that using auxiliary datasets as source of OOD examples for supervisory signal significantly improves the performance of existing anomaly detection models on image and text data.",impact-revealing,highlighting the influence of recent work on generative models evaluation
1033,,c47ccf809366a34ad1b0a5adeb0dee03d215d54e,Neurobiology of Depression: Chronic Stress Alters the Glutamatergic System in the Brain—Focusing on AMPA Receptor,,,"###The plasma levels of corticosterone, a stress hormone, have been widely used as biomarkers of stress [16], depression [17] and anxiety [18] in mice, and such a phenomenon was also observed in humans [19] in the form of plasma cortisol.",impact-revealing,highlighting the significance of plasma corticosterone levels as biomarkers in stress and related conditions
584,5b3d98cc17c44a510f801c3a,f722b0a7a9b7709d693b9d39195c779832a943fe,end-to-end speech-driven facial animation with temporal gans,58437722ac44360f1082f58e,Generating Videos with Scene Dynamics.,"However, GANs are not limited to these applications and can be extended to handle videos [14, 16, 24, 25].###Some video generation methods have dealt with this problem by generating the entire sequence at once [25] or in small batches [20].###Straight-forward adaptations of GANs for videos are proposed in [20, 25], replacing the 2D convolutional layers with 3D convolutional layers.",impact-revealing,acknowledge the adaptability of GANs for video applications
3582,5eccb534e06a4c1b26a83a46,e582444763f8b1e3e2c725aab1f37c5d64b69123,Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations,58437722ac44360f1082f4d8,Towards Evaluating The Robustness Of Neural Networks,"Note that similar techniques of clamping the logits have also been used in [5], however, their motivation is to obtain minimum-magnitude (image-dependent) perturbations.###The existing attacks are commonly categorized under image-dependent attacks [42, 14, 23, 31, 5] and universal ( i.e . image-agnostic) attacks [28, 19, 33, 27, 36, 46, 35] which devise one single perturbation to attack most images.###Image-dependent attack techniques have been explored in a variety of works ranging from optimization based techniques [42, 5] to FGSM related techniques [14, 23, 7, 45].",other,providing context on existing attack techniques in image processing
124,5cede0f9da562983788d862a,44d43bfbd23d55b1e7c4c4fd91fe101c0eaf1a06,Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks,5c0495ae17c44a2c747019af,Boosting Adversarial Attacks With Momentum,"…[10] (TI-FGSM) has the following update rule Also, the integration of the translation-invariant method into the basic iterative method [15] yields the TI-BIM algorithm The translation-invariant method can be similarly integrated into MI-FGSM [7] and DIM [38] as TI-MI-FGSM and TI-DIM, respectively.###We adopt the ensemble method proposed in [7], which fuses the logit activations of different models.###Although many attack methods [7, 38] can generate adversarial examples with very high transferability across normally trained models, they are less effective to attack defense models in the black-box manner.###It has been shown that BIM induces much more powerful white-box attacks than FGSM at the cost of worse transferability [16, 7].###In our experiments, we integrate our method into the fast gradient sign method (FGSM) [10], momentum iterative fast gradient sign method (MI-FGSM) [7], and diverse inputs method (DIM) [38].###For black-box attacks based on the transferability [10, 19, 7], an adversarial example is usually generated for a single input against a white-box model.###Momentum Iterative Fast Gradient Sign Method (MI-FGSM) [7] proposes to improve the transferability of adversarial examples by integrating a momentum term into the iterative attack method.###Several methods [7, 38] have been proposed to improve the transferability, which enable powerful black-box attacks.###We do not include the basic iterative method [15] and C&W’s method [5] since that they are not good at generating transferable adversarial examples [7].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
703,57d063e0ac443673542947b3,23430d75292d150ddd1987b6f2e0a3626ab76f9b,Opportunistic Competition Overhead Reduction for Expediting Critical Section in NoC Based CMPs,558b1d6384ae84d265c18f42,GARNET: A detailed on-chip network model inside a full-system simulator,"We implement and integrate our design in GEM5 [3], on which the OS kernel (Linux 4.2) and the embedded network GARNET [1] are enhanced according to our technique.###[1] N. Agarwal, T. Krishna, L.-S. Peh, and N. K. Jha, “GARNET: A
Detailed On-chip Network Model Inside a Full-system Simulator,” in International Symposium on Performance Analysis of Systems and Software (ISPASS), 2009.###2) and the embedded network GARNET [1] are enhanced according to our technique.###At the hardware-level, we modify the default VA and SA (class VCallocator and SWallocator) of the GARNET VC router in GEM5 to support our priority based arbitration.",impact-revealing,describing the implementation of a design in a specific simulation environment
14,5d08be648607575390f908ca,b30b850e726ee41f80a28c2a81da9c640a8541fc,QoSMT: supporting precise performance control for simultaneous multithreading architecture,53e9b2e0b7602d9703d99006,Per-thread cycle accounting in SMT processors,"PTA uses MLP correction to achieve higher accuracy [12].###Eyerman et al. [12] proposed the per-thread cycle accounting (PTA) mechanism that is able to estimate a work-load’s solo performance in a co-running mode on SMT processors.###Referring to prior work [12], we design shadow solo-cycle accounting (SSCA) approach to estimate workloads’ execution time in solo mode by T solo = T share − T interf , where T share is the execution time in SMT mode and the interference time, T interf , is the sum of the contention stall cycles…",impact-revealing,reporting prior findings on PTA and its mechanisms
1614,,e7be3740ecea0b46eea51841bae4a36578e8f7a7,Single- and multimodal subvoxel registration of dissimilar medical images using robust similarity measures,,,"###Registration experiments were performed both with D and D images A rst class of experiments consisted in applying a known transformation translations and rotations to a set of MRI slices or volumes to create a second image set of the transformed images was then corrupted by salt and pepper noise to simulate outliers For each method the estimated registration parameters were compared to the true ones to determine the accuracy of the registration Statistics on the registration errors were computed on a set of di erent registrations problems
involving translation parameters between and voxels and rotations between and degrees As we can see in Table the robust algorithms achieved subvoxel registration errors while the non robust LS and IU techniques failed The MI method also achieved subvoxel registration but its performance is slightly inferior to the results obtained by the RLS technique Figure c shows an example where the standard method LS failed to correctly register the MR slices shown in gures a and b but where the RLS achieved accurate matching by discarding the outliers The registration error shown in gures c and d is the squared image di erence after registration The registration errors c and d are normalized to the maximum display value for better visualization
Figure D robust registration a Reference image b Image in a rotated by deg translated by pixels along the x axis pixels along the y axis and corrupted at with salt and pepper noise c Di erence between the noise free registered image and the image in a when the non robust technique is applied d Di erence between the noise free registered image and the image in a using the Geman McClure robust estimation function
a b c d
Table D registration results A set of D image volumes was arti cially transformed using di erent rigid transformations and the images were corrupted at by salt and pepper noise The average and the standard deviation of the registration errors computed from the registrations are presented for the di erent approaches The translation error is given in voxels and the rotation error in degrees
Single Modal Registration D
Approach ""tx ""ty ""tz "" x "" y "" z LS IU MI RLS RIU
Complementary experiments with known ground truth were obtained with a D test object acquired under di erent rigid transformations by modifying the read and phase gradients during acquisition as explained previously Table presents the registration errors for the di erent techniques in this case In the absence of signi cant noise all of the techniques achieved subvoxel accuracy but the RLS gave the best results and appears to be a good choice for the single modal registration problem Let us notice that the image uniformity approaches IU or RIU are not appropriate methods for single modal image registration as can be seen from the results in Tables and as already noticed they have rather been devised for multimodal images
Finally we have applied the RLS algorithm to a set of MRI slices of a multiple sclerosis MS patient acquired at di erent dates Figure illustrates an accurate alignment where small di erences due to lesion evolution which were not well distinguished previously due to misalignment by standard methods g c are now identi ed by simple image subtraction see g d The robust algorithm achieved better registration than the standard one g Fewer registration artifacts are observed on the cortical sulcus the falx and the periventricular hyperintensities and multiple sclerosis lesions evolution is clearly displayed The robust registration technique allowed better follow up of the disease
Table D Test object registration results A D test object was acquired with di erent rigid transformations The average and the standard deviation of the registration errors are presented for the di erent approaches The translation error is given in pixels and the rotation error in degrees
Single Modal Test Object Registration D
Approach ""tx ""ty "" LS IU MI RLS RIU
Figure a A MS patient s MR image b Image of the same patient acquired some months later c Di erence between the registered image and the image in a when the least squares technique is applied d Di erence between the registered image and the image in a using the Geman McClure robust estimation function
a b c d###To evaluate the di erent cost functions and registration algorithms the following data sets have been acquired
D MR images of a phantom and of selected multiple sclerosis patients were acquired on a Tesla Bruker system Each MR image set was obtained with a multi slice multi echo sequence echo time TE ms repe tition time TR ms The size of the images was FOV cm with pixel size of mm mm and a slice thickness of mm
D MR scans were acquired on partially epileptic patients using a Tesla Bruker system The images were obtained with a gradient echo sequence TR ms and TE ms !ip angle deg image size is FOV cm
SPECT imaging was performed on a double headed camera Elscint Helix with low energy and high resolution parallel hole collimators using MBq of mTc HMPAO or mTc ECD The camera was operated in the stop and shoot mode with acquisition at deg intervals acquiring views at s per interval projections matrix Slices were reconstructed with a matrix System resolution was measured at mm full width at half maximum FWHM in all planes at the center of the eld
For epileptic patients the interictal SPECT studies were performed when patients had been seizure free for at least hours EEG recording was performed during isotope injections to insure interictal status at the time of injection For ictal studies patients underwent continuous video EEG monitoring were injected during ongoing spontaneous seizure activity
To test the registration algorithms with ground truth data a part of the MR images were acquired with di erent o sets in demodulation frequency to simulate translations of the data set Di erent directions of the read gradient were also used to generate rotations Thanks to the above manipulation the true values of the D translation and rotation parameters were known accurately for these data sets and could be used to compare the performances of the di erent approaches
Both computation and display were performed on a Hewlett Packard workstation by using a D D image analysis software MEDIMAX developed at the IPB This software running under Unix is developed in C language and uses the standard graphics interface X R and the Motif windows manager All registrations techniques presented in this paper were implemented under this software environment and are easily available to users The software is presented on the laboratory s web server http alsace u strasbg fr
RESULTS
We have compared the robust least squares RLS and robust image uniformity RIU approaches to the standard least squares LS method and to the image uniformity IU technique and to the recently pro posedmutual information criterion MI###We have compared the robust least-squares (RLS) (7) and robust image uniformity (RIU) (8) approaches to the standard least-squares (LS) method (2) and to the image uniformity (IU) technique (3) and to the recently proposedmutual information criterion (MI) .###The calculation of the registration parameters ® involves the minimization of the non-linear cost functions (7) or (8) which depend on the scale parameter C.###In the single modal case (7) , the cost function is simply the robust error norm of the residual differences between the two registered images.###To evaluate the multimodal image registration algorithms a D SPECT image volume has been manually registered to its corresponding MRI volume with the aid of a neurologist The manually registered SPECT image was then transformed using the same D translation and rotation parameters as for the previously described experiments To simulate outliers of the SPECT image was corrupted by salt and pepper noise The robust image uniformity technique RIU has been compared to the image uniformity IU technique and to the MI method which is also suited to multimodal image registration Table illustrates the robustness of our technique to outliers The error for the RIU method is around pixel for the translation and degree for the rotation This is signi cantly more accurate than IU approach We also notice the good performance of the MI technique which provides results that are always better than the IU but generally slightly inferior to RIU
Finally gure shows a real example of a patient s SPECT image volume interictal registered with respect to its MRI counterpart by the robust algorithm After robust registration of the ictal SPECT volume to the same MRI the SPECT hyperintensity region di erence between ictal and interictal has been segmented and superimposed onto the MR image g
Table D MRI SPECT registration results A set of D SPECT image volumes manually pre registered by an expert to its MRI counterpart was arti cially transformed using di erent translation and rotation parameters and corrupted at by salt and pepper noise The average and the standard deviation of the registration errors are presented for the di erent approaches The translation error is given in voxels and the rotation error in degrees
Multimodal Registration D
Approach ""tx ""ty ""tz "" x "" y "" z IU MI RIU
Figure Robust MRI SPECT registration The SPECT and MRI volume with the SPECT contours superimposed are shown multiplanar visualization a Before registration b After robust registration by the RIU technique
a
b
Figure D MRI SPECT representation of a patient presenting partial complex seizures of right temporal origin the di erence image ROI between ictal SPECT and interictal SPECT demonstrating areas of increased perfusion is shown superimposed onto the corresponding MR image
DISCUSSION AND CONCLUSION
The registration methods described in this paper were motivated by the algorithm proposed by in the case of single modality medical image registration and by the model proposed in in the case of multimodal image matching These approaches have been improved by the non straightforward extensions proposed in this paper The new robust multigrid stochastic registration technique has two major advantages over previous methods
No manual initialization near the optimal solution is required to obtain an accurate registration Local minima a major problem in standard medical image registration techniques are avoided automatically by the use of fast simulated annealing optimization algorithms This results in a fully data driven method that requires no human interaction
Gross image di erences due to lesion evolution etc are taken into account e ciently by robust estimation techniques The robust functions decide whether a measure is an outlier or not To our knowledge robust registration has never been evoked for multimodal images until now
We have compared our approach to the maximization of the mutual information technique and to the commonly used image uniformity algorithm The IU algorithm does not perform well when the images exhibit signi cant di erences since its cost function based on standard image statistics does not account for outliers The MI method presents a good robustness to outliers but its performance is not as good as that observed with the robust image uniformity technique Let us notice that the LS and RLS techniques require approximately the same computation times min cpu time for images on our HP workstation On the same data set the IU method takes min the MI technique min and the RIU method needs h cpu time As can be seen the
additional computational complexity introduced by the robust estimation remains moderate and these methods may thus be used with pro t to improve the accuracy in many critical multimodal image registration problems",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3691,5e5e18ad93d709897ce2654c,19605cad33f79d3070b7c4b24aa49653ab7c90da,Inductive Matrix Completion Based on Graph Neural Networks,53e9b083b7602d9703aecb96,Content-Based Recommender Systems: State Of The Art And Trends,"Other content-based recommender systems (Lops et al., 2011) face similar problems.",other,acknowledge challenges faced by content-based recommender systems
776,57aa28de0a3ac518da9896d5,36ee2c8bd605afd48035d15fdc6b8c8842363376,node2vec: scalable feature learning for networks,53e9b253b7602d9703cf4028,DeepWalk: online learning of social representations,"Recent attempts in this direction [24, 28] propose efﬁcient algorithms but rely on a rigid notion of a network neighborhood, which results in these approaches being largely insensitive to connectivity patterns unique to networks.###We contrast the performance of node2vec with state-of-the-art feature learning algorithms [24, 28].###DeepWalk [24] proposes search using uniform random walks.###The resulting algorithm is ﬂexible, giving us control over the search space through tunable parameters, in contrast to rigid search procedures in prior work [24, 28].###Inspiredbythe Skip-gram model, recent research established an analogy for networks by representing a network as a “document” [24, 28].###This is a major shortcoming of prior work which fail to offer any ﬂexibility in sampling of nodes from a network [24, 28].###We proceed by extending the Skip-gram architecture to networks [21, 24].###We showed how random walks, also used in DeepWalk [24], allow the sampled nodes to be reused as neighborhoods for different source nodes appearing in the walk.###We exclude other matrix factorization approaches which have already been shown to be inferior to DeepWalk [24].###• DeepWalk [24]: This approach learns d -dimensional feature representations by simulating uniform random walks.",impact-revealing,highlighting the limitations of existing algorithms and contrasting them with a more flexible approach
95,5f64211c9e795e0286c313a2,f5316f15c665e5a5f89b8b70de13438892e21207,ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks,53e9be09b7602d9704abe386,Semi-supervised Learning by Entropy Minimization,"Entropy minimisation is the most widely used principle in machine learning [14, 38, 9, 10, 22].###This is surrounded by minimum entropy regularisation, which is widely evaluated in unsupervised and semi-supervised scenarios [9, 10].",impact-revealing,highlighting the prevalence of entropy minimization in machine learning
1188,,e0dea7c925b743a976e185045fdcae8a6947bd0f,A Lightweight Domain Adversarial Neural Network Based on Knowledge Distillation for EEG-based Cross-subject Emotion Recognition,,,"###Moreover, the design of feature interaction module is inspired by multi-scale feature fusion in the Feature Fusion Single Shot Multibox Detector (FSSD) [16].",impact-revealing,providing context for feature interaction module design
3070,5c234870da562935fc1d4db3,0b6a531754e67379518a946f3a3cf685f59358cc,CritICs Critiquing Criticality in Mobile Apps,555044e045ce0a409eb518a1,A measurement study of google play.,"Software Proﬁling for Mobile Platforms: A number of software proﬁling frameworks have been proposed [35], [104]– [107] - studying library usage [35], [106], app-market level changes to the source/advertisement models, [104], [105], dynamic instrumentation mechanisms [107], developer side…###…A number of software proﬁling frameworks have been proposed [35], [104]– [107] - studying library usage [35], [106], app-market level changes to the source/advertisement models, [104], [105], dynamic instrumentation mechanisms [107], developer side debugging/optimizations [108], [109] etc.",other,acknowledge existing software profiling frameworks
2469,5aed14d617c44a4438158e20,9e788f1530af08a1f2140e6016fd4aeaa8b29033,Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform,573696ce6e3b12023e5ce95a,Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift,Batch normalization (BN) is a widely used technique to ease network training by normalizing feature statistics [19].,other,providing context on batch normalization technique
2131,,6c143cdde619b19129786b7444bb3053c3d6498a,A Node-Based Modeling Approach for the Continuous Dynamic Network Loading Problem,,,"###…and n i ω,p(t), which are, respectively, the position of the head of (ω, p) on link i at time t(εiω,p(t) = 0 if packet (ω, p) is not on the link) and the number of vehicles in (ω, p) on link i at time t, and V i(t) can be derived as
εiω,p(t) = ε ( V i(t− Δt), V i(t), εiω,p(t− Δt) ) (10)
niω,p(t)…###Substituting 4/γ i max with α yields V F i = (4 · Ci/γimax).###…in model structure is heavily dependent on both the assumptions made to obtain a solution for the problem, i.e., the discretization dimension, the size of discretization, and queuing, and the criteria that affect the computation of link loads and path–link traveling times, as summarized in [3].###In addition, the solution approach of the proposed methodology to a complete continuous-time DNL model is inspired by the optimization scheme in [3], the discretization scheme in [32], and the cumulative flow approach in [5].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1392,,9eb10021a808fed390db69a046101a6f74cbd50f,A System for Experiments with Dependency Parsers,,,"###In their further work (see (Nivre and McDonald, 2008)) the authors present a hybrid system that combines the two models.###Our focus on ensemble models for parsing is motivated by a number of works like (McDonald and Nivre, 2007) and our previous experiments with MaltParser and MSTParser (Simov et al., 2013) which have shown the usefulness of this technique.",impact-revealing,highlighting the motivation for using ensemble models in parsing
3942,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,57d063f1ac44367354296402,Elfen Scheduling: Fine-Grain Principled Borrowing From Latency-Critical Workloads Using Simultaneous Multithreading,Other works reduce coscheduled job interference [109–114] or schedule them in a machine characteristics-aware manner [115–118].,other,acknowledge existing methods for job scheduling
2496,5f50ba4291e01182e69239cb,20454697ea082975db2503a52418efb8f65b8ae6,clocs: camera-lidar object candidates fusion for 3d object detection,5c2c7a9217c44a4e7cf31224,Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object   Detection for Autonomous Driving,[18] and [19] explore using stereo images to generate dense point cloud and conduct object detection using that cloud.,other,reporting prior findings on stereo images and object detection
3300,573695886e3b12023e4a8d45,bd908f1aa7818412e8e09b8c17ee8324d2346dfe,Optimizing drug–target interaction prediction based on random walk on heterogeneous networks,55a4ad2165ceb7cb02d5d050,Drug-target interaction prediction by random walk on the heterogeneous network.,"Recent work has demonstrated the power of network-based approaches in drug discovery [1–3].###In this work, we apply a random walk-based link prediction algorithm based on Chen et al. [3] to a more extensive drug–target network and evaluated its performance using an external dataset.###First, we offer a general approach that takes the whole drug target network into account without sepa-rating protein categories, in contrast to the previous study [3].###It has been shown that the weight parameters w d and w t are robust among the prediction results [3].###The calculation of each of the transition matrix in discussed in Chen et al. [3].###This is the first time that the random walk-based method is evaluated using a binding assay dataset (cf. [3, 5]).###Because of these evidences, we here simply adopt the previously used value of 0.3 [3].",other,highlighting the application of network-based approaches in drug discovery
1060,,9692886ba8e2c9d8990b0505e9c67a696d9f28a7,A Closer Look into Transformer-Based Code Intelligence Through Code Transformation: Challenges and Opportunities,,,"###Due to the format similarity between source code and text [5], Transformer [6], an attention-based neural network architecture for learning textual semantics [7], is now widely used for source code representation learning [1], [2], [3], [4], and becomes a state-of-the-art architecture in several code intelligence tasks, including code completion [8], [9], code summarization [10], [11], and program repair [12].###Over the past few years, deep neural networks (DNNs) have been continuously expanding their real-world applications for source code intelligence tasks [1], [2], [3], [4].",impact-revealing,highlighting the widespread application and effectiveness of Transformer models in source code representation learning
94,5a73cb4d17c44a0b3035672d,7cd5d7f8295b219b029a4231ae5cffb261e00ebe,Early Active Learning with Pairwise Constraint for Person Re-identification,5c75697df56def97982a172b,Person Re-Identification By Unsupervised L(1) Graph Learning,"Following the assumption that visually similar images of a person have a high probability of sharing the similar representation features in re-id [9], this will make early active learning schema more suitable for re-id applications.###As discussed in [9], minimizing the pairwise constraint will force the similar representations to be close to each other.###The primary target of person re-identification (re-id) is to identify a person from camera shots across pairs of non-overlapping camera views, and research on this topic has attracted considerable attention in recent years [8,9,10,15,29].###It is a tough task even for humans to identify the same person in different camera views among a potentially huge number of imposters [9,20].",impact-revealing,highlighting the challenges and attention in person re-identification research
125,5c0495ae17c44a2c747019af,8e37a3b227b68953f8067215828dc8b8714cb21b,Boosting Adversarial Attacks with Momentum,5550417845ce0a409eb3b9b3,Explaining and Harnessing Adversarial Examples.,"For DNNs trained on a dataset with thousands of output categories such as the ImageNet dataset, finding targeted adversarial examples by only one model to fool a black-box model is impossible [12].###One-step gradient-based approaches, such as the fast gradient sign method (FGSM) [5], find an adversarial example x∗ by maximizing the loss function J(x∗, y), where J is often the cross-entropy loss.###The proposed methods alleviate the trade-off between the white-box attacks and the transferability, and act as a stronger attack algorithm than one-step methods [5] and vanilla iterative methods [9].###Beyond iterative gradient-based methods that iteratively perturb the input with the gradients to maximize the loss function [5], momentum-based methods accumulate a velocity vector in the gradient direction of the loss function across iterations, for the purpose of stabilizing update directions and escaping from poor local maxima.###With the knowledge of the structure and parameters of a given model, many methods can successfully generate adversarial examples in the white-box manner, including optimization-based methods such as box-constrained LBFGS [23], one-step gradient-based methods such as fast gradient sign [5] and iterative variants of gradient-based methods [9].###Although the decision boundaries are similar, they are unlikely the same due to the highly non-linear structure of DNNs.###Among many attempts [13, 3, 15, 10, 24, 17, 11], adversarial training is the most extensively investigated way to increase the robustness of DNNs [5, 10, 24].###Deep neural networks (DNNs) are challenged by their vulnerability to adversarial examples [23, 5], which are crafted by adding small, human-imperceptible noises to legitimate examples, but make a model output attackerdesired inaccurate predictions.",impact-revealing,highlighting the challenges and methods related to adversarial examples in deep neural networks
2380,5d0b003a8607575390fb4f6a,43d74cd04fb22bbe61d650861766528e369e08cc,An Encoding Strategy Based Word-Character LSTM for Chinese NER,5736960c6e3b12023e51f0a0,End-To-End Sequence Labeling Via Bi-Directional Lstm-Cnns-Crf,Ma and Hovy (2016) and Chiu and Nichols (2016) use CNN to capture spelling characteristics and Lample et al. (2016) use LSTM instead.,other,reporting prior findings on the use of CNN and LSTM for spelling characteristics
1776,,ba4e12a6085d0da695dbd5feb10a69b8de4dfd8d,Single case research designs,,,"###Replication is an integral part of scientific advancement and is necessary for garnering support for the operation of causal relations (Kazdin & Nock, 2003).###Although broad, case studies generally are similar in their inclusion of several primary characteristics (Kazdin, 2003).###…of coverage of this material is limited by the length of this chapter; the reader interested in more extensive coverage of each should consult several excellent sources on these topics (Barlow, Hayes, & Nelson, 1984; Barlow & Hersen, 1984; Franklin, Allison, & Gorman, 1997; Kazdin, 1982, 2003).",impact-revealing,emphasizing the importance of replication in scientific advancement
1106,,407c8d45152d5b7a56fbb3561fa9864c50282000,EgoSonics: Generating Synchronized Audio for Silent Egocentric Videos,,,"###We build upon the state-of-the-art generation capabilities of Latent Diffusion Models [20, 38, 44] to generate realistic audio tracks that are not only semantically meaningful to the visual content of videos but also synchronized to events in them (see Figure 1).###Diffusion Models [20, 38, 44] have demonstrated remarkable efficacy in a multitude of generative tasks, spanning image generation, audio synthesis, and video creation.",impact-revealing,highlighting the effectiveness and application of Latent Diffusion Models in generative tasks
3343,5f0d85c69fced0a24be4f019,6817b5d48cf8f665b680de07d23b91814d2923b1,Bouquet of Instruction Pointers: Instruction Pointer Classifier-based Spatial Hardware Prefetching,573698426e3b12023e70bf13,Best-Offset Hardware Prefetching,"We get marginal performance improvements with a 128 and 256 entry IP tables, corroborating with recent works [38] and [13] that use IP-stride at the L1 with 64 entries.###Offset prefetchers: Offset based prefetchers such as Best-offset Prefetcher (BOP) [38] and Sandbox [42] prefetcher explore multiple offsets.###Well-established and recent spatial L2 prefetchers (prefetchers that prefetch within a spatial region) [33], [13], [14], [38], [11], [45] have pushed the limits of data prefetching.",other,reporting performance improvements and corroborating with recent works
365,5e15adcb3a55ac47ab5b0b8c,aa63ac11aa9dcaa9edd4c88db18bec87e0834328,Graph Transformer Networks,5cd7fa07ced107d4c65bf2eb,Heterogeneous Graph Attention Network,"Previous works [37, 43] require manually deﬁned meta-paths and perform Graph Neural Networks on the meta-path graphs.###We used the GCN [19], GAT [33], and HAN [37] as GNN based methods.###Meta-Path [37] denoted by p is a path on the heterogeneous graph G that is connected with heterogeneous edges, i.e., v , where t l ∈ T e denotes an l -th edge type of meta-path.###Then conventional GNNs can operate on the transformed homogeneous graphs [37, 43].###Here, we test HAN on the selected sub-graphs whose nodes are linked with meta-paths as described in [37].###The metapath2vec [10] learns graph representations by using meta-path based random walk and HAN [37] learns graph representation learning by transforming a heterogeneous graph into a homogeneous graph constructed by meta-paths.",impact-revealing,describing the use of various GNN methods and their applications
3097,5e7232fe93d709897cfa3461,e39ec42bc0b393fd2d21e4fe9ae55d361e2a752b,A New Method of Fuzzy Support Vector Machine Algorithm for Intrusion Detection,58d82fb2d649053542fd19e4,A three-way decision making approach to malware analysis using probabilistic rough sets.,The enumerating sequences-based [17–19] methods are simple and e ﬃ cient to implement by removing system call parameters.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1629,,654605956618571c07340ac7c29d7af3180c891b,Reliable Fix Paterns Inferred from Static Checkers for Automated Program Repair,,,"###Fault Localization: for evaluation purposes, we apply diferent fault localization settings to the experiment of each research question, while the default setting of Avatar is to use the GZoltar framework with the Ochiai ranking metric.###For example, ELIXIR [56], PraPR [9] and GenPat [15] rely on the Ochiai technique to identify potential buggy statements, but more details about of-the-shelf fault localization techniques are not provided.###7.2/Ochiai.###To that end, we attempt to replicate two scenarios of fault localization used in APR assessments: the irst scenario assumes that the faulty method name is known [24] and thus focuses on ranking the inner-statements based on Ochiai suspiciousness scores; the second scenario makes no assumption on fault location and thus uses the default setting of Avatar.###1.1 + Ochiai GZoltar-0.###1.1 + Ochiai + prioritization) against the normal fault localization (normal FL, i.e., GZoltar-0.###In the framework, we leverage the Ochiai [1] ranking metric to actually compute the suspiciousness scores of statements that are likely to be the faulty code locations.###The usage of GZoltar and Ochiai reduces the comparison biases since both are widely used by APR systems in the literature.###CapGen [63] applies GZoltar and Ochiai to detect bug positions, but the exact version of GZoltar is not clariied.###7.2 and the Ochiai ranking metric) by Liu et al. [34] as Avatar.###1.1 + Ochiai + prioritization # ixed bugs correctness ratio # ixed bugs correctness ratio
Avatar 26/82 31.7% (26+2+6)/(82+0+8) 37.8%
refined FL
normal FL
0 1000 2000 3000 # patch candidates generated by AVATAR
Fig.###The research community has developed generate-and-validate repair pipelines [4, 16, 23, 25, 31, 62, 63, 68] where program test cases are leveraged not only for localizing the bug locations [1, 29, 52, 72] but also as the oracle for validating the generated patches [22, 37, 53, 67].###1.1 + Ochiai).###Overall, we straightforwardly rerank suspicious statements exposed by GZoltar + Ochiai by prioritizing statements �1, �2, and �3 over other suspicious statements for the fault localization of Avatar.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1272,,b37ee51baf478d43444b0ba4e30a7bfe13e99581,Treating Postpartum Depression: What Do We Know about Brexanolone?,,,"###Flowchart su arizing ethods and findings of rando ized controlled trial by Kanes et al. 2017 [21].###The specific dosage divided into hourly time periods is provided in Table 1 [21].###This study provided the first placebo-controlled clinical trial to support the use of allopregnanolone analog in the treatment of PPD; however, this trial was limited by a small sample size, a strict severe PPD definition of HAM-D ≥ 26, possible respondent fatigue in HAM-D assessment, and the short follow-up period (30 days) [12,21,28].###’s RCT [21], continued for 30 days with a contin-###’s RCT [21], continued for 30 days with a continuous 60 h infusion of brexanolone or a placebo administered to participants, with monitoring at specific intervals till hour 72 (post-infusion) and follow-ups on days 7 and 30.###[21] as me tioned above) of statistical significance in both the studies.###Flowchart summarizing methods and findings of randomized controlled trial by Kanes et al. 2017 [21].###As of now, a search of the literature for empirical evidence of brexanolone’s clinical assessment yielded three separate studies, consisting of a total of three randomized control trials (RCTs, Table 2) [14,21] and one proof-of-concept study [24].",impact-revealing,highlighting the significance of the first placebo-controlled clinical trial for allopregnanolone analog in treating PPD
419,599c7945601a182cd2629f72,000178cd12c8a6e5da8215b6365fae03c20fd18d,End-to-End Representation Learning for Correlation Filter Based Tracking,57a4e91dac44365e35c98be3,Fully-Convolutional Siamese Networks For Object Tracking,"Our starting point is a network similar to that of [3], which we later modify in order to allow the model to be interpreted as a Correlation Filter tracker.###We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s [3], Staple [2] and LCT [22].###Our baseline diverges slightly from [3] in two ways.###1Note that this differs from [3], in which the target object and search area were instead denoted z and x respectively.###For our method, we prefer to build upon the fully-convolutional Siamese architecture [3], as it enforces the prior that the appearance similarity function should commute with translation.###Recently, several methods based on Siamese networks have been introduced [28, 12, 3], raising interest in the tracking community for their simplicity and competitive performance.###Recent works have focused on learning deep embeddings that can be used as universal object descriptors [3, 12, 28, 17, 5].###Similar to [3], we employ a simplistic tracking algorithm to assess the utility of the similarity function.###Notice that the forward pass of the architecture in Figure 1 corresponds exactly to the operation of a standard CF tracker [13, 6, 22, 3] with CNN features, as proposed in previous work [21, 7].###We follow the procedure of [3] to minimize the loss (equation 2) through SGD, with the Xavier-improved parameters initialization and using mini-batches of size 8.###We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s [3], Staple [2] and LCT [23].",impact-revealing,providing context for the methodology and comparisons with existing trackers
3862,5f8d6be69fced0a24bbaaf7b,6427b12aa3ddb4c89b7879c43267cd4a9f0ad1c7,DE-RRD: A Knowledge Distillation Framework for Recommender System,57d063b4ac4436735428dc44,Discrete Collaborative Filtering,"adopted hash techniques to reduce the inference cost [10, 15, 16, 30].###However, a growing scale of users (and items) and sophisticated model architecture to capture complex patterns make the size of the model continuously increasing [13, 25, 28, 30].###Recently, the size of the recommender model is continuously increasing, and the computational time and memory cost required for the inference are also increasing accordingly [13, 25, 28, 30].",other,highlighting the increasing complexity and resource demands of recommender models
3129,5b3d98cc17c44a510f802054,8a564ee07fa930ebc1176019deacdc9951063a99,Collaborative Learning for Deep Neural Networks.,5aed14d117c44a4438158a3b,Large scale distributed neural network training through online distillation.,Co-distillation of two instances of the same neural network is studied in [2] with a focus on training speed-up in a distributed learning environment.,other,reporting prior findings on co-distillation in neural networks
3028,5f7fdd328de39f0828397c88,edcb65ea0954067d9137599423790fbd331de7b3,how hard is to distinguish graphs with graph neural networks,5c8a11324895d9cbc6121c34,Neural Message Passing for Quantum Chemistry.,"[7], MPNN has been extended to include edge [8] and global features [9].###Communication capacity is an effective generalization of the previously considered product between depth and width [23], being able to consolidate more involved properties, as well as to characterize MPNN with global state [8, 9, 28] and adaptive architecture [29–32].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1389,,a687a739572b9f06f65f75efa5780553a98925d6,Synchronization power depends on the register size,,,"###The key to the proof is a novel method of reducing a multi-valued decision task with limited size compare&swap registers to the set-consensus problem [3] with read/write registers, allowing us to build on the recent powerful impossibility results of [2, 9, 18].###From [2, 9, 18] we know that the l-set consensus problem cannot be solved with atomic registers for l < n.###The reduction actually emulates a full-information version of algorithmB [2, 7, 9, 18].",impact-revealing,highlighting a novel method for solving a multi-valued decision task
597,5cb06564ced107d4c6006f1c,19351711295bddc627f761d59a1ef58ab2fa7e2c,Identifying SDC-Causing Instructions Based on Random Forests Algorithm,53e9b77db7602d9704323f58,SmartInjector: Exploiting intelligent fault injection for SDC rate analysis,"We extract features of instructions according to our analysis and prior work [12, 13, 16, 17 and 18].###SmartInjector [12] proposes an intelligent fault injection framework to identify the SDC-prone instructions.",impact-revealing,reporting prior findings on intelligent fault injection framework
1905,,e8cd869056f9e52d8b77a90c8843d11a34995a07,Interrelation between Pedagogical Design and Learning Interaction Patterns in different Virtual Learning Environments,,,"###Teacher presence is the connecting element of the CoI model [1].###The model of this Community of Inquiry assumes that learning occurs within the Community through the interaction of three core elements [1] CoI model core elements are three presences: Cognitive presence, Social presence and Teacher presence and each of the presences contain hierarchies.###Based on the review of several similar studies [1,2,23,24] with the same coding template, we chose the whole message as a unit of analysis.###The study used content analysis based on the coding template, which is was validated by several studies [1,2,23,24].###Within the model of the communities of inquiry different types of interactions are crucial for the learning [1,9,19].###Cognitive presence is the basic element for success of educational experience [1].###The typology of the pedagogical design and interaction patterns will be based on the Communities of Inquiry [1,2,3,4].",impact-revealing,providing context for the Community of Inquiry model and its elements
3576,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e9a26bb7602d9702b74894,Combining Final Score With Winning Percentage By Sigmoid Function In Monte-Carlo Simulations,See also the use of the history heuristic for improving simulation estimates (6.1.5).,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2174,,440cf298cd78de46c07865fb24f436a6cb29f0c8,Cloud Separation: Stuck Inside the Cloud,,,###Cloud Computing builds on diﬀerent forms of distributed computing tying together distributed computing and virtualization [1].###Cloud Computing is a fast growing industry and is becoming part of most enterprises [1].,impact-revealing,highlighting the significance and growth of Cloud Computing in enterprises
3957,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,53e9bb5ab7602d970479a5b1,Communities of interest for internet traffic prioritization,[59] proposed a heuristic method to differentiate wanted and unwanted traffic based on the sampled NetFlow data.,other,reporting prior findings on heuristic methods for traffic differentiation
2950,5ee8986891e011e66831c3b6,38f93092ece8eee9771e61c1edaf11b1293cae1b,Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,573696136e3b12023e5258e2,Colorful Image Colorization,"Linear evaluation on ImageNet We first evaluate BYOL’s representation by training a linear classifier on top of the frozen representation, following the procedure described in [48, 74, 41, 10, 8], and appendix D.###In particular, relative patch prediction [23, 40], colorizing grayscale images [41, 42], image inpainting [43], image jigsaw puzzle [44], image super-resolution [45], and geometric transformations [46, 47] have been shown to be useful.",other,reporting various evaluation methods for image representation
2529,5dcbd5da3a55ac789b0dbc7f,f0efc23ecb6d4fb9745d555450b2c4a97e8ac4d5,Robust Design of Deep Neural Networks Against Adversarial Attacks Based on Lyapunov Theory,5c04967517c44a2c74708fc4,Generalizable Adversarial Training via Spectral Normalization.,"[11] uses PAC-Bayes generalization analysis to estimate the robustness of DNNs trained by spectral regularization against adversarial attacks.###Previous works [11, 28, 37, 8] have used a penalty on the spec-###[11] for instance, obtained an accuracy of 62% at ✏ = 0.###All the previous works on this subject [11, 28, 37, 8], keep constant across layers.###The closest to our work are the results given in [11, 28, 37, 8].",other,reporting prior findings on PAC-Bayes generalization analysis
164,5a9cb65d17c44a376ffb83f3,e06357ac23811054acb23e2ea30d087f5beaef90,an interpretable reasoning network for multi-relation question answering,57a4e921ac44365e35c9930e,Key-Value Memory Networks for Directly Reading Documents.,"Recent reasoning models (Miller et al., 2016; Wang et al., 2017) mainly concentrate on Reading Comprehension (RC) which requires to answer questions according to a given document.###MemNN [Weston et al., 2015] and KVMemN2N [Miller et al., 2016] transferred the reading comprehension framework to QA where a set of triples is treated as a document and a similar reasoning process can be applied.###IRN is better than MemN2N and KVMemN2N on most datasets, and both models are much better than other baselines with path information.###, 2015), KVMemN2N (Miller et al., 2016) and EviNet (Savenkov and Agichtein, 2017) transferred the reading comprehension framework to QA where a set of triples is treated as a document and a similar reasoning process can be applied.###KVMemN2N [Miller et al., 2016] improves the MemN2N for KBQA as it divides the memory into two parts: the key memory stores the head entity and relation while the value memory stores the tail entity.###KVMemN2N (Miller et al., 2016) improves the MemN2N for KBQA as it divides the memory into two parts: the key memory stores the head entity and relation while the value memory stores the tail entity.",impact-revealing,highlighting advancements in reasoning models for reading comprehension and QA
431,5eb3df3191e011cea6a7c3c8,86746ca4e3fb61cbcfb8dc29d6779d51b03692e0,Effect of Character and Word Features in Bidirectional LSTM-CRF for NER,5d9edc1647c8f76646032985,Named Entity Recognition with Bidirectional LSTM-CNNs,"This paper, we combined two techniques proposed in Chiu et al., 2015 (CNN + Bidirectional LSTM) and Huang et al., 2015 (Bidirectional LSTM + CRF), respectively, using public word embedding, character features and word features [6,4].###A combination of word embedding with word and character features is proven to be reliable for increasing the accuracy of NER system by Chiu et al., 2015 [6].",impact-revealing,reporting a combination of techniques for improving NER accuracy
1051,,0f391ea799eff6ecf042d16fcfae0133605d8445,MacroBase: Analytic Monitoring for the Internet of Things,,,"###Base, dataﬂow is a means to an end rather than an end in itself. In designing a specialized engine, we were inspired by several past projects, including Gigascope (specialized for network monitoring) [28], WaveScope (specialized for signal processing) [36], MCDB (specialized for Monte Carlo-based operators) [52], and Bismarck (providing extensible aggregation for gradient-based optimization) [33]. In ",impact-revealing,drawing inspiration from past projects for engine design
1240,,09437ecf936af7b03b845d129e0fcdf1e2b2aa9b,"Alcohol withdrawal hallucinations in the general population, an epidemiological study",,,"###Furthermore, self-report (here, of psychosis)usually has good specificity but low sensitivity(Perälä et al., 2007), and is limited by factors such as recall bias, lack of laboratory markers, and lack of validity measures(Stephane et al., 2006).###Our data show over 2.2% lifetime prevalence of AWH in the general population—a prevalence twice as high as that of schizophrenia(van Os and Kapur, 2009), and about third of that of all psychotic disorders combined(Perälä et al., 2007).",impact-revealing,highlighting the prevalence of auditory verbal hallucinations in the general population compared to schizophrenia
2769,5c04967517c44a2c74708b7e,c18663fea10c8a303d045fd2c1f33cacf9b73ca3,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,5a260c8117c44a4ba8a30771,Squeeze-and-Excitation Networks.,"…where researchers compete to create the most accurate model that classiﬁes given im-Figure 1: Strong correlation between top-1 accuracy on ImageNet 2012 validation dataset and model size for representative state-of-the-art image classiﬁcation models in recent years [49, 50, 23, 54, 24, 57, 45].###The winner of 2017 ImageNet challenge went to Squeeze-and-Excitation Networks [24], which achieved 82 .",other,highlighting competition and advancements in image classification models
1201,,2076543d6af56c35c1d74a407d97576e42c12c00,Sequential sum-of-squares programming for analysis of nonlinear systems⋆,,,###We prove local convergence of the sequence of convex problems using a result from variational analysis [41] that builds upon the implicit function theorem for strongly regular generalized equations by Robinson [42].,impact-revealing,reporting a mathematical result
3764,58437722ac44360f1082f160,8aa3358a34a17abd0a65622aad8c85317b851af4,very deep convolutional networks for end-to-end speech recognition,53e9aef1b7602d970391c7bd,Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups,"Unlike Deep Neural Networks (DNNs) [12], CNNs explicitly exploit structural locality in the spectral feature space.",other,highlighting the differences between DNNs and CNNs in exploiting structural locality
2310,53e9b54ab7602d97040825b6,e4fc3adca44206ecb5dc2c4960e578fe2d0994fe,Secure Untrusted Data Repository (SUNDR),53e998bfb7602d97020f9f12,Pastry: Scalable distributed object location and routing for large-scale peer-to-peer systems,"Several new distributed hash tables such as Chord [26] and Pastry [24] show the potential to scale to millions of separately administered volunteer nodes, with CFS [5] layering a read-only file system on top of such a highly distributed architecture.",other,highlighting the scalability potential of new distributed hash tables
4005,5eabf3cd91e011664efc496f,916595fc4e701ac6125725408912a1fac3d7a60b,CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows,5736982b6e3b12023e6fd299,Long Term Parking (Ltp): Criticality-Aware Resource Allocation In Ooo Processors,"In [5], [56], instructions dependent on long-latency operations are temporarily kept in a small buffer until their dependences are resolved.###For decades, researchers have tried to make an OoO core more energy efﬁcient by addressing the complexity of the scheduling logic [2], [4] or reducing the accesses to power-hungry structures [5], [6], [7], [8].",other,acknowledging ongoing research efforts in energy-efficient scheduling for OoO cores
1624,,d31b4758a1affc51db41665da9ac20d9d6aa50b7,Can Automated Program Repair Refine Fault Localization?,,,"###For example, pioneering spectrum-based fault localization (SBFL) techniques [3, 12, 22] compute the code elements covered by more failed tests or less passed tests as more suspicious, pioneering mutation-based fault localization (MBFL) techniques [42, 46, 72] inject code changes (e.###Therefore, besides our default Ochiai [3] formula, all the other formulae in SBFL can be adopted in ProFL.###Note that, SBFL and Metallaxis can adopt different SBFL formulae, and we by default uniformly use Ochiai [3] since it has been demonstrated to perform the best for both SBFL and MBFL [31, 48, 73].###, the Ochiai [3] SBFL technique is leveraged in many recent program repair techniques, including PraPR [17], CapGen [64], and SimFix [21].###of techniques, fault localization [3, 12, 22, 33, 42, 71, 72] and program repair [24, 28, 29, 35, 36, 52, 53, 63].###, the default Ochiai [3]) to compute the suspiciousness for each statement, e.###, Tarantula [22], Ochiai [3], and Ample [12]) or learning techniques [7, 54–###, Ochiai [3]) at the statement level, and then perform suspiciousness aggregation [59] to calculate the initial suspiciousness value for each program element.###We attempted to improve the effectiveness of traditional SBFL based on Ochiai formula [3], which has been widely recognized as one of the most effective SBFL formulae [31, 48, 73].",impact-revealing,Highlighting the effectiveness of the Ochiai formula in fault localization
1621,,f6d93eb5e4dd8893ca6ee77ced2bf934db431f6b,Seeing the Whole Elephant: Systematically Understanding and Uncovering Evaluation Biases in Automated Program Repair,,,"###Then, based on the spectrum, SFL uses various ranking metrics (e.g., Ochiai [1]) to calculate the suspiciousness score for each statement.###For the concrete ranking metric, we employ Ochiai [2], which is shown to be an effective metric [2, 84] and widely adopted by existing APR tools [26, 59, 61, 128].",impact-revealing,describing the method for calculating suspiciousness scores
914,53e9afd3b7602d9703a24a4b,10c1bfa7fe190b3fc6d64c4909cb7ef0911a7b90,Dead-block prediction & dead-block correlating prefetchers,53e9a55cb7602d9702e82bac,"Selective, accurate, and timely self-invalidation using last-touch prediction","The key intuition behind why a larger history depth increases an MCP’s accuracy and coverage is that while data structures are often referenced in multiple distinct program contexts or phases [7] (e.###These results indicate that the potential for trace-based predictors to predict memory system events is beyond just predicting memory invalidation and sharing for scientific applications in multiprocessors [7].###Due to control flow irregularities in applications, multiple cache blocks may have dead-block signatures that are proper subsequences of each other resulting in subtrace aliasing [7].###In a recent paper [7], we proposed trace-based predictors that record a trace of shared memory references to predict a last reference to a cache block prior to an invalidation in a multiprocessor.###In a recent paper [7], we proposed Last-Touch Predictors (LTPs) to predict memory invalidations for shared data in a multiprocessor.###We use truncated addition (as before [7]) to maintain a fixed-size encoding for every instruction trace.",impact-revealing,highlighting the significance of larger history depth in improving MCP accuracy and coverage
811,5c20b1fcda5629702063afe6,bb221fa131ac1aa89857b7dca2117bc0e70e32cc,Performance Improvement by Prioritizing the Issue of the Instructions in Unconfident Branch Slices,53e9b1f8b7602d9703c8e414,Power- and Complexity-Aware Issue Queue Designs,A comprehensive survey was carried out by Abella et al [28].,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2847,5e5e18a493d709897ce22b32,3345443925cec95381c2cf2f0b029c411945bfef,GraphSAINT: Graph Sampling Based Inductive Learning Method,5a9cb66717c44a376ffb8667,FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling,"To mitigate such “neighbor explosion”, state-of-the-art methods [14, 6, 5, 33, 18] use various layer sampling techniques.###The work in [18] improves [6] by adding an additional sampling neural network.###Point (2) improves accuracy and robustness on sparse graphs and deep nets by avoiding the overly sparse minibatches of [6] (see Section 3.###In order to scale GCNs to large graphs, layer sampling techniques [14, 6, 33, 5, 9, 18] have been proposed for efficient minibatch training.###Point (3) reduces training time by decreasing the number of sampler invocations (compared with [6, 18]), and the cost of each invocation (compared with [18]).###Current works on Graph Convolutional Networks (GCNs) [14, 6, 9, 18, 5] mostly focus on shallow models (2 layers) on relatively small graphs.###Under the independent layer sampling assumption of [6], one would sample a connection ( u, v ) with probability p (l) u,v ∝ 1 deg(u) + 1 deg(v) .###This sampler is similar to the layer sampler in [6].###FastGCN [6] performs sampling from another perspective.###The works in [6, 18] further propose samplers that ensure a fixed number of samples in all layers, which potentially restricts the neighbor expansion factor to 1.",other,acknowledge existing methods for neighbor explosion mitigation in GCNs
77,5edf5dd891e011bc656deed4,ae0988c55095827384bf70e346e192a89216e75c,IterefinE: Iterative KG Refinement Embeddings using Symbolic Knowledge,53e997f5b7602d9701ff9c47,Knowledge Graph Identification,"They can also make use of ontological rules effectively, and specifically, the PSL-KGI implementation uses rules defined on schema-level features [Pujara et al., 2013].###We reproduce the list of information used in [Pujara et al., 2013] in tabular form in Table 1.###NELL: The NELL subset taken from its 165th iteration [Carlson et al., 2010]) has been used for the KG refinement task [Pujara et al., 2013, Jiang et al., 2012].###We have looked at the KG refinement task and methods for the same, from probabilistic rule based methods like PSL-KGI [Pujara et al., 2013] to KG embedding methods like type-ComplEx [Jain et al., 2018].###We have looked at the KG refinement task and methods for the same, from probabilistic rule based methods like PSL-KGI [Pujara et al., 2013] to KG embedding methods like type-ComplEx [Jain et al.###Of these methods, PSL-KGI [Pujara et al., 2013, 2017] is shown not only to perform better with KG noise and sparsity, but also to be quite scalable.###2) for added noisy facts [Pujara et al., 2013].###To capture realistic KG refinement settings, we further add extraction scores generated by sampling them from two different normal distributions: N(0.7, 0.2) for facts in the original KG and N(0.3, 0.2) for added noisy facts [Pujara et al., 2013].###We use a single hyper-parameter threshold as the cutoff for classifying a test triple based on the prediction score [Pujara et al., 2013].###…australia and austalia), incorrect relationships –both due to wrong relation label as well as incorrect linkage altogether– between entities (e.g., 〈matt flynn, athleteplayssport, baseball〉 is false since Matt Flynn is an NFL player), incompatible entity types, and many more [Pujara et al., 2013].###They can also make use of ontological rules effectively, and specifically, the PSL-KGI implementation uses rules defined on schema-level features [Pujara et al., 2013].
ar X
iv :2
00 6.###An important input to these formulations are the probabilistic sources of information such as the confidence scores obtained during extraction [Pujara et al., 2013, Jiang et al., 2012] from multiple sources.###, 〈matt flynn, athleteplayssport, baseball〉 is false since Matt Flynn is an NFL player), incompatible entity types, and many more [Pujara et al., 2013].",impact-revealing,reporting on the use of ontological rules and methods in knowledge graph refinement
3203,58437722ac44360f1082f15c,cc16e43cce64b649da00892d1493425620c2d61c,Learning to Match Using Local and Distributed Representations of Text for Web Search.,5736977f6e3b12023e665d1c,Search Retargeting Using Directed Query Embeddings,"Other papers incorporating word embeddings include [10, 11, 34].###In IR, a significant number of these works have focused on word embeddings [6, 8, 10, 11, 27, 28, 34, 41] and modelling short-text similarities [15, 16, 29, 35–37].",other,acknowledge existing research on word embeddings and their applications
3044,5dea04309e795e693620e97c,b0c35bf9ddffefb0dab4f76c20b30e00a22b1e0a,unsupervised author disambiguation using heterogeneous graph convolutional network embedding,57aa28de0a3ac518da9896d5,node2vec: Scalable Feature Learning for Networks,"DeepWalk [16] and Node2Vec [29] use random walk strategy on network and skip-gram [30], [31] model to learn the representation of each node in network.",other,describing methods for learning node representations in networks
3542,5f75aa6a9fced0a24b64599d,69fb130409d48478fab0c4545dc2ff102a2630ef,The forward slice core microarchitecture,53e9af0db7602d9703945fad,Understanding The Backward Slices Of Performance Degrading Instructions,"Proposals such as speculative-slice execution [23], flea-flicker multi-pass pipelining [3], braid processing [22] and OUTRIDER [6] also exploit critical instruction slices [24] for improving performance.",other,acknowledge existing proposals for performance improvement
3989,5b67b4b917c44aac1c867dbc,d18b48f77eb5c517a6d2c1fa434d2952a1b0a825,hierarchical graph representation learning with differentiable pooling,53e9a018b7602d97028fa3e5,Automatic generation of complementary descriptors with molecular graph networks.,"GNNs have been applied to a wide variety of tasks, including node classification [16, 21], link prediction [31], graph classification [7, 11, 39], and chemoinformatics [14, 19, 27, 28, 32].",other,reporting applications of GNNs in various tasks
954,,9c0c6fcfeb9eee6f5e5025950597db972894aaf4,Nutrition Management of Gastrointestinal Symptoms in Children with Autism Spectrum Disorder: Guideline from an Expert Panel.,,,"###Evidence suggests children with ASD have a fivefold increase in problematic eating and feeding behaviors compared with typically developing peers.(14) Food selectivity, defined as a limited food repertoire (eg, only eating a few foods and/or rejection of one or more food groups) or high intake of a single food,(15) is the most frequently documented feeding issue associated with ASD.###In doing so, the project represents a critical first step toward developing standards of care that take into consideration the unique combination of dietary restriction and related medical/nutrition concerns in this population.(14) As emphasized by the algorithm, nutrition management in ASD should involve a tiered approach.###DESCRIPTION OF THE ALGORITHM Evaluation and intervention recommendations were designed to assist clinicians with navigating potential barriers associated with food selectivity, caregiver-initiated complementary/alternative diet therapies, and/or nutritional deficits/excesses often observed in this population.(14,37) The committee also emphasized the importance of a multidisciplinary approach during assessment and intervention to elucidate the causal relationship between nutritional intake and possible organic etiology, as well as provide complementary treatment avenues in cases involving severe food selectivity and/or lack of response to dietary intervention.###Finally, RDNs should assess a child’s possible behavioral response (eg, tantrums, aggression) to change in the meal, a consideration that should be foremost in the minds of clinicians when planning intervention, given the ubiquity of feeding problems in this population.(14) A practice guideline of this nature reflects a more general need to further elucidate the role of nutrition management in ASD.###In addition, food selectivity increases the risk of nutrition and/or medical concerns in ASD, including significant specific deficits (eg, lower intake of calcium and protein) and a higher number of overall nutritional deficits.(14) This risk, however, may go undetected in pediatric settings without a detailed examination of nutrient intake because it does not necessarily translate into compromised growth or decreased energy intake, which typically trigger attention in pediatric settings.",impact-revealing,highlighting the significance of nutrition management in children with ASD and the need for a multidisciplinary approach
3683,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,53e9a102b7602d97029ee8a5,Discriminative model fusion for semantic concept detection and annotation in video.,"[57, 58] Audio (MFCC), video (DCT of the face region) and the synchrony score Monologue detection, semantic concept detection and annotation in video###Features from visual, audio and caption tracks in TRACVID datasets are extracted and used in fusion for various multimedia analysis tasks, such as video shot retrieval [83], semantic video analysis [121], news video story segmentation [52], video concept detection [58, 143] and so on.###[58] for semantic concept detection and annotation in video, Lucey et al.###For example, NIST average precision metric is used to determine the accuracy of semantic concept detection at the video shot level [58, 121, 143].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1930,,b97eb76941bf2bf281ea89e82c2f99485cfbcbf0,"Mucoadhesive vaginal tablets as veterinary delivery system for the controlled release of an antimicrobial drug, acriflavine",,,"###It owns biocompatibility and biodegradability [18] and is widely used as a pharmaceutical excipient [19].###Furthermore, it is known that chitosan reacts with polyanions, such as sodium alginate, resulting in a gel [19].",impact-revealing,providing context on the properties and applications of chitosan
2091,,910c83a6b4785c254910850085bb91dd5ec6b3be,Abelian Noncyclic Orbit Codes and Multishot Subspace Codes,,,"###See [5] for more detailed information.###The method for constructing multishot subspace codes proposed in [23] is inspired by the so-called multi-level construction given by [5] for block-coded modulation schemes, originally proposed by Imai and Hirakawa in [15].",impact-revealing,providing context for a method
1765,,386335e5853b6fe8b552c42bd64b0c75d1287fb4,A Multi-Functional In-Memory Inference Processor Using a Standard 6T SRAM Array,,,"###Reusing data read from external memory (data reuse) [12], [13] is highly effective in saving energy as shown by DianNao [12] and Eyeriss [1], [14], but results in on-chip memory access still accounting for 35% to 45% of total energy, and does not address the energy and delay of SRAM reads.###effective in saving energy as shown by DianNao [12] and Eyeriss [1], [14], but results in on-chip memory access still###As a result, a number of integrated circuit (IC) implementations of ML kernels and algorithms have appeared recently [1]–[10] to address the problems of designing energyefficient ML systems in silicon.###2782087 systems such as deep neural networks [1], [12].",impact-revealing,highlighting the effectiveness of data reuse in energy savings while noting its limitations
2034,,59b2e8136cfc70076647106fd545f2820655771c,Preparation and cellular uptake behaviors of uniform fiber-like micelles with length controllability and high colloidal stability in aqueous media,,,"###Although the cellular internalization mechanism is still unclear, previous results indicated that stiﬀ ﬁber-like NPs only can be internalized by macrophages when the contact point is at the end of NPs [7] .###…delivery vectors is dependent on the chemical structure and composition, surface chemistry, size and shape of NPs. Diﬀerent from spherical NPs widely used for drug delivery, ﬁber-like NPs can mimic ﬁber-like natural bacteria and viruses, aiming to obtain unique beneﬁts over spherical NPs [7–9] .###Intriguingly, ﬁber-like NPs indeed exhibit several advantages over corresponding spherical NPs in some cases, such as longer circulation time in bloodstream [ 10 , 11 ], higher eﬃciency in tumor penetration [ 12 , 13 ], lower phagocytosis [ 7 , 14 , 15 ], and faster renal clearance [10] .",impact-revealing,highlighting advantages of fiber-like nanoparticles over spherical nanoparticles
394,5a260bfb17c44a4ba8a1c61e,a55970013b984f344dfbbbba677d89dce0ba5f81,Image Super-Resolution via Deep Recursive Residual Network,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"where x̂ is the output of the residual unit, h(x) is an identity mapping [8] : h(x) = x, W is a set of weights (the biases are omitted to simplify notations), function σ denotes ReLU, F(x,W ) is the residual mapping to be learned, and U denotes the function of the residual unit structure.###In ResNet [8], the basic residual unit is formulated as Eq.###Further, very deep networks could suffer from the performance degradation problem, as observed in visual recognition [8] and image restoration [17].###The main idea of ResNet [8] is to use a residual learning framework to ease the training of very deep networks.###1 overviews DL-based SISR, this section focuses on three most related work to ours: ResNet [8], VDSR [13] and DRCN [14].###Strategies used in ResNet [8], VDSR [13], DRCN [14] and DRRN.###Simplified structures of (a) ResNet [8].###Noted that in ResNet [8], different residual units use different inputs for the identity branch (green dashed boxes in Fig.###Inspired by the success of very deep networks [8, 27, 28] on ImageNet [21], Kim et al.",impact-revealing,highlighting the significance of ResNet in training deep networks
1900,,0673fe2941f5011d5e376162aceafc5a098859a2,Indigenizing Curriculum Development and Online Course Design: a Caribbean Study,,,"###…presence, learner presence) not examined in this study point to the continuous evolution of CoI; however, the focus of this study will be the Community of Inquiry framework and its three presences as introduced by Garrison et al. (2000) and validated by a wide range of subsequent studies.",impact-revealing,acknowledge the foundational framework of the Community of Inquiry and its validation
1391,,a638f4e1d36c7cae05d950fce38fe6fffed90044,Improving Quality and Efficiency in Plan-based Neural Data-to-text Generation,,,"###This mechanism is inspired by transition based parsing (Nivre and McDonald, 2008).",impact-revealing,providing context for the mechanism used
1046,,67b2918c1804a76552002aa2ea6f35e0722d3b8f,Algorithmic Self-Assembly of DNA Sierpinski Triangles,,,"###How crystal morphology and patterning can be programmed by tile design in an inherently asynchronous assembly process is addressed by the abstract Tile Assembly Model (aTAM) (Winfree 1996, 1998a).###This work inspired a theoretical proposal (Winfree 1996) that builds on Wang's (1961, 1962) embedding of computation in geometrical tilings to show that twodimensional (2D) self-assembly of DNA can perform Turinguniversal computation—which implies that any algorithm can in principle be embedded in,…###(Winfree 1996) that builds on Wang’s (1961, 1962) embedding###DAE-E and DAO-E molecules (Winfree 1996), resulting in two",impact-revealing,highlighting the influence of prior work on theoretical proposals in DNA self-assembly
1788,,35bd5247616fd21526cee5cf28b9db07f96b574b,Causation-Driven Visualizations for Insurance Recommendation,,,"###Data visualization methods are widely used for recommendation systems in many fields, such as finance [1], education [2], also fostering research in academia [3, 4].",impact-revealing,highlighting the application of data visualization methods in various fields
2455,5b3d98cc17c44a510f80212a,cb91c2f8d3cac0b655a39be318b603334eb18987,learning to optimize tensor programs,58d83045d649053542fe8b34,Lift: a functional data-parallel IR for high-performance GPU code generation.,"There are many domain-specific languages (DSLs) for code generation [32, 36, 15, 37, 20, 30], each with with a different E , Se and g.",other,acknowledge variations in domain-specific languages for code generation
1328,,b40f6faebe9ad6c91c915feb2a2b1d762b6e1cf8,Urban health: Access to health care for vulnerable patients in the context of migration and detention,,,###Pap test underuse corresponded to a lack of lifetime screening in many Latin American countries and underscores the need for language- and culturally-appropriate education.(35;36),impact-revealing,highlighting the significance of culturally-appropriate education for Pap test screening
3006,5aed14e217c44a4438159868,d3707cf521e3596313af1f53acba6413d0d528a6,Training Tips for the Transformer Model,57a4e91aac44365e35c979f6,Optimization Methods for Large-Scale Machine Learning,", 2017) and also theoretical explanations (Bottou et al., 2016; Smith and Le, 2017; Jastrzebski et al., 2017). Smith and Le (2017) interpret SGD (and its variants) as a stochastic differential equation and show that the gradient noise scale g = ε ( N B − 1 ) , where ε is the learning rate, N is the training set size, and B is the effective batch size. This noise “drives SGD away from sharp minima, and therefore there is an optimal batch size which maximizes the test set accuracy”. In other words for keeping the optimal level of gradient noise (which leads to “flat minima” that generalize well), we need to scale the learning rate linearly when increasing the effective batch size. However, Hoffer et al. (2017) suggest to use √ k scaling instead of the linear scaling and provide both theoretical and empirical support for this claim.###, 2017) and also theoretical explanations (Bottou et al., 2016; Smith and Le, 2017; Jastrzebski et al., 2017). Smith and Le (2017) interpret SGD (and its variants) as a stochastic differential equation and show that the gradient noise scale g = ε ( N B − 1 ) , where ε is the learning rate, N is the training set size, and B is the effective batch size.###by Bottou et al. (2016). Thus our initial hypothesis was that 0.###, 2017) and also theoretical explanations (Bottou et al., 2016; Smith and Le, 2017; Jastrzebski et al., 2017).",other,discussing theoretical insights and findings related to SGD and its implications for batch size and learning rate
2277,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,5736982b6e3b12023e6fd21d,Microarchitectural Implications Of Event-Driven Server-Side Web Applications,"Many works tune individual server knobs, such as selective voltage boosting [98–100], exploiting multicore heterogeneity [101–103], trading memory latency/bandwidth [104–107], or reducing front-end stalls [70, 96, 108].",other,acknowledge various tuning methods in server optimization
1954,,1ee656b84bcce7c0fa8b1ce83be2ee251995ed54,Seagull -- A Real-Time Coflow Scheduling System,,,"###But improvements can not lead to a higher performance of data-parallel applications [13], because they are agnostic to flow dependency semantics from data-intensive tasks.###Coflow-level scheduling: Researchers [13]–[17] recently demonstrated the importance of semantics dependency in flows of data-parallel application.###It’s proved by many related works that minimizing average coflow completion time (CCT) results in a better performance of data-parallel applications [13], [15]–[17].###The main idea is the same as Varys [13].###Take the SEBF (Smallest-Effective-Bottleneck-First) and MADD (Minimum-Allocation-for-Desired-Duration) algorithm in Varys for example [13].",impact-revealing,highlighting the importance of semantics dependency in data-parallel applications
1731,,38431764dc980aa9318651145cc08d7e51c4183d,Rise and fall of the two visual systems theory.,,,"###The text from the original article also described the absence of motor errors when patients visualized the target in foveal vision (Perenin and Vighetto 1988, Table 9.4), and the existence of errors for perceptual responses (see Rossetti et al 2010, Figure 10.2).###We can observe that the figure reproduced by Milner and Goodale in their book (1995, part 4.2.1.) takes out the central areas (CVF) of these figures (from Perenin and Vighetto 1988).###First of all, a critical data figure reproduced by Milner and Goodale (1995), to illustrate the proposed dissociation between perception and action, comes from works by Vighetto and Perenin (Vighetto 1980; Perenin and Vighetto 1988).###Patients who were asked to insert the hand in a slit with a variable orientation did not direct their hand towards the location of this visual goal, but also did not rotate their wrist to the correct orientation (Vighetto, 1980; Vighetto and Perenin 1981; Perenin and Vighetto 1988).###Optic ataxia The effects of neurological lesions involving a limited part of the posterior parietal cortex were described in groups of patients by Jeannerod (1986a) and Perenin and Vighetto (1988).###Patients with optic ataxia make substantial errors only in the peripheral visual hemifield, validated by pointing and grasping tasks (Vighetto 1980; Perenin and Vighetto 1988; Revol et al 2003; Milner et al 1999, 2003; Rossetti et al 2003, 2005), whereas the absence of perceptual impairments has…###…of the figure presented by Milner and Goodale selectively omitted the central panel, which showed that patients made hardly any errors in the central part of their visual field, except for one patient (n° 3) who presented with multiple associated impairments (Perenin and Vighetto 1988, Figure 4).",impact-revealing,discussing critical findings related to optic ataxia and visual perception
3531,5a9cb66717c44a376ffb89eb,651adaa058f821a890f2c5d1053d69eb481a8352,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,58437722ac44360f1082f4d8,Towards Evaluating The Robustness Of Neural Networks,"Much of what we describe below has been discussed in prior work (Carlini & Wagner, 2017a; Madry et al., 2018); we repeat these points here and offer our own perspective for completeness.###To generate (cid:96) ∞ bounded adversarial examples we use Projected Gradient Descent (PGD) conﬁned to a speciﬁed (cid:96) ∞ ball; for (cid:96) 2 , we use the Lagrangian relaxation of Carlini & Wagner (2017c).###Instead of actively attacking the detection method, we ﬁnd that LID is not able to detect high conﬁdence adversarial examples (Carlini & Wagner, 2017a), even in the unrealistic threat model where the adversary is entirely oblivious to the defense and generates adversarial examples on the original…###If g ( · ) is smooth and differentiable, then computing gradients through the combined network ˆ f is often sufﬁcient to circumvent the defense (Carlini & Wagner, 2017b).###As benchmarking against iterative optimization-based attacks (e.g., Kurakin et al. (2016a); Madry et al. (2018); Carlini & Wagner (2017c)) has become standard practice in evaluating defenses, new defenses have arisen that appear to be robust against these powerful optimization-based attacks.",other,acknowledging prior work and providing context for completeness
1958,,e12da7526f0154fb36488d4af2fe4cc7eb4d1b57,Analysis of High-Perimeter Planar Electrodes for Efficient Neural Stimulation,,,"###Oxide coatings including platinized platinum and iridium oxide reduce interface impedance and increase charge capacity for stimulation (Weiland and Anderson, 2000; Merrill et al., 2005), but charge capacity during short-duration pulses (∼100 μs) is limited by the rate of electron and ion transport (Cogan, 2008).###Oxide coatings including platinized platinum and iridium oxide reduce interface impedance and increase charge capacity for stimulation (Weiland and Anderson, 2000; Merrill et al., 2005), but charge capacity during short-duration pulses (∼100 µs) is limited by the rate of electron and ion transport…###Similarly, high surface area porous electrodes reduce interface impedance and pacing thresholds (Mond and Grenz, 2004), but diffusion limitations prevent accessing the full surface area during short-duration stimulation pulses (Elliott and Owen, 2000; Weiland and Anderson, 2000).",impact-revealing,highlighting the limitations of oxide coatings and porous electrodes in stimulation
1786,,3b29d5324a7614ddab0137e42257b01981de6557,Draco 2: An Extensible Platform to Model Visualization Design,,,"###Voy-ager’s [22, 23] CompassQL [21] and Draco 1 both build on the Vega-Lite [17] grammar and combine rules that model ﬁne-grained design knowledge with hand-tuned scores.###Modeling Visualization Design Knowledge: Visualization recommendation researches on algorithms including rule-based meth-ods considering theoretical principles [13,14,22,23] or proposing new metrics [2,9,19], and ML-based approaches [8,11,12] learning from a vast corpus of empirical results.",impact-revealing,acknowledge existing visualization recommendation research
1996,,b9bed699b5d170715bb6639d40fa18a26395baaa,CRISPR/Cas9 Editing of Duck Enteritis Virus Genome for the Construction of a Recombinant Vaccine Vector Expressing ompH Gene of Pasteurella multocida in Two Novel Insertion Sites,,,"###Available vaccines showed variable results in protecting ducks against fowl cholera in natural outbreaks [2,3].###Meanwhile, the live attenuated DEV vaccines are widely used for reduction of diseases impact in ducks [3].",impact-revealing,acknowledging the effectiveness of available vaccines and their variable results
1327,,647c5e0602bb26819f9ad18e5dc06334af417c43,Effect of Severity of Leaf and Crop Removal on Grape and Wine Composition of Merlot Vines in Hawke’s Bay Vineyards,,,"###…have been reported with Merlot (Spayd et al. 2002), Pinot noir (Price et al. 1995), and Shiraz (Haselgrove et al. 2000, Downey et al. 2004), while Cortell and Kennedy (2006) found model extractions had a lower concentration of flavonols in shaded treatments, similar to the nil LR treatment…###In Pinot noir, cluster shading decreased levels of skin tannins (Cortell and Kennedy 2006), but in ripe Shiraz there was no significant effect of shading on either skin or seed tannin levels (Downey et al. 2004), although both studies noted decreases in skin flavonols from shading.###Shading Pinot noir grapes reduced the proportions of delphinidin, cyanidin, petunidin, and malvidin relative to unshaded, but there was a large increase in peonidin glucosides (Cortell and Kennedy 2006).###This conclusion is validated by the findings of Cortell and Kennedy (2006) who reported that shading Pinot noir clusters from the E–###Reductions in berry anthocyanins were reported in some seasons from fruit shading and anthocyanin composition was also significantly altered by shading (Cortell and Kennedy 2006, Downey et al. 2004).",impact-revealing,reporting findings on the effects of shading on grape composition
1820,,0a8aa98d680caba14134f3d826d0ea803e53446d,Parenting in context: parents’ experiences of caring for a child with autism in Bangladesh,,,###Professionals have underscored the importance of raising awareness about the influence of cultural factors on ASD (Dyches et al. 2004; Welterlin and LaRue 2007).,impact-revealing,highlighting the significance of cultural factors on ASD
3451,5f576c1591e011f4c3d5dd7e,c36571ac50808c75fa8a5d37f1041af22e89e6ee,GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training,5c8a11324895d9cbc6121c34,Neural Message Passing for Quantum Chemistry.,"a neighbor aggregation (or message passing) scheme [10], where node features are recursively aggregated from their neighbours.###[10] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl.",other,providing context for neighbor aggregation scheme
1562,,4581578742385fe22b6ac3cb6ee4a565c63dff46,The associated random walk and martingales in random walks with stationary increments,,,"###The work below is motivated by that of Lu (1991) on branching processes in random environments (Smith & Wilkinson (1969), Athreya & Karlin (1971)) and by a convergence result given a straightforward proof by the author (Grey (2001)).###Then the waiting times {Wn} satisfy
Wn+1 = (Wn + Un − Tn) +
and it follows by a standard argument, dating back to Lindley (1952) in the case of i.i.d. inter-arrival times and exploited, among others, by Kingman (1964), that Wn has an equilibrium distribution which is the same as the distribution of…",impact-revealing,acknowledging foundational work and motivation for current research
2412,5f92ba1691e011edb3573ba0,268d347e8a55b5eb82fb5e7d2f800e33c75ab18a,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,5f645c489e795e0286c904b1,Big Transfer (BiT): General Visual Representation Learning,"We de-duplicate the pre-training datasets w.r.t. the test sets of the downstream tasks following Kolesnikov et al. (2020).###For these datasets, pre-processing follows Kolesnikov et al. (2020).###Therefore, in large-scale image recognition, classic ResNet-like architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020).###For VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter setting for all tasks.###Moreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov et al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from large scale datasets such as ImageNet-21k and JFT-300M.###Finally, further scaling of ViT would likely lead to improved performance. et al., 2020)).###It is often beneﬁcial to ﬁne-tune at higher resolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020).###The ﬁrst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which performs supervised transfer learning with large ResNets.###These modiﬁcations improve transfer (Kolesnikov et al., 2020), and we denote the modiﬁed model “ResNet (BiT)”.###We follow the pre-processing used in Kolesnikov et al. (2020), except that we do not use task-speciﬁc input resolutions.",other,acknowledge existing methods and findings in image recognition
1159,,aea27667f4ba81c7f7482c9c49bb1a9bcf1184d6,Mechanism design for Data Replica Placement (DRP) problem in strategic settings,,,"###This modeling is inspired by real-world applications, in particular data replication in content delivery networks, or content delivery networks (CDNs) (Dilley et al., 2002; Douglis & Kaashoek, 2001; Nygren, Sitaraman, & Sun, 2010; Pallis & Vakali, 2006; Vakali & Pallis, 2003).",impact-revealing,highlighting inspiration from real-world applications in content delivery networks
1473,,62f047962729e02a00f4551e33c94d7f0b04a0b4,Auto-LUT: Auto Approximation of Non-Linear Operations for Neural Networks on FPGA,,,"###, GELU [8], ELU [9], and SiLU [10], which are used in state-of-the-art networks for improved performance.###This approach is inadequate for more complex activations, e.g., GELU [8], ELU [9], and SiLU [10], which are used in state-of-the-art networks for improved performance.###GELU, ELU, SiLU, and Tanh.",impact-revealing,acknowledge activation functions used in state-of-the-art networks
2698,5bdc315017c44a1f58a05c5e,5201efab94c9376ef894f6f33cab06a5c5e00073,Learning Named Entity Tagger using Domain-Specific Dictionary,53e9a718b7602d970304d2e6,Latent aspect rating analysis without aspect keyword supervision.,"For the laptop review domain, we use the Amazon laptop review dataset3, which is designed for the aspect-based sentiment analysis (Wang et al., 2011).###For the laptop review domain, we use the Amazon laptop review dataset 3 , which is designed for the aspect-based sentiment analysis (Wang et al., 2011).",other,reporting data source for aspect-based sentiment analysis
3529,5cf48a3eda56291d582a1174,05c4eb154ad9512a69569c18d68bc4428ee8bb83,Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"eddings at layer L −1, which again requires their neighbors’ embeddings at layer L−2 and recursive ones in the downstream layers. This leads to time complexity exponential to the GCN depth. GraphSAGE [5] proposed to use a fixed size of neighborhood samples during back-propagation through layers and FastGCN [1] proposed importance sampling, but the overhead of these methods is still large and will bec###ed to acquire the gradient of one node. To make mini-batch SGD work, previous approaches try to restrict the neighborhood expansion size, which however do not improve embedding utilization. GraphSAGE [5] uniformly samples a fixed-size set of neighbors, instead of using a full-neighborhood set. We denote the sample size as r. This leads to O(rL)embedding computations for each loss term but also makes ###ring embeddings. For simplicity we omit the memory for storing the graph (GCN) or sub-graphs (other approaches) since they are fixed and usually not the main bottleneck. GCN [9] Vanilla SGD GraphSAGE [5] FastGCN [1] VR-GCN [2] Cluster-GCN Time complexity O(L∥A∥0F +LNF2) O(dLNF2) O(rLNF2) O(rLNF2) O(L∥A∥0F +LNF2 +rLNF2) O(L∥A∥0F +LNF2) Memory complexity O(LNF +LF2) O(bdLF +LF2) O(brLF +LF2) O(brLF +LF###ch is efficient, the convergence of gradient descent is slow since the parameters are updated only once per epoch. [memory: bad; time per epoch: good; convergence: bad] •Mini-batch SGD is proposed in [5]. Since each update is only based on a mini-batch gradient, it can reduce the memory requirement and conduct many updates per epoch, leading to a faster convergence. However, mini-batch SGD introduces### is only updated once per epoch, the training requires more epochs to converge. It has been shown that mini-batch SGD can improve the training speed and memory requirement of GCN in some recent works [1, 2, 5]. Instead of computing the full gradient, SGD only needs to calculate the gradient based on a mini-batch for each update. In this paper, we use B⊆[N]with size b = |B|to denote a batch of node indices,",other,discussing challenges and complexities in graph convolutional networks
707,5bdc315017c44a1f58a05ce0,4930de1aff4b1948157a15ac9cdb02364bee97bb,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,5a260c2817c44a4ba8a236bf,Position-aware Attention and Supervised Data Improve Slot Filling.,"We conduct experiments on two relation extraction datasets: (1) TACRED: Introduced in (Zhang et al., 2017), TACRED contains over 106k mention pairs drawn from the yearly TAC KBP4 challenge.###Successful relation extraction is the cornerstone of applications requiring relational understanding of unstructured text on a large scale, such as question answering (Yu et al., 2017), knowledge base population (Zhang et al., 2017), and biomedical knowledge discovery (Quirk and Poon, 2017).###Earlier, our group compared (1) and (2) with sequence models (Zhang et al., 2017), and we report these results; for (3) we report results with our own implementation.###Our group presented a competitive sequence model that employs a position-aware attention mechanism over LSTM outputs (PA-LSTM), and showed that it outperforms several CNN and dependency-based models by a substantial margin (Zhang et al., 2017).###More recently, Adel et al. (2016) and Zhang et al. (2017) have shown that relatively simple neural models (CNN and augmented LSTM, respectively) can achieve comparable or superior performance to dependency-based models when trained on larger datasets.###For fair comparisons on the TACRED dataset, we follow the evaluation protocol used in (Zhang et al., 2017) by selecting the model with the median dev F1 from 5 independent runs and reporting its test F1.",impact-revealing,acknowledge the importance of relation extraction and its applications
3935,5eccb534e06a4c1b26a83a46,e582444763f8b1e3e2c725aab1f37c5d64b69123,Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations,53e9a479b7602d9702d98afa,Microsoft COCO: Common Objects in Context,"As the proxy datasets, we use images from MS-COCO [24] and Pascal VOC [9], two widely used object detection datasets, and Places365 [50], a large-scale scene recognition dataset.",other,reporting data sources used for proxy datasets
3355,5ed7796e91e011e6e91120f0,8ade0f3bbe8d8251c5de9ef39886a13055f463e5,TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding,53e9be4ab7602d9704b0ab7e,Predicting the Political Alignment of Twitter Users.,"Ideology detection in general could be naturally divided into two directions, based on the targets to predict: of the politicians [7, 24, 28], and of the ordinary citizens [1, 2, 5, 8, 13, 15–17, 20, 23, 29].###Darkest red represents p ∈ [0, 8 ] of users in that area are liberal, remaining [ 7 8 , 1] are conservative; darkest blue areas have [ 8 , 1] users being liberal,###Most existing approaches of ideology detection on social networks focus on text [5, 8, 15–17].###On the other hand, on social network datasets, it is quite intuitive trying to extract information from text data to do ideology-detection [5, 8, 15–17], only a few paid attention to links [9, 13].###[0, 8 ] conservative.",other,acknowledge existing approaches in ideology detection
1948,,45306e839a728559ca21da1efe10da10f2cf96ca,MXDAG: A Hybrid Abstraction for Cluster Applications,,,"###, Baraat [11], Varys [9], Aalo [7] fundamentally consider Coflow abstraction and perform application-aware network scheduling.###Coflow abstraction [6] tries to bridge this gap by jointly considering collection of network flows among multiple compute stages, which enables application-aware network scheduling to some extent [9, 7].###The Coflow abstraction [6] is proposed a decade ago and is widely used by many network schedulers to optimize resource sharing [9, 7, 22].###Explicit network schedulers e.g., Baraat [11], Varys [9], Aalo [7] fundamentally consider Coﬂow abstraction and perform application-aware network scheduling.",impact-revealing,acknowledge the significance of Coflow abstraction in network scheduling
1884,,f78c716ebe11a2c051bbe7f2cf238aa42e4383c2,Mechanisms of dopamine quantal size regulation.,,,"###While cyclic voltammetry has proved extremely helpful to monitor DA release with high specificity in acute slices and in vivo (14-16), the temporal resolution of this method is limited by the electrode’s response to the oxidation-reduction wave and it is not optimal for monitoring rapid quantal release events.",impact-revealing,highlighting the limitations of cyclic voltammetry in monitoring rapid quantal release events
1828,,0283c205686110c3a9f030fd5d57d3db8a39f788,Strategies to enhance the therapeutic efficacy of antidepressants: targeting residual symptoms,,,"###A number of agents are currently used clinically for augmenting primary antidepressants, of which lithium and triiodothyronine (T3) are most commonly used as first-line augmenting agents [16–20].",impact-revealing,reporting common clinical practices for augmenting antidepressants
2823,558a49f584ae84d265bcb7dc,2fd637ff36c131ad82b2fcf0b1723196ea0ce05c,StatCache: a probabilistic approach to efficient and accurate data locality analysis,53e9aac3b7602d9703444af6,"DIOTA: Dynamic Instrumentation, Optimization and Transformation of Applications","Many tools have been built using binary code instrumentation tools like DIOTA [16], ATOM [7] or EEL [13].",other,acknowledge existing tools for binary code instrumentation
2287,5db80dc83a55acd5c14a24b9,0af061849aa325b41a213e8730b3d1e84aa26c0d,CONNA: Addressing Name Disambiguation on the Fly,573697556e3b12023e63e9e3,HLTCOE Efforts in Entity Linking at TAC KBP 2010.,"This two-step strategy which is widely adopted in entity linking [23], [28], [48], [51] is proved to be effective.###The main solutions usually include the NIL threshold methods [9], [32] which predict the mention as unlinkable if the score of the top ranked entity is smaller than a NIL threshold, the classification methods [23], [28], [51] which predict the unlinkable mentions by a binary classifier based on the same features as the entity matching model, and the unified models that incorporate the unlinkable mention prediction process into the entity matching process [3], [11].",other,describing widely adopted strategies in entity linking
2725,5ce3afafced107d4c65f7c4c,d9f6ada77448664b71128bb19df15765336974a6,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,59ae3c592bbe271c4c7211cf,SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation.,"…in GLUE can be found in Wang et al. (2019a) and in Warstadt et al. (2019, CoLA), Socher et al. (2013, SST-2), Dolan and Brockett (2005, MRPC), Cer et al. (2017, STS-B), and Williams et al. (2018, MNLI), and Rajpurkar et al. (2016, the original data source for QNLI). remains substantial scope…",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1617,,1708ca453c3b896362f8ca29c60bf39507be6964,Empathy in schizophrenia: impaired resonance,,,"###Impaired empathy has recently been assumed to be involved in schizophrenia [2, 19, 51, 54].###This shared representation serves as a basis for the ability to share physiological and emotional states of others and makes up one component of empathy [19, 22, 42, 61].",impact-revealing,highlighting the role of impaired empathy in schizophrenia
252,5f75eed591e0111c1eb4da5f,400389ca8b23ff77fa9ee96717fe6447df7469af,Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning Attacks,5d3ed25a275ded87f97deae1,Robust Graph Convolutional Networks Against Adversarial Attacks,"The implementations for Mettack, PGD and FGA were based on the publicly available DeepRobust Jin et al. (2020) library.###Following (Zhu et al. 2019), we set hidden dimensions at 16 and assume a diagonal covariance for each node.###RGCN (Zhu et al. 2019): This is a recently proposed approach that explicitly enhances the robustness of GCNs.###…robustness to structural perturbations; • Across a suite of global poisoning attacks, UM-GNN consistently outperforms existing methods including the recent Robust GCN Zhu et al. (2019); • UM-GNN achieves signiﬁcantly lower misclassiﬁcation rate ( > 50% improvement) against targeted attacks.###Following Zhu et al. (2019), we set hidden dimensions at 16 and assume a diagonal covariance for each node.###In comparison, GAT appears to be the most sensitive to random structural perturbations and its low performance strongly corroborates with the ﬁndings in Zhu et al. (2019).###• Across a suite of global poisoning attacks, UM-GNN consistently outperforms existing methods including the recent Robust GCN (Zhu et al. 2019);###Following the progress in graph adversarial attacks, designing defense mechanisms or building robust variants of GNNs have become critical Zhu et al. (2019).###In comparison, GAT appears to be the most sensitive to random structural perturbations and its low performance strongly corroborates with the findings in (Zhu et al. 2019).###While there exist very few GNN formulations for speciﬁcally defending against adversarial attacks, the recent robust GCN (RGCN) approach Zhu et al. (2019) has been the most effective, when compared to standard GCN and GAT models.###RGCN Zhu et al. (2019): This is a recently proposed approach that explicitly enhances the robustness of GCNs. RGCN models node features as distributions as opposed to deterministic vectors in GCN and GAT models.###The key contributions of this work are summarized as follows: • A novel architecture for semi-supervised learning, UM-GNN , that can be built upon any existing GNN model and is immune to evasion attacks by design; • An uncertainty matching-based knowledge transfer strategy for achieving robustness to structural perturbations; • Across a suite of global poisoning attacks, UM-GNN consistently outperforms existing methods including the recent Robust GCN Zhu et al. (2019); • UM-GNN achieves signiﬁcantly lower misclassiﬁcation rate ( > 50% improvement) against targeted attacks.###(Zhu et al. 2019) introduced a robust variant of GCN based on a variance-weighted attention mechanism, and showed it to be effective against different types of attacks.###While there exist very few GNN formulations for specifically defending against adversarial attacks, the recent robust GCN (RGCN) approach (Zhu et al. 2019) has been the most effective, when compared to standard GCN and GAT models.###Following the progress in graph adversarial attacks, designing defense mechanisms or building robust variants of GNNs have become critical (Zhu et al. 2019).",impact-revealing,highlighting the effectiveness of UM-GNN against adversarial attacks and comparing it to existing methods
1012,,2238672d9b3e0d7adb24042fa9af95cc7c5e2320,Volunteer work in the church among older Mexican Americans.,,,"###Although pardcipadng in formal worship services is a key mechanism for transmitdng religious percepts, researchers have known for some dme that the informal social networks that arise in places of worship play an important role in this process as well (Krause, 2008). Evidence of this may be found in the work of Ysseldyk, Matheson, and Anisman (2010), who maintain that religious social idenddes, and the self-concepts that arise from them, are maintained and reinforced by members of die rehgious community. Krause's (2008) nodon of informal spiritual support suggests one way in which this might be accomplished.###This measure was taken from the work of Krause (2009). As shown in Table 1, the quesdon on volunteering was carefully###These measures were taken from the work of Krause (2008). The indicators in this brief composite assess whether fellow church members share their own religious experiences with study participants, whether they help study participants lead a better rehgious life, and whether they help study participants get to know God better.###Although pardcipadng in formal worship services is a key mechanism for transmitdng religious percepts, researchers have known for some dme that the informal social networks that arise in places of worship play an important role in this process as well (Krause, 2008). Evidence of this may be found in the work of Ysseldyk, Matheson, and Anisman (2010), who maintain that religious social idenddes, and the self-concepts that arise from them, are maintained and reinforced by members of die rehgious community.",impact-revealing,highlighting the role of informal social networks in transmitting religious concepts
3479,5ec49a639fced0a24b4de82e,724d182e2b0217ce10ff05e3ad0b2f548fb39b9f,Iterative Edit-Based Unsupervised Sentence Simplification,53e9be28b7602d9704ae7001,Sentence Simplification by Monolingual Machine Translation.,"Such simpliﬁ-cation systems are typically trained in a supervised way by either phrase-based machine translation (PBMT, Wubben et al., 2012; Narayan and Gardent, 2014; Xu et al., 2016) or neural machine translation (NMT, Zhang and Lapata, 2017; Guo et al., 2018; Kriz et al., 2019).###First, we consider non-neural phrasebased machine translation (PBMT) methods: PBMT-R (Wubben et al., 2012), which re-ranks###First, we consider non-neural phrase-based machine translation (PBMT) methods: PBMT-R (Wubben et al., 2012), which re-ranks sentences generated by PBMT for diverse simpliﬁ-cations; SBMT-SARI (Xu et al., 2016), which uses an external paraphrasing database; and Hybrid (Narayan and Gardent, 2014),…###Later, researchers adopted machine learning meth-ods for text simpliﬁcation, modeling it as mono-lingual phrase-based machine translation (Wubben et al., 2012; Xu et al., 2016).###First, we consider non-neural phrase-based machine translation (PBMT) methods: PBMT-R (Wubben et al., 2012), which re-ranks sentences generated by PBMT for diverse simpliﬁ-cations; SBMT-SARI (Xu et al., 2016), which uses an external paraphrasing database; and Hybrid (Narayan and Gardent, 2014), which uses a combination of PBMT and discourse representation structures.###Later, researchers adopted machine learning methods for text simplification, modeling it as monolingual phrase-based machine translation (Wubben et al., 2012; Xu et al., 2016).",other,acknowledge existing methods in text simplification
3445,5ee8986891e011e66831c452,a9872078cc6dabd2428750543862b45f4a482dfc,Graph Meta Learning via Local Subgraphs,5b67b45517c44aac1c860876,Graph Convolutional Neural Networks for Web-Scale Recommender Systems.,", [56, 26]), many graph-structured data resources remain under-utilized because of the sparsity and scarcity of labels.###Graph Neural Networks (GNNs) have achieved remarkable results in domains such as recommender systems [56], molecular biology [65, 19], and knowledge graphs [49, 18].",other,highlighting the success of Graph Neural Networks in various domains
2927,5f0277e911dc830562231dab,6deae79dec438eaaa524bca3b82c6b8d93553b20,A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data,5b3d98cc17c44a510f802154,Label Refinery: Improving ImageNet Classification through Label Progression.,", multiple teachers [8] and cascade distillations [4]) and different forms of knowledge (e.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1228,,c9f4f26a47ab7fda7c5485009ba087b525ccdb81,Modelling of Pandemic Influenza in Canada: Predicted Burden and Hospital-Resource Adequacy,,,"###Effectiveness (susceptibility) (%)* 0–100 10 30 50 [231, 302]###The community group size was fixed at 100 people, reflecting the number of random contacts an individual is likely to have in their home and work neighbourhoods [232, 302].###Of those who have been infected, two thirds experience symptomatic, “clinical” infection (IC), while one third develop asymptomatic infections (IA) and are half as infectious as symptomatic individuals [232, 299, 302, 333, 334].###InFluNet is a population-level, discrete-time simulation model that builds on previous deterministic and stochastic influenza models [157, 232, 301-306].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1217,,7e40f89fb94b0b250bbe67d5714398cda59e5981,"Pattern, Growth, and Control",,,"###For example, secreted polypeptide growth factors (e.g., chalones) are thought to act within epithelial tissues at ranges up to a few hundred microns, due to the depleting effects of receptor-mediated uptake (e.g., Lander et al., 2009a; Shvartsman et al., 2001).###Upregulating morphogen destruction can produce arbitrary robustness to fluctuations in morphogen levels (Eldar et al., 2003), but because it makes gradients shallower far from the morphogen source, it also ends up increasing the effect of noise on precision (Lander et al., 2009b).###Reverse engineering of this system (Lander et al., 2009a) entailed mathematically exploring what performance objectives could potentially be met by a multiplicity of progenitor###Reverse engineering of this system (Lander et al., 2009a) entailed mathematically exploring what performance objectives could potentially be met by a multiplicity of progenitor
958 Cell 144, March 18, 2011 ª2011 Elsevier Inc.
cell stages, a multiplicity of feedback factors, and the specificity of…###…a morphogen gradient spreads also depends upon the rate of morphogen uptake and turnover, it has been argued that, for morphogen gradients of typical biological length scales (50–200 mm), receptor binding noise will usually be too slow to remove by simple time averaging (Lander et al., 2009b).###Production of chalones, such as GDF11, by differentiated cells in the olfactory epithelium inhibits progenitor self-renewal (B), providing a feedback signal that increases (decreases) in time as long as the probability of progenitor cell renewal is greater (lesser) than 50% (Lander et al., 2009a).###, the probability that the target cell’s progeny remain of the same type instead of progressing to the next lineage stage (Lander et al., 2009a).###…out that significant saturation of receptors near the source of a morphogen gradient dramatically degrades robustness such that small changes in
morphogen production rate (e.g., due to environmental or genetic variation) produce large changes in the shape of the gradient (Lander et al., 2009b).###Thus, reverse engineering suggests that the detailed interaction of lineage, feedback, and regulation of self-renewal found in the olfactory epithelium constitutes a system for simultaneous robust size control and rapid regeneration (Lander et al., 2009b; Lo et al., 2009).###This in turn led to experiments showing that GDF11 also controls the renewal probability of its target cell, i.e., the probability that the target cell’s progeny remain of the same type instead of progressing to the next lineage stage (Lander et al., 2009a).###This work has been followed by many studies from other groups exploring ways in which other known mechanisms of pattern formation can also be robust (reviewed by Barkai and Shilo, 2009; Eldar et al., 2004; Lander et al., 2009b).###Other molecules have recently been suggested to act as chalones in a variety of tissues (reviewed by Lander et al., 2009a).###Consistent with this, the pattern of gradual progenitor pool expansion, contraction, and extinction that occurs in the developing brain closely follows the expected consequences of negative feedback control of progenitor selfrenewal (Lander et al., 2009a).###Panels B and C are adapted from Lander et al. (2009b).
morphogen) will extend its range, but this strategy is limited by another problem: receptor saturation.###It has recently been argued (Lander et al., 2009b) that such tradeoffs offer a more plausible explanation for the relatively short distances (50– 100 cells) (Wolpert, 1969) over which morphogens act than physical limitations on the speed at which morphogens spread (Crick, 1970).",impact-revealing,highlighting the significance of morphogen gradients and their regulatory mechanisms in biological systems
1493,,fc21d461727bf1289dc29abfb6c4507a816914bc,Exceeding the limit for microscopic image translation with a deep learning-based unified framework,,,"###Diffusion models, which are inspired by nonequilibrium thermodynamics (30), have recently demonstrated their superior results on various image processing tasks (31 – 35).",impact-revealing,highlighting the recent success of diffusion models in image processing
3936,5d9ed2d847c8f76646f797b7,c3715947bbbf648dcf29a1aa4b35cfb68044f919,GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification,57a4e91dac44365e35c9851e,A Decomposable Attention Model for Natural Language Inference.,"Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM.###These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et…###Luken et al. (2018) adopt the de-composable attention model (DAM) (Parikh et al., 2016) to generate NLI predictions for each claim-evidence pair individually and then aggregate all NLI predictions for ﬁnal veriﬁcation.",other,highlighting the advancements in neural models for claim-evidence relevance inference
1382,,2fecb2565c8d9782842a9e74317f5daa72b14341,How symmetry constrains evolutionary optimizers,,,###Salomon [6] emphasized the importance of rotational invariance.,impact-revealing,highlighting the significance of rotational invariance
3255,5f4f6ec291e0111f07b30a2b,3259c9ab1714a4cfdf6439cca6bdc5f78d78fda3,NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size,5550415945ce0a409eb3a820,ImageNet Large Scale Visual Recognition Challenge,"In sum, ImageNet-16-120 contains 151.7K training images, 3K validation images, and 3K test images with 120 classes.###(I) Compared to candidates in St, ResNet shows competitive performance in three datasets, however, it still has room to improve, i.e., about 2% compared to the best architecture in CIFAR-100 and ImageNet-16-120, about 1% compared to the best one with the same amount of parameters in CIFAR-100 and ImageNet-16-120.###This is because the significantly increased searching data on CIFAR-100 and ImageNet-16-120 over CIFAR-10 alleviate the problem of incorrect gradient estimation in bi-level optimization.###However, on CIFAR-100 and ImageNet16-120, they perform relatively well.###ImageNet-16-120: We build ImageNet-16-120 from the down-sampled variant of ImageNet (ImageNet16×16).###(3) On ImageNet-16-120, BOHB significantly outperforms the other methods.###As indicated in [34], down-sampling images in ImageNet can largely reduce the computation costs for optimal hyperparameters of some classical models while maintaining similar searching results.###On ImageNet-16-120, we use a similar strategy but with random crop 16×16 patch and 2 pixels padding on each border.###The ranking of every architecture in our search space is shown in Figure 2, where the architectures ranked in CIFAR-10 (x-axis) are shown in relation to their respective ranks in CIFAR-100 and ImageNet-16-120 (y-axis), indicated by green and red markers respectively.###We train and evaluate each architecture on CIFAR-10, CIFAR-100 [33], and ImageNet-16-120 [34].###Observations on the size search space Ss. (1) REA significantly outperforms the other methods on all datasets in the size search space Ss. (2) On CIFAR-100 and ImageNet16-120, results of BOHB, REINFORCE, and RANDOM are similar.###[34] down-sampled the original ImageNet to 16×16 pixels to form ImageNet16×16, from which we select all images with label ∈ [1, 120] to construct ImageNet-16-120.###We choose these three datasets because CIFAR and ImageNet [35] are the most popular image classification datasets.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2589,58d82fcbd649053542fd669e,a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c,Deep Variational Information Bottleneck.,53e9a15bb7602d9702a4bb9c,The Im Algorithm: A Variational Approach To Information Maximization,"Variational bounds on mutual information have previously been explored in Agakov (2004), though not in conjunction with the information bottleneck objective.",other,acknowledging prior work on variational bounds
1781,,115fe1ed7a6f5a2518647eb59f087dbfff8dbe44,Piperine Enhances the Antimalarial Activity of Curcumin in Plasmodium berghei ANKA-Infected Mice: A Novel Approach for Malaria Prophylaxis,,,"###Piperine is a natural alkaloid isolated from black pepper (Piper nigrum) [33].###It is widely used as a preservative and seasoning in diets, medical procedures (to cure intermittent fever, colds, asthma, diarrhea, colic pain, cholera, and malaria), in perfumery, and even as an insecticide [33, 34].",impact-revealing,providing context and applications of piperine
1435,,c17985a669522e7e85ae3d34754c7df49c7187d1,Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges,,,"###…al., 2014; Bahdanau et al., 2014) have been widely adopted as the state-of-the-art approach for machine translation, both in the research community (Bojar et al., 2016a, 2017, 2018b) and for large-scale production systems (Wu et al., 2016; Zhou et al., 2016; Crego et al., 2016; Hassan et al.,…",impact-revealing,acknowledging the widespread adoption of state-of-the-art machine translation approaches
3888,58437725ac44360f1082f992,e70b9a38fcf8373865dd6e7b45e45cca7ff2eaa9,pate: semi-supervised knowledge transfer for deep learning from private training data,57aa28dd0a3ac518da9896a6,Smart Reply: Automated Response Suggestion for Email,"…learning applications with great beneﬁts are enabled only through the analysis of sensitive data, such as users’ personal contacts, private photographs or correspondence, or even medical records or genetic sequences (Alipanahi et al., 2015; Kannan et al., 2016; Kononenko, 2001; Sweeney, 1997).",other,highlighting the ethical concerns regarding the use of sensitive data in learning applications
1096,,c84539a89bc0a2a2425ca1b0046833f57d86e7b0,Using Bayesian networks to analyze expression data,,,"###To facihtate efficient learning, we need to be able to focus the attention of the search procedure on relevant regions of the search space, giving rise to the Sparse Candidate algorithm [ 18 ].###We refer the reader to [ 18 ] for more details on the algonthm and ~ts complexity, as well as empmcal results companng ~ts performance to traditional search techmques.###The approach builds on two techmques that were motivated by the challanges posed by this domain: a novel search algorithm [ 18 ] and an approach for estimating statistical confidence [16].",impact-revealing,providing context and details about the Sparse Candidate algorithm and its performance
3395,5db929b747c8f766461fa94f,2a6d160b529272964ce1a6707adf52f3d6ba4861,Diffusion Improves Graph Learning,5b67b45517c44aac1c860876,Graph Convolutional Neural Networks for Web-Scale Recommender Systems.,"However, they only became widely adopted in recent years, when they started to outperform classical models in many graph-related tasks [19, 33, 42, 82].###PPNP [33] propagates the node predictions generated by a neural network using personalized PageRank, DCNN [4] extends node features by concatenating features aggregated using the transition matrices of k-hop random walks, GraphHeat [79] uses the heat kernel and PAN [45] the transition matrix of maximal entropy random walks to aggregate over nodes in each layer, PinSage [82] uses random walks for neighborhood aggregation, and MixHop [2] concatenates embeddings aggregated using the transition matrices of k-hop random walks before each layer.",other,highlighting the evolution and advancements in graph-related tasks
512,57d063f6ac44367354296741,908f7931de8768786d9ef7d64f5a8156860709dd,Dynamic Pricing and Traffic Engineering for Timely Inter-Datacenter Transfers,53e9b4d4b7602d9703ff2f93,Achieving high utilization with software-driven WAN,"Similar to prior work [18, 20], we execute SAM once every few minutes.###Traffic engineering for datacenter WANs has been drawn recent attention from both industry and academia [18, 20, 24].###As described earlier, Pretium sets aside some capacity to account for ad hoc high priority traffic; the volume to be set aside is estimated based on historical usage [18].###Other portions of the WAN traffic may not be governed by any TE scheme [18, 20, 22].###A sizeable portion of inter-datacenter transfers have deadlines, and can be modeled using this abstraction [18, 22].###, from historical usage, as in [18, 22]), and is appropriately reserved on all links of the network.###Centralized traffic engineering (TE) techniques have been proposed to improve network utilization [18, 20] without affecting low latency traffic and with explicit support for deadlines [22, 34].###SWAN [18] and B4 [20] aim to improve the utilization of inter-DC WAN.",impact-revealing,acknowledging existing traffic engineering techniques and their applications
3074,53e99845b7602d9702071dfc,178deeb441faf2892719fa24f5a0593794d704a9,matrix scheduler reloaded,53e997fcb7602d97020076b4,Hierarchical Scheduling Windows,"Multilevel partitioning techniques [ 4 , 18] are a related approach which resemble cache hierarchies by using a small, fast wait-match buffer backed by larger, slower buffer.",other,providing context on multilevel partitioning techniques
2987,5f7aeb7691e011983cc81e80,572c12e81319ccd47cc0c637c82efadd03fd05ab,Autoregressive entity retrieval,53e9a366b7602d9702c7095a,KORE: keyphrase overlap relatedness for entity disambiguation.,"We evaluate on seven out-of-domain test sets: MSNBC, Derczynski (Der) (Der-czynski et al., 2015), KORE 50 (K50) (Hoffart et al., 2012), N3-Reuters-128 (R128), N3-RSS-500 (R500) (R ¨ oder et al., 2014), and OKE challenge 2015 and 2016 (OKE15 and OKE16) (Nuzzolese et al., 2015).###, 2015), KORE 50 (K50) (Hoffart et al., 2012), N3-Reuters-128 (R128), N3-RSS-500 (R500) (Röder et al.",other,reporting evaluation on various test sets
2266,5cede0f9da562983788d862a,44d43bfbd23d55b1e7c4c4fd91fe101c0eaf1a06,Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks,58d82fcbd649053542fd64c5,Adversarial Machine Learning at Scale.,"It has been shown that BIM induces much more powerful white-box attacks than FGSM at the cost of worse transferability [16, 7].###FGSM can generate more transferable adversarial examples but is usually not effective enough for attacking white-box models [16].",other,comparing the effectiveness of different attack methods
2609,5d1eb9e4da562961f0b1eb04,f80be25edf309ab595fc76fddd8cefe8eb2e5a54,GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation,55a6d07465ce054aad76d8ac,Quantum chemistry structures and properties of 134 kilo molecules,"QM9 All models were additionally evaluated on the graph-level regression task on the QM9 molecule data set (Ramakrishnan et al., 2014), considering 13 different quantum chemical properties.###All models were additionally evaluated on graph-level regression tasks on the QM9 molecule data set (Ramakrishnan et al., 2014), considering thirteen different quantum chemical properties.###• QM9 property prediction (Ramakrishnan et al., 2014): „ 130 000 graphs of „ 18 nodes represent molecules, where nodes are atoms and undirected, typed edges are bonds between these atoms, different edge types indicating single/double/etc.",other,reporting evaluation on QM9 dataset for graph-level regression tasks
1212,,1f305ae29596a30c7066b2e1c46d671bcd74bbe4,Effect of feedback regulation on stem cell fractions in tissues and tumors: understanding chemoresistance in cancer.,,,"###An ordinary differential equation model has been used to describe tissue hierarchy dynamics in a healthy tissue (Kunche et al., 2016; Lander et al., 2009), and the models presented here build on these approaches.###This adds to the mathematical literature quantifying the role of feedback regulation for tissue and tumor dynamics (Arino and Kimmel, 1986; Komarova, 2013; Komarova and van den Driessche, 2018; Konstorum et al., 2016; Kunche et al., 2016; Lander et al., 2009; Rodriguez-Brenes et al., 2011; Rodriguez-Brenes et al., 2013b; Rodriguez-Brenes et al., 2017; Stiehl et al., 2018; Yang et al., 2015; Youssefpour et al., 2012), and builds upon the wider mathematical literature concerned with the dynamics of hierarchically structured cell populations, e.###, in the olfactory epithelium, where GDF11 and Activin βB negatively regulate self­ renewal rates in progenitor and stem cells (Gokoffski et al., 2011; Lander et al., 2009).",impact-revealing,building on prior mathematical models to contribute to tissue dynamics literature
2415,5bdc315017c44a1f58a05e7e,7a71941e60894ae7e1f5af8e79c37cec6cd6c6ad,Multi-scale Residual Network for Image Super-Resolution,5550472145ce0a409eb64ae3,Learning A Deep Convolutional Network For Image Super-Resolution,"proposed a model for SISR problem termed SRCNN [1], which was the first successful model adopting CNNs to SR problem.###proposed the SRCNN [1] model, various CNNs architectures have been used on SISR problem.###3 Comparisons with State-of-the-art Methods We compare our model with 10 state-of-the-art SR methods, including Bicubic, A+ [23], SelfExSR [20], SRCNN [1], ESPCN [2], FSRCNN [3], VDSR [4], DRCN [5], LapSRN [6] and EDSR [9].###In this work, we have reconstructed some classic SR models, such as SRCNN [1], EDSR [9] and SRResNet [8].",other,reporting on the development and comparison of super-resolution models
3574,5ce2d184ced107d4c6438f01,cd26a0ae3c5a65d807b8fa7134f3a44cfd0392bd,exploiting edge features for graph neural networks,53e9afe8b7602d9703a3c50f,Node Classification in Social Networks,", degrees) [5], kernel functions [28][24] or other hand-crafted features which measure local neighborhood structures.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
353,5e79da4491e0115bb1157b77,8e74106415626a21bb909ff489ef310625c769e1,Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles,5d1eb9d4da562961f0b0e960,XLNet: Generalized Autoregressive Pretraining for Language Understanding.,"[41], XLNet surpasses BERT on the related GLUE benchmark [39], so we were expecting a similar outcome.###As the third model category, we employ two language models for deep contextual text representations based on the Transformer architecture [37], named BERT [15] and XLNet [41].###Recently, Transformer-based [37] neural language models introduced a shift from context-free word embeddings, like GloVe [31], to contextual embeddings as the ones used in BERT [15] and XLNet [41].###While BERT is trained on English Wikipedia and the BooksCorpus [43] alone, XLNet uses additional Web corpora for pretraining [41].###We hypothesize that this difference may be attributed to two reasons, pretraining on different corpora, and smaller models compared to [41].###Second, we implement six different models using word-based document embeddings from GloVe [31] and Paragraph Vectors [23] (as Doc2vec implementation [33]), and deep contextual language models from BERT [15] and XLNet [41] in a vanilla and Siamese architecture [9].",impact-revealing,discussing model expectations and differences in training data
1148,,183aec8376b39ae2a1707a436266cddaf8f05596,Denoising Likelihood Score Matching for Conditional Score-based Data Generation,,,"###Such a time-inhomogeneous variant is commonly adopted by recent works on score-based generative models (Song & Ermon, 2019; Song et al., 2021b), as it provides flexibility in controlling pt(x̃t) and t.###In this work, we adopt the PC sampler (VE-SDE) identical to that in (Song et al., 2021b), which is a time-inhomogeneous sampling algorithm.###The score model architecture is exactly the same as the one used in (Song et al., 2021b), while the architecture of the classifier is based on ResNet (He et al., 2016) with a conditional branch for encoding the information of the standard deviation σ (Song et al., 2021b).###Recent endeavors followed this approach and further extended the concept of conditional score-based models to a number of application domains, including colorization (Song et al., 2021b), inpainting (Song et al., 2021b), and source separation (Jayaram & Thickstun, 2020).###The network architecture for the score model is the same as (Song et al., 2021b).###In particular, some recent researchers (Song et al., 2021b; Dhariwal & Nichol, 2021) applied this method to the field of classconditional image generation tasks, and proposed the classifier-guidance method.###The experimental results in (Song & Ermon, 2019; Song et al., 2021b) demonstrated that such a time-inhomogeneous sampling process
can improve the sampling quality on real-world datasets.###For the sampling algorithm, we adopt the predictor-corrector (PC) sampler described in (Song et al., 2021b) with the sampling steps set to T=1,000.###Since the computational cost of denoising score matching is relatively lower in comparison to other reformulation techniques (Hyvärinen, 2005; Song et al., 2019), it is extensively adopted in recent score-based generative models (Song & Ermon, 2019; 2020; Song et al., 2021b).###Their success inspired several succeeding works (Song & Ermon, 2020; Ho et al., 2020; Song et al., 2021a;b; Dhariwal & Nichol, 2021), which together contribute to making score-based generative models an attractive choice for contemporary image generation tasks.###Following the assumptions in the previous study (Song et al., 2021b), the decomposition can be achieved by taking the log-gradient on both sides of the equation, expressed as follows:
∇x̃ log pσ,τ (x̃|ỹ) = ∇x̃ log pσ,τ (ỹ|x̃) +∇x̃ log pσ(x̃)−∇x̃ log pτ (ỹ)︸ ︷︷ ︸ =0 , (5)
where ∇x̃ log pσ,τ…",impact-revealing,describing the methodology and its applications in score-based generative models
324,5db9295f47c8f766461f5135,2c1006c856fefdbd6cd710e840e57153f2d6cd04,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,5550417845ce0a409eb3b9b3,Explaining and Harnessing Adversarial Examples.,"As the adversarial examples reside in a large, contiguous region and a significant portion of the adversarial subspaces is shared [24, 19, 59, 40], pure label-guided adversarial examples will clutter as least in the shared adversarial subspace.###Recently, [2] showed that many existing defence methods suffer from a false sense of robustness against adversarial attacks due to gradient masking, and adversarial training [24, 32, 58, 36] is one of the effective defense method against adversarial attacks.###Adversarial examples, initially demonstrated in [4, 55], have attracted great attention recently [4, 24, 58, 36, 2, 5].###In terms of the image classification, an adversarial example for a natural image is a modified version which is visually indistinguishable from the original but causes the classifier to produce a different label prediction [4, 55, 24].###The inner maximization can be solved approximately, using for example a one-step approach such as FGSM [24], or a multi-step projected gradient descent (PGD) method [36] x = PSx ( x + α · sign ( ∇xL(x, y;θ) )) , (2) where PSx(·) is a projection operator projecting the input into the feasible region Sx.###The inner maximization can be solved approximately, using for example a one-step approach such as FGSM [23], or a multi-step projected gradient descent (PGD) method [35] x t +1 = P S x x t + α · sign ∇ x L ( x t , y ; θ ) , where P S x ( · ) is a projection operator projecting the input into the feasible region S x .###For testing , model robustness is evaluated by approximately computing an upper bound of robustness on the test set, by measuring the accuracy of the model under different adversarial attacks, including white-box FGSM [23], PGD [35], CW [8] (CW-loss [8] within the PGD framework) attacks and variants of black-box attacks.###OT-solver CIFAR10 SVHN CIFAR100 Clean FGSM PGD20 PGD100 CW20 CW100 Clean FGSM PGD20 PGD100 CW20 CW100 Clean FGSM PGD20 PGD100 CW20 CW100 Sinkhorn 90.0 78.4 To further verify if a degenerate minimum is obtained, we evaluate the robustness of the model trained with the proposed approach w.r.t.black-box attacks (B-Attack) following [57].###A fast gradient sign method (FGSM) for adversarial attack generation is developed and used in adversarial training in [23].###It improves model robustness by solving a minimax problem as [24, 36]: min θ [ max x′∈Sx L(x′, y;θ) ] (1)###A fast gradient sign method (FGSM) for adversarial attack generation is developed and used in adversarial training in [24].###The proposed formulation deviates from the conventional minimax formulation for adversarial training [24, 36].###Among them, adversarial training [24, 36] is one of the most popular technique [2], which conducts model training using the adversarially perturbed images in place of the original ones.###, Dθ≡ ∑ i Lθ(xi, yi), the proposed approach reduces to the conventional adversarial training setup [24, 36].",impact-revealing,highlighting the significance of adversarial examples and their impact on model robustness
3372,5e5e190b93d709897ce4997e,cb4571fa905abb70868d0bb9d4681f0a612c2d0f,Differentiable Reasoning On Large Knowledge Bases And Natural Language,53e99b21b7602d97023b612a,On our best behaviour.,The main focus of Artiﬁcial Intelligence is building systems that exhibit intelligent behaviour (Levesque 2014).,other,providing context on the focus of Artificial Intelligence
2040,,0afab3e6f04a47ee4154ac126fd093bc3541411f,The role of adhesion molecule NCAM in ovarian cancer progression and its correlation with intrabdominal cancer dissemination,,,"###Following the observation that NCAM/FGFR interplay is necessary for EOC cell migration and invasion, we asked whether it is also sufficient.###We tested whether mAbs that prevent the binding of NCAM to FGFR had any impact on NCAMdependent EOC cell invasion.###This supports the hypothesis that interfering with the NCAM/FGFR association has a dramatic impact on the promalignant function of NCAM in EOC cells.###FIG.3 A model of intraabdominal dissemination in EOC.###To address this question, we selected two human EOC cell lines, SKOV3 and OVCA-433, which express no endogenous NCAM.###We employed an assay based on the intraperitoneal injection of SKOV3 cells into immunodeficient mice, which is widely used as a model for peritoneal metastasis of human EOC (43-45).###NCAM stimulates EOC cell migration via its interaction with###The neural cell adhesion molecule ( NCAM)
Cell adhesion molecules mediating either cell-cell interactions or cell-matrix adhesion have emerged as key players throughout the natural history of EOC development, in that they have been implicated both in cancer cell survival upon detachment from the primary tumor and in the subsequent adhesion to and invasion of metastatic sites (25, 29).###Based on the ability of NCAM to modulated FGFR function and on the proposed role of FGFR activity in ovarian cancer, we hypothesized that the NCAM/FGFR signaling axis is causally involved in EOC development.###To investigate whether NCAM is involved in the malignant phenotype of EOC cells, we utilized the MOVCAR cell line, originally isolated from the cancer tissue of MISIIR-TAg transgenic mice, a genetic model of ovarian carcinoma (Connolly et al,2003).###In agreement with the data on NCAM silencing in MOVCAR cells, ectopic expression of NCAM in human EOC cells had no effect on cell proliferation (Suppl.###Tumor cell invasion is a key step during cancer progression and, therefore, we determined the role of the NCAM/FGFR interaction in the ability of EOC cells to invade Matrigel, a reconstituted basement membrane.###Thereafter, some authors have provided extensive evidence of a physical association between the two proteins on different, non-neural cell types (33- 36) All four members of the FGFR family as well as various FGFs have been found in EOC tissue (37-39) suggesting that dysregulated FGFR signaling contributes to ovarian carcinogenesis (39-41).and therefore it may represent a suitable therapeutic target (42).###The primary objective of the present study was to investigate the expression and functional role of NCAM in EOC both in vitro and in vivo (mouse models).###NCAM stimulate EOC cell invasion via its interaction with",impact-revealing,highlighting the role of NCAM in EOC cell invasion and its potential as a therapeutic target
1545,,554a175c8c59e7610d58d82ec7f090d037d2cae4,Neuroscience and Biobehavioral Reviews Self-organization of Multiple Spatial and Context Memories in the Hippocampus,,,"###The place cells representation can be used as part of a scaffold, to provide the spatio-temporal context to the episodic memories (O'Keefe and Nadel, 1978; Eichenbaum et al., 1999; Eichenbaum, 2000b).###Although an accumulating body of evidence has indicated that hippocampal activity is not exclusively related to space (Eichenbaum, 2000a, 2004), the prevalence of spatial correlates in the rat has encouraged speculations on the evolution of the hippocampus based on spatial function, reinforced by…###By doing so, they inevitably impose a rigid schema, to divide work which is possibly much more distributed across different areas (Eichenbaum, 2000a).",impact-revealing,highlighting the role of place cells in episodic memory context
4027,5edcbb9a91e0110f1d6db1c2,c99e2877d4b1f171746a3aa73d5fd5fd680c982d,Hierarchical Bipartite Graph Neural Networks: Towards Large-Scale E-commerce Applications,573696096e3b12023e51c873,Discriminative Embeddings of Latent Variable Models for Structured Data,"GNNs treat the underlying graph as a computation graph and generate individual node embeddings by passing, transforming, and aggregating node feature information across the graph [23]–[26].",other,providing context on GNNs and their functionality
2347,5eda19c991e01187f5d6d814,befc296197edcf5431e6042ee58f48d5dc2cf970,Convergence and Stability of Graph Convolutional Networks on Large Random Graphs,5b67b4b417c44aac1c8675e1,Diffusion Scattering Transforms on Graphs.,"To compare two graph representations, a standard approach in the study of stability (and graph theory in general) has been to deﬁne a metric that minimizes over permutations σ of the nodes ( e.g., [16, 18]), thus we deﬁne MSE Σ ( Z, Z ′ ) def.###A more recent line of work has studied stability properties of GCNs or scattering representations on discrete graphs, by considering certain well-chosen discrete perturbations and metrics [16–18, 47] ,###However the notion of deformations is not well-deﬁned on discrete graphs, and most stability studies for GCNs use purely discrete metrics that are less intuitive for capturing natural changes in structure [16, 18, 47].###…Φ( ): where − ) is the deformed signal and N ( τ ) quantiﬁes the size of the deformation, typically through norms of its jacobian ∇ τ , such as k∇ τ k ∞ = sup x k∇ τ ( x ) . the deformation k As we have seen in the introduction, it is not clear how to extend notion of on discrete graphs [16, 18].###…a sub-manifold can be studied under the light of Riemannian geometry, stability bounds on SBMs may be expressed with a direct dependence on their parameters, or more explicit stability bounds may be obtained when the (c)GCN is a structured architecture like the scattering transform on graphs [16].",other,discussing stability properties and metrics in graph representations
3569,5b1642388fbcbf6e5a9b5740,3913d2e0a51657a5fe11305b1bcc8bf3624471c0,learning structured representation for text classification via reinforcement learning,5736974d6e3b12023e6389d7,"Molding CNNs for text: non-linear, non-consecutive convolutions","Sequence representation models such as convolutional neural network (Kim 2014; Kalchbrenner, Grefenstette, and Blunsom 2014; Lei, Barzilay, and Jaakkola 2015) and recurrent neural network (Hochreiter and Schmidhuber 1997; Chung et al. 2014) consider word order but do not use any structure.",other,providing context on sequence representation models
820,5f8d6be69fced0a24bbab005,93e513de7bc55a6d9319b7861435f435ed85a03f,dimension relation modeling for click-through rate prediction,5843774bac44360f1083973e,Field-aware Factorization Machines for CTR Prediction,"Usually, the raw fields are not independent, thus it’s effective to learn information from their interactions, such as low-order interactions [4, 8] or high-order interactions [3, 5], as shown in Fig.",impact-revealing,providing context on the effectiveness of learning from interactions
3592,5ee8986891e011e66831c3b6,38f93092ece8eee9771e61c1edaf11b1293cae1b,Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,57a4e91aac44365e35c98084,Deeper Depth Prediction with Fully Convolutional Residual Networks,The setup is based on [83] and detailed in Appendix E.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
327,5843777eac44360f108417ec,455afd748e8834ef521e4b67c7c056d3c33429e2,Hierarchical Attention Networks for Document Classification,5550411c45ce0a409eb3897f,Neural Machine Translation by Jointly Learning to Align and Translate.,"The GRU (Bahdanau et al., 2014) uses a gating mechanism to track the state of sequences without using separate memory cells.###We use a bidirectional GRU (Bahdanau et al., 2014) to get annotations of words by summarizing information from both directions for words, and therefore incorporate the contextual information in the annotation.###To include sensitivity to this fact, our model includes two levels of attention mechanisms (Bahdanau et al., 2014; Xu et al., 2015) — one at the word level and one at the sentence level — that let the model to pay more or less attention to individual words and sentences when constructing the…",impact-revealing,describing the architecture and functionality of the GRU model
1455,,2afc4772593a0f3a068e883a9184df1edd4e6d09,The destruction of benzene by calcium peroxide activated with Fe(II) in water.,,,"###[40] Z. Miao, X. Gu, S. Lu, D.D. Dionysiou, S.R. Al-Abed, X. Zang, X. Wu, Z. Qiu, Q. Sui,
M. Danish, Mechanism of PCE oxidation by percarbonate in a chelated Fe(II)-based catalyzed system, Chem.###14 by the solution pH [30-31], thus the generation of H2O2 in the CaO2/Fe(II) system was constrained by the solution pH, resulting in the decrease of HO generation and benzene destruction.###Northup [30] also reported that the yield of H2O2 and the dissolution rate of CaO2 increased with the decreasing of solution pH.###In addition, the removal of a wide variety of pollutants, including 2,4,6-trinitrotoluene (TNT) [27], total petroleum hydrocarbons (TPHs) [22], polycyclic aromatic hydrocarbons (PAHs) [28], trichloroethene (TCE) [29], and tetrachloroethene (PCE) [30] have been reported as well.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1049,,f4fdaaf864ca6f73ced06f937d3af978568998eb,Network Monitoring as a Streaming Analytics Problem,,,"###The data processing component of Sonata follows a long line of related efforts in the database community [1, 9, 13, 30, 36] and also builds on the prior work on streaming data in the form of network traffic [5, 13, 30].###Gigascope [13] uses query partitioning to minimize the data transfer within the stream processor.",impact-revealing,acknowledge prior work in data processing and streaming
33,5ce2d0feced107d4c63dd498,d07284a6811f1b2745d91bdb06b040b57f226882,Decoupled Weight Decay Regularization,5c8f296d4895d9cbc62e76a9,A unified theory of adaptive stochastic gradient descent as Bayesian filtering.,"(Aitchison, 2018).###Decoupled weight decay very naturally fits into this unified framework as part of the state-transition distribution: Aitchison (2018) assumes a slow change of the optimizer according to the following Gaussian: P (θt+1 | θt) = N ((I −A)θt,Q), (4)###We now discuss a justification of decoupled weight decay in the framework of Bayesian filtering for a unified theory of adaptive gradient algorithms due to Aitchison (2018).###”(Aitchison, 2018).###Aitchison (2018) views stochastic optimization of n parameters x1, . . . , xn as a Bayesian filtering problem with the goal of inferring a distribution over the optimal values of each of the parameters xi given the current values of the other parameters θ−i(t) at time step t.###Aitchison (2018) goes on to show that popular adaptive gradient methods, such as Adam and RMSprop, as well as Kronecker-factorized methods are special cases of this framework.###Aitchison (2018) assumes a Gaussian state transition distribution P (θt+1 | θt) and an approximate conjugate likelihood P (yt+1 | θt+1), leading to the following closed-form update of the filtering distribution’s mean:
µpost = µprior + Σpost × g, (3) where g is the gradient of the log likelihood of…###Decoupled weight decay very naturally fits into this unified framework can express weight decay as part of the state-transition distribution: Aitchison (2018) assumes a slow change of the optimizer according to the following Gaussian:
P (θt+1 | θt) = N ((I −A)θt,Q), (4)
where Q is the covariance of…",impact-revealing,discussing a theoretical framework for adaptive gradient algorithms
1862,,79e1072782c7e7658722552353eecf52b2a3ba21,Driving the ambulance: an essential component of emergency medical services: an integrative review,,,"###A systematic integrative review [6] was conducted.###Under the integrative review methodology, a comprehensive appraisal of the overall quality of each study was made with the support from the CASP checklists [6].###The analysis conducted was inspired by the integrative review methodology [6].###Finally, a final result was formulated [6].",impact-revealing,describing the methodology of a systematic integrative review
460,5f7af09591e011983cc81efc,87b008a6289fa22c72e1726a8929e815dfbbc65f,Hard Negative Mixing for Contrastive Learning,5cede0e6da562983788c532a,Hardness-Aware Deep Metric Learning,"Our work is also related to metric learning works that employ generators [14, 51].",impact-revealing,acknowledge related work in metric learning
870,558b3e9384ae84d265c24c24,fed9ac8dc6ea64141e5526894a146e476b8fea52,SCD: A scalable coherence directory with flexible sharer set encoding,558bee32e4b0cfb70a1aa6c1,The ZCache: Decoupling Ways and Associativity,"Caches and directories use H3 hash functions, which are simple to implement and work well in practice [5, 25].###Moreover, Tagless relies on the tracked caches being set-associative, and would not work with other array designs, such as skew-associative caches [27] or zcaches [25].###Second, as in zcaches [25], the array is pipelined, and we allow concurrent non-conflicting lookups and writes, but only allow one replacement at a time.###Prior work leverages these models to show that associativity depends only on the number of replacement candidates, not ways [25], and to implement scalable and efficient cache partitioning [26].###Both L2 and L3 are 4-way zcaches [25] with 16 and 52 replacement candidates, respectively.###In caches, zcaches have the latency and energy efficiency of a low-way cache on hits, but replacements incur similar energy costs as a set-associative cache of similar associativity [25].###ZCache implements the replacement process differently: it first retrieves all possible replacement candidates in a breadth-first fashion, selects the least desirable candidate using replacement policy information, and performs a few moves to evict that candidate and insert the new one.###We have shown that in practice, this is an accurate assumption for zcaches [25, 26].###We leverage recent prior work on efficient highly-associative caches (ZCache [25] and Cuckoo Directory [10]), which, due to their multiple hash functions and replacement process, work in###We leverage recent prior work on efficient highly-associative caches (ZCache [25] and Cuckoo Directory [10]), which, due to their multiple hash functions and replacement process, work in
978-1-4673-0826-7/12/$26.00 ©2011 IEEE
practice as if replacement candidates were selected randomly, independently of the addresses tracked [25].###Both ZCache [25] and Cuckoo Directory [10] build on skewassociative caches [27] and Cuckoo hashing [24].###practice as if replacement candidates were selected randomly, independently of the addresses tracked [25].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3518,5bdc316717c44a1f58a071ff,ec3071fb918ad69ec80df1ca9cf1fdeb386a9603,TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.,573696056e3b12023e518112,MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems,"Computational graphs are a common way to represent programs in deep learning frameworks [3, 4, 6, 8].###Barrier inserted automatically by compiler All threads cooperatively load AS and BS in different parallel patterns for thread_group (by, bx) in cross(64, 64): for thread_item (ty, tx) in cross(2, 2): local CL[8][8] = 0 shared AS[2][8], BS[2][8] for k in range(1024): for i in range(4): AS[ty][i*4+tx] = A[k][by*64+ty*8+i*4+tx] for each i in 0.###inp_buffer AL[8][8], BL[8][8] acc_buffer CL[8][8] for yo in range(128): for xo in range(128): vdla.###Current deep learning frameworks, such as TensorFlow, MXNet, Caffe, and PyTorch rely on a computational graph intermediate representation to implement optimizations such as auto differentiation and dynamic memory management [3, 4, 8].###Barrier inserted automatically by compiler
All threads cooperatively load AS and BS in different parallel patterns for thread_group (by, bx) in cross(64, 64): for thread_item (ty, tx) in cross(2, 2): local CL[8][8] = 0 shared AS[2][8], BS[2][8] for k in range(1024): for i in range(4): AS[ty][i*4+tx] = A[k][by*64+ty*8+i*4+tx] for each i in 0..4: BS[ty][i*4+tx] = B[k][bx*64+ty*8+i*4+tx] memory_barrier_among_threads() for yi in range(8): for xi in range(8): CL[yi][xi] += AS[yi] * BS[xi] for yi in range(8): for xi in range(8): C[yo*8+yi][xo*8+xi] = CL[yi][xi]
We introduce the concept of memory scopes to the schedule space so that a compute stage (AS and BS in the code) can be marked as shared.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
764,5a260c8417c44a4ba8a31186,4ec75b95946c1339571410cbb0d4ff005c8bb53b,compiling deep learning models for custom hardware accelerators,5550450545ce0a409eb52fc8,Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks,"Operations in a compute window are independent, hence it is well suited to multi-core processors: GPUs [21] and other designs using ASIC [3, 2] and FPGAs [6, 26].",impact-revealing,highlighting the suitability of operations for multi-core processors
2741,5d3ed25a275ded87f97deaab,025ea689e6ab3b544101df17233e87536a1e578a,Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation,5550411645ce0a409eb38730,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.,"Recurrent Neural Network (RNN), especially the Long Short Term Memory (LSTM) [2 5] has been proved to perform well for sequential data.",other,reporting prior findings on RNN performance
329,5843777eac44360f108417ec,455afd748e8834ef521e4b67c7c056d3c33429e2,Hierarchical Attention Networks for Document Classification,5550489045ce0a409eb6f76a,A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval,"Attention serves two beneﬁts: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classiﬁcation decision which can be of value in applications and analysis (Shen et al., 2014; Gao et al., 2014).",impact-revealing,highlighting the benefits of attention mechanisms in classification
3048,5ecbc8889fced0a24b51eb0e,57c38661af2d1ac5ac79cc51a443f5f1cca4b03b,single shot video object detector,5d0b000a8607575390fa578c,Adaptive Convolution for Object Detection,"Inspired by the recent advances in image representation using deep Convolutional Neural Networks (CNN) [1], [3], [4], [5], remarkable progresses have been witnessed for object detection [8], [10], [11], [12], [13], [25], [26], [27], [28], [29], [30], [31], [32].",other,highlighting the influence of CNN advancements on object detection
713,53e99ae2b7602d97023691f2,f71f02c06192cd8ec06c30f5d373dcf509edd8ba,temporal instruction fetch streaming,53e9af67b7602d97039a85ee,Last-Touch Correlated Data Streaming,"The TIFS design is based on recent proposals for address-correlated prefetch of recurring temporal data streams [7, 8, 21, 30, 37].###Our key observation, inspired by recent studies of data prefetching [7, 8, 21, 30, 37], is that repetitive control flow graph traversals lead to recurring instruction-cache miss sequences, much like repetitive data-structure traversals lead to recurring data-cache miss sequences.###The basic operation of TIFS mirrors prior address-corre-lated prefetching proposals that target data accesses [8, 21, 37].",impact-revealing,highlighting the inspiration from prior work on data prefetching
43,5ec49a639fced0a24b4de7ed,1eed0659d561354d5c471a723cf3381430561d04,Graph Neural News Recommendation with Unsupervised Preference Disentanglement,5ce2d08fced107d4c6390bc9,DAN : Deep Attention Neural Network for News Recommendation,"Following DAN (Zhu et al., 2019), we use two parallel convolutional neural networks (PCNN) taking the title T and proﬁle P of news as input to learn the title-level and proﬁle-level representation (cid:98) T and (cid:98) P for news.###DAN (Zhu et al., 2019), a deep attention neural network for news recommendation which can capture the dynamic diversity of news and user’s interests, and consider the users’ click sequence information.###GNUD improves the best deep neural models DKN and DAN more than 6.45% on AUC and 7.79% on F1 on both datasets.###However, these methods (Wu et al., 2019b; Zhu et al., 2019; An et al., 2019) usually focus on news contents, and seldom consider the collaborative signal in the form of high-order connectivity underlying the user-news interactions.###Following (Zhu et al., 2019), we deﬁne the proﬁle embedding###DKN and DAN further improve other deep neural models by incorporating external knowledge and applying a dynamic attention mechanism.###Following DAN (Zhu et al., 2019), we just select user id, news id, time-stamp, the title and proﬁle of news to build our datasets, and preprocess the data by removing the stopwords in the news content.###Some works (Wang et al., 2018; Zhu et al., 2019) propose to improve news representations via external knowledge, and learn representations of users from their browsed news using an attention module.",impact-revealing,describing the method and improvements in news recommendation systems
3789,5db929e147c8f766461fc024,e03b5bc5edeb44d4b47d225c0c26ac54088fe528,An Improved Neural Baseline for Temporal Relation Extraction,573696116e3b12023e523bae,Relation Classification via Recurrent Neural Network,"Since TempRel is a speciﬁc relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016).###The XML markups, which was initially proposed under the name of position indicators for relation extraction (Zhang and Wang, 2015), uniquely indicate the event positions to LSTM, such that the ﬁnal output of LSTM can be used as a representation of those events and their context.###The XML markups, which was initially proposed under the name of position indicators for relation extraction (Zhang and Wang, 2015), uniquely indicate the event positions to LSTM, such that the final output of LSTM can be used as a representation of those events and their context.",other,describing the use of XML markups in relation extraction
1343,,4efd19e446c08f8a7cec1c68f89dfa88d63c007c,Survey of information theory in visual quality assessment,,,"###Information theoretic FR image QA (IQA) indices [9,10, 12] that have been developed to date generally use measures of the mutual information between reference and distorted images to quantify losses of visual information arising from distortion.###In [9], the Information Fidelity Criterion (IFC) is developed as the amount of information shared between the reference and distorted wavelet coefficients.###By contrast, the method explained in [19] builds on the information fidelity criterion (IFC) introduced in [9] for images, to design information theoretic VQA algorithms.###While information theoretic quantities can be directly used to predict quality of the distorted image in local patches [9,10], they may also be indirectly used to weight quality evaluations at different locations in “quality maps” [11].###For a more detailed description of these aspects, we refer the reader to [9,10].",impact-revealing,providing context on information theoretic image quality assessment methods
2357,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,53e9a952b7602d97032a5d35,Research on Errors of Utilized Bandwidth Measured by NetFlow,[159] studied the errors of utilized bandwidth measurement of NetFlow and provided guidance for correctly estimating the utilized bandwidth.,other,reporting prior findings on bandwidth measurement errors
3262,5c04966a17c44a2c74708401,51203e9d5620abdcdf6c9be93b1e221e79cda67d,Transfer Learning of Language-independent End-to-end ASR with Language Model Fusion,5b3d98cc17c44a510f801eb2,Improved training of end-to-end attention models for speech recognition,"Therefore, shallow fusion shows performance gains in many ASR tasks [5, 18, 19].###The research question in this paper is: Is linguistic context also helpful for adaptation to new languages? The most common approach to integrate the external language model (LM) is referred to as shallow fusion, where LM scores are interpolated with scores from the S2S model [5,18,19].",other,highlighting the effectiveness of shallow fusion in ASR tasks
2969,5f8cf5159e795ea21aee7f07,0ee0801ba010a441403f9ed666ef9bf006b3aa07,Adaptive Universal Generalized PageRank Graph Neural Network,5db929b747c8f766461fa94f,Diffusion Improves Graph Learning,"In contrast, recent GNN models that utilize Personalized PageRanks (PPR) with fixed weights (Wu et al., 2019; Klicpera et al., 2018; 2019) inevitably act as low-pass filters.",other,highlighting limitations of recent GNN models
172,5cf48a2cda56291d5828e868,c42816f497d663c681df20d48a6e66a5632600d8,Mixmatch: A holistic approach to semi-supervised learning,5a260c8417c44a4ba8a31511,mixup: Beyond Empirical Risk Minimization.,We also use MixUp [47] on its own as a baseline.###We also use MixUp [47] in MixMatch to encourage convex behavior “between” examples.,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1448,,42d4057b9f03b538349c7c66a02fdc6c84c066f1,Gramformer: Learning Crowd Counting via Graph-Modulated Transformer,,,"###The idea is motivated by the existence of the perspective geometry of pin-hole cameras in crowd images, which usually generates perspective or scale variations only along the vertical axis (Shi et al. 2019).",impact-revealing,highlighting the motivation behind the proposed idea based on existing geometric principles
1826,,12ccad1c383eea7443e5cc7aad757724377a2146,Emerging theory of teacher resilience: a situational analysis,,,"###As such, resilience is shaped by a dynamic interplay between personal and contextual factors and resources (Mansfield et al., 2016), an interaction over time between risk and protective factors (Beltman et al.###The challenge of work–life balance is often depicted in literature on teacher resilience (Mansfield et al., 2016).###In this paper, the researchers posit that teacher resilience is related to, often includes, but is distinct from teacher agency, as well as other similar constructs such as teacher efficacy, emotional intelligence and motivation (Beltman et al., 2011; Mansfield et al., 2016; Tait, 2008).###If resilience leads to increased well-being, sense of belonging, passion and engagement (Mansfield et al., 2016), teachers with resilience could operate with agency.###Robust international evidence links teacher resilience with teacher effectiveness, job satisfaction, motivation and teacher self-efficacy (Beltman et al., 2011; Greenfield, 2015; Johnson et al., 2014; Mansfield et al., 2016; Papatraianou and Le Cornu, 2014; Peters and Pearce, 2012).###As such, resilience is shaped by a dynamic interplay between personal and contextual factors and resources (Mansfield et al., 2016), an interaction over time between risk and protective factors (Beltman et al., 2011).###Second, the situational map extends the work of Mansfield et al. (2012, 2016) and Sullivan and Johnson (2012) to visually depict the complexity of the dynamic interplay of internal, external, constraining and enabling factors to explain how resilience is mediated by efficacy and ideally leads to…###…constraining factors is depicted on the situational map (see Figure 1) informed by study findings, confirmed by international literature on teacher resilience (Mansfield et al., 2016; Sullivan and Johnson, 2012) and related to literature on medical professional resilience (Brigham et al., 2018).###The dynamic interaction of internal and external, enabling and constraining factors is depicted on the situational map (see Figure 1) informed by study findings, confirmed by international literature on teacher resilience (Mansfield et al., 2016; Sullivan and Johnson, 2012) and related to literature on medical professional resilience (Brigham et al.###Importance of teacher resilience Robust international evidence links teacher resilience with teacher effectiveness, job satisfaction, motivation and teacher self-efficacy (Beltman et al., 2011; Greenfield, 2015; Johnson et al., 2014; Mansfield et al., 2016; Papatraianou and Le Cornu, 2014; Peters and Pearce, 2012).",impact-revealing,highlighting the significance of teacher resilience and its impact on effectiveness and job satisfaction
4046,58437785ac44360f108432a7,92527ace7f75188b5ec209ff7d59f431343075e4,Video-based emotion recognition using CNN-RNN and C3D hybrid networks,573696f46e3b12023e5f12ae,Learning Spatiotemporal Features with 3D Convolutional Networks,C3D can model appearance and motion information simultaneously and the C3D features with a linear classifier can achieve good performance on different video analysis benchmarks [6].###The confusion matrices of our submissions on the validation and testing sets are given in Figure 5.###The category with the highest score is taken to be the final recognition result.###And we found that the CNN-RNN and C3D hybrid network can further improve the performance.,other,highlighting the performance capabilities of C3D in video analysis
1593,,c0a0b0e3c99aa8fb49706e9617d0deb9cfa54cb0,Potential implications of research on genetic or heritable contributions to pedophilia for the objectives of criminal law.,,,"###Enhanced understanding of the underlying reasons and motivations for the offender’s behavior, especially through the inclusion and discussion of research or information on the complex biological and genetic predispositions and associations of pedophilia [26-65], could in turn lead to a more satisfying experience for victims and communities [137].",impact-revealing,highlighting the importance of understanding offender behavior for victim satisfaction
3564,5d3c234c3a55acd386d4e112,a9ec03dbe702f6909acd1f1f14a3395d0141043b,generative models for graph-based protein design,5c0f70ebda562944ac628fae,"The Rosetta All-Atom Energy Function for Macromolecular Modeling and Design (vol 13, pg 3031, 2017)","We found that our model was more accurate and signiﬁcantly faster than Rosetta (Table 4).###In the ﬁrst, we used the latest version of Rosetta (3.10) to design sequences for our ‘Single chain’ test set with the ﬁxbb ﬁxed-backbone design protocol and default parameters (Table 4, a).###Many of the major ‘ﬁrsts’ in protein design are due to Rosetta [30, 31], a leading framework for protein design.###In the second, we also compared to a prior benchmark from members of the Rosetta community [47, 48] across 40 diverse proteins.###Although this reduced the size of the training set from ∼ 18,000 to ∼ 10,000 chains, we found our model to be both more accurate than and several orders of magnitude faster than Rosetta (Table 4, b).###For ﬁxed-backbone sequence design, our model achieves considerably improved statistical performance over a neural-network based model and also achieves higher accuracy and efﬁciency than Rosetta fixbb , a state-the-art program for protein design.###Comparison to Rosetta To evaluate the performance of our model at generating realistic protein sequences, we performed two experiments that compare with Rosetta [30], a state-of-the-art framework for computational protein design.###When evaluated on unseen folds, the model achieves signiﬁcantly improved perplexities over recent neural network-based generative models and more accurate and efﬁcient sequence generation than the state-of-art program Rosetta.###We will focus on comparisons to Rosetta, since it is based on a shared parametric energy function that captures the sequence-structure relationship.###Many of the major ‘firsts’ in protein design are due to Rosetta [30, 31], a leading framework for protein design.",other,Highlighting the superiority of the proposed model over Rosetta in protein design
1575,,245effed5f0eb2674c79c7b5f897573d3b7c0433,Reply to comment by Stefano Orlandini and Giovanni Moretti on “Global search algorithm for nondispersive flow path extraction”,,,"###0148-0227/08/2007JF000964$09.00
F04001 1 of 9
al., 1991; Costa-Cabral and Burges, 1994; Tarboton, 1997; Seibert and McGlynn, 2007].###However, this algorithm has been criticized [e.g., Costa-Cabral and Burges, 1994] in that the reasoning behind the introduction of randomness is weakly supportive; the randomness generates different flow paths at every run for the same DEM, and extracted flow paths over a plane are not parallel,…###…terrains chosen here are those frequently used for similar tests in earlier studies [Fairfield
F04001 PAIK: NONDISPERSIVE FLOW PATH RETRIEVAL
4 of 9
F04001
and Leymarie, 1991; Freeman, 1991; Costa-Cabral and Burges, 1994; Tarboton, 1997; Orlandini et al., 2003; Seibert and McGlynn, 2007].",impact-revealing,Critique of algorithm's randomness and its implications
2822,5d0616bd8607575390f86730,635b079447ee46acc813a08305988e77dfe3d1dd,Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi,5c0495fa17c44a2c747051a6,Boosting fine-grained activity sensing by embracing wireless multipath effects,"Due to complexity of human activity, existing approaches extract signal features, either statistical [14, 15, 23, 28, 30, 45, 49] or physical [6, 31, 34, 38, 39, 44, 51, 52] ones, and map them to discrete activities.###Niu et al. [30] uses signal waveforms for fine-grained gesture recognition.",other,acknowledge existing approaches to human activity recognition
2024,,cb438eeb68b710128873c2ed43f581fbfd6dc203,Microgrid Operations Planning Based on Improving the Flying Sparrow Search Algorithm,,,"###Most metaheuristic algorithms are inspired by physical phenomena or natural species [13], e.###Because of the high chance of a successful search and the fast convergence speed, metaheuristic algorithms are increasingly used to solve complex engineering, healthcare, finance, and military issues [13].",impact-revealing,highlighting the growing application of metaheuristic algorithms in various fields
3261,5e3940c73a55ace46ed436d2,845b4941d8c016aa5f8967da2f86d38ef6c18fa3,a survey on knowledge graphs representation acquisition and applications,5b1642388fbcbf6e5a9b5765,Neural Knowledge Acquisition via Mutual Attention Between Knowledge Graph and Text.,"[64] put them under the same roof and proposed a joint learning framework with mutual attention for information sharing between knowledge graph and text.###[64] proposed a joint learning framework with mutual attention for data fusion between knowledge graphs and text, which solves KGC and relation extraction from text.",other,reporting prior findings on joint learning frameworks
990,,f5fa1f1bb8bdfc012270970f178102f89914765e,Conceptualizing Knowledge Creation: A Critique of Nonaka's Theory,,,"###…other parts of the theory (Nonaka, 1994, pp. 20–35; Nonaka and Takeuchi, 1995, pp. 70–90) have undergone considerable modification since the 1990s (Nonaka et al., 2001a, 2001b) the ‘engine’ remains a central element, recently being described as the way firms synthesize contradictions (Nonaka…###While other parts of the theory (Nonaka, 1994, pp. 20–35; Nonaka and Takeuchi, 1995, pp. 70–90) have undergone considerable modification since the 1990s (Nonaka et al., 2001a, 2001b) the ‘engine’ remains a central element, recently being described as the way firms synthesize contradictions (Nonaka and Toyama, 2003).###…a metaphor suggesting that each ‘circuit’ builds on the previous one; knowledge creation is also, implicitly, knowledge accumulation (Nonaka, 1991a; 1994, pp. 15, 18; 1995; Nonaka and Takeuchi, 1995, pp. 56, 61–2, 71–2, 89, 237–8; Nonaka and Toyama, 2003; Nonaka et al., 1994; 2001a, pp. 14–18).###…“belief”, and to emphasize the importance of the “justification” of knowledge’ (Nonaka, 1994, p. 15), a ‘critical’ distinction because ‘traditional epistemology emphasizes the absolute, static, and nonhuman nature of knowledge’ (Nonaka and Takeuchi, 1995, p. 58; Nonaka et al., 2001a, pp. 14–15).###In so far as the matrix continues to figure in Nonaka and his colleagues’ later model (e.g. Nonaka and Toyama, 2003) that too may be in need of revision, an issue beyond the scope of this paper.###Tacit knowledge is difficult to communicate or share, but, they claimed, is a ‘rich untapped source of new knowledge’ (Nonaka, 1994, p. 16; Nonaka and Takeuchi, 1995, pp. 8, 59–60, 72, 85; Nonaka et al., 2001a, p. 15).###This process is a ‘spiral’ one, a metaphor suggesting that each ‘circuit’ builds on the previous one; knowledge creation is also, implicitly, knowledge accumulation (Nonaka, 1991a; 1994, pp. 15, 18; 1995; Nonaka and Takeuchi, 1995, pp. 56, 61–2, 71–2, 89, 237–8; Nonaka and Toyama, 2003; Nonaka et al., 1994; 2001a, pp. 14–18).",impact-revealing,acknowledging the evolution and significance of Nonaka's knowledge creation theory
3171,5ee8986891e011e66831c3b6,38f93092ece8eee9771e61c1edaf11b1293cae1b,Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,556f622a2401b4b38c23635c,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,"Learning good image representations is a key challenge in computer vision [1, 2, 3] as it allows for efficient training on downstream tasks [4, 5, 6, 7].",other,highlighting the importance of learning good image representations in computer vision
1739,,e5fb523f7036ea5ed011289467f18cdb4f64fb0a,Exploring Math + CS in a Secondary Education Methods Course,,,"###The authors recommend using methods courses as a context for developing “preservice teachers’ understanding of computational thinking in the context of the discipline"" [31].###…utilizes university-level mathematics secondary education majors and builds upon the natural connections between mathematics and CS, infusing the CS concepts into teacher education to uncover the resulting affordances of bridging these disciplines together as proposed by Yadav et al. 2017 [31].",impact-revealing,highlighting the integration of computer science concepts into teacher education
2436,58437725ac44360f108302aa,03a5b2aac53443e6078f0f63b35d4f95d6d54c5d,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,53e9b512b7602d9704045da3,Cardiac image super-resolution with global correspondence using multi-atlas patchmatch.,"This task, referred to as super-resolution (SR), finds direct applications in many areas such as HDTV [15], medical imaging [28, 33], satellite imaging [38], face recognition [17] and surveillance [53].",other,highlighting the applications of super-resolution in various fields
2206,5da2f8aa3a55ac3402d8c2e1,17f2f3f7e58b916175d495109bc74b2757ef952a,Barrage of Random Transforms for Adversarially Robust Defense,5ca600ae6558b90bfa4d76e9,Towards Deep Learning Models Resistant to Adversarial Attacks,"[25], which is the computational cost of a threat model.###PGD has been conjectured to be a near-optimal first-order attack [25].###[25] used adversarial training on the cifar dataset, which still has the best empirical robustness to attack [24] and has been repeatedly validated as effective and capable of fully defending against the best known adversaries under the whitebox threat model [4].",other,reporting findings on adversarial training effectiveness
2109,,9eab27b98d4aaae826e009d377cc895d8467ddb8,Response-consequence contingency discriminability when positive and negative reinforcement compete in concurrent schedules,,,"###This possibility is related to the first because presumably a longer COD helps to differentiate the two response options from each other (Findley, 1958; Herrnstein, 1961).###(1980) investigated the role of varying stimulus disparity in a free-operant changeoverkey procedure (Findley, 1958) using Baum’s (1974) original GMR.###(1980) investigated the role of varying stimulus disparity in a free-operant changeoverkey procedure (Findley, 1958) using Baum’s (1974) original GMR. They exposed three groups of pigeons to different line orientations on the response key. Group 1 was the 0degree disparity group, that is, the line orientations indicating which component of the concurrent schedule was operating were not different from each other within subject but were different across subjects within group. Group 2 was the 15-degree disparity group and Group 3 was the 45-degree disparity group. All subjects were exposed to comparable relative reinforcement rate conditions. Their results clearly showed that Baum’s (1974) measure of sensitivity (a) increased with increasing stimulus disparity and with increased absolute rates of reinforcement (see also Nevin, et al., 1982 for more on absolute rates of reinforcement). The fundamental problem was that the Davison-Tustin model did not address the factors that determine sensitivity to reinforcement. They simply continued to include it, as Baum (1974) had originally proposed, as a free parameter to be derived from the data. In light of the evidence from Miller, et al. (1980) this position no longer was tenable.###In effect, the button on the small panel functioned as a changeover key (Findley, 1958) and the button on the main panel was the response button associated with both the white and the orange lights.###This is in contrast to Ruddle et al’s apparatus that more closely approximated Findley’s (1958) procedure (i.###(1980) investigated the role of varying stimulus disparity in a free-operant changeoverkey procedure (Findley, 1958) using Baum’s (1974) original GMR. They exposed three groups of pigeons to different line orientations on the response key. Group 1 was the 0degree disparity group, that is, the line orientations indicating which component of the concurrent schedule was operating were not different from each other within subject but were different across subjects within group. Group 2 was the 15-degree disparity group and Group 3 was the 45-degree disparity group. All subjects were exposed to comparable relative reinforcement rate conditions. Their results clearly showed that Baum’s (1974) measure of sensitivity (a) increased with increasing stimulus disparity and with increased absolute rates of reinforcement (see also Nevin, et al., 1982 for more on absolute rates of reinforcement). The fundamental problem was that the Davison-Tustin model did not address the factors that determine sensitivity to reinforcement. They simply continued to include it, as Baum (1974) had originally proposed, as a free parameter to be derived from the data.###Findley (1958) set out to replicate the findings of###In effect, the button on the small panel functioned as a changeover key (Findley, 1958) and the button###In fact, the only circumstance where it might be plausible to consider dsb = 1 would be in an unsignaled Findley switching-key procedure (Findley, 1958).###’s (1982) methods arises from the unique situation that resulted from their programming independent schedules using a changeover-key (Findley, 1958) arrangement with VC schedules of negative reinforcement.###Outside the context of Davison, McCarthy, and Nevin’s work, Miller et al. (1980) investigated the role of varying stimulus disparity in a free-operant changeoverkey procedure (Findley, 1958) using Baum’s (1974) original GMR.###A fourth problem with Ruddle et al.’s (1981) and Ruddle et al.’s (1982) methods
arises from the unique situation that resulted from their programming independent schedules using a changeover-key (Findley, 1958) arrangement with VC schedules of negative reinforcement.###Findley (1958) was the first to characterize behavior under this arrangement beyond mere description.###Findley (1958) arranged his concurrent schedules so that one schedule remained
constant across conditions and the other varied the rate of reinforcers delivered.###(1980) investigated the role of varying stimulus disparity in a free-operant changeoverkey procedure (Findley, 1958) using Baum’s (1974) original GMR. They exposed three groups of pigeons to different line orientations on the response key. Group 1 was the 0degree disparity group, that is, the line orientations indicating which component of the concurrent schedule was operating were not different from each other within subject but were different across subjects within group. Group 2 was the 15-degree disparity group and Group 3 was the 45-degree disparity group. All subjects were exposed to comparable relative reinforcement rate conditions. Their results clearly showed that Baum’s (1974) measure of sensitivity (a) increased with increasing stimulus disparity and with increased absolute rates of reinforcement (see also Nevin, et al.###Findley (1958) set out to replicate the findings of
Ferster and Skinner (1957).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2215,53e9a415b7602d9702d30d91,1753c2dc85cc40e0a2e8b4a405c1690eab066d8d,FENNEL: streaming graph partitioning for massive scale graphs,53e9aeaab7602d97038cd2a4,Partitioning graphs into balanced components,"√ When ν = 2 there exists an O ( log k log n ) approximation algorithm based on semideﬁnite programming (SDP) [38].###When ν = 2 there exists an O( √ log k log n) approximation algorithm based on semidefinite programming (SDP) [20].###This suggests that the oﬄine SDP solver will yield signiﬁcantly better performance for small values of k .###Algorithm SDP-Relax is a Ω( log ) approximation algorithm for the optimal k graph partition problem.###, [19, 20], is to impose hard cardinality constraints, namely |S i | ≤ ν nk for some small constant ν, i = 1, .###Then, As in Goemans-Williamson [22], given a random hyper-plane with normal vector r that goes through the origin, the probability of sgn( v Ti r ) = sgn( v Tj r ), i.e., i and j fall on the same side of the hyperplane, is 1 − We wish to ﬁnd a ρ such that E [ C k ] ≥ ρ OPT SDP .###, [20], and heuristics that are used in practice [16, 26, 29].###For this special case, because of the combinatorial meaning of the objective function, we design a non-trivial, semideﬁnite programming (SDP) algorithm and show that it provides provides the following guarantee.###In what follows, we refer to the optimal value of the integer program as OPT IP and of the semideﬁnite program as OPT SDP .###We will then show that one can do better by using an SDP-based rounding algorithm, which guarantees an Ω(log( k ) /k ) approximation.###The proof is based on a partitioning algorithm that is derived by using a randomized rounding of a solution to our SDP relaxation of the original combinatorial optimization problem.###Our contributions can be summarized in the following points: • We introduce a general framework for graph partitioning that relaxes the hard cardinality constraints on the number of vertices in a cluster [3, 20].###Since, OPT SDP ≥ OPT IP , this would then imply E [ C k ] ≥ ρ OPT IP .###We obtain the following semideﬁnite programming relaxation: The above SDP can be solved within an additive error of δ of the optimum in time polynomial in the size of the input and log ( 1 δ ) by interior point algorithms or the ellipsoid method [6].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3891,5db929e147c8f766461fc024,e03b5bc5edeb44d4b47d225c0c26ac54088fe528,An Improved Neural Baseline for Temporal Relation Extraction,5736987e6e3b12023e73e2c2,Bidirectional Long Short-Term Memory Networks for Relation Classification.,"Since TempRel is a speciﬁc relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016).###Since TempRel is a specific relation type, it is natural to borrow recent neural relation extraction approaches (Zeng et al., 2014; Zhang et al., 2015; Zhang and Wang, 2015; Xu et al., 2016).",other,acknowledge the use of recent neural relation extraction approaches
101,5f02f17c91e011ee5e0258c8,3da4626411d83c19c9919bb41dba94fff88da90e,Scaling Graph Neural Networks with Approximate PageRank,5ce2d032ced107d4c635260c,Predict then Propagate: Graph Neural Networks meet Personalized PageRank,"Recent work shows that personalized PageRank [28] can be used to directly incorporate multi-hop neighborhood information of a nodewithout explicit message-passing [21].###[21]’s approach does not easily scale to large graphs since it performs an expensive variant of power iteration during training.###In addition to the two scalable baselines, we also evaluate how PPRGo compares to the APPNP model [21] which we build upon.###[21] used K = 10 to achieve a good approximation) is prohibitively expensive for large graphs since they need to be computed###[21] suggest decoupling the feature transformation from the propagation.###Both of these approaches are a special case of the PPNP model [21] which experimentally shows higher classification performance [21, 17].",impact-revealing,highlighting the limitations of existing approaches in scaling to large graphs
17,5cede0edda562983788cb3c2,1e43c7084bdcb6b3102afaf301cce10faead2702,BioBERT: a pre-trained biomedical language representation model for biomedical text mining.,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"BERT (Devlin et al., 2019) is a contextualized word representation model that is based on a masked language model and pretrained using bidirectional transformers (Vaswani et al., 2017). Due to the nature of language modeling where future words cannot be seen, previous language models were limited to a combination of two unidirectional language models (i.e. left-to-right and right-toleft). BERT uses a masked language model that predicts randomly masked words in a sequence, and hence can be used for learning bidirectional representations. Also, it obtains state-of-the-art performance on most NLP tasks, while requiring minimal task-specific architectural modification. According to the authors of BERT, incorporating information from bidirectional representations, rather than unidirectional representations, is crucial for representing words in natural language. We hypothesize that such bidirectional representations are also critical in biomedical text mining as complex relationships between biomedical terms often exist in a biomedical corpus (Krallinger et al., 2017). Due to the space limitations, we refer readers to Devlin et al. (2019) for a more detailed description of BERT.###Fine-tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by Devlin et al. (2019). On the other hand, it takes more than 20 epochs for BioBERT to reach its highest performance on the NER datasets.###For computational efficiency, whenever the Wiki þ Books corpora were used for pre-training, we initialized BioBERT with the pre-trained BERT model provided by Devlin et al. (2019). We define BioBERT as a language representation model whose pre-training corpora includes biomedical corpora (e.",impact-revealing,providing context and significance of BERT in NLP and biomedical text mining
3173,5ea013159fced0a24b9cf180,351140fe9b4186a1ea17984397be022046f39946,AccelTCP: Accelerating Network Applications with Stateful TCP Offloading,5c96086e3cb210d2716c49e9,TAS: TCP Acceleration as an OS Service,"One can optimize them by exploiting flow-level parallelism [5, 30, 41, 59] or by steering the tasks into fast and slow paths [48] on kernel-bypass stacks.###While NIC offload is logically desirable, conventional wisdom suggests otherwise due to complexity [15, 48].",other,discussing optimization strategies for network interface card offload
2719,53e9b11db7602d9703b99bc4,41f128798ca097aec531ff7804aafe1043c019a1,Stream chaining: exploiting multiple levels of correlation in data prefetching,53e9bdfdb7602d9704aafae1,Using a user-level memory thread for correlation prefetching,"Also orthogonal to our work, [18] proposed using a user-level thread running in a processor-in-memory configuration to execute the prefetching algorithms.",other,acknowledge related work in prefetching algorithms
4057,5e5e18d493d709897ce3320c,222b9a7b8038120671a1610e857d3edbc7ac5550,Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models,53e9aa7ab7602d97033f434c,Regularization of Neural Networks using DropConnect.,"Dropconnect (Wan et al., 2013) chooses a parameter to drop with a probability of p.###, 2015), and dropconnect (Wan et al., 2013), as an adaptive L-penalty toward the origin (all zero parameters 0) and generalize dropout by considering a target model parameter u (instead of the origin), to which we refer as mixout(u).###Dropconnect (Wan et al., 2013) chooses a parameter to drop with a probability of p .###…of dropout and its variants, such as Gaussian dropout (Wang & Manning, 2013), variational dropout (Kingma et al., 2015), and drop-connect (Wan et al., 2013), as an adaptive L 2 -penalty toward the origin (all zero parameters 0 ) and generalize dropout by considering a target model…###However, neither Wan et al. (2013) nor Mianjy & Arora (2019) gives theoretical analysis for the extension of dropout which uses a point other than 0 .###The theoretical analysis for dropout as an L 2 -regularizer toward 0 was explored by Wan et al. (2013) where 0 is the origin.",other,describing dropout and its variants in model training
1275,,9aa364f00845752553343c87ccf21d678df78d5c,Dirac mixture distributions for the approximation of mixed effects models⋆,,,"###This model is widely used for method evaluation (Raue et al., 2009; Hass et al., 2016; Fröhlich et al., 2016) and possesses 9 state variables, 16 parameters and 3 observables.",impact-revealing,reporting on a widely used model for method evaluation
409,5f7d893591e011346ad27d16,ff8a988d88ebe5d5ca116340baa34ae00ce011e8,PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"A BERT-based planning model is ﬁrst designed to assign and position keyphrases into different sentences.###We apply the BPE tokenization (Sennrich et al., 2016) for the generation model as BART does, and use WordPiece (Wu et al., 2016) for BERT-based planner.###We ﬁrst study a planning model trained from BERT (Devlin et al., 2019) to produce the initial content plan, which assigns keyphrases to different sentences and predicts their positions.###Large pre-trained language models are the cornerstone of many state-of-the-art models in various natural language understanding and generation tasks (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), yet they are far from perfect.###The content planner is built on top of BERT base , which has 110 M parameters.###We choose BERT because it has been shown to be effective at both language modeling and sequence tagging.###We further design a separate keyphrase positioning layer to predict token position s j as the relative distance from each sentence’s beginning: where H L is the last layer hidden states of the Transformer, and W s are the newly added keyphrase positioning parameters learned during BERT ﬁne-tuning.###We further report results by using content plans predicted by our BERT-based planner.###In this section, we ﬁrst introduce content planning built upon BERT, that assigns keyphrases into sentences and predicts their positions (§ 3.1).###However, BERT relies on bidirectional self-attentions to attend to both left and right.###Our content planner is trained from BERT to assign keyphrases to different sentences and predict their corresponding positions.###We extract keyphrases and acquire their ground-truth positions from human-written references, and ﬁne-tune BERT with cross-entropy losses for both assignment and positioning, with a scaling factor 0 .",impact-revealing,describing the design and application of a BERT-based planning model
424,5e5e18ca93d709897ce315f0,68f86237dadcf2f570f0cd5b5e56161693619a74,Residual Energy-Based Models for Text Generation,53e9a645b7602d9702f7362e,Noise-contrastive estimation: A new estimation principle for unnormalized statistical models,"With the theoretical guarantee of NCE, we can show that the optimum of the above objective is reached at data distribution with inﬁnite amount of data and model with enough capacity, which is also proved in Ma & Collins (2018) 2 .###Instead, we train our residual energy function using Noise Contrastive Estimation (NCE) (Gutmann & Hyv¨arinen, 2010), and more speciﬁcally its conditional version (Ma & Collins, 2018).###NCE then trains a binary classiﬁer on the difference of log-probability scores of these two models.###NCE requires two distributions: The model distribution and a noise distribution.###In particular, we adopt the conditional noise contrastive estimation (NCE) objec-tive (Ma & Collins, 2018; Gutmann & Hyv¨arinen, 2010) to our residual model energy function and then sample from the joint model using importance sampling.###While Ma & Collins (2018) used conditional NCE to predict the next word in a sequence, we apply it to produce a whole sequence at once with the pretrained auto-regressive language model as the noise distribution.",impact-revealing,describing the application of noise contrastive estimation in model training
2201,5eb789d3da5629cf24430b41,1739466ac1411788cf1de60a3a6b59d739dc41ff,Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder,58d82fc8d649053542fd5862,Feature Pyramid Networks for Object Detection,"The ever-growing ability of deep learning has found numerous applications mainly in image classification, object detection, and natural language processing [12, 17, 28].",other,highlighting the applications of deep learning across various domains
3120,5550414c45ce0a409eb39fa8,081651b38ff7533550a3adfc1c00da333a8fe86c,How transferable are features in deep neural networks?,53e9b40eb7602d9703f04187,Visualizing And Understanding Convolutional Networks,"For example, Zeiler and Fergus (2013) found that it is better to decrease the first layer filters sizes from 11 × 11 to 7 × 7 and to use a smaller stride of 2 instead of 4.###A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.###…Recent studies have taken advantage of this fact to obtain state-of-the-art results when transferring from higher layers (Donahue et al., 2013a; Zeiler and Fergus, 2013; Sermanet et al., 2014), collectively suggesting that these layers of neural networks do indeed compute features that are…",other,highlighting findings on neural network layer initialization and its impact on generalization
3387,5c5ce4fd17c44a400fc38abb,d524f10d653ba09e36456475da0aed92d244f795,Image Super-Resolution As A Defense Against Adversarial Attacks,599c795f601a182cd263581a,A Simple Neural Network Module For Relational Reasoning,"SUCCESS of Convolutional Neural Networks (CNNs) over the past several years has lead to their extensive deployment in a wide range of computer vision tasks [1], [2], [4], including image classification [3], [5], [6], object detection [7], [8], semantic segmentation [9], [10] and visual question answering [11].",other,highlighting the success and application of CNNs in computer vision tasks
1488,,9c58634e8040eccfdeb21e4b8884476318e282f3,Learning Interpretable Style Embeddings via Prompting LLMs,,,"###Our approach is motivated by recent works showing models trained on synthetic datasets annotated by prompting LLMs can match and sometimes even outperform models trained on human-labeled datasets (Wang et al., 2022; Gilardi et al., 2023; Huang et al., 2022; Honovich et al., 2022).###Instead, we generate a synthetic dataset annotated by a large language model, a technique that has been applied to other domains (Han et al., 2021; Gilardi et al., 2023).",impact-revealing,highlighting the motivation behind using synthetic datasets annotated by LLMs
1494,,3852b468bd6c5a22e1c13a425fdd7604b5bcb7e2,SODA: Bottleneck Diffusion Models for Representation Learning,,,"###Consequently, diffusion models have been widely adopted for numerous tasks and modalities [29–33], synthesizing images, videos, audio and text [34–38], and even advancing planning [39] and drug discovery [40], effectively becoming one of the leading paradigms for generative modeling nowadays.",impact-revealing,highlighting the widespread adoption and significance of diffusion models in various tasks and modalities
3107,5a9cb66717c44a376ffb89eb,651adaa058f821a890f2c5d1053d69eb481a8352,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,599c796f601a182cd263cd58,Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection   Methods,"Much of what we describe below has been discussed in prior work (Carlini & Wagner, 2017a; Madry et al., 2018); we repeat these points here and offer our own perspective for completeness.###To generate (cid:96) ∞ bounded adversarial examples we use Projected Gradient Descent (PGD) conﬁned to a speciﬁed (cid:96) ∞ ball; for (cid:96) 2 , we use the Lagrangian relaxation of Carlini & Wagner (2017c).###Instead of actively attacking the detection method, we ﬁnd that LID is not able to detect high conﬁdence adversarial examples (Carlini & Wagner, 2017a), even in the unrealistic threat model where the adversary is entirely oblivious to the defense and generates adversarial examples on the original…###If g ( · ) is smooth and differentiable, then computing gradients through the combined network ˆ f is often sufﬁcient to circumvent the defense (Carlini & Wagner, 2017b).###As benchmarking against iterative optimization-based attacks (e.g., Kurakin et al. (2016a); Madry et al. (2018); Carlini & Wagner (2017c)) has become standard practice in evaluating defenses, new defenses have arisen that appear to be robust against these powerful optimization-based attacks.",other,acknowledging prior work and providing context for adversarial example generation
2781,5dbab2523a55acea3c05b02b,395de0bd3837fdf4b4b5e5f04835bcc69c279481,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",58d82fc8d649053542fd5ae7,Regularizing Neural Networks by Penalizing Confident Output Distributions.,"During finetuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.###During ﬁne-tuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.",other,reporting the method used for fine-tuning
261,5e3a93a93a55ac06c6119df5,cad9e682ddec3b1dd532cb8301737109d9eda7d7,Collaborative Distillation for Top-N Recommendation,5550489145ce0a409eb6f817,Dual-Regularized One-Class Collaborative Filtering,[19] proposed dual regularization by combining the weighted- and imputation-based methods.###Such ambiguity has been explicitly discussed in one-class collaborative filtering (OCCF) [12]– [19].,impact-revealing,reporting prior findings on dual regularization in collaborative filtering
1577,,653814c5f8111428fb492724c70e5947864a50c0,Error assessment of grid-based flow routing algorithms used in hydrological models,,,"###The shortfalls of D8 have been recognised and well discussed and analysed by previous studies (Fair eld and Leymarie 1991, Costa-Cabral and Burges 1994, Wilson and Gallant 2000).###Thus, by integrating DEMON (Costa-Cabral and Burges 1994) and the aspect-driven algorithm proposed by Lea (1992), he proposed the Deterministic in nite-node (Dinf, or D2 ) algorithm.###Numerous grid DEM-based hydrological modelling algorithms have been developed (O’Callaghan and Mark 1984, Freeman 1991, Quinn et al. 1991, Fair eld and Leymarie 1991, Costa-Cabral and Burges 1994, Meisels et al. 1995, Tarboton 1997, Mackay and Band 1998, Liang and Mackay 2000).###This also con rms the conclusion of Costa-Cabral and Burges (1994).###Costa-Cabral and Burges (1994) proposed the Digital Elevation Model Networks (DEMON ) algorithm in a conceptually similar way to the stream-tube approach used with contour-based DEM by Moore and Grayson (1991), but implemented on a grid DEM.###Thus it is not suitable for areas where divergent ow occur, such as convex slopes and ridges (Costa-Cabral and Burges 1994, Moore 1996, Wilson and Gallant 2000).###…have been developed to compute, for example, Total Contributing Area (TCA) (Freeman 1991, Moore et al. 1994), Speci c Catchment Area (SCA) (Costa-Cabral and Burges 1994, Gallant and Wilson 1996), Topographic Index (also known as ln(a/tanb) index, Quinn et al. 1991, 1995), and Stream Power…",impact-revealing,acknowledging limitations and previous discussions on D8 and related algorithms
2896,5b3d98cc17c44a510f80212a,cb91c2f8d3cac0b655a39be318b603334eb18987,learning to optimize tensor programs,5550410f45ce0a409eb384f8,Sequence to Sequence Learning with Neural Networks.,"DL models can now recognize images [23], understand natural language [38], play games [27], and automate system decisions (e.",other,highlighting the capabilities of deep learning models across various tasks
639,55465e4c0cf2939c2feea7c8,ff917ed73fa5491928c44e8f8850f0d311b0400c,Branch prediction and the performance of interpreters — Don't trust folklore,5c7958b34895d9cbc63d780d,A case for (partially) TAgged GEometric history length branch prediction,"And while many compilers now support this extensions (in particular, we checked GCC, icc, LLVM), older versions and proprietary, processor speciﬁc compilers may not support it.###As mentioned, a signiﬁcant part of the overhead of the dispatch loop is thought to come from the poorly predicted indirect jump that implements the switch statement.",impact-revealing,providing context on compiler support and performance issues
2426,5736974d6e3b12023e638aca,5b9c2b3f85920bc0e160b484ffa7a5f0a9d8f22a,Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction,53e9b89bb7602d97044763cc,Distant supervision for relation extraction without labeled data.,"This is easily seen in the line of work known as distantly-supervised relation extraction (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012); these models use the relation instances in a knowledge base as their only supervision, performing some heuristic mapping of the entities in…###We will touch on three broad categories related to KB completion: the task of relation extraction, embedding methods for KB completion, and graph meth-ods for KB completion.",other,acknowledge existing methods in relation extraction
1953,,59b9710d95d0e329f0fe08b0cf580000dc3d4b31,Scheduling Dependent Coflows with Guaranteed Job Completion Time,,,"###This abstraction has been widely used for simple, yet practical design of flow scheduling in data center networks [6, 16, 17].###The coflow scheduling can capture such application-level semantics to some extent, but it cannot account for the coflow dependencies, thus being efficient to provide guaranteed job completion time [4, 6].###[5, 6] have shown that scheduling those flow transfers at the level of coflow rather than the individual flow level can bring potential benefits for job completion time.",impact-revealing,highlighting the significance of coflow scheduling in data center networks
3939,5f7fdd328de39f0828397fae,645054d31fa26b29bbfb0cf73b75f8906c359415,spectral temporal graph neural network for multivariate time-series forecasting,59a02d92b161e8ad1a7b6d75,Stock Price Prediction via Discovering Multi-Frequency Trading Patterns,"We compare the performances of StemGNN on nine public datasets, ranging from traffic, energy and electrocardiogram domains with other state-of-the-art models, including FC-LSTM [26], SFM [32], N-BEATS [19], DCRNN [17], LSTNet [14], ST-GCN [31], DeepState [21], TCN [3], Graph Wavenet [29] and DeepGLO [25].###The overall time complexity is O(N3)
5 Experiments
5.1 Setup
We compare the performances of StemGNN on nine public datasets, ranging from traffic, energy and electrocardiogram domains with other state-of-the-art models, including FC-LSTM [26], SFM [32], N-BEATS [19], DCRNN [17], LSTNet [14], ST-GCN [31], DeepState [21], TCN [3], Graph Wavenet [29] and DeepGLO [25].###SMF [32] improves the LSTM model by breaking down the cell states of a given univariate time-series into a series of different frequency components through Discrete Fourier Transform (DFT).###%) on COVID-19
FC-LSTM [26] SFM [32] N-BEATS [19] TCN [3] DeepState [21] GraphWaveNet [29] DeelpGLO [25] StemGNN (ours)
7 Day 20.3 19.6 16.5 18.7 17.3 18.9 17.1 15.5 14 Day 22.9 21.3 18.5 23.1 20.4 24.4 18.9 17.1 28 Day 27.4 22.7 20.4 26.1 24.5 25.2 23.1 19.3
0
5000
10000
Brazil StemGNN
0
2000
4000
6000 Germany StemGNN
4/1 4/11 4/21 5/1 0
500
1000 1500 Singapore StemGNN
(a) Forecasting result for the 28th day 0 5 10 15 20
US Canada Mexico Russia
UK Italy Germany France
Belarus Brazil Peru
Ecuador Chile India Turkey Saudi Arabia Pakistan
Iran Singapore
Qatar Bangladesh
Arab China Japan Korea
0.4
0.5
0.6
0.7
(b) Inter-country correlations
Figure 3: Analysis on COVID-19
To investigate the feasibility of StemGNN for real problems, we conduct additional analyses on daily number of newly confirmed COVID-19 cases.###Table 4: Forecasting results (MAPE%) on COVID-19 FC-LSTM [26] SFM [32] N-BEATS [19] TCN [3] DeepState [21] GraphWaveNet [29] DeelpGLO [25] StemGNN (ours)###SFM models the time-series data in the frequency domain and shows stable improvement over FCLSTM.###For instance, State Frequency Memory (SFM) network [32] combines the advantages of DFT and LSTM jointly for stock price prediction; Spectral Residual (SR) model [23] leverages DFT and achieves state-of-the-art performances in###Time-series forecasting is an emerging topic in machine learning, which can be divided into two major categories: univariate techniques [20, 22, 18, 27, 32, 19, 18] and multivariate techniques [24, 21, 17, 31, 3, 29, 25, 16, 15].###For instance, State Frequency Memory (SFM) network [32] combines the advantages of DFT and LSTM jointly for stock price prediction; Spectral Residual (SR) model [23] leverages DFT and achieves state-of-the-art performances in ∗The work was done when the author did internship at Microsoft.",other,reporting performance comparisons of StemGNN with other models
2826,5e5794b791e011545375102b,38bccac4f05585ec595c7bb7c0e747561dcad886,DNN-Chip Predictor: An Analytical Performance Predictor for DNN Accelerators with Various Dataflows and Hardware Architectures,573696106e3b12023e523461,EIE: Efficient Inference Engine on Compressed Deep Neural Network,"While DNN accelerators can be 1000× more efficient than general purpose computing platforms [15], developing DNN accelerators presents significant challenges, because: (1) mainstream DNNs have millions of parameters and billions of operations; (2) the design space of DNN accelerator is large due to numerous design choices of architectures, hardware IPs, DNN-to-accelerator-mappings, etc.",other,highlighting challenges in developing DNN accelerators
637,5c234870da562935fc1d4db3,0b6a531754e67379518a946f3a3cf685f59358cc,CritICs Critiquing Criticality in Mobile Apps,53e9a186b7602d9702a7b885,Criticality-based optimizations for efficient load processing,"Without loss in generality, we have taken two representative, well-studied and well-proven criticality optimizations in prioritizing two important resources - one for memory which issues prefetches for critical loads [18] and another for ALU resources in instruction scheduling [4], [32], [33].###…level BPU [20], [59] Memory 2-way 32KB i-cache, 64KB d-cache, 2 cycle hit latency; 8-way 2MB L2 with System CLPT prefetcher (1024 × 7bits entries) [18]; Data Processing call), and the 3-bit argument with it to denote that the next l + 1 instructions would be 16-bit format to inform the decoder…###These proposals identify high-fanout loads to mark them as critical to issue prefetch [18] and prioritize the critical instructions for ALU resource allocations [32], [33].###Different optimizations can be employed upon fetching a critical instruction - prioritizing CPU resources [26]–[28], caches [8], [9], [18], memory requests [11], predicting instruction results [14], [29]–[31], issuing prefetches [18], etc.###For instance, we show that one such recent optimization [18], which prioritizes critical loads, does very well for SPEC workloads (as in prior works), but provides a measly 0.7% speedup for a wide spectrum of mobile apps.###Over the years, numerous criticality based optimizations have been proposed and studied for high-end workloads - prioritizing CPU resources [4]–[7], caches [8]–[10], memory request queues [11]–[13], predicting the result of the instruction [14]–[17], issuing prefetch requests [18], etc.###Traditional criticality based optimizations [9], [11], [12], [18] have targeted one critical instruction at a time, rather than groups or chains.###The high-fanout based optimization has also been shown to outperform the latency based ways of identifying and exploiting criticality [18], [34].###…be seen, the performance gains from prefetching high-fanout loads and prioritizing them at ALU resource scheduling are both quite signiﬁcant for SPEC.int (15% from prefetching, 9% from prioritizing) and SPEC.ﬂoat (34% from prefetching, 25% from prioritizing), re-afﬁrming prior results [18], [33].",impact-revealing,highlighting the significance of criticality optimizations in resource allocation
3366,5e09a76bdf1a9c0c41677a7b,71bd6b3a2bfa54ee4ea8499be0c4ff478fd735f6,POLAR++: Active One-Shot Personalized Article Recommendation,5736958b6e3b12023e4abf2f,Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval.,"The first group, called representation-based models, get the distributed semantic representation of an article with neural networks and then take as the similarity score the similarity (often cosine similarity) between distributed representations of two articles [11], [13], [27].",other,describing representation-based models for article similarity
3032,5c8b99db4895d9cbc69c7956,add350d0c5605c98d285b87493fc77c1d68281df,architectural support for server-side php processing,53e9b69eb7602d9704217429,Trace-based just-in-time type specialization for dynamic languages,"Prior works [21, 31–33, 38, 40, 42, 43, 49, 62, 71] attempt to mitigate abstraction overheads (see Section 3) associated with these languages.",other,acknowledge prior attempts to address abstraction overheads
2712,5a260c8117c44a4ba8a30b08,79cfb51a51fc093f66aac8e858afe2e14d4a1f20,Focal Loss for Dense Object Detection,558bc444e4b00c3c48de599f,Cascade object detection with deformable part models,"This inefficiency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping [32, 28] or hard example mining [36, 8, 30].###Class Imbalance: Both classic one-stage object detection meth-ods, like boosted detectors [19], [25] and DPMs [20], and more recent methods, like SSD [9], face a large class imbalance during training.###A common solution is to perform some form of hard negative mining [32, 36, 8, 30, 21] that samples hard examples during training or more complex sampling/reweighing schemes [2].###DPMs [20] helped extend dense detectors to more general object categories and had top results on PASCAL [26] for many years.###Class Imbalance: Both classic one-stage object detection methods, like boosted detectors [36, 5] and DPMs [8], and more recent methods, like SSD [21], face a large class imbalance during training.###DPMs [8] helped extend dense detectors to more general object categories and had top results on PASCAL [7] for many years.",other,highlighting common challenges and solutions in object detection
3140,555048d145ce0a409eb71b05,df787a974fff59f557ed1ec620fc345568aec491,learning deep representations for graph clustering,53e9b1d7b7602d9703c6c253,The Database of Interacting Proteins: 2004 update.,This is an unweighted protein-protein interaction (PPI) network from the Database of Interacting Proteins (Salwinski et al. 2004).,other,providing context about a specific protein-protein interaction network
3621,5dbebb7447c8f766462c2328,b0d941cfa0a3c43703e5222221addb1ad0f9e68d,Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks,59ae3bf12bbe271c4c71bf6a,Session-aware Information Embedding for E-commerce Product Recommendation.,[37] proposed a list-wise deep neural network model to train a ranking model.,other,reporting prior findings on ranking models
3002,5d3ed25a275ded87f97deb36,37f7eaf57b88ef03efd4196f2f57ae0b4c657a79,Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommendation,5bbacb2c17c44aecc4eab5f7,Graph Convolutional Matrix Completion for Bipartite Edge Prediction.,"[31] use GNNs on user/item intrinsic structure graphs to learn user/item representations.###Recently, several works developed GNN architectures for recommender systems [14, 19, 31, 32], but these approaches are all designed for homogeneous bipartite user-item interaction graphs or user/item-similarity graphs.",other,acknowledge existing GNN architectures for recommender systems
1218,,92118faf27749fe1b98083d20ed3fac0f36dba41,"Relationship Between Obesity, Physical Activity, and Cardiorespiratory Fitness Levels in Children and Adolescents in Bosnia and Herzegovina: An Analysis of Gender Differences",,,"###To define underweight, overweight and obese according to BMI, the 5th, 85th and 95th BMI for age, the Centers for Disease Control and Prevention (CDC) reference percentiles were used (Ogden et al., 2002; Goncalves et al., 2015).###, 2014) reaching epidemic proportions both in developed (Ogden et al., 2002; Gomes et al., 2014) and less developed countries (Ebbeling et al.###Although BMI has been commonly used as a sensible indicator of overall adiposity in children (Ogden et al., 2002), it seems that BMI has limitations in assessing the prevalence of age- and sex-specified obesity as well as body composition and fat distribution (Savva et al.",impact-revealing,highlighting the limitations of BMI as an indicator of obesity
385,5e9ef9b69fced0a24b1b65a2,69f1ab7fd22c3df3c9900430566be890e1529b4e,NetTaxo: Automated Topic Taxonomy Construction from Text-Rich Network,53e99c8bb7602d9702539072,Network Motifs: Simple Building Blocks Of Complex Networks,"Meta-paths [31] and motif patterns [5, 18] have been widely adopted to extract useful structural information from networks.###Network motifs are higher-order subgraph structures that are critical in complex networks across various domains, such as neuroscience [30], bioinformatics [18], and information networks [5].",impact-revealing,highlighting the significance of network motifs in various domains
2385,5f92ba1691e011edb3573ba0,268d347e8a55b5eb82fb5e7d2f800e33c75ab18a,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,573696ce6e3b12023e5ce95a,Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift,"For the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization layers (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized convolutions (Qiao et al., 2019).",other,reporting modifications to baseline CNNs
685,53e9aeebb7602d970391ac0a,8681e808a9ebd7f7f155590e75fb63563a8aae6e,performance prediction based on inherent program similarity,53e99b16b7602d97023aa8bf,Structures for phase classification,The data stream is characterized with respect to local and global data strides [10].,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1227,,f47edb85adbfabf43c107be7f482778b36a47154,One-Health Simulation Modelling: Assessment of Control Strategies Against the Spread of Influenza between Swine and Human Populations Using NAADSM.,,,"###However, other studies have highlighted the importance of investigating the spread and control strategies targeted at the household level together with zones of certain radius (Ferguson et al., 2005, Longini et al., 2005, Wu et al., 2006, Fraser, 2007).###…guide and inform the development of contingency plans and policy for preparedness and response to future pandemic threats (Ferguson et al., 2005, Longini et al., 2005, Ferguson et al., 2006, Germann et al., 2006, Halloran et al., 2008, Basta et al., 2009, Gojovic et al., 2009, Tuite et al.,…###Few studies in humans have also investigated the spread and control of influenza at the household level (Ferguson et al., 2005, Longini et al., 2005, Wu et al., 2006, Fraser, 2007, Shaban et al., 2009).###…the largest impact on the modelled outcomes in this study was consistent with these other studies that compared the similar control strategies targeted at the household level and used zones of a certain radius around infected cases (Ferguson et al., 2005, Longini et al., 2005, Shaban et al., 2009).###A similar time-frame for speed of detection and implementation of control measures was used for pandemic influenza spread in humans by Longini et al (2005), where delay times of 7, 14, or 21 days after the detection of the first case were investigated.",impact-revealing,highlighting the importance of household-level strategies in pandemic preparedness and response
115,5ca600ae6558b90bfa4d76e9,7aa38b85fa8cba64d6a4010543f6695dbf5f1386,Towards Deep Learning Models Resistant to Adversarial Attacks,5550417845ce0a409eb3b9b3,Explaining and Harnessing Adversarial Examples.,"Due to the growing body of work on adversarial examples (Gu & Rigazio, 2014; Fawzi et al., 2015; Torkamani, 2016; Papernot et al., 2016; Carlini & Wagner, 2016a; Tramèr et al., 2017b; Goodfellow et al., 2014; Kurakin et al., 2016), we focus only on the most related papers here.###Unfortunately, ERM often does not yield models that are robust to adversarially crafted examples (Goodfellow et al., 2014; Kurakin et al., 2016; Moosavi-Dezfooli et al., 2016; Tramèr et al., 2017b).###For instance, the `∞-ball around x has recently been studied as a natural notion for adversarial perturbations (Goodfellow et al., 2014).###While trained models tend to be very effective in classifying benign inputs, recent work (Szegedy et al., 2013; Goodfellow et al., 2014; Nguyen et al., 2015; Sharif et al., 2016) shows that an adversary is often able to manipulate the input so that the model produces an incorrect output.###On the attack side, prior work has proposed methods such as the Fast Gradient Sign Method (FGSM) and multiple variations of it (Goodfellow et al., 2014).###Computer vision presents a particularly striking challenge: very small changes to the input image can fool state-of-the-art neural networks with high probability (Szegedy et al., 2013; Goodfellow et al., 2014; Nguyen et al., 2015; Sharif et al., 2016; Moosavi-Dezfooli et al., 2016).",impact-revealing,highlighting the challenges and limitations of existing models in the context of adversarial examples
3889,5c45b4d03a55ac25e7f0f55c,e5badcfa663c30a983da24dd682288141d00fcc3,GraphSAR: A Sparsity-Aware Processing-in-Memory Architecture for Large-Scale Graph Processing on ReRAMs,573695cf6e3b12023e4e7f37,Technological Exploration of RRAM Crossbar Array for Matrix-Vector Multiplication,"The emergingmetal-oxide resistive random accessmemory (ReRAM) and ReRAM crossbars show huge potential in energy efficient processing-in-memory operations [15, 16], especially for matrixvector multiplications [17].",other,highlighting the potential of ReRAM technology in energy-efficient computing
2664,5bdc316717c44a1f58a071ff,ec3071fb918ad69ec80df1ca9cf1fdeb386a9603,TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.,5a9cb66717c44a376ffb8cce,Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions.,"We include the TensorComprehension (TC, commit: ef644ba) [37] a recently introduced auto-tuning framework as an additional baseline.###Tensor comprehension [37] applied black-box autotuning together with polyhedral optimizations to optimize CUDA kernels.",other,reporting the inclusion of a new baseline method
2444,5db9298647c8f766461f8ed6,784b018c87c7dcbbe772374e45d5191bae9938ee,Hyperbolic Graph Neural Networks,5736986c6e3b12023e7308a0,Convolutional Networks on Graphs for Learning Molecular Fingerprints,"Applications include molecular design [31], ﬁngerprinting [14] and poly pharmaceutical side-effect modeling [52].###Due to their ability to learn inductive models of graphs, GNNs have found promising applications in molecular ﬁngerprinting [14] and quantum chemistry [18].",other,highlighting applications of GNNs in various fields
3573,5e5e18ba93d709897ce2b48e,04f3203f1214063436d81ce0c2ad7623204da488,Geom-GCN: Geometric Graph Convolutional Networks,5a260c8117c44a4ba8a30d2e,"A Comprehensive Survey of Graph Embedding: Problems, Techniques and   Applications",", citation networks (Kipf & Welling, 2017) and community networks (Chen et al., 2019)) where node homophily holds (i.e., similar nodes are more likely to be proximal, and vice versa.), but may be inappropriate to the disassortative graphs where node homophily does not hold (Newman, 2002). For example, Ribeiro et al. (2017) shows disassortative graphs where nodes of the same class exhibit high structural similarity but are far apart from each other.###One can employ various embedding methods to infer the latent space (Cai et al., 2018; Wang et al., 2018).",other,highlighting the limitations of existing methods in disassortative graphs
997,,ed92545537832ba066152e342a93f50c2f0d53f4,The neural correlates of attention deficit hyperactivity disorder: an ALE meta-analysis.,,,"###In particular, frontostriatal and fronto-parietal networks supporting an array of top-down or executive processes, such as dorsolateral prefrontal cortices, anterior cingulate cortices, and associated striatal regions, are frequently cited as loci of dysfunction in ADHD (e.g., Barkley, 1997;  Castellanos & Tannock, 2002 ).",impact-revealing,highlighting key brain networks associated with ADHD dysfunction
3330,5f03f3b611dc83056223206d,75c8466a0c1c3b9fe595efc83671984ef95bd679,XGNN: Towards Model-Level Explanations of Graph Neural Networks,5e5e18d793d709897ce34a43,StructPool: Structured Graph Pooling via Conditional Random Fields,"In addition, extensive efforts have been made towards different graph operations, such as graph convolution [13, 16, 19], graph pooling [20, 44], and graph attention [10, 36, 37].",other,acknowledge existing research on graph operations
3615,5a260c0c17c44a4ba8a1e113,c751ab01aedc2888a7fe6e8b4f77ab1afa94072f,protein interface prediction using graph convolutional networks.,5c756865f56def97981ee27e,Deep learning for computational biology.,"The remarkable success of deep neural networks in a wide range of challenging machine learning tasks from computer vision [14, 15] and speech recognition [12] to machine translation [24] and computational biology [4], has resulted in a resurgence of interest in this area.",other,highlighting the broad impact and success of deep neural networks across various fields
1344,,45d0d9a6b71f4ede12edf226280f0e52f434763f,Local binary pattern statistics feature for reduced reference image quality assessment,,,"###Some are motivated by the natural scene statistics (NSS) and they use the information fidelity criteria to quantify the information shared between the distorted and the reference images [6, 7].",impact-revealing,acknowledge motivations behind certain methods in image processing
1372,,1f83e2b9ffadca2a57a5677b32887eb7144fc6be,DNA-based Diagnosis of Uncharacterized Inherited Macrothrombocytopenias Using Next-generation Sequencing Technology with a Candidate Gene Array,,,"###Authors commented that the laboratory may also simply elect to exclude the target from the report if Sanger sequencing is not performed despite low coverage [39].###Moreover, relative to WES and WGS, it provides good gene coverage and representation of exons, is relatively fast and cheap and minimizes the problems with unexpected findings and development of complex downstream bioinformatic pipelines for analysis [39].###The question as to whether confirmatory Sanger sequencing need be performed is debated in the literature [39, 67].",impact-revealing,Highlighting the advantages and ongoing debates regarding Sanger sequencing in comparison to other methods
808,5ef3247091e0110c353da5ff,06bf758b7e7fd675ceb2d008520db51631716d42,Embedding-based Retrieval in Facebook Search,53e9986eb7602d97020a7ef9,Representation learning: a review and new perspectives.,"n, computer vision, and natural language understanding [10]. Among them embedding, which is also called representationlearning, has been proven to be successful techniques contributing to the success [2]. In essence, embedding is a way to represent a sparse vector of ids as a dense feature vector, which is also called semantic embedding in that it can often learn the semantics. Once the embeddings ar",impact-revealing,providing context on the concept of embedding in representation learning
765,5aed14d617c44a4438158d78,7d89abfe87ed7d1b40391d37364560656d208117,learning memory access patterns,573695fe6e3b12023e5116f7,Pixel recurrent neural networks,"In (Oord et al., 2016b), they predict 16-bit integer values from an acoustic signal.###We take inspiration from recent works in image and audio generation that discretize the space, namely PixelRNN and Wavenet (Oord et al., 2016a;b).###This idea of treating the output prediction problem as one of classiﬁcation instead of regression has been successfully used in image (Oord et al., 2016a) and audio generation (Oord et al., 2016b).",impact-revealing,drawing inspiration from successful methods in image and audio generation
2010,,9ec0c968996495a5987c28250a65e97ca93d6995,Rectal sensory threshold for pain is a diagnostic marker of irritable bowel syndrome and functional abdominal pain in children.,,,"###Parallel to visceral hypersensitivity, abnormal somatic referrals in response to rectal distension have been reported in patients with IBS and FAP as compared with control subjects.(2,6) Whether visceral hypersensitivity is a biologic marker of FGID is debated in the literature largely because no prospective study has been attempted to measure rectal sensory threshold for pain (RSTP) in a cohort of patients suffering from symptoms of abdominal pain.###Previous studies have reported that in control subjects without any gastrointestinal complaints, rectal isobaric distension provokes sensations mainly referred to the S3 dermatome (perineal area), and most patients with IBS refer their sensation to aberrant sites (abdominal projections to dermatomes T8 to L1).(2,26) We hypothesize that subjects with protracted complaints of abdominal pain not related to FGID may have a normal visceral sensitivity but in contrast to ‘‘true’’ control subjects may have an abnormal###8 mm Hg, the fifth percentile for the control subjects, the RSTP had a sensitivity rate of 89% and a specificity rate of 83% for IBS and FAP diagnosis.(2) However, in both of these studies patients were initially recruited and classified into subgroups accord-",impact-revealing,highlighting the debate on visceral hypersensitivity as a biologic marker of FGID
2074,,ec2c9fb733aba6c014b7bd8d88eb95c69f9ea103,The JuliaConnectoR: a functionally oriented interface for integrating Julia in R,,,"###Due to the disadvantage of JSON mentioned above, XRJulia also deviates from this strategy for large data by writing vectors in intermediate files (see largeVectors documentation item in the manual of XRJulia).###XRJulia lets R and Julia communicate via JSON messages.###This avoids transformations which are necessary for using text-based exchange formats like JSON, where numbers have to be converted to strings containing decimal representations.###The format is inspired by BSON (MongoDB, Inc. 2009), a format that is an alternative binary format to JavaScript Object Notation (JSON) (Bray 2017).###2009), a format that is an alternative binary format to JavaScript object notation (JSON; Bray 2017).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
454,5f91548b91e011126509bd5a,75739ed2ddebd7982042f516f407553f8d3110f8,Self-supervised Graph Learning for Recommendation,5e3d353b3a55ac4de4104f40,LightGCN: Simplifying and Powering Graph Convolution Network for  Recommendation,"trainable parameters, the space complexity remains the same as LightGCN [17].###• LightGCN [17].###This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage [48] and LightGCN [17].###The technique is inspired from the graph convolution networks (GCNs), which provide an effective end-to-end way to integrate multi-hop neighbors into node representation learning and achieve state-ofthe-art performance for recommendation [17, 36, 44, 48].###We conduct experiments on three widely used benchmark datasets: Yelp2018[17, 44], Amazon-Book[17, 44], and Alibaba-iFashion [6]1.###which can be simply set as the last-layer representation [36, 48], concatenation [44], or summation [17] over the representations of all layers.###Most existing models [17, 36, 44] construct a bipartite graph G = (V, E), where the node set V = U ∪ I involves all users and items, and the edge set E = O+ represents observed interactions.###Most models approach the recommendation task under a supervised learning paradigm [17, 19, 32], where the supervision signal comes from the observed user-item interactions.###Following [17, 44], we use the same 10-core setting for Yelp2018###Here we implement it on a state-of-the-art GCN-based model, LightGCN [17].",impact-revealing,describing the effectiveness of graph convolution networks for recommendation
2952,5b1642388fbcbf6e5a9b54be,b3dae9529f3caeeec9cc6872e94aa690418acb22,Reinforcement Learning for Relation Classification from Noisy Data,56d81390dabfae2eee6255de,Using active learning and semantic clustering for noise reduction in distant supervision,"There are other approaches to reduce the noise of distant supervision using active learning (Sterckx et al. 2014) and negative patterns (Takamatsu, Sato, and Nakagawa 2012).",other,acknowledge alternative approaches to reduce noise in distant supervision
1511,,3cd624c6dc313f9fcca83c861f38adbe05994cd8,Combined intrarectal lidocaine gel and periprostatic nerve block: A ‘balanced’ anaesthesia for transrectal ultrasound-guided prostate biopsy?,,,"###Extended biopsy protocol has been adopted by many urologists following the evidence that sextant protocol fails to adequately sample the prostate.[1,2] The proportional increase in the pain with the number of biopsy cores underscores the need for adequate anaesthesia.",impact-revealing,highlighting the adoption of extended biopsy protocols due to limitations of sextant protocol
2023,,ef777ac4a61425617d55b83a5c389d94e0a2ed67,Mutigroup-Based Phasmatodea Population Evolution Algorithm with Mutistrategy for IoT Electric Bus Scheduling,,,"###Metaheuristic algorithms are inspired by relevant experiences, behaviors, rules, and mechanisms in the fields of physics, chemistry, biology, society, and art [1].",impact-revealing,providing context on the inspiration behind metaheuristic algorithms
1596,,0ac2f38c110364b60cf6925474accf0b5627381e,Muscarinic control of rostromedial tegmental nucleus GABA neurons and morphine‐induced locomotion,,,"###RMTg directly disinhibits VTA DA neurons; however, the disinhibition alone is not sufficient for sustained DA efflux (Jalabert et al., 2011).###Previous studies have demonstrated that inhibition of RMTg GABA neurons (with morphine, WIN55 or muscimol) resulted in EPSPs in VTA DA neurons (Jalabert et al., 2011; Lecca et al., 2011, 2012).###These GABA neurons then strongly inhibit DA neurons in the VTA (Balcita-Pedicino et al., 2011; Jalabert et al., 2011; Matsui & Williams, 2011) and substantia nigra compacta (Bourdy et al., 2014).###RMTg neurons express high levels of l-OR (Jhou et al., 2009a, 2012; Jalabert et al., 2011).###…excitatory inputs from LDT and PPT through M5 and nicotinic ACh receptors and AMPA glutamatergic receptors (Forster & Blaha, 2000; Watabe-Uchida et al., 2012); these inputs facilitate the excitation of VTA DA neurons following opioid administration (Jalabert et al., 2011; Steidl et al., 2011).###7) (Jalabert et al., 2011; Lecca et al., 2011; Matsui & Williams, 2011).###facilitate the excitation of VTA DA neurons following opioid administration (Jalabert et al., 2011; Steidl et al., 2011).",impact-revealing,reporting prior findings on the role of RMTg in dopamine neuron activity
3059,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,53909f2d20f70186a0e3961f,Quantifying the cost of context switch.,"We then estimate upper and lower context switch penalty bounds using switching latencies reported by prior works [52, 53].",other,reporting methodology for estimating context switch penalties
2922,5c234870da562935fc1d4da6,94bd59e507ba8496b36605be0f6740e5731e91d5,CounterMiner: Mining Big Performance Data from Hardware Counters,53909fbd20f70186a0e42dda,"Time Interpolation: So Many Metrics, So Few Registers","We take the sampled values of an event as a time series since it is important to observe the time varying behaviors of the event [30].###However, MLPX incurs large measurement errors due to time-sharing and sampling [4], [30]–[32].###However, large measurement errors occur with MLPX because information may be lost when the event does not happen during a sampled interval [30], [33], [34] but happens during a un-sampled interval.###Based on the samples, the full behavior of each event is extrapolated [30].",other,highlighting the importance of time series analysis and measurement errors in event observation
1462,,fb894a5dfdbab36678814de8da668e4adb725ff0,Algorithms in the historical emergence of word senses,,,"###Pioneering research by Rosch [5] suggested that common semantic categories signiﬁed by words such as bird and furniture may exhibit a prototype structure, such that some members of a category are viewed as more representative than others.###This algorithm is motivated by Rosch [5] and Geeraerts [7] and predicts the emerging sense at t + 1 with a probability based on semantic similarity with the prototypical sense at time t .###We hypothesize that chaining might be a preferred mechanism of lexical evolution because it facilitates sense extensions that are cognitively “cheap,” conforming to the general principle of cognitive economy [5].",impact-revealing,highlighting the significance of prototype structure in semantic categories
2553,5f7fdd328de39f08283980ba,2fce1ef37391cd685fc5459e1cbfcb8490b85242,Graph information bottleneck,5bdc31c217c44a1f58a0cd24,"Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow.","Other methods apply IB to various domains [40,41].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3659,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,53e99809b7602d970201f4fe,A distributed sensor network for video surveillance of outdoor environments,"Examples include Foresti and Snidaro [40], Yang et al. [152] for detecting and tracking people, and Wang et al. [136] and Kankanhalli et al. [67] for video surveillance and traffic monitoring.###Linear weighted fusion Feature Foresti and Snidaro [40] Video (trajectory coordinates) Human tracking###Examples include Foresti and Snidaro [40], Yang et al.###However, unlike Foresti and Snidaro [40], Yang et al.###Foresti and Snidaro [40] used a linear weighted sum method to fuse trajectory information of the objects.###However, many of them either have considered equal weights [83, 152] or have not elaborated the issue of weight determination [55, 67, 136], and have left it to the users to decide [40].###The methods in this category are the support vector machine, Table 1 A list of the representative works in the rule-based fusion methods category Fusion method Level of fusion The work Modalities Multimedia analysis task Linear weighted fusion Feature Foresti and Snidaro [40] Video (trajectory coordinates) Human tracking Wang et al. [136] Video (color, motion and texture Human tracking Yang et al. [152] Video (trajectory coordinates) Human tracking Kankanhalli et al. [67] Video (color, motion and texture) Face detection, monologue detection and traffic monitoring Decision Neti et al. [87] Audio (phonemes) and visual (visemes)###However, unlike Foresti and Snidaro [40], Yang et al. [152] assigned equal weights to the different modalities.",other,acknowledge existing methods in human tracking and their limitations
921,573695886e3b12023e4a8d45,bd908f1aa7818412e8e09b8c17ee8324d2346dfe,Optimizing drug–target interaction prediction based on random walk on heterogeneous networks,55a4ad2165ceb7cb02d5d050,Drug-target interaction prediction by random walk on the heterogeneous network.,"Recent work has demonstrated the power of network-based approaches in drug discovery [1–3].###In this work, we apply a random walk-based link prediction algorithm based on Chen et al. [3] to a more extensive drug–target network and evaluated its performance using an external dataset.###First, we offer a general approach that takes the whole drug target network into account without sepa-rating protein categories, in contrast to the previous study [3].###It has been shown that the weight parameters w d and w t are robust among the prediction results [3].###The calculation of each of the transition matrix in discussed in Chen et al. [3].###This is the first time that the random walk-based method is evaluated using a binding assay dataset (cf. [3, 5]).###Because of these evidences, we here simply adopt the previously used value of 0.3 [3].",impact-revealing,highlighting the application and evaluation of a network-based approach in drug discovery
3497,5f0d85c69fced0a24be4f052,0dd3e9f581c617eb826bc0fabac5ae1394f9cef1,Data Compression Accelerator on IBM POWER9 and z15 Processors : Industrial Product,53e9b708b7602d970429dac5,Stateful hardware decompression in networking environment.,"Simple hardware designs can handle at most one code per cycle [27], because the next code’s ﬁrst bit position cannot be known until the current one is decoded.###We describe a speculative decoder capable of decoding 8 codes per cycle in Section VI where the related work [27], [28] is also discussed.",other,describing a speculative decoder's capabilities in comparison to simple hardware designs
1255,,802882a514d4203227735bb56f4695fa5c524332,On sequential and parallel non-monotone derivative-free algorithms for box constrained optimization,,,"###It is for instance the approach commonly used for solving mixed integer problems ([1,35] and references therein).",impact-revealing,providing context for a common approach in mixed integer problems
2499,5c7a561ff56def9798e6a297,abe4e92d6e4868115aef57a4eba97ec4d48ac124,language-adversarial transfer learning for low-resource speech recognition,5a73cb6317c44a0b3035844d,2016 But Babel System: Multilingual Blstm Acoustic Model With I-Vector Based Adaptation,"More recently in [6], Karaﬁat et al. use bi-directional long-short term memory (BLSTM) based source model to transfer shared parameters.###Various ASR efforts in the last couple of years have shown improved performance with i-vector features in addition to the acoustic features in the ASR modeling [3], [6].",other,reporting recent advancements in ASR modeling
430,5eb3df3191e011cea6a7c3c8,86746ca4e3fb61cbcfb8dc29d6779d51b03692e0,Effect of Character and Word Features in Bidirectional LSTM-CRF for NER,573695fe6e3b12023e511e25,Bidirectional LSTM-CRF Models for Sequence Tagging,"This paper, we combined two techniques proposed in Chiu et al., 2015 (CNN + Bidirectional LSTM) and Huang et al., 2015 (Bidirectional LSTM + CRF), respectively, using public word embedding, character features and word features [6,4].###, 2015 [4] presented a word LSTM with conditional random field (CRF) to improve the NER model.###, 2015 [4], and adjust masks of numbers of Ratinov et al.###Huang et al., 2015 [4] presented a word LSTM with conditional random field (CRF) to improve the NER model.###, 2015 (Bidirectional LSTM + CRF), respectively, using public word embedding, character features and word features [6,4].###(2015) implemented using CNN, LSTM, and Bidirectional LSTM respectively [3,4].",impact-revealing,reporting on the combination of techniques for NER improvement
944,5d64ff713a55acf547f20de0,26d3f8db3a4275225d355a9ce8e132b8c19c225b,Once-for-All: Train One Network and Specialize it for Efficient Deployment on Diverse Hardware Platforms,5c2c7a9217c44a4e7cf31437,ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware.,"To derive a sub-network that has D layers in a unit that originally has N layers, we keep the first D layers and skip the last N −D layers, rather than keeping any D layers as done in current NAS methods (Cai et al., 2019; Wu et al., 2019).",impact-revealing,describing a method for deriving a sub-network in neural architecture search
3980,5f64211c9e795e0286c313a2,f5316f15c665e5a5f89b8b70de13438892e21207,ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks,5b3d98cc17c44a510f80181e,Born Again Neural Networks,"For example, multiple networks are trained for KD [7].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1362,,f7ae14879f9fba229a709209184e8986a63783b8,Privacy and usability of image and text based challenge questions authentication in online examination,,,"###Challenge questions authentication is widely used for credential recovery and secure transactions in the online banking sectors [12],[10].",impact-revealing,highlighting the application of challenge questions in online banking
2522,5f3f917891e011d38f9242d9,14156438bafed28a626738630b5181b83ed5d79c,Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters,599c797d601a182cd2643e8a,Modeling Relational Data with Graph Convolutional Networks.,"For the GCN, GAT, RGCN, GraphSAGE, GeniePath, we use the source code provided by authors.###We select GCN [17], GAT [34], RGCN [31], and GraphSAGE [12] to represent general GNN models.",other,reporting the use of source code for various graph neural network models
2221,5da1a6d447c8f766460688bc,9570147aee646e15affffc30a6b18be0568cfa82,SWQUE: A Mode Switching Issue Queue with Priority-Correcting Circular Queue,55465e110cf2939c2fee990e,IBM POWER8 processor core microarchitecture,"For example, the CAM type is used in the AMD Bulldozer [11], whereas the RAM type is used in the IBM POWER8 [26].###A circuit called the age matrix [22, 24], which selects the single oldest ready instruction, is used together with RAND in current processors [11, 22, 26].###Although all processor vendors do not publish their IQ organizations, AGE is generally used in modern processors [11, 22, 26].",other,providing context on processor types and their components
2795,5e2d653a3a55acc8374367eb,f4be875eb05424e7606b03686750a7fb41684579,Privacy-Aware Recommendation with Private-Attribute Protection using Adversarial Learning,53e9a44fb7602d9702d6cf39,"A Privacy-Preserving Framework for Personalized, Social Recommendations."," to construct private covariance Privacy-Aware Recommendation with Private-Attribute Protection WSDM ’20, February 3–7, 2020, Houston, TX, USA matrices to be further used by recommender. Another work [26] clusters users w.r.t. the social relations and generates differentially private average of users’ preferences in each cluster. Hua et al. [23] propose a private matrix factorization which adds noise ###against re-identification attacks in which an adversary tries toinferatargeteduser’sactualratingsandinvestigateifthetargetis in the database. They could be categorized into differential privacy based [23, 26, 33, 45] and perturbation based [32, 38, 41] approaches. Some methods utilize differential privacy strategy [14] to modify the answers of the recommendation algorithm so the the presence of a user’s data (eit",other,reporting on various approaches to privacy-aware recommendation systems
2146,,7175b6530cf01690b9648d57fc3bf12e53c29019,Examining the impact of computer-mediated social networks on individual consumerism environmental behaviors,,,"###By identifying the source of the message and examining how this source plays a role in the belief formation and behavioral change, this study attempts to more precisely capture the SIP effect within the CMSN.
Watson et al. (2010) highlighted the importance of social norms in changing ‘‘citizens’ behavior in an environmentally desirable direction.’’###CMSNs can play a role in furthering the solidification of environmental consumerism in society through message cues from influential actors within the CMSN. Hedonic intentions are also positively related to recycling behaviors, where individuals enjoy and find satisfaction in engaging in the relatively simple way to help the environment by recycling.###Individuals are constantly assessing the social environment they are operating in and adjusting their behavior based on the prompts and feedback that they receive within the CMSN.
Additionally, SCT highlights the concept of individual self-efficacy, described as an individual’s perception of what they are able to do, greatly impacts their interpretation of the positive outcomes that are associated with a particular behavior (Compeau et al., 1999).###Individual’s attitudes and behaviors are ‘‘responsive to cuing during prolonged and intense group interaction’’ (Fulk et al., 1987) as what is experienced in a CMSN. Understanding how the features of these types of computer-mediated information social structures, such as CMSNs, can influence individual environmental behavior is an important research focus (Watson et al., 2010).###Understanding how the features of these types of computer-mediated information social structures, such as CMSNs, can influence individual environmental behavior is an important research focus (Watson et al., 2010).###To further understand the influential impact that CMSNs have on an individual’s behavior, we examine CMSN Intensity, which we define as the user’s engagement in and usage of the CMSN.###IS researchers have also been interested in understanding the influential impact of information channels that do not ‘‘provide the full range of social cues [and] nonverbal signals’’ (Rice & Aydin, 1991), such as a CMSN. Human–computer interaction research has also directly called for an examination of ‘‘primes associated with green consumerism would activate norms of social responsibility and environmentally sustainable conduct and thereby increase corresponding behaviors’’ (Chiou et al., 2012).###…intense group interaction’’ (Fulk et al., 1987) as what is experienced in a CMSN. Understanding how the features of these types of computer-mediated information social structures, such as CMSNs, can influence individual environmental behavior is an important research focus (Watson et al., 2010).###Individuals in this social environment begin to engage in sense-making (Fulk et al., 1987) of all the information cues that they encounter within the CMSN.###Additionally, the CMSN can be viewed as a positional network where proximity of the CMSN influencer refers to the extent that individuals occupy the same positions (Rice & Aydin, 1991) within the CMSN.###CMSNs have a role in the process of spreading the message for taking care of the environment that we all take a part in. Research has yet to show what aspects of CMSNs are responsible for the changes that are found in one’s understanding of various topics, after they have been exposed to various messages in the CMSN.###This means that in addition to the level of awareness of what one can do for the environment, the message-giver in a CMSN is what has an impact on their level of responsibility for the environment, not simply their increased usage and engagement of the CMSN.###…how this source plays a role in the belief formation and behavioral change, this study attempts to more precisely capture the SIP effect within the CMSN.
Watson et al. (2010) highlighted the importance of social norms in changing ‘‘citizens’ behavior in an environmentally desirable direction.’’",impact-revealing,highlighting the role of social norms in influencing environmentally desirable behavior
1090,,b38c7cf19f1a779046afbf189090ce7f4d0e2258,Citrus Fruit Quality Classification using Support Vector Machines,,,"###The feature vector built for the training was inspired by [8], using 64 colors features, 7 texture features, 8 shape features.###The metrics chosen to evaluate the model was f1-score, accuracy and confusion matrix one of the most used metrics to evaluate pattern recognition models [1],[2],[8],[15].",impact-revealing,reporting the feature vector and evaluation metrics used in the model
494,5e09a801df1a9c0c41680233,1438d8c68b4495947fd5de001b87e6ef5a263a3e,Learning to Jointly Generate and Separate Reflections,5a260c8417c44a4ba8a31302,A Generic Deep Architecture for Single Image Reflection Removal and Image Smoothing,"The comparison methods include Wan18 [24], Zhang18 [29], CycleGAN [30], and FY17 [5].###Existing method [5] can be treated as a special instance of our method when the generator is sim-pliﬁed as a linear function.###In contrast with previous methods [24, 13, 5], that heavily rely on the simpliﬁed model in Equation 1 and regard the image generation and separation as two independent stages, the proposed model leverages the mutual beneﬁts of the image generation and separation in a joint learning manner to…###Instead of the one-to-one mapping in previous methods, our generator learn the mapping as G : ( B , R ) → M , where the non-linear mappings can produces more realistic reﬂection appearances (ﬁrst to third columns in Figure 4 1 ) than previous linear functions [5, 26, 17, 1] with ﬁxed coefﬁcients.###Recently, deep learning based reﬂection removal meth-ods [24, 5] with better generalization ability have been proposed to address the limitations arising from the hand-crafted image priors.###In contrast to the conventional pipelines [5, 29, 24] that treat the image generation and separation as two independent stages, we come up with a uniﬁed model, such that the mutual effects between two stages can beneﬁt the robustness.###Here, α and β are the mixing coefﬁcients [5, 27, 26].###Moreover, we introduce the gradient constraints [5, 26] to make the model learning more effective, in which the edge map estimation is ele-gantly dealt with as an auxiliary task via a multi-task learning structure.###Instead of one-to-one framework in previous methods [5, 29], our separator learns the mapping function as S : M → ( B , R , E ) , where the multi-task learning framework models the image separation process in a more reasonable way, especially the auxiliary task of edge map estimation, that provides…###The framework introduced in [5] exploited the edge information when training the whole network to better preserve the image details.",impact-revealing,highlighting the advantages of the proposed model over previous methods
1333,,cabd5edd9a2500020aec02a183e3e7e454d0a84f,Applying methods of formative and summative assessment to problem-based learning in computer courses,,,"###…which generally takes place after a period of instruction and requires making a judgment’ and giving score ‘about the learning that has occurred’ (Boston 2002), students would benefit from more opportunities to build on their strengths and learn from their weaknesses through the feedback…###(Boston 2002) ‘In contrast to summative assessment, which generally takes place after a period of instruction and requires making a judgment’ and giving score ‘about the learning that has occurred’ (Boston 2002), students would benefit from more opportunities to build on their strengths and learn…",impact-revealing,providing context on assessment types and their implications for student learning
3908,5e2ac357df1a9c0c41e7fa48,73bc990757d54f2161eceacd330e0aeb189393bf,Building attention and edge message passing neural networks for bioactivity and physical–chemical property prediction,58d82fced649053542fd7295,Low Data Drug Discovery with One-shot Learning.,The SIDER set [58] is a collection of drugs and corresponding potential adverse reactions grouped following MedDRA classifications [59] according to previous usage [60].,other,reporting on a specific dataset related to drugs and adverse reactions
3293,53e99fe3b7602d97028bddfb,ecf5fd423c117ffb87730d75a473bc05beaae2b8,self-optimizing memory controllers: a reinforcement learning approach,558aee7184ae84d265c07d5e,Stall-Time Fair Memory Access Scheduling for Chip Multiprocessors,"QoS-aware memory controllers were recently proposed [29, 31, 34] to provide fair access to threads sharing the DRAM system.###In particular, several QoS-aware memory controller proposals have been published lately [29, 31, 34].",other,acknowledge recent proposals in QoS-aware memory controllers
3725,5b67b45517c44aac1c860884,9fa3e53b5937a0ec92499ed415e339ede6c92010,DeepInf: Social Influence Prediction with Deep Learning,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"Typical examples include GCN [25], GraphSAGE [21], and the state-of-the-art model GAT [49].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
305,5c5ce50d17c44a400fc38e42,350c5f528b557cde46177866121c40e250201a0f,Goal-based Course Recommendation,5843774bac44360f10839740,Parallel Recurrent Neural Network Architectures for Feature-rich Session-based Recommendations,"While RNNs have been previously applied to make recommendations based on collaborative filtering principles [15, 16, 23], they have not been re-purposed to make more targeted personalized goal-based recommendations in any domain.",impact-revealing,highlighting a gap in the application of RNNs for personalized recommendations
2853,5e16fa233a55acac60fd363d,f3058ac35927720d2a229984b10524e36d87d7dc,HyGCN: A GCN Accelerator with Hybrid Architecture,5a9cb66717c44a376ffb8667,FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling,"The former indices for edge sampling are based on dynamic generation while the latter ones are predefined and can be read from offchip memory like in [11, 26].###In order to decrease the computational complexity, the Sample function is usually applied before the Aggregate function to sample a subset from the neighbor vertices of each vertex [5, 26] as the new neighbors, specifically, S(v) = Sample ( N(v) ) .",other,describing computational methods for edge sampling
3981,56d86073dabfae2eee7807ea,6928b1bf7c54a4aa8d976317c506e5e5f3eae085,Deception detection using real-life trial data,53e9b55db7602d9704093e1f,Syntactic Stylometry for Deception Detection.,"Given the difﬁculties associated with the use of polygraph-like methods, learning-based approaches have been proposed to address the deception detection task using a number of modalities, including text [13] and speech [20, 29].###…ﬁndings have shown the effectiveness of features derived from text analysis, which frequently includes basic linguistic representations such as n-grams and sentence count statistics [28], and also more complex linguistic features derived from syntactic CFG trees and part of speech tags [13, 43].",other,highlighting the effectiveness of learning-based approaches for deception detection
1197,,08d73733faac48516d290e5e227ac6a03555d13b,"Cross-Modal Retrieval: A Review of Methodologies, Datasets, and Future Perspectives",,,###Bronstein et al. [65] introduced an innovative approach for acquiring cross-modal hash functions by leveraging feature decomposition and boosting techniques.,impact-revealing,reporting prior findings on cross-modal hash functions
2400,5eb3df3191e011cea6a7c3c8,86746ca4e3fb61cbcfb8dc29d6779d51b03692e0,Effect of Character and Word Features in Bidirectional LSTM-CRF for NER,56d8d654dabfae2eeece4f2d,Exploring Word Embedding for Drug Name Recognition,"The machine learning approach involves the usage of structured and unstructured techniques, such as CRF that was implemented for DrugNER [11].",other,describing the machine learning approach and techniques used
3215,5f7fdd328de39f0828397afd,c841c9704bf35873a051f228a15f67b30d650c2f,Scalable Graph Neural Networks via Bidirectional Propagation,5bdc31b817c44a1f58a0c14d,GAMENet: Graph Augmented MEmory Networks for Recommending Medication   Combination,"Recently, the field of Graph Neural Networks (GNNs) has drawn increasing attention due to its wide range of applications such as social analysis [23, 20, 28], biology [10, 26], recommendation systems [36], and computer vision [39, 7, 13].",other,highlighting the growing interest and applications of Graph Neural Networks
864,5ec49a639fced0a24b4de922,0d965ed237a3b4592ecefdb618c29f63adedff76,Towards Debiasing Sentence Representations,5d1eb9ebda562961f0b25f0f,Measuring Bias In Contextualized Word Representations,"Finally, Kurita et al. (2019) only measure bias on BERT by extending the word-level Word Embedding Association Test (WEAT) (Caliskan et al., 2017) metric in a manner similar to May et al. (2019).",impact-revealing,reporting prior findings on bias measurement in BERT
903,57a4e92bac44365e35c9ab55,d98063b0eb446c99e98684b34cb53914ca6b7206,a survey of techniques for architecting dram caches,53e9a3e1b7602d9702cf7c20,A 1.2V 12.8GB/s 2Gb mobile Wide-I/O DRAM with 4×128 I/Os using TSV-based stacking,"These features have motivated the development and release of high performance stacked DRAM from several leading vendors [13], [14], [15], [16], [17] and as such, stacked DRAM is poised to be integrated in the memory hierarchy of both CPUs and GPUs [9], [18], [19].",impact-revealing,highlighting the significance of stacked DRAM development and its potential integration
4048,5ea16b2b91e011fa08b8f8d9,9cc444d4deb0a291059c43bc5657bb7743a846cc,torchgpipe: On-the-fly Pipeline Parallelism for Training Giant Models,5a9cb65d17c44a376ffb820b,Regularized Evolution for Image Classifier Architecture Search,"We show that each component is necessary to fully benefit from pipeline parallelism in such environment, and demonstrate the efficiency of the library by applying it to various network architectures including AmoebaNet-D [23] and U-Net [24].###We show that each component is necessary to fully benefit from pipeline parallelism in such environment, and demonstrate the efficiency of torchgpipe by conducting the speed and memory benchmarks on AmoebaNet-D [23] and U-Net [24] when trained with the library.###For example, AmoebaNet-B [23] scaled with GPipe [11] has 557 million parameters and has achieved top-1 accuracy 84.",other,demonstrating the efficiency of a library for pipeline parallelism in neural networks
13,5736973b6e3b12023e62b0a8,97e3bb4af723f43927317e9b9f2d794a9e398e8e,PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks,5736977f6e3b12023e66632b,LINE: Large-scale Information Network Embedding,"The essential idea is to make use of the second-order proximity [27] between vertices, which assumes vertices with similar neighbors are similar to each other and thus should be represented closely in a low dimensional space.###1 Bipartite Network Embedding In our previous work, we introduced the LINE model to learn the embedding of large-scale information networks [27].###(3) The objective (3) can be optimized with stochastic gradient descent using the techniques of edge sampling [27] and negative sampling [18].###Our previous work proposed a novel large-scale network embedding model called the “LINE,” which is suitable for arbitrary types of information networks: undirected or directed, binary or weighted [27].###The network is embedded into a low dimensional vector space that preserves the second-order proximity [27] between the vertices in the network.###The proposed method naturally extends our previous work of unsupervised information network embedding [27] and first learns a low dimensional embedding for words through a heterogeneous text network.###For the detailed optimization process, readers can refer to [27].###A straightforward solution to optimize the objective (4) is to merge the all the edges in the three sets Eww, Ewd, Ewl and then deploy edge sampling [27], which samples an edge for model updating in each step, with the sampling probability proportional to its weight.",impact-revealing,describing the method for network embedding and its optimization
646,5c8b99db4895d9cbc69c7956,add350d0c5605c98d285b87493fc77c1d68281df,architectural support for server-side php processing,56d87e3fdabfae2eee523b75,Hardware Accelerators for Regular Expression Matching and Approximate String Matching,"Previous work, such as [68], propose methods for string matching in hardware.###Prior works [68] have realized the potential of hardware specialization for string matching, but do not support all the necessary string functions frequently used in these",impact-revealing,acknowledge prior work on string matching in hardware
1322,,2b7b217312e9ee8a9a42d542242193e8ef6d5d3d,Inequality in Black and White High School Students’ Perceptions of School Support: An Examination of Race in Context,,,"###Mplus software adjusts for missingness using full-information maximum-likelihood (FIML) estimation, which is widely recognized as an appropriate means of handling missing data assumed to be MAR (Schafer and Graham 2002).",impact-revealing,reporting a widely recognized method for handling missing data
1352,,61f4267fd8b1ce5cdbedcca2c6a6fd91cbd05387,TRPP2 and TRPV4 Form an EGF-Activated Calcium Permeable Channel at the Apical Membrane of Renal Collecting Duct Cells,,,"###[6,7] we focused on a potential interaction between TRPP2 and TRPV4.###Recent studies have also reported that TRPP2 and TRPV4 colocalize to the cilia of polarized madin-darby canine kidney epithelial (MDCK) cells [6].###Similar results were observed in HEK293 cells coexpressing these channels [6].###In addition, TRPV4-deficient animal models do not develop cysts suggesting that TRPV4, although an essential component of the ciliary mechanosensor in vivo, its absence is not sufficient for cystic development [6].",impact-revealing,highlighting the interaction and colocalization of TRPP2 and TRPV4 in cellular contexts
1923,,913613d28b27e7155f2cf120f9ac4bf386e62c94,Comparison of Body Shape Descriptors for Biometric Recognition Using MMW Images,,,"###This is inspired by previous works, which show that recognition through the shape and boundary of traits such as the hand [6], [7] or the signature are fairly reliable [8], [9].###In the biometric field, it was first used for signature verification [8].",impact-revealing,highlighting the reliability of recognition methods in biometric applications
1795,,85724298e9bd76118b40244fcfc79717fb6ee936,Information Interactions in Outcome Prediction: Quantification and Interpretation using Stochastic Block Models,,,"###Typically in [26], the authors study interaction between multiple entity types via a heterogeneous network representation and define clusters of entities based on structural properties of the resulting graph.###Research in recommender systems applied to multiple pieces of information is motivated by numerous descriptive studies on multimodal networks structure [11, 24, 26].",impact-revealing,acknowledge existing research on multimodal networks and their applications
432,53e9b593b7602d97040d7111,a0d4e1b410a85fea0452be729cfc8a999032eeee,Research on Improved TBL Based Japanese NER Post-Processing,53e9acd3b7602d97036b29a9,Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging,Transformation Based Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992([14]).,impact-revealing,reporting prior findings on an effective learning algorithm
3788,5cd7fa07ced107d4c65bf34f,371c799bde8b162e7f8fa2b2a0a8cfb29765f89f,Knowledge Graph Convolutional Networks for Recommender Systems,599c7ce9601a182cd27e7834,Neural Collaborative Filtering.,"A traditional recommendation technique is collaborative filtering (CF), which assigns users and items ID-based representation vectors, then models their interactions by specific operation such as inner product [16] or neural networks [8].###com/library/bing/bing-satori 8We have tried NCF [8], i.",other,describing a traditional recommendation technique
2834,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,5736960c6e3b12023e51f0a0,End-To-End Sequence Labeling Via Bi-Directional Lstm-Cnns-Crf,"Many recent sequence labeling frameworks (Ma and Hovy, 2016b; Misawa et al., 2017) share a very basic structure: a bidirectional LSTM network followed by a CRF tagging layer (i.e. BLSTM-CRF).###Recent research efforts in neural network models have shown that end-to-end learning like convolutional neural networks (CNNs) (Ma and Hovy, 2016a) or bidirectional long short-term memory (BLSTMs) (Lam-ple et al., 2016) can largely eliminate human-crafted features.",other,acknowledge existing sequence labeling frameworks and their structures
3870,5a4aef9e17c44a2190f7a8b8,ff772950f66ac6a57f4201ce1f02f0013ccdc1bb,Receptive Field Block Net for Accurate and Fast Object Detection,53e99a8cb7602d9702302822,Learning Local Image Descriptors,"A few shallow descriptors coincidentally make use of such a mechanism to design [36] or learn [1, 39, 31] their pooling schemes, and show good performance in matching image patches.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3345,5aed147c17c44a4438153ea5,665c0dde22c2f8598869d690d59c9b6d84b07c01,domino temporal data prefetcher,53e9ae6fb7602d970388fc05,Address-Value Delta (Avd) Prediction: Increasing The Effectiveness Of Runahead Execution By Exploiting Regular Memory Allocation Patterns,"Temporal prefetching is suitable for accelerating chains of dependent data misses, which are common in pointerchasing applications [25], [26] (e.",other,providing context for temporal prefetching applications
1522,,e24e34855c2f84b1246a975c8aad9aeb09c7a6d9,Rhabdomyolysis-Induced Acute Kidney Injury Evidence for Sustained Renal Hypoxia and Transient Hypoxia Adaptation,,,"###However, recent studies have demonstrated that hypoxic upregulations of PHDs may represent a feedbackadaptive mechanism in response to high levels of HIF-1 , which limit an accumulation of this transcription factor to prevent the overexpression of its target genes by accelerated degradation of HIF-1 (11, 34).###Furthermore, activities of PHDs are mainly dependent on tissue PO2, rather than the expression levels of PHDs (11).###It has been reported that this inhibitor of PHD activity upregulates the HIF-1 levels (11).###Because PO2 is lower in renal medullary than in renal cortical tissue (5, 15, 54) and hypoxic upregulation of the expression of PHDs has been reported in other tissues or cells (9, 11, 14, 29, 34), the increased levels of PHDs in the renal medulla may be associated with a low-PO2 milieu in this kidney region.###This reaction has been widely used for the determination of PHD activity (11, 43).",impact-revealing,highlighting the role of PHDs in response to hypoxia and their relationship with HIF-1 levels
2780,5fd0a7f691e01147f1d1e367,d4e9bf1d5fab52698a3aa48c65b098ea327deea6,CTRLsum: Towards Generic Controllable Text Summarization,5c8dc31f4895d9cbc69e72ee,Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.,"We detail g control next, but describe the automatic keyword extraction later in §2.3.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
4019,5cede10fda562983788ef75c,2a69ddbafb23c63e5e22401664bea229daaeb7d6,Res2Net: A New Multi-Scale Backbone Architecture,573696f46e3b12023e5f108f,Holistically-Nested Edge Detection,"Unsurprisingly, multi-scale features have been widely used in both conventional feature design [1], [31] and deep learning [6], [18], [29], [39], [44], [52].",other,acknowledge the use of multi-scale features in various approaches
439,5de632503a55ac4f55c25481,6da4ab0ace4dfd1670919dccc03eed5afdbd6264,Correction Filter for Single Image Super-Resolution: Robustifying Off-the-Shelf Deep Super-Resolvers,5cede10dda562983788ed6ee,Blind Super-Resolution With Iterative Kernel Correction,"These strategies include: modifying the training phase such that it covers a predefined set of downscaling kernels [39, 14]; using DNNs to capture only a natural-image prior which is decoupled from the SISR task [38, 5]; or completely avoid any offline training and instead train a CNN super-resolver from scratch at test time [32, 27].###One approach trains a CNN super-resolver that gets as inputs both the LR image and the degradation model, and assumes that the downscaling kernels belong to a certain set of Gaussian filters [14, 39].###Finally, we would like to highlight major differences between this paper and the work in [14], whose ”kernel correction” approach may be misunderstood as our ”correction filter”.###In general, only a few works have considered the blind SISR setting and developed kernel estimation methods [28, 22, 14, 3].###In [14], three different DNNs (super-resolver, kernel estimator, and kernel corrector) are offline trained under the assumption that the downscaling kernel belongs to a certain family of Gaussian filters (similarly to [39]), and the CNN super-resolver gets the estimated kernel as an input.###Thirdly, the concepts of these works are very different: The (iterative) correction in [14] modifies the estimated downscaling kernel, while our correction filter modifies the LR image.###So, the first major difference is that contrary to our approach, no pre-trained existing DNN methods (other than SRMD [39]) can be used in [14].",impact-revealing,highlighting differences in approaches to super-resolution
3167,573697f96e3b12023e6d2f31,97acdfb3d247f8250d865ef8a9169f06e40f138b,EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding,5550414d45ce0a409eb3a0ad,End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results.,"Meanwhile, another line of work [6, 7, 8, 9, 10, 11, 12] has focused on end-to-end ASR, i.",other,acknowledge existing research in end-to-end ASR
1865,,6072f9766b6412f8f8ebf33216638938e03f6357,A Web-Based Dementia Education Program and its Application to an Australian Web-Based Dementia Care Competency and Training Network: Integrative Systematic Review,,,"###Learning that leads to the best possible provision of care for people living with dementia [17,21-49].###Second, integrative reviews can also combine empirical literature together with theoretical frameworks [22].###Integrative review methodology [22] was chosen for several reasons.###Multimedia Appendix 2 displays a summary of the 46 included papers [6-56].###Learners require a strong commitment to external studies, requiring a level of discipline, a willingness to develop self-direction, and a capacity for resilience [14,19-37,42,43].###It was frequently reported that learners who are engaged in Web-based studies require a level of commitment and willingness, the ability to develop self-direction, and a capacity for flexibility [14,19-37,42,43].###Hence, this review included both qualitative and quantitative studies as well as literature that looked at theories and empirical studies [22].###In total, 8 studies in this review highlighted the importance of including the evaluation of staff confidence, knowledge, and attitudes toward those living with dementia [19,21-49].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3906,5d3ed25a275ded87f97dea9b,0ddd7c9a1955c76eb831a230ef20a68b662886bb,Mining Algorithm Roadmap in Scientific Publications,555048eb45ce0a409eb72996,Relation Classification via Convolutional Deep Neural Network.,"network based methods have achieved great success in relation extraction, including CNN-based approaches [40, 41] and LSTMbased approaches [31].",other,acknowledge successful methods in relation extraction
2247,5ea6adfa91e011a546871d52,7066df8fd89cca546d1ef3d66679cb15eba48d50,flat: chinese ner using flat-lattice transformer,5aed14d117c44a4438158014,Core techniques of question answering systems over knowledge bases: a survey,"Named entity recognition (NER) plays an indispensable role in many downstream natural language processing (NLP) tasks (Chen et al., 2015; Diefenbach et al., 2018).",other,highlighting the importance of named entity recognition in NLP tasks
133,5a260c8117c44a4ba8a30f54,33998aff64ce51df8dee45989cdca4b6b1329ec4,Graph Attention Networks,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,"Transductive learning For transductive learning tasks, we compare against the same strong base-lines and state-of-the-art approaches as speciﬁed in Kipf & Welling (2017).###It is worth noting that, as Kipf & Welling (2017) and Atwood & Towsley (2016), our work can also be reformulated as a particular instance of MoNet (Monti et al., 2016).###• As opposed to GCNs, our model allows for (implicitly) assigning different importances to nodes of a same neighborhood, enabling a leap in model capacity.###We also directly compare our model against GCNs (Kipf & Welling, 2017), as well as graph convolutional models utilising higher-order Chebyshev ﬁlters (Defferrard et al., 2016), and the MoNet model presented in Monti et al. (2016).###This complexity is on par with the baseline methods such as Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017).###Finally, Kipf & Welling (2017) simpliﬁed the previous method by restricting the ﬁlters to operate in a 1-step neighborhood around each node.###More speciﬁcally, we are able to improve upon GCNs by a margin of 1.5% and 1.6% on Cora and Cite-seer, respectively, suggesting that assigning different weights to nodes of a same neighborhood may be beneﬁcial.###For the transductive tasks, we report the mean classiﬁcation accuracy (with standard deviation) on the test nodes of our method after 100 runs, and reuse the metrics already reported in Kipf & Welling (2017) and Monti et al. (2016) for state-of-the-art techniques.",impact-revealing,comparing model performance and methodologies in transductive learning
3260,58437722ac44360f1082f13a,d2e4587744a89bad95fea69e08842cad6c8ff0dd,Temporal ensembling for semi-supervised learning,5736960c6e3b12023e51ec0c,Semi-Supervised Learning with Ladder Networks,"Γ -model is a subset of a ladder network (Rasmus et al., 2015) that introduces lateral connections into an encoder-decoder type network architecture, targeted at semi-supervised learning.###The Π -model can also be seen as a simpliﬁcation of the Γ -model of the ladder network by Rasmus et al. (2015), a previously presented network architecture for semi-supervised learning.###Finally, temporal ensembling lets us dispose of the notion of having multiple copies of the same network as in our Π-model, or in the Γ-model [13].###Γ-model is a subset of a ladder network [13] that introduces lateral connections into an encoderdecoder type network architecture, targeted at semi-supervised learning.###Of our comparison methods the Γ-model [13] is the only one that explicitly says that augmentation was not used for CIFAR-10.###Our approach is somewhat similar to the Γ -model of the ladder network by Rasmus et al. (2015), but conceptually simpler.###Our Π-model can be seen as a simplification of the Γ-model of the ladder network [13], a previously presented network architecture for semi-supervised learning.###This approach is similar to the Γ-model of the ladder network [13], but conceptually simpler.",other,describing the relationship and differences between the Γ-model and Π-model in semi-supervised learning
3838,5e5e18e493d709897ce3a0f2,7a064df1aeada7e69e5173f7d4c8606f4470365b,albert: a lite bert for self-supervised learning of language representations,573696016e3b12023e5152fd,Learning Distributed Representations of Sentences from Unlabelled Data.,"Skipthought (Kiros et al., 2015) and FastSent (Hill et al., 2016) sentence embeddings are learned by using an encoding of a sentence to predict words in neighboring sentences.",other,reporting prior findings on sentence embedding methods
2675,58d82fced649053542fd692f,29e944711a354c396fad71936f536e83025b6ce0,categorical reparameterization with gumbel-softmax,53e9b577b7602d97040b5c23,The Neural Autoregressive Distribution Estimator,"We use the MNIST dataset with ﬁxed binarization for training and evaluation, which is common practice for evaluating stochastic gradient estimators (Salakhutdinov & Murray, 2008; Larochelle & Murray, 2011).",other,reporting common practice in dataset usage
118,5a9cb66717c44a376ffb89eb,651adaa058f821a890f2c5d1053d69eb481a8352,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,58437722ac44360f1082f4d8,Towards Evaluating the Robustness of Neural Networks,"Much of what we describe below has been discussed in prior work (Carlini & Wagner, 2017a; Madry et al., 2018); we repeat these points here and offer our own perspective for completeness.###To generate (cid:96) ∞ bounded adversarial examples we use Projected Gradient Descent (PGD) conﬁned to a speciﬁed (cid:96) ∞ ball; for (cid:96) 2 , we use the Lagrangian relaxation of Carlini & Wagner (2017c).###Instead of actively attacking the detection method, we ﬁnd that LID is not able to detect high conﬁdence adversarial examples (Carlini & Wagner, 2017a), even in the unrealistic threat model where the adversary is entirely oblivious to the defense and generates adversarial examples on the original…###If g ( · ) is smooth and differentiable, then computing gradients through the combined network ˆ f is often sufﬁcient to circumvent the defense (Carlini & Wagner, 2017b).###As benchmarking against iterative optimization-based attacks (e.g., Kurakin et al. (2016a); Madry et al. (2018); Carlini & Wagner (2017c)) has become standard practice in evaluating defenses, new defenses have arisen that appear to be robust against these powerful optimization-based attacks.",impact-revealing,acknowledging prior work and providing a perspective on adversarial examples
1478,,4d7a228ea268a2be85e25bbef60dc14a77cb80a8,Fast generalization error bound of deep learning without scale invariance of activation functions,,,"###Note that usual activation functions including the sigmoid function, ReLU, and ELU satisfy this assumption.###it (ReLU; Nair and Hinton, 2010) satisﬁes the scale invariance, the other famous activation functions including the sigmoid and the hyperbolic tangent functions, and the exponential linear unit (ELU; Clevert et al., 2016) does not satisfy this condition. The existing analysis indicates a possibility that a deep learning with the non scale invariant activations may have a slower convergencerate ofO(1/ √ n) when onewith###In recent years, the non scale invariant activation functions including ELU (Clevert et al., 2016) are proposed, and such activation functions empirically provide higher or comparable performance compared to ReLU.###In contrast, recently, Clevert et al. (2016) propose a new important activation function, called exponential linear unit (ELU).###ale invariant activations may have a slower convergence rate of O(1/ √ n) even when one with the scale invariant activations can reach a convergence rate faster than O(1/ √ n). In contrast, recently, Clevert et al. (2016) propose a new important activation function, called exponential linear unit (ELU). Although 2 FAST LEARNING RATE OF DEEP LEARNING WITHOUT SCALE INVARIANCE the ELU function is not scale invariant, the###framework proposed by Suzuki (2018) can be widely applied for analysis of deep learning with general activation functions. In recent years, the non scale invariant activation functions including ELU (Clevert et al., 2016) are proposed, and such activation functions empirically provide higher or comparable performance compared to ReLU. Our analysis can be applied for the deep neural network with these activations. Ther###Whereas the rectified linear unit (ReLU; Nair and Hinton, 2010) satisfies the scale invariance, the other famous activation functions including the sigmoid and the hyperbolic tangent functions, and the exponential linear unit (ELU; Clevert et al., 2016) does not satisfy this condition.###Although
the ELU function is not scale invariant, the ELU function does not only speed up learning in deep neural networks but also leads to higher (or comparable) classification accuracies compared to other activation functions including the ReLU and LReLU functions in many practical situations.",impact-revealing,highlighting the performance advantages of non-scale invariant activation functions in deep learning
1995,,8233e74aef012c114419411b367b80e8c87e9c2d,Cariporide enables hemodynamically more effective chest compression by leftward shift of its flow-depth relationship.,,,"###Current techniques are not only limited by reduced hemodynamic efficacy but also by substantial risk of injury to the chest wall and intrathoracic viscera (24,26,31).###However, this flow is limited by a maximum compression depth beyond which there is no additional hemodynamic benefit but disproportionate risk of injury to the chest wall and intrathoracic viscera (24,26,31).",impact-revealing,highlighting limitations in current techniques
2943,599e96ec9c05cae4992b4ef3,95b803d07c37e8349bd7b1318367d8237c76cbc0,Audio-driven facial animation by joint end-to-end learning of pose and emotion,53e9abbeb7602d970356ea4c,Deformation Transfer For Triangle Meshes,"In many applications we would like to drive several different meshes with audio, and we support that via deformation transfer [Sumner and Popović 2004], as shown in Figure 8.",other,providing context for audio mesh applications
1443,,7e2ec35f98bcf23ab85540db3a6b3e8f31b56e0c,A Comprehensive Review on Current Progressions in Combating The Antimicrobial Resistance,,,"###Secondly, the light which is being used to trigger the photosensitizer is not suitable for penetration deeper into the tissues (Li et al., 2018) .###Recently the supramolecular systems are developed to counter these shortcomings (Li et al., 2018).",impact-revealing,highlighting advancements in supramolecular systems to address limitations in photosensitizer light penetration
153,5cf48a48da56291d582ab75a,c5f5f179d80a3bf9b4f29750283a87eaca42e91b,neural graph collaborative filtering,599c7ce9601a182cd27e7834,Neural Collaborative Filtering.,"While inner product can force user and item embeddings of an observed interaction close to each other, its linearity makes it insufficient to reveal the complex and nonlinear relationships between users and items [14, 15].###Other more complicated choices, such as neural network-based interaction functions [14], are left to explore in the future work.###when the embeddings are insufficient in capturing CF, the methods have to rely on the interaction function tomake up for the deficiency of suboptimal embeddings [14].###Modern recommender systems [5, 14, 33] parameterize users and items by vectorized representations and reconstruct user-item interaction data based on model parameters.###In traditional recommender models like MF and neural collaborative filtering [14], these ID embeddings are directly fed into an interaction layer (or operator) to achieve the prediction score.###Following mainstream recommender models [1, 14, 26], we describe a user u (an item i) with an embedding vector eu ∈ Rd (ei ∈ Rd ), where d denotes the embedding size.###To implement the assumption, a common paradigm is to parameterize users and items for reconstructing historical interactions, and predict user preference based on the parameters [1, 14].###Towards this end, recent efforts [11, 14, 15, 36] focus on exploiting deep learning techniques to enhance the interaction function, so as to capture the nonlinear feature interactions between users and items.###For instance, neural CF models, such as NeuMF [14], employ nonlinear neural networks as the interaction function; meanwhile, translationbased CF models, such as LRML [28], instead model the interaction strength with Euclidean distance metrics.###information of items [30]; neural collaborative filtering models replace the MF interaction function of inner product with nonlinear neural networks [14]; and translation-based CF models instead use Euclidean distance metric as the interaction function [28], among others.",impact-revealing,highlighting the limitations of inner product in capturing complex relationships in recommender systems
3972,5b1643ba8fbcbf6e5a9bc79b,b3f83e8416010e9c3a705a0b6390d268e5ddf5c0,Black-box adversarial attacks with limited queries and information,573696076e3b12023e51a63f,The Limitations of Deep Learning in Adversarial Settings,"Papernot et al. (2016a; 2017) have successfully used this method to attack commercial classiﬁers like the Google Cloud Prediction API, the Amazon Web Services Oracle, and the MetaMind API, even evading various defenses against adversarial attacks.###Prior work considers various threat models (Papernot et al., 2016b; Carlini & Wagner, 2017).###Papernot et al. (2016a; 2017) trained the Cloud Prediction API with small datasets like MNIST and successfully demonstrated an untargeted attack.###Copyright 2018 by the author(s). a substitute network to emulate the original network and then attacks the substitute with ﬁrst-order white-box meth-ods (Papernot et al., 2016a; 2017).###To attack this setting, we can use “standard” ﬁrst-order techniques for generating adversarial examples Goodfellow et al. (2015); Papernot et al. (2016b); Madry et al. (2017); Carlini & Wagner (2017), substituting the gradient of the loss function with an estimate of the gradient, which is…",other,highlighting the effectiveness of adversarial attacks on commercial classifiers
83,5ee7495191e01198a507f6a4,d2599ccb2401198b5e6e1d867c7d0f22b5055f5e,CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models,53e9b43db7602d9703f3a053,A Linear Non-Gaussian Acyclic Model for Causal Discovery,"[28] proposed an effective method called LiNGAM to learn the causal graph and they prove the model identiﬁability under the linearity and non-Gaussianity assumption.###Discovering the causal graph from pure observations has attracted large amounts of attention in the past decades [7, 33, 28].###A Causal Layer, which essentially describes a Structural Causal Model (SCM) [28], is introduced to a conventional VAE network.",impact-revealing,reporting prior findings on causal graph learning
335,5d9ed4a047c8f76646fb6da2,0fd26ed185aaf860f2db491c194884914fc29311,A Neural Multi-digraph Model for Chinese NER with Gazetteers,573696106e3b12023e5227c8,Neural Architectures for Named Entity Recognition.,"We use BiLSTMCRF (Lample et al., 2016) with character+bigram embedding without using any gazetteer as the comparison baseline6.###We use BiLSTM-CRF (Lample et al., 2016) with character+bigram embedding without using any gazetteer as the comparison baseline 6 .###Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) (Li et al., 2016) and a standard bidirectional LSTM-CRF (Lample et al., 2016) (BiLSTM-CRF), our model learns a weighted combination of the information from different gazetteers and resolves matching conﬂicts based on contextual…###, 2016) and a standard bidirectional LSTM-CRF (Lample et al., 2016) (BiLSTM-CRF), our model learns a weighted combination of the information from different gazetteers and resolves matching conflicts based on contextual information.",impact-revealing,describing the model architecture and comparison baseline
2334,5db80dc83a55acd5c14a24b9,0af061849aa325b41a213e8730b3d1e84aa26c0d,CONNA: Addressing Name Disambiguation on the Fly,53e9b457b7602d9703f54cf4,Two Supervised Learning Approaches For Name Disambiguation In Author Citations,"The problem has been extensively studied for decades [10], [14], [22], [36], [41], [43], [48] and most of the works focus on how to group the papers belonging to same persons together into a cluster from scratch.###Existing work either leverage the disambiguating results by algorithms in some well-known academic websites such as Scopus [29] CiteSeerX [46], Web of Science [1] and PubMed [39], or annotate a much smaller datasets by human beings, such as 8,453 [10], 6,921 [16], 7,528 [36] and 2,946 annotated persons [25].",other,acknowledge extensive research on paper clustering and disambiguation
407,5aed14d617c44a4438158e20,9e788f1530af08a1f2140e6016fd4aeaa8b29033,Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform,58d82fced649053542fd729f,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,"Perceptual loss [21, 3] and adversarial loss [27, 38] are introduced to solve the regression-to-the-mean problem that is usually caused by conventional MSE-oriented loss functions.###[38] further propose adversarial loss to encourage the network to favor solutions building x4 plant x4###Figure 5 shows the qualitative results of different models including PSNR-oriented methods, such as SRCNN [7], VDSR [22], LapSRN [26], DRRN [43], MemNet [44], and GAN-based methods, such as SRGAN [27] and EnhanceNet [38].###Our final results show that an SR network equipped with SFT can generate more realistic and visually pleasing textures in comparison to state-of-the-art SRGAN [27] and EnhanceNet [38].###We draw inspiration from [27, 38] and apply perceptual loss and adversarial loss in our model.###GAN-based methods (SRGAN [27], EnhanceNet [38] and ours) clearly outperform PSNR-oriented approaches in term of perceptual quality.###Our framework is based on adversarial learning, inspired by [27, 38].###[38] develop a similar approach and further explore the local texture matching loss, partly reducing visually unpleasant artifacts.###Comparing different SR approaches with downsampling factor ×4: SRCNN [7], SRGAN [27], EnhanceNet [38], our proposed SFT-GAN and the original HR image.###Bicubic SRCNN[7] VDSR[22] LapSRN[26] DRRN[43] MemNet[44] EnhanceNet[38] SRGAN[27] SFT-GAN (ours) GT",impact-revealing,highlighting the effectiveness of GAN-based methods in super-resolution
1422,,19c647c8a315613f4573dc1ab4c29c91cf3337cf,Conservative Offline Distributional Reinforcement Learning,,,"###A shortcoming of most existing approaches to ofﬂine RL [11, 46, 20, 21, 48, 18] is that they are designed to maximize the expected value of the cumulative reward (which we call the return ) of the policy.###An alternative is to regularize the Q -function estimates to be conservative at OOD actions [21, 48, 18].###CQL [21] obtains a similar lower-bound; thus, CDE generalizes CQL to other objectives—e.g., it can be used in conjunction with any distorted expectation objective (e.g., CVaR, Wang, CPW, etc.) for risk-sensitive ofﬂine RL. Note that Theorem 3.6 does not preclude the possibility that the lower…###Following [21], we propose a min-max objective where the inner loop chooses the current policy to maximize the CDE objective, and the outer loop minimizes the CDE objective for this policy: where we have used c 0 as in (6).###First, we assume that our dataset D has nonzero coverage of all actions for states in the dataset [23, 21].###Recall that the original objective is We ﬁrst provide a derivation of the above objective; this portion largely follows from [21].###…Critic (ORAAC) [43], a state-of-the-art ofﬂine risk-averse RL algorithm that combines a distributional critic with an imitation-learning based policy to optimize a risk-sensitive objective, and to Conservative Q-Learning (CQL) [21], a state-of-art ofﬂine RL algorithm that is non-distributional.###…does not account for estimation error, especially for pairs ( s, a ) that rarely appear in the given dataset D ; thus, ˆ Z k ( s, a ) may be an overestimate of Z k ( s, a ) [12, 20, 21], even in distributional RL (since the learned distribution does not include randomness in the dataset) [14, 3].###We build on conservative Q -learning [21], which penalizes Q values for OOD state-action pairs to ensure that the learned Q -function lower bounds the true Q -function.###This objective adapts the conservative penalty in prior work [21] to the distributional RL setting; in particular, the ﬁrst term in the objective is a penalty that aims to shrink the quantile values for out-of-distribution (OOD) actions compared to those of in-distribution actions; intuitively, c 0…",impact-revealing,highlighting the limitations of existing offline reinforcement learning approaches
2202,5d64ff713a55acf547f20de0,26d3f8db3a4275225d355a9ce8e132b8c19c225b,Once-for-All: Train One Network and Specialize it for Efficient Deployment on Diverse Hardware Platforms,53e9b844b7602d970440513c,Imagenet: A Large-Scale Hierarchical Image Database,"4 EXPERIMENTS In this section, we first apply the progressive shrinking algorithm to train the once-for-all network on ImageNet (Deng et al., 2009).",other,describing the application of a specific algorithm in experiments
3264,5c96086e3cb210d2716c4a08,bb76749fab841ccbe20ace74111a518f65d9870b,make the most out of last level cache in intel processors,53e9ad68b7602d970374e57d,Reflections on the memory wall.,"One of the known problems in achieving high performance in computer systems has been the Memory Wall [43], as the gap between Central Processing Unit (CPU) and Direct Random Access Memory (DRAM) speeds has been increasing.",other,highlighting a known problem in computer systems performance
1791,,c87589588b1a43a88d216ca79f74e8dd87f141a5,Ventilator-Associated Pneumonia,,,"###There have been concerns raised, however, regarding the diagnostic validity of CPIS, with one meta-analysis reporting the sensitivity and specificity for CPIS as 65% and 64% respectively.(10) Furthermore, there is significant user variability in CPIS calculation despite its seemingly simple calculation.###VAP prevention bundles provide an effective method in which to reduce individual intensive care unit rates of VAP.###One scoring system described is the clinical pulmonary infection score (CPIS), which takes a number of different investigations into account.(10)###Furthermore, there is significant user variability in CPIS calculation despite its seemingly simple calculation.(10)###Every patient exhibiting signs of VAP should have a chest X-ray and those patients who exhibit changes consistent with infection should have a sample of their respiratory tract secretions sent for Gram stain, culture, and sensitivity.(10) A normal chest X-ray should prompt the clinician to consider an alternative diagnosis.###A CPIS score of 6 or higher out of a maximum score of 12 indicates a likely diagnosis of VAP.(10) There have been concerns raised, however, regarding the diagnostic validity of CPIS, with one meta-analysis reporting the sensitivity and specificity for CPIS as 65% and 64% respectively.###Early diagnosis and adoption of practices known to prevent VAP can reduce mortality and decrease the development of multidrug–resistant organisms.1 Throughout this article we will discuss the incidence, scoring systems, pathophysiology, prevention, risk factors, diagnosis, and treatment of VAP.###One scoring system described is the clinical pulmonary infection score (CPIS), which takes a number of different investigations into account.10
Subscribe to ATOTW tutorials by visiting www.wfsahq.org/resources/anaesthesia-tutorial-of-the-week
ATOTW 382 — Ventilator-Associated Pneumonia (27 June 2018) Page 4 of 6
A CPIS score of 6 or higher out of a maximum score of 12 indicates a likely diagnosis of VAP.10 There have been concerns raised, however, regarding the diagnostic validity of CPIS, with one meta-analysis reporting the sensitivity and specificity for CPIS as 65% and 64% respectively.10 Furthermore, there is significant user variability in CPIS calculation despite its seemingly simple calculation.10
The Hospitals in Europe Link for Infection Control through Surveillance (HELICS) criteria shown below are commonly used for monitoring VAP rates.16 A diagnosis of VAP is made using the HELICS criteria when each of the radiological, systemic, and pulmonary criteria have been met.",impact-revealing,highlighting concerns regarding the diagnostic validity of CPIS
2322,5d1eb9d5da562961f0b0fa03,037aeb767ab431eeebc74a0b85df0d2f5641c652,Pre-Training With Whole Word Masking for Chinese BERT,5b67b4b417c44aac1c8678f5,DRCD: a Chinese Machine Reading Comprehension Dataset,"• Machine Reading Comprehension (MRC) : CMRC 2018 [27], DRCD [28], CJRC [29].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2119,,d441b29fe7eed6d285ca489a437028e67ba77038,Performance of continuum models for realworld traffic flows: Comprehensive benchmarking,,,"###…models has been criticized for exhibiting some traffic waves as traveling faster than traffic, and thereby, violating the anisotropy property which suggests that upstream vehicles should not systematically affect downstream vehicles (see Daganzo, 1995c ; Helbing and Johansson, 2009 ; Zhang, 2009 ).###For instance, the minimum supply-and-demand method ( Daganzo, 1995a ), which is widely used for the CTM, is a famous example of these methods.###CTM Daganzo (1994) • To represent the basic LWR-type models • To be a baseline for evaluation of equilibrium models • Widely used for control purposes • All model parameters are observable and###Meanwhile, Daganzo (1995c) criticized NE models for violating the anisotropy property of traffic flow, which suggests that following vehicles should not systematically affect the dynamics of leading vehicles. Daganzo (1995c) argued that the existing NE models were ill-behaved because the second characteristic speed in such models can travel faster than the average traffic speed, thereby reaching vehicles from behind and affecting their dynamics. These criticisms have motivated researchers to propose NE models devoid of characteristic speeds faster than traffic speed. These models are sometimes called “anisotropic”, and the second characteristic speed in the “anisotropic” models is the same as the traffic speed. Aw and Rascle (20 0 0) presented the first phenomenological model in this category. Zhang (2002) independently derived a similar model from car-following relationships, which can be considered as a particular case of the Aw-Rascle model. The two models are commonly referred to as ARZ models in the literature. Goatin (2006) argued that the ARZ model could become unstable for small densities and proposed a bi-phase extension of the ARZ model, where the LWR model is used for the entire free-flow regime. The ARZ model does not explicitly account for the maximum density, and can prescribe “non-physical” solutions with respect to the expected traffic flow properties at specific phase transitions ( Lebacque et al., 2007a ). To address these issues, Colombo (2002) presented another phenomenological model in this category known as the phase-transition model.###Meanwhile, Daganzo (1995c) criticized NE models for violating the anisotropy property of traffic flow, which suggests that following vehicles should not systematically affect the dynamics of leading vehicles.###Daganzo (1995c) argued that the existing NE models were ill-behaved because the second characteristic speed in such models can travel faster than the average traffic speed, thereby reaching vehicles from behind and affecting their dynamics.###Meanwhile, Daganzo (1995c) criticized NE models for violating the anisotropy property of traffic flow, which suggests that following vehicles should not systematically affect the dynamics of leading vehicles. Daganzo (1995c) argued that the existing NE models were ill-behaved because the second characteristic speed in such models can travel faster than the average traffic speed, thereby reaching vehicles from behind and affecting their dynamics. These criticisms have motivated researchers to propose NE models devoid of characteristic speeds faster than traffic speed. These models are sometimes called “anisotropic”, and the second characteristic speed in the “anisotropic” models is the same as the traffic speed. Aw and Rascle (20 0 0) presented the first phenomenological model in this category. Zhang (2002) independently derived a similar model from car-following relationships, which can be considered as a particular case of the Aw-Rascle model. The two models are commonly referred to as ARZ models in the literature. Goatin (2006) argued that the ARZ model could become unstable for small densities and proposed a bi-phase extension of the ARZ model, where the LWR model is used for the entire free-flow regime. The ARZ model does not explicitly account for the maximum density, and can prescribe “non-physical” solutions with respect to the expected traffic flow properties at specific phase transitions ( Lebacque et al., 2007a ). To address these issues, Colombo (2002) presented another phenomenological model in this category known as the phase-transition model. Lebacque et al. (2007b) presented a generic variation of ARZ and Colombo’s models, which is commonly referred to as GSOM 3 .###The throughflow is computed based on the minimum supply and demand method ( Daganzo, 1995b ), which reads: Q ∗i ( t ) = min ( Suppl y i +1 ( t ) , Deman d i ( t ) ) (16) where for homogeneous sections of roads, Supply and Demand flows are defined as:
Suppl y i +1 ( t ) = {
Q i ( t ) i f ρi +1 ( t )…###Meanwhile, Daganzo (1995c) criticized NE models for violating the anisotropy property of traffic flow, which suggests that following vehicles should not systematically affect the dynamics of leading vehicles. Daganzo (1995c) argued that the existing NE models were ill-behaved because the second characteristic speed in such models can travel faster than the average traffic speed, thereby reaching vehicles from behind and affecting their dynamics. These criticisms have motivated researchers to propose NE models devoid of characteristic speeds faster than traffic speed. These models are sometimes called “anisotropic”, and the second characteristic speed in the “anisotropic” models is the same as the traffic speed. Aw and Rascle (20 0 0) presented the first phenomenological model in this category. Zhang (2002) independently derived a similar model from car-following relationships, which can be considered as a particular case of the Aw-Rascle model. The two models are commonly referred to as ARZ models in the literature. Goatin (2006) argued that the ARZ model could become unstable for small densities and proposed a bi-phase extension of the ARZ model, where the LWR model is used for the entire free-flow regime. The ARZ model does not explicitly account for the maximum density, and can prescribe “non-physical” solutions with respect to the expected traffic flow properties at specific phase transitions ( Lebacque et al., 2007a ). To address these issues, Colombo (2002) presented another phenomenological model in this category known as the phase-transition model. Lebacque et al. (2007b) presented a generic variation of ARZ and Colombo’s models, which is commonly referred to as GSOM 3 . In recent years, the GSOM model has been studied both analytically and for real-world applications ( Fan et al., 2017 ; Goatin, 2007 ; Goatin and Laurent-Brouty, 2018 ; Ou et al., 2007 ; Seo and Bayen, 2017 ; Villa et al., 2016 ; Yu and Krstic, 2019 ), and it has been argued that this model can capture the complex traffic phenomena, including scattering and hysteretic phase transitions. Meanwhile, Jiang et al. (2002) followed a distinct approach for developing a NE model devoid of faster-than-traffic waves.###Daganzo (1994) proposed the cell transmission model (CTM), a simplified LWR model, which can be analytically solved due to its piece-wise linear (also called triangular) fundamental diagram.###Meanwhile, Daganzo (1995c) criticized NE models for violating the anisotropy property of traffic flow, which suggests that following vehicles should not systematically affect the dynamics of leading vehicles. Daganzo (1995c) argued that the existing NE models were ill-behaved because the second characteristic speed in such models can travel faster than the average traffic speed, thereby reaching vehicles from behind and affecting their dynamics.###Meanwhile, Daganzo (1995c) criticized NE models for violating the anisotropy property of traffic flow, which suggests that following vehicles should not systematically affect the dynamics of leading vehicles. Daganzo (1995c) argued that the existing NE models were ill-behaved because the second characteristic speed in such models can travel faster than the average traffic speed, thereby reaching vehicles from behind and affecting their dynamics. These criticisms have motivated researchers to propose NE models devoid of characteristic speeds faster than traffic speed. These models are sometimes called “anisotropic”, and the second characteristic speed in the “anisotropic” models is the same as the traffic speed. Aw and Rascle (20 0 0) presented the first phenomenological model in this category. Zhang (2002) independently derived a similar model from car-following relationships, which can be considered as a particular case of the Aw-Rascle model.###Meanwhile, Daganzo (1995c) criticized NE models for violating the anisotropy property of traffic flow, which suggests that following vehicles should not systematically affect the dynamics of leading vehicles. Daganzo (1995c) argued that the existing NE models were ill-behaved because the second characteristic speed in such models can travel faster than the average traffic speed, thereby reaching vehicles from behind and affecting their dynamics. These criticisms have motivated researchers to propose NE models devoid of characteristic speeds faster than traffic speed. These models are sometimes called “anisotropic”, and the second characteristic speed in the “anisotropic” models is the same as the traffic speed. Aw and Rascle (20 0 0) presented the first phenomenological model in this category. Zhang (2002) independently derived a similar model from car-following relationships, which can be considered as a particular case of the Aw-Rascle model. The two models are commonly referred to as ARZ models in the literature. Goatin (2006) argued that the ARZ model could become unstable for small densities and proposed a bi-phase extension of the ARZ model, where the LWR model is used for the entire free-flow regime.",impact-revealing,highlighting the criticisms and developments in NE models for traffic flow
798,5550424345ce0a409eb411c6,334da45ee9a6325cd8ca0142586b820f88de6a77,crema-d: crowd-sourced emotional multimodal actors dataset,53e9b672b7602d97041d8857,Emotion Inferences from Vocal Expression Correlate Across Languages and Cultures,"Parallel research exists in vocal expression [4], [5].",impact-revealing,acknowledge related research in vocal expression
3946,5aed146117c44a44381527f9,0aef7a464840621c384380ac8877ae53b73d23a8,Blasting through the Front-End Bottleneck with Shotgun,53e9b054b7602d9703ab5b01,SMARTS: Accelerating Microarchitecture Simulation via Rigorous Statistical Sampling.,We use SMARTS [19] multiprocessor sampling methodology for sampled execution.,other,method used for sampled execution
3917,5f7ee07491e011a5faf0feb2,c54fb59319288d0d17ad09e6b7dfee6d042f83a8,Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer,53e9a488b7602d9702da7685,A Three-Way Model for Collective Learning on Multi-Relational Data,"There are also various methods falling into the groups of Bilinear models such as RESCAL (Nickel et al., 2011) and DistMult (Yang et al., 2015), as well as neural models like HolE (Nickel et al., 2016) and ConvE (Dettmers et al., 2018).",other,acknowledge existing methods in bilinear and neural models
3640,5d3ed25a275ded87f97deb36,37f7eaf57b88ef03efd4196f2f57ae0b4c657a79,Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommendation,57aa28de0a3ac518da98974f,Collaborative Knowledge Base Embedding for Recommender Systems,"In general, existing KG-aware recommender systems can be classified into three categories: (1) Embedding-based methods [9, 26, 34] pre-process a KG with knowledge graph embedding (KGE) [30] algorithms, then incorporate learned entity embeddings into recommendation.###The settings of dimension and learning rate are the same as SVD. • CKE [34] is a representative of embedding-based methods, which combines CF with structural, textual, and visual knowledge in a unified framework.###• CKE [34] is a representative of embedding-based methods, which combines CF with structural, textual, and visual knowledge in a unified framework.###Existing KG-aware recommender systems can be classified into path-based methods [8, 33, 36], embedding-based methods [9, 26, 27, 34], and hybrid methods [18, 24, 28].###We implement CKE as CF plus a structural knowledge module in this paper.",other,describing categories of existing KG-aware recommender systems
1584,,144f2e06cbb5a1ea4f3bbb0f99c4a8854ce02150,A New Metaheuristic Inspired by the Vapour-Liquid Equilibrium for Continuous Optimization,,,"###Some of these techniques are: grey wolf optimizer (GWO) [29] (it imitates the command hierarchy and hunting strategy of grey wolves), the pity beetle algorithm (PBA) [30] (it was inspired by the grouping behaviour of the beetle Pityogenes chalcographus, looking for food and nests), shark smell optimization (SSO) [31] (it simulates the skill of a shark for finding their prey by using its sense of smell and moving toward the source of the odour), symbiotic organisms search (SOS) [32] (mimics the symbiotic interaction strategies followed by organisms to survive and propagate in the ecosystem), dolphin echolocation (DOE) [33] (it considers the echolocation system used by dolphins in searching for food), the whale optimization algorithm (WOA) [34] (it mimics the social behaviour of humpback whales), and the emperor penguin optimizer (EPO) [35] (it simulates the huddling behaviour of emperor penguins (Aptenodytes forsteri)).",impact-revealing,providing examples of optimization techniques inspired by nature
2858,5aed14d617c44a4438158cff,b77b179522ac01b6903c2719d9b5d29c1efa652e,Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba,57d063c7ac44367354290656,Text-Enhanced Representation Learning for Knowledge Graph.,"ther embedded a complicate knowledge graph with the node in a hierarchical structure, like sub-categories, etc. Besides, textual information related to the nodes are incorporated into graph embedding [19, 24, 26, 27]. Moreover, in [5], Chang et al. propose a deep learning framework to simultaneously deal with the text and image features for heterogeneous graph embedding. In this paper, we mainly processing discre",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3245,5cede0eada562983788c8fb4,940016df38b80c5f3fd9db411e722f58a9d7e227,Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation,5a4aef9e17c44a2190f7a399,Parallel WaveNet: Fast High-Fidelity Speech Synthesis.,"Since it is impractical to have single speaker record many hours of speech in clean acoustic environment, we use Google’s Parallel WaveNet-based TTS [31] system to generate training targets from a large hand-transcribed speech corpus.###Such behavior is qualitatively different from what one would obtain by simply running an ASR followed by a TTS for example.###Es-sentially this reduces the task to reproducing any input speech in the voice of a single-speaker TTS system.###Finally, we conduct another listening test to evaluate whether the model consistently generates normalized speech with the same TTS voice.###Such a multitask trained encoder can be thought of as learning a latent representation of the input that maintains information about the underlying transcript, i.e. one that is closer to the latent representation learned within a TTS sequence-to-sequence network.###Using this corpus, we run a TTS system to generate target utterances in a synthetic female voice.###Using TTS to generate this parallel corpus ensures that: (1) the target is always spoken with a consistent predeﬁned speaker and accent; (2) without any background noise or disﬂuencies.###The core architecture is based on recent attention-based end-to-end ASR models [2,22] and TTS models such as Tacotron [4,23].###Note that one advantage of directly converting speech to speech over cascading a ﬁnetuned ASR engine with TTS is as follows.###The WER using Google’s ASR system on the TTS-synthesized reference transcripts is 14.8%.###The only difference here is that Parrotron is trained on a male TTS speech, obtained form our production WaveNet-based TTS. Testing on KTEST , we ﬁnd that the output of this model was rated as natural as the original speech, but our ASR engine performs even more poorly on the converted speech than the original speech.###They have also achieved state-of-the-art results in end-to-end Text-To-Speech (TTS) synthesis [4] and Automatic Speech Recognition (ASR) [5], using a single neural network that directly generates the target sequences, given virtually raw inputs.",other,describing the methodology and advantages of using TTS for generating training targets
1454,,daa2d41a17b51811349ef48e70c0497bb344f2af,Methods in Text Mining for Diagnostic Radiology,,,"###For instance, the cTAKES pipeline [24, 61] was developed at the Mayo Clinic and has been extended by the YTEX project at Yale [26].###One of the earliest clinical text processing pipelines, MedLEE [24], was speci cally developed for the analysis of radiology records.",impact-revealing,acknowledging the development and extension of clinical text processing pipelines
1714,,4cf52192ba41815a2758a0c3fa89917a9c278dc7,‘Autism is the Arena and OCD is the Lion’: Autistic adults’ experiences of co-occurring obsessive-compulsive disorder and repetitive restricted behaviours and interests,,,"###…has been associated with pursuing interests and pleasure (Turner-Brown et al., 2011) with the earlier mentioned qualitative findings adding nuance in that, for some individuals, these behaviours are an adaptive way of coping with negative emotional states (Kapp et al., 2019; Collis et al., 2022).###Qualitative research findings report the suppression and substitution of a range of phenomena described in the RRBI domain (Collis et al., 2022; Kapp et al., 2019) suggesting that for many autistic individuals, these phenomena are not always experienced positively within a neurotypical society or…###For example, both Collis et al. (2022) and Kapp et al. (2019) delineated participants’ narratives of supressing and substituting RRBIs as their search to be seen as ‘socially acceptable’, while feeling conflicted due to also wanting to express and accept RRBI.###This would raise crucial ethical dilemmas as some RRBI can be experienced as part of someone’s autistic identity and can support self-regulation (Kapp et al., 2019).###For example, the term ‘stimming’ is widely used by many autistic people (e.g. Kapp et al., 2019) with qualitative research describing physical movements which help to self-regulate, self-soothe and cope with conditions of boredom and stress (Collis et al., 2022).",impact-revealing,highlighting the complexity of autistic behaviors and the ethical dilemmas surrounding their expression
2219,5ac1829d17c44a1fda917e29,ef2ec69e7c94b4194ba01719ac76d4595e6b4bdf,L2-Nonexpansive Neural Networks,5a9cb66717c44a376ffb89eb,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,"…of networks including image classiﬁcation (Carlini & Wagner, 2017a; Goodfellow et al., 2014), speech recognition (Kreuk et al., 2018; Alzantot et al., 2018; Carlini & Wagner, 2018), image captioning (Chen et al., 2017) and natural language processing (Gao et al., 2018; Ebrahimi et al., 2017).###For example, Athalye et al. (2018) reported that out of eight recent defense works, only Madry et al. (2017) survived strong attacks.###Adversarial defense is a well-known difﬁcult problem (Szegedy et al., 2014; Goodfellow et al., 2014; Carlini & Wagner, 2017a; Athalye et al., 2018; Gilmer et al., 2018).",other,highlighting the challenges and significance of adversarial defense across various domains
1735,,99e50ecd95d3ecd9b6e941e3de0bf231c6fc2f81,Mentalization-Based Treatment in Clinical High-Risk for Psychosis: A Rationale and Clinical Illustration,,,###Mentalization (thinking about thinking; (Fonagy 1991)) is a concept used both clinically and empirically to characterize a set of social cognitive processes that convey an understanding of human behavior as motivated by intentional mental states.,impact-revealing,providing context for the concept of mentalization
244,5736974d6e3b12023e6388bf,1d7e580740a3f96f1529e3d771915af4e996a259,Verbal and Nonverbal Clues for Real-life Deception Detection,5550488e45ce0a409eb6f579,Deception detection using a multimodal approach,", 2014), which was then used to develop a multimodal deception detection system (Abouelenien et al., 2014).###A multimodal deception dataset consisting of linguistic, thermal, and physiological features was introduced in (PérezRosas et al., 2014), which was then used to develop a multimodal deception detection system (Abouelenien et al., 2014).",impact-revealing,reporting the development of a multimodal deception detection system
2056,,e6f347a376820ac47f1962b22cbf1801aef2fee7,Machine Learning Algorithms to Classify and Quantify Multiple Behaviours in Dairy Calves Using a Sensor: Moving beyond Classification in Precision Livestock,,,"###However, such an approach fails to consider the fact that positive predictive value decrease with prevalence (i.e., [23]) and possible differences in behaviour prevalence between the training/test dataset and a new unlabelled dataset.",impact-revealing,highlighting limitations in predictive value approaches
71,5e5e190b93d709897ce4997e,cb4571fa905abb70868d0bb9d4681f0a612c2d0f,Differentiable Reasoning On Large Knowledge Bases And Natural Language,599c7974601a182cd263f32d,End-to-end Differentiable Proving.,"Results reported in Rocktäschel and Riedel (2017) were calculated with an incorrect evaluation function, causing artificially better results.###Among such systems, NTPs (Rockt¨aschel and Riedel 2017; Minervini et al. 2018) are end-to-end differentiable deductive reasoners based on Prolog’s backward chaining algorithm, where discrete uniﬁcation between atoms is replaced by a differentiable operator computing the similarities between their…###Rockt¨aschel and Riedel (2017) show that it is possible to learn rules from data by specifying rule templates , such as H : Parameters θ p : , θ q : , θ r : ∈ R k , denoting rule-predicate embeddings, can be learned from data by minimising the loss in Eq.###Furthermore, the expansion of a rule like p ( X , Y ) :– q ( X , Z ) , r ( Z , Y ) via backward chaining causes an increase of the sub-goals to prove, both because all atoms in the body need to be proven, and because Z is a free variable with many possible bindings (Rockt¨aschel and Riedel 2017).###We report the re-sults of experiments on benchmark datasets — Countries (Bouchard, Singh, and Trouillon 2015), Nations, UMLS, and Kinship (Kemp et al. 2006) — following the same evaluation protocols as Rockt¨aschel and Riedel (2017).###This optimisation is also present in Rocktäschel and Riedel (2017). where k denotes the embedding size, and it may be computationally inefficient to learn each of the embedding vectors if k is large.###We follow the protocol used by Rockt¨aschel and Riedel (2017) and split every dataset into training, development, and test facts, with a 80% / 10% / 10% ratio.###We compare GNTPs and NTPs on a set of link prediction benchmarks, also used in Rockt¨aschel and Riedel (2017).###A promising strategy for overcoming these issues consists of combining neural models and symbolic reasoning, given their complementary strengths and weaknesses (d’Avila Garcez et al. 2015; Rocktäschel and Riedel 2017; Yang, Yang, and Cohen 2017; Evans and Grefenstette 2018; Weber et al. 2019).###NTPs (Rocktäschel and Riedel 2017) recursively build a neural network enumerating all the possible proof paths for proving a query (or goal) on a given KB, and aggregate all their proof scores via max pooling.###For consistency, we use the same notation as Rocktäschel and Riedel (2017). We consider binary predicates, without loss of generality.###Rocktäschel and Riedel (2017) show that it is possible to learn rules from data by specifying rule templates, such as H :– B with H = [θp:, X, Y] and B = [[θq:, X, Z], [θr:, Z, Y]].###Although NTPs can be used for learning interpretable rules from data, the solution proposed by Rocktäschel and Riedel (2017) can be quite inefficient, as the number of parameters associated to rules can be quite large.###Furthermore, the expansion of a rule like p(X, Y) :– q(X, Z),r(Z, Y) via backward chaining causes an increase of the sub-goals to prove, both because all atoms in the body need to be proven, and because Z is a free variable with many possible bindings (Rocktäschel and Riedel 2017).###Among such systems, NTPs (Rocktäschel and Riedel 2017; Minervini et al. 2018) are end-to-end differentiable deductive reasoners based on Prolog’s backward chaining algorithm, where discrete unification between atoms is replaced by a differentiable operator computing the similarities between their embedding representations.###A related field is differentiable interpreters—program interpreters where declarative or procedural knowledge is compiled into a neural network architecture (Bošnjak et al. 2017; Rocktäschel and Riedel 2017; Evans and Grefenstette 2018).###Although NTPs can be used for learning interpretable rules from data, the solution proposed by Rockt¨aschel and Riedel (2017) can be quite inefﬁcient, as the number of parameters associated to rules can be quite large.",impact-revealing,highlighting the inefficiencies in existing neural theorem proving methods
348,5c5c55bfe1cd8e03e71689a9,5babbf2ed9f6e36b83ed246927b270db320fe866,A Simple Convolutional Generative Network for Next Item Recommendation,5c79a19c4895d9cbc65c34b3,Session-based Recommendations with Recurrent Neural Networks.,"…the comparison with content-or context-based sequential recommendation models, such as the 3D CNN recommender [30] and other RNN variants [9, 20, 25, 27]; (2) the GRURec baseline could be regarded as the state-of-the-art Improved GRURec [28] when dealing with the long-range session data sets…###It is worth noting that in previous sequential recommendation literatures, such as Caser , GRURec and [20, 25, 28, 30], they only model a single conditional distribution p ( x i | x 0: i − 1 , θ ) rather than all conditional probabilities (cid:206) ti = 1 p ( x i | x 0: i − 1 , θ ) p ( x 0 ) .###(3) ( see [20, 25, 29, 30]).###In the follow-up papers, various RNN variants have been designed to extend the typical one for different application scenarios, such as by adding personalization [25], content [9] and contextual features [27], attention mechanism [7, 20] and different ranking loss functions [14].",impact-revealing,acknowledge existing sequential recommendation models and their limitations
3746,59ec02da0cf22f5df7319dc3,c27db32efa8137cbf654902f8f728f338e55cd1c,mastering the game of go without human knowledge,56ab70cd0cf2c98bf5bc717a,Mastering the game of Go with deep neural networks and tree search,"Our program, AlphaGo Zero, differs from AlphaGo Fan and AlphaGo Lee 12 in several important aspects.###In our evaluation, all programs were allowed 5 s of thinking time per move; AlphaGo Zero and AlphaGo Master each played on a single machine with 4 TPUs; AlphaGo Fan and AlphaGo Lee were distributed over 176 GPUs and 48 TPUs, respectively.###We follow the formalism of alter­ nating Markov games described in previous work 12 , noting that algorithms based on value or policy iteration extend naturally to this setting 39 .###AlphaGo Zero and AlphaGo Master played on a single machine on the Google Cloud; AlphaGo Fan and AlphaGo Lee were distributed over many machines.###…data, we trained a second neural network (using the same architecture) to predict expert moves in the KGS Server data­ set; this achieved state­of­the­art prediction accuracy compared to pre­ vious work 12,30–33 (see Extended Data Tables 1 and 2 for current and previous results, respectively).###The published version 12 , which we refer to as AlphaGo Fan, defeated the European champion Fan Hui in October 2015.###This neural network combines the roles of both policy network and value network 12 into a single architecture.###Each simulation starts from the root state and iteratively selects moves that maximize an upper confidence bound Q ( s , a ) + U ( s , a ), where U ( s , a ) ∝ P ( s , a ) / (1 + N ( s , a )) (refs 12, 24), until a leaf node s ′ is encountered.",other,describing differences between AlphaGo versions and their evaluation methods
1332,,0c640df6a64fcaece4b7d08c7f903b1076e939cb,Peer and Self-Assessment in High Schools,,,"###Boston (2002), Rolheiser and Ross (2000) and others have emphasized the importance of training and professional development for teachers to help them better understand and implement effective practices that are the important elements of formative assessment.",impact-revealing,highlighting the importance of teacher training and professional development in formative assessment
2208,5bdc31b417c44a1f58a0b894,510d98681e5e85fb1265513728f16e2543ae1b4b,Hypergraph Neural Networks,5550401245ce0a409eb3205c,Dropout: a simple way to prevent neural networks from overfitting,The feature dimension of the hidden layer is set as 16 and the dropout (Srivastava et al. 2014) is employed to avoid overﬁtting with drop rate p = 0 .,other,providing details on model configuration and techniques
1962,,d5a4d88b465432b94e8186b74d5aa6dba9f71284,Remote Photoplethysmography: Rarely Considered Factors,,,"###To evaluate the impact of rolling shutter effect on the retrieving of the phase shift over the picture, several PPG records from MMSE-HR dataset [50] were used.###The following datasets are commonly used for training and evaluation of rPPG approaches: Mahnob-HCI [49], MMSE-HR [50], PURE [26], and VIPL-HR [25].",impact-revealing,reporting datasets used for training and evaluation of rPPG approaches
3172,5a9cb65d17c44a376ffb83f3,e06357ac23811054acb23e2ea30d087f5beaef90,an interpretable reasoning network for multi-relation question answering,573696026e3b12023e516133,Large-scale Simple Question Answering with Memory Networks,"The memory units consist of the related triples in a local subgraph of the corresponding answer path, where the settings are the same as (Bordes et al., 2015).###Single-relation questions, such as “How old is Obama?”, can be answered by finding one fact triple in KB, and this task has been widely studied (Bordes et al., 2015; Xu et al., 2016; Savenkov and Agichtein, 2017).",other,reporting prior findings on answering single-relation questions
760,5fbe5cf091e011e6e11b3cf5,15a84047e5145891d0c7ee054c00a00f2f5d38a1,Boosting Contrastive Self-Supervised Learning with False Negative Cancellation,5e4672c93a55ac14f595d8b5,A Simple Framework for Contrastive Learning of Visual Representations,"SimCLR v1 [9] eschews a momentum encoder in favor of a large batch size, and proposes updates to the projection head and data augmentation.###In fact, selfsupervised visual representation learning has been closing the gap with, and in some cases even surpassing its supervised counterpart [9, 22, 11, 10].###While recent efforts focus on improved architectures [9, 10, 22] and data augmentation [9, 43], relatively little work considers the effects of negative samples, especially that of false negatives.###Without labels, recent breakthroughs in self-supervised visual representation learning rely on the instance discrimination task in which positive pairs are defined as different views of the same image, while negative pairs are formed by sampling views from different images, regardless of their semantic information [22, 9, 34].",impact-revealing,highlighting advancements and challenges in self-supervised visual representation learning
3560,5def6ca63a55ac6095fe0607,2a6f2656c716082e34f9b86d9e589390842e0853,DistTC: High Performance Distributed Triangle Counting,5a260c4617c44a4ba8a26ecd,Collaborative (CPU + GPU) algorithms for triangle counting and truss decomposition on the Minsky architecture: Static graph challenge: Subgraph isomorphism,"[19] present a GPU algorithm that leverages the CPU as well to improve the performance of both triangle counting and ktruss: they use GPU zero-copy memory and unified memory capabilities to decrease CPU-GPU data transfer overhead, and they use the CPU to perform some computations.",other,reporting prior findings on GPU algorithm performance
3225,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e9b833b7602d97043ede44,Understanding Sampling Style Adversarial Search Methods,This algorithm is the same as AMAF but also updates nodes that can be reached by permutations of moves in the simulation that preserve the eventual state reached [101].,other,providing context for an algorithm
669,5f69d1e09fced0a24bc32bb1,614e222b921974f2a8b48c9fbbc8868c9362e99d,A Communication-Aware DNN Accelerator on ImageNet Using In-Memory Entry-Counting Based Algorithm-Circuit-Architecture Co-Design in 65-nm CMOS,599c7cda601a182cd27e0d74,SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks.,"To exploit the sparsity of DNNs to improve energy efﬁciency, many architectures, such as [25], are proposed to detect and skip the multiplications associated with zeros.",impact-revealing,acknowledge existing architectures for energy efficiency in DNNs
792,53e9b49bb7602d9703fa7aed,65cd7e21193ced281fecb0894a1245a8c3988286,The evicted-address filter: A unified mechanism to address both cache pollution and thrashing,53e9bac2b7602d97046f3e49,High performance cache replacement using re-reference interval prediction (RRIP),"In addition, like TA-DIP, since TA-DRRIP operates at a thread (rather than a block) granularity, it suffers from the short-comings of TA-DIP described before.###, [17, 19, 34, 35]), two problems degrade cache performance signiVcantly.###We compare our EAF-augmented cache (or just EAF-cache) with five state-of-the-art cache management approaches that aim to prevent pollution or thrashing: 1) Thread-aware dynamic insertion policy [16] (TA-DIP), 2) Thread-aware dynamic re-reference interval prediction policy [17] (TA-DRRIP), 3) Signature-based Hit Prediction using Instruction pointers (SHIP) [55], 4) Run-time cache by-passing [19] (RTB), and 5) Miss classification table [9] (MCT).###Thread-Aware Dynamic Re-Reference Interval Prediction (TADRRIP) [17] improves upon TA-DIP by using a better replacement policy than LRU, RRIP.###As the figure shows, EAF-cache consistently improves performance in both cases for almost all workloads (11% on average for LRU and 12% for RRIP).###These workloads are known to have scans [17] (accesses to a large number of blocks with no reuse) which pollute the cache, making pollution the major problem.###TA-DRRIP [17] Partly Yes 2 bytes per application No changes to cache No changes to cache hits###We compare our EAF-augmented cache (or just EAF-cache) with Vve state-of-the-art cache management approaches that aim to prevent pollution or thrashing: 1) Thread-aware dynamic insertion policy [16] (TA-DIP), 2) Thread-aware dynamic re-reference interval prediction policy [17] (TA-DRRIP), 3) Signature-based Hit Prediction using Instruction pointers (SHIP) [55], 4) Run-time cache bypassing [19] (RTB), and 5) Miss classiVcation table [9] (MCT).###However, as identified by later work [55], TA-DRRIP does not completely address the pollution problem as it monitors the reuse behavior of a block after inserting the block into the cache.###Prior work proposed to modify the cache insertion policy to mitigate the negative eUects of pollution and thrashing [16, 17, 19, 34, 35, 53, 55].###Figure 8 shows the beneVts of augmenting our EAF mechanism to 1) a cache following the LRU replacement policy, and 2) a cache following the RRIP [17] policy for all 4-core workloads.###This allows RRIP to reduce the performance degradation caused by low-reuse blocks, when compared with LRU.###Our tech report provides the full results [47]. best mechanisms to address pollution (SHIP) or thrashing (DRRIP) for most benchmarks.###Unlike the LRU policy, which inserts all in-coming blocks with the highest priority (MRU position), RRIP in-serts all incoming blocks with a lower priority.###Figure 8 shows the benefits of augmenting our EAF mechanism to 1) a cache following the LRU replacement policy, and 2) a cache following the RRIP [17] policy for all 4-core workloads.###When multiple threads share the cache, prior approaches [16, 17] learn the thrashing behavior of individual threads using a technique called set-dueling [35, 36], and use BIP for those threads that are determined to suUer from thrashing.###One shortcoming of EAF-cache is its storage overhead compared to certain other prior approaches [9, 16, 17] to address cache thrashing or pollution individually (see Table 1).###For all mechanisms, except baseline LRU and DIP, the last-level cache uses the re-reference interval prediction replacement policy [17].###Thread-Aware Dynamic Re-Reference Interval Prediction (TA-DRRIP) [17] improves upon TA-DIP by using a better replacement policy than LRU, RRIP.",impact-revealing,comparing cache management approaches and their performance
2956,5d3ed25a275ded87f97deae9,2c6097792ed9e4e8a664ce2dc7492377bfd57139,LightNet: A Dual Spatiotemporal Encoder Network Model for Lightning Prediction,53e9b8dbb7602d97044bf738,The relationship between lightning activity and ice fluxes in thunderstorms,Deierling et al. [6] used ground-based dual-polarimetric radar and total lightning data to investigate the relationship between total lightning activity and precipitation ice mass on a storm scale.,other,reporting prior findings on lightning activity and precipitation
3122,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,558c650f84ae6766fdf2d55f,SCRIPT: A framework for Scalable Real-time IP Flow Record Analysis,DIPStorage [91] is a distributed flow storage platform for IP flow records based on DHT. SCRIPT [92] is a distributed flow analysis framework that distributed flow records equally to multiple nodes.###SCRIPT [92] is a distributed flow analysis framework that distributed flow records equally to multiple nodes.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2701,5db929e147c8f766461fc024,e03b5bc5edeb44d4b47d225c0c26ac54088fe528,An Improved Neural Baseline for Temporal Relation Extraction,5b1643998fbcbf6e5a9bc27f,A Multi-Axis Annotation Scheme For Event Temporal Relations,"We also show our performance on another dataset,
5http://cogcomp.org/page/publication_ view/834
TCR6 (Ning et al., 2018a), which contains both temporal and causal relations and we only need the temporal part.###In addition, Ning et al. (2018d) only reported F1 scores, while we also use another two metrics for a more thorough comparison: classification accuracy (acc.) and temporal awareness Faware, where the awareness score is for the graphs represented by a group of related TempRels (more details in the…###A recent annotation scheme, Ning et al. (2018c), introduced the notion of multi-axis to represent the temporal structure of text, and identified that one of the sources of confusions in human annotation is asking annotators for TempRels across different axes.###Recently, Ning et al. (2018c) introduced a new dataset called Multi-Axis Temporal RElations for Start-points (MATRES).###Note that the TEMPROB we use is reconstructed using the same
method described in Ning et al. (2018b) with the base method changed to CogCompTime.###Temporal relation (TempRel) extraction has been considered as a major component of understanding time in natural language (Do et al., 2012; UzZaman et al., 2013; Minard et al., 2015; Llorens et al., 2015; Ning et al., 2018a).###Ning et al. (2018b) was an initial attempt to acquire such knowledge, by aggregating automatically extracted TempRels from a large corpus.###We compare with the most recent version of CogCompTime, the state-of-the-art on MATRES.7 Note that in Table 2, CogCompTime performed slightly different to Ning et al. (2018d): CogCompTime reportedly had F1=65.9 (Table 2 Line 3 therein) and here we obtained F1=66.6.###This paper uses MATRES to show that a long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) system can readily outperform the previous state-of-the-art system, CogCompTime (Ning et al., 2018d), by a large margin.###Moreover, we further improve the LSTM system by injecting knowledge from an updated version of TEMPROB, an automatically induced temporal common sense knowledge base that provides typical TempRels between events1 (Ning et al., 2018b).###Table 3 furthermore applies CogCompTime and the proposed Concat+CSE system on a different test set called TCR (Ning et al., 2018a).###More details in Ning et al. (2018c).
agate to subsequent modules.###6K Table 1: TimeBank (TB), AQUAINT (AQ), and Platinum (PT) are from MATRES (Ning et al., 2018c) and TCR from Ning et al.###…2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Jurafsky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017).",other,acknowledge existing work and datasets in temporal relation extraction
1032,,f2ee0726d80a569c9e1ca7c6494916c205c86a14,Data gathering ability contributes to visual organization and probabilistic reasoning,,,"###It has been widely used as a measure of probabilistic reasoning in schizophrenia studies to show delusional ideation may be associated with placing disproportionate import on prematurely gathered data (Dudley et al., 2016; Fine et al., 2007; Garety and Freeman, 2013; Speechley et al., 2010).",impact-revealing,highlighting the significance of a measure in schizophrenia studies
1523,,3f8770d27ca36e3a556ac762c1d1e57b6f63c2c3,Edge Bundling by Rapidly-Exploring Random Trees,,,"###The development of RRTs was inspired by ideas from optimal control theory [6], non-holonomic planning [7], and randomized path planning [8], [9].",impact-revealing,highlighting the inspiration for RRT development from various theories
3933,5fd0a7f691e01147f1d1e367,d4e9bf1d5fab52698a3aa48c65b098ea327deea6,CTRLsum: Towards Generic Controllable Text Summarization,599c797a601a182cd2642797,Get To The Point: Summarization With Pointer-Generator Networks,"…extractive summarization where models find and copy important portions of a documents (Cheng and La-pata, 2016; Nallapati et al., 2017; Narayan et al., 2018), and abstractive summarization where models freely generates novel sentences (Rush et al., 2015; See et al., 2017; Paulus et al., 2018).",other,describing different approaches to summarization
4042,5736960c6e3b12023e51ee06,492f57ee9ceb61fb5a47ad7aebfec1121887a175,Gated Graph Sequence Neural Networks,53e9bd59b7602d97049f3661,A New Model For Learning In Graph Domains,"More closely related to our goal in this work are methods that learn features on graphs, including Graph Neural Networks (Gori et al., 2005; Scarselli et al., 2009), spectral networks (Bruna et al., 2013) and recent work on learning graph ﬁngerprints for classiﬁcation tasks on graph representations of chemical molecules (Duvenaud et al., 2015).###A secondary contribution is highlighting that Graph Neural Networks (and further extensions we develop here) are a broadly useful class of neural network model that is applicable to many problems currently facing the ﬁeld.###Our main contribution is an extension of Graph Neural Networks that outputs sequences.###We now describe Gated Graph Neural Networks (GG-NNs), our adaptation of GNNs that is suitable for non-sequential outputs.###Here, (1) is mostly achieved by previous work on Graph Neural Networks (Scarselli et al., 2009); we make several minor adaptations of this framework, including changing it to use modern practices around Recurrent Neural Networks.###More closely related to our goal in this work are methods that learn features on graphs, including Graph Neural Networks (Gori et al., 2005; Scarselli et al., 2009), spectral networks (Bruna et al., 2013) and recent work on learning graph ﬁngerprints for classiﬁcation tasks on graph representations…###GNNs have been applied in several domains (Gori et al., 2005; Di Massa et al., 2006; Scarselli et al., 2009; Uwents et al., 2011), but they do not appear to be in widespread use in the ICLR community.###In this section, we review Graph Neural Networks (GNNs) (Gori et al., 2005; Scarselli et al., 2009) and introduce notation and concepts that will be used throughout.",other,highlighting the relevance and application of Graph Neural Networks in various problems
1337,,01ac9b0d9ef6c1511f0d9002ac7bbbf5237c75d4,IQMA Network: Image Quality Multi-scale Assessment Network,,,"###Our method is motivated by the fact that traditional FRIQA metrics may fail when evaluating GAN-based distor-
tion.###Various objective methods have been proposed for FRIQA [4,18,20,22,23,26,27].",impact-revealing,highlighting the limitations of traditional FRIQA metrics
3707,57d063e0ac44367354294777,9c62532d78d6eb31f7b0489b6bcb4c2baf517860,CASH: Supporting IaaS Customers with a Sub-core Configurable Architecture,53e9b350b7602d9703e268f6,"Utility-Based Cache Partitioning: A Low-Overhead, High-Performance, Runtime Mechanism To Partition Shared Caches","For instance, configurable cache hierarchies [20, 23,
28, 44, 52] and configurable pipeline architectures (modifiable issue width, instruction window size, number of physical registers, etc.) [15, 41, 57, 64] allow fine grain control over resource scheduling.",other,acknowledge existing configurable architectures
564,58437722ac44360f1082f160,8aa3358a34a17abd0a65622aad8c85317b851af4,very deep convolutional networks for end-to-end speech recognition,5550415645ce0a409eb3a69e,Very Deep Convolutional Networks for Large-Scale Image Recognition.,"Recently, very deep CNNs architectures [15] have also been shown to be successful in ASR [16, 17], using more non-linearities, but fewer parameters.###We are driven by same motivation that led to the success of very deep networks in vision [14, 18, 21, 23] – add depth of processing using more non-linearities and expressive power, while keeping the number of parameters manageable, in effect increasing the amount of computation per parameter.###CNNs have shown improvement over traditional fully-connected deep neural networks on many ASR tasks [14, 12], we investigate the effect of convolutional layers in seq2seq models.###We explored very deep CNNs for end-to-end speech recognition.###Moreover, strided convolutions are an essential element of CNNs.###While very deep CNNs have been successfully applied to ASR, recently there have been several advancements in the computer vision community on very deep CNNs [15, 18] that have not been 1.###Unlike fully connected layers, Convolutional Neural Networks (CNNs) take into account the input topology, and are designed to reduce translational variance by using weight sharing with convolutional ﬁlters.###The standard formulation of BN for CNNs can be readily applied to DNN acoustic models and cross-entropy training.###Our strategy to alleviate this problem is to apply striding in the ﬁrst and second layer of the CNNs to reduce the time dimensionality and memory footprint.###In our work, we replace Listen with a network of very deep CNNs and BLSTMs.###Recently, very deep CNNs architectures [14] have also been shown to be successful in ASR [15, 16, 17], using more non-linearities, but fewer parameters.###Unlike Deep Neural Networks (DNNs) [13], CNNs explicitly exploit structural locality in the spectral feature space.###Convolutional Neural Networks (CNNs) [9] have been successfully applied to many ASR tasks [10, 11, 12].###CNNs use shared weight ﬁlters and pooling to give the model better spectral and temporal invariance properties, thus typically yield better generalized and more robust models compared to DNNs [14].###While very deep CNNs have been successfully applied to ASR, recently there have been several advancements in the computer vision community on very deep CNNs [14, 18] that have not been",impact-revealing,highlighting the success and advancements of very deep CNNs in ASR and their potential benefits
1056,,760848b14a00e20abe8546ca018a2ccac8084cf0,A Temporal Foundation for Continuous Queries over Data Streams,,,"###In order to express continuous queries, different query languages have been proposed recently [1, 10, 29, 3, 13].###TelegraphCQ [9] relies on a declarative language to express a sequence of windows over a stream, whereas Gigascope [10] and [27] try to unblock operations by using stream constraints instead of windows.###Their need is motivated by a variety of applications [4, 13, 8, 25, 10, 29] like network and traffic monitoring.",impact-revealing,acknowledge different query languages and their applications
2716,5da1a6d447c8f7664606888c,404d82a8da658f16c46714442155490d79a413f1,temporal prefetching without the off-chip metadata,53e9992ab7602d9702165fe1,Access Map Pattern Matching for High Performance Data Cache Prefetch.,"Many prefetchers predict sequential [21, 28, 42] and strided [3, 15, 22, 32, 35, 37, 39] accesses, and while this class of prefetchers has enjoyed commercial success due to their extremely compact metadata, their benefits are limited to regular memory accesses.",other,highlighting the limitations of existing prefetchers
2414,57d063e8ac443673542950ad,0509facb30efa81c4730f98bfc28cdbd6178822e,"Image sentiment analysis using latent correlations among visual, textual, and sentiment views",53e9b18fb7602d9703c1b31b,SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining,"For this, we use an exter-nal knowledge base, called SentiWordNet [10].###To highlight the sentiment information in the text, we introduce an external sentiment knowledge base, Senti-WordNet [10], which forms the sentiment view.###…feature-based method [4] (denoted as Low ), a mid-level visual feature-based method [5] (denoted as SentiBank ), a method that concatenates low-level visual features with the mid-level features (denoted as Low&SentiBank ), and a textual feature-based method [10] (denoted as SentiStrength 3 ).",other,reporting the use of an external knowledge base for sentiment analysis
1815,,2c7b1bcfe59a5d67157a828b5d5f342c407ad5d0,Sustained FVIII expression and phenotypic correction of hemophilia A in neonatal mice using an endothelial-targeted sleeping beauty transposon.,,,"###While viralbased vectors have shown some success, the utility of several of these vectors has been limited by induction of host immune and inflammatory responses [7,8] as well as the risk of inducing tumorigenic mutations [9,10].",impact-revealing,highlighting limitations of viral-based vectors
832,53e999cab7602d970220f327,7aca628a75775530a5b946900af827890a4208de,"A PPM-like, Tag-based Predictor",53e998cdb7602d97021066c4,Data compression using adaptive coding and partial string matching,"5 is a global-history based predictor derived from PPM. PPM was originally introduced for text compression [1], and it was used in [2] for branch prediction.",impact-revealing,reporting prior findings on PPM and its applications
2029,,d8410ca7ef94267f649647d8d3efff25db98d4e6,"Diabetes, glucose level, and risk of sudden cardiac death.",,,"###In addition to be a marker of microvascular kidney disease, proteinuria might also reflect to some extent macrovascular kidney disease.(29) The present analysis did not address the issue of a decrease in the risk of SCD with a decrease in the level of glycaemia.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3081,5da2f8aa3a55ac3402d8c2e1,17f2f3f7e58b916175d495109bc74b2757ef952a,Barrage of Random Transforms for Adversarially Robust Defense,599c796f601a182cd263cd58,Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection   Methods,"In our experiments, we will test a range of ∈ [2, 32].###As far as we are aware, these types of defenses have all been defeated in the white-box threat model, either by correctly incorporating the defense into the adversary’s search procedure [2], or by properly accounting for obfuscated gradients [4].###This is largely due to the success of modern deep learning techniques within the realm of computer vision tasks and the surprising ease with which such systems are fooled into giving incorrect decisions [2].###Below we will further detail all the steps we take to implement the adversary’s attack, so that we fully account for the gradient obfuscation and other issues that have thwarted previous defenses [2, 4].###We will show that our defense outperforms adversarial training across all ∈ [2, 16], and even continues to provide a robust defense up to = 32.",other,Highlighting the challenges faced by existing defenses in adversarial settings
1279,,b96cb1895cfac5ed4410e9fce0015b985779bf86,"An eigen-analysis of the relationships between model structure, discrete data, measurement error and resulting parameter identification distributions",,,"###These methods determine when the data quantity and quality
is insufficient for the size of a model, resulting in mutual interference of two or more parameters (Docherty et al., 2011, Raue et al., 2009, Saccomani, 2013).###Infinite confidence intervals indicate practical non-identifiability (Raue et al., 2009) and since identifiability is a continuous artefact (Docherty et al., 2011), smaller finite intervals could be useful in evaluating whether the degree of identifiability is acceptable, subject to the needs of…###This is a highly novel area of research with only one other group in the field (Raue et al., 2009, Raue et al., 2014, Saccomani, 2013).",impact-revealing,highlighting the novelty and limited research in a specific area
222,573696026e3b12023e515eec,2c03df8b48bf3fa39054345bafabfeff15bfd11d,Deep residual learning for image recognition,53e99da4b7602d970265df42,Aggregating local image descriptors into compact codes.,"In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD.",impact-revealing,providing context on image recognition representations
1371,,da007a4fad7036fce49dfa7e38668ce1ba09a20d,RIPK1-RIPK3-MLKL-dependent necrosis promotes the aging of mouse male reproductive system,,,"###Given that aged animals carry significantly more DNA damage than younger animals, their elimination from the mating pool results in healthier pups overall, an outcome that would confer an evolutionary advantage over (a population) of animals that do not thusly employ a necroptosis program in their testes.###The cause for these unhealthy pups may be multiple, including age-related accumulation of DNA damages in the old sperm and other organs.###…mice, measured as the level of 8-hydroxydeoxyguanosine (8-OHdG), a biomarker for the oxidative damage of DNA (Chigurupati et al., 2008; Johnson et al., 2015; Paul et al., 2011), was significantly higher in the sperm of 18-month-old wild type and Ripk3-knockout mice than in 4-month-old…###A study into the possible reason for the unhealthy offspring revealed accumulated oxidative damage in the sperm DNA of aged Ripk3-knockout mice, measured as the level of 8-hydroxydeoxyguanosine (8-OHdG), a biomarker for the oxidative damage of DNA (Chigurupati et al., 2008; Johnson et al., 2015; Paul et al., 2011), was significantly higher in the sperm of 18-month-old wild type and Ripk3-knockout mice than in 4-month-old mice (Figure 1—figure supplement 7B and C).###Founders were screened with T7E1 assays and were validated by DNA sequencing.",impact-revealing,highlighting the evolutionary advantage of eliminating aged animals from the mating pool due to DNA damage
2663,5736974d6e3b12023e6388bf,1d7e580740a3f96f1529e3d771915af4e996a259,Verbal and Nonverbal Clues for Real-life Deception Detection,5550446545ce0a409eb4d504,Towards A General Rule For Identifying Deceptive Opinion Spam,", 2010; Joinson and Dietz-Uhler, 2002), social networks (Ho and Hollister, 2013), and consumer report websites (Ott et al., 2011; Li et al., 2014).###…of deceptive content in a variety of domains, including online dating websites (Toma and Hancock, 2010; Guadagno et al., 2012), forums (Warkentin et al., 2010; Joinson and Dietz-Uhler, 2002), social networks (Ho and Hollister, 2013), and consumer report websites (Ott et al., 2011; Li et al., 2014).",other,acknowledge variations in existing research on deceptive content
3883,53e99fe3b7602d97028bddfb,ecf5fd423c117ffb87730d75a473bc05beaae2b8,self-optimizing memory controllers: a reinforcement learning approach,53e9a81fb7602d9703168ac1,Smart Refresh: An Enhanced Memory Controller Design For Reducing Energy In Conventional And 3d Die-Stacked Drams,"Other work has proposed techniques for intelligent address remapping [8], eliminating bank conﬂicts [44, 47], and refresh scheduling [15] in DRAM controllers.",other,acknowledge existing techniques in DRAM controllers
3896,5c04967517c44a2c74708f29,15c9684321f03744051bb73b4c1141507dc8ddb2,Embedding Uncertain Knowledge Graphs,53e9a922b7602d9703276ed9,Open Question Answering with Weakly Supervised Embedding Models,"Hence, they have been the crucial feature models that beneﬁt numerous knowledge-driven tasks (Bordes, Weston, and Usunier 2014; He et al. 2017; Das et al. 2018).",other,highlighting the significance of feature models in knowledge-driven tasks
1647,,a775677c0d4648e5067a65603432d3c0adfe9ea4,"COVID-19 coverage from six network and cable news sources in the United States: Representation of misinformation, correction, and portrayals of severity",,,"###The model builds upon the well-established Social Identity Theory (Tajfel and Turner, 1979) and the Reasoned Action Approach (Fishbein and Ajzen, 2010) to suggest that the attitudinal and behavioral differences between the political right and left regarding COVID-19 stem from different perceptions…###The model builds upon the well-established Social Identity Theory (Tajfel and Turner, 1979) and the Reasoned Action Approach (Fishbein and Ajzen, 2010) to suggest that the attitudinal and behavioral differences between the political right and left regarding COVID-19 stem from different perceptions and misperceptions systematically formed over time within highly discrepant information environments.",impact-revealing,building on established theories to explain political differences in COVID-19 perceptions
879,5def6ca63a55ac6095fe0607,2a6f2656c716082e34f9b86d9e589390842e0853,DistTC: High Performance Distributed Triangle Counting,5d79a6ff3a55ac5b650357d7,CuSP: A Customizable Streaming Edge Partitioner for Distributed Graph Analytics,The partitioning strategy described in this section is application-agnostic and is implemented in the Customizable Streaming Partitioner (CuSP) framework [36].###We implemented the proposed graph partitioning policy using the Customizable Streaming Partitioner (CuSP) [36] framework.,impact-revealing,describing the implementation of a graph partitioning policy
1480,,2c91faf6ac4c9fcdd4686e67b867a2b8c20fc3a0,Semantic segmentation-based traffic sign detection and recognition using deep learning techniques,,,###Our choice was motivated by the work of [26].,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
555,5c2c7a9217c44a4e7cf3161b,e6926981ef9c1d06d6c075cdae7b298d3dbf3a7d,Learning Latent Representations for Style Control and Transfer in End-to-end Speech Synthesis,5c8ce3904895d9cbc628ae7f,Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions,"Therefore, in this paper we intend to introduce VAE to Tacotron2 [1], a state-of-the-art end-to-end speech synthesis model, to learn the latent representation of speaker state in a continuous space, and further to control the speaking style in speech synthesis.###While the single style TTS, usually neutral speaking style, is approaching the extreme quality close to human expert recording [1, 3], the interests in expressive speech synthesis also keep rising.###The attention module and decoder have the same architecture as Tacotron 2 [1].",impact-revealing,introducing a novel approach to enhance speech synthesis
3513,599c797a601a182cd2642797,668db48c6a79826456341680ee1175dfc4cced71,Get To The Point: Summarization with Pointer-Generator Networks,57d063b9ac4436735428ea8e,Modeling Coverage For Neural Machine Translation,"Originating from Statistical Machine Translation (Koehn, 2009), coverage was adapted for NMT by Tu et al. (2016) and Mi et al. (2016), who both use a GRU to update the coverage vector each step.###We propose a novel variant of the coverage vector (Tu et al., 2016) from Neural Machine Translation, which we use to track and control coverage of the source document.###We adapt the coverage model of Tu et al. (2016) to solve the problem.###Repetition is a common problem for sequence-to-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1).",other,highlighting the adaptation of coverage models in neural machine translation
2659,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",53e9be03b7602d9704ab732c,Semi-supervised Learning on Directed Graphs.,"As an alternative, some authors have proposed to approximate directed graphs by undirected ones, using such approaches as the hub-authority model [94], [95].",other,acknowledge alternative approaches in graph modeling
2493,5dc9327d3a55acc1042498de,2cf3bd0cc1382f35384e259d99e4f9744eeaed28,Blockwise Self-Attention for Long Document Understanding,5d3ed2653a55ac61d998598b,RoBERTa: A Robustly Optimized BERT Pretraining Approach,"This technique has been widely used in Transformer training (Ott et al., 2019; Liu et al., 2019).###RoBERTa-2seq & RoBERTa-1seq We compare with two versions of RoBERTa (Liu et al., 2019).###Notice that although our analysis is done on BERT-Base, it can also be generalized to BERTLarge and other models such as RoBERTa (Liu et al., 2019) and XLNet (Yang et al.###Notice that although our analysis is done on BERT-Base, it can be easily generalized to BERT-Large and other models such as RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019).###, 2017) and its successful application to masked language model pre-training (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019), several approaches have been proposed to simplify the model and its training process.###…of the pre-training and ﬁne-tuning paradigm, exempliﬁed by methods like ELMo (Peters et al., 2018), GPT-2 (Radford et al., 2019), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019), has drastically reshaped the landscape of the natural language processing research.###RoBERTa-2seq and RoBERTa-1seq We compare with two versions of RoBERTa (Liu et al., 2019).###In order to reduce the pre-training time of RoBERTa to 1 day, Liu et al. (2019) use 1,024 V100 GPUs.###, 2019), RoBERTa (Liu et al., 2019) and ALBERT (Lan et al.###…the invention of Transformer (Vaswani et al., 2017; Dai et al., 2019) and its successful application on language model pre-training (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019), there have been several studies attempted to simplify it from different perspectives.",other,highlighting the significant impact of Transformer models on NLP research
2046,,363850a782ad92811a5f4a72b89d6136db32b8a3,Optimal Co-Segmentation of Tumor in PET-CT Images With Context Information,,,"###Our work is inspired by the multiple surface detection method [45], [47] and the multi-region segmentation method [48] based on the graph theoretical framework.###In the future, we plan to simultaneously segment those boundary surfaces from the CT scan as well as the tumor from both PET and CT datasets by integrating the graph searching method [44], [45], [65] with the graph cut method [10].",impact-revealing,highlighting inspiration from existing methods and suggesting future research directions
1390,,24af21d0e286dedbad04522884a3e726ec564f2d,Identifying Risk Factors for Diabetic Ketoacidosis Associated with SGLT2 Inhibitors: a Nationwide Cohort Study in the USA,,,"###Their primary mechanism of action lowers plasma glucose by inhibiting reabsorption at the nephron.(1,2) Two of the SGLT2 inhibitors, empagliflozin and canagliflozin, also reduce the risk of myocardial infarction, stroke, and cardiovascular mortality.###Sodium glucose co-transporter 2 (SGLT2) inhibitors are commonly used for the treatment of type 2 diabetes mellitus.(1,2) Their primary mechanism of action lowers plasma glucose by inhibiting reabsorption at the nephron.",impact-revealing,reporting on the mechanism and benefits of SGLT2 inhibitors
2374,5d1eb9beda562961f0af981f,934d7bffdba0b560a80a518b99a791a16b3e198c,A Fourier Perspective on Model Robustness in Computer Vision,5a4aef9e17c44a2190f7a221,Measuring the tendency of CNNs to Learn Surface Statistical Regularities.,"While this problem is far from being completely understood, perhaps the simplest explanation proposed is that models lack robustness to distributional shift simply because there is no reason for them to be robust (Jo & Bengio, 2017).",other,highlighting a potential explanation for model robustness issues
3701,5b1643ba8fbcbf6e5a9bc5b5,8c1b00128e74f1cd92aede3959690615695d5101,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,599c7987601a182cd2647a85,Triviaqa: A Large Scale Distantly Supervised Challenge Dataset For Reading Comprehension,"Their popularity can be attributed to an increase in publicly available annotated datasets, such as SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), CNN/Daily News (Hermann et al., 2015), WikiReading (Hewlett et al., 2016), Children Book Test (Hill et al., 2015), etc.###In this section, we test our model on another dataset TriviaQA (Joshi et al., 2017), which consists of 650K context-query-answer triples.###As the text could be long, we adopt the data processing similar to Hu et al. (2017); Joshi et al. (2017).###, 2016), TriviaQA (Joshi et al., 2017), CNN/Daily News (Hermann et al.###We also conduct similar studies on TriviaQA (Joshi et al., 2017), another Q&A dataset, to show that the effectiveness and efficiency of our model are general.###In addition to the full development set, the authors of Joshi et al. (2017) also pick a veriﬁed subset that all the contexts inside can answer the associated questions.###We also conduct similar studies on TriviaQA (Joshi et al., 2017), another Q&A dataset, to show that the effectiveness and efﬁciency of our model are general.###According to the previous work (Joshi et al., 2017; Hu et al., 2017; Pan et al., 2017), the same model would have similar performance on both Wikipedia and Web, but the latter is five time larger.###Full Verified Single Model EM / F1 EM / F1 Random (Joshi et al., 2017) 12.",other,highlighting the impact of publicly available annotated datasets on model testing
3440,5eede0b091e0116a23aafb82,1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af,Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,599c798a601a182cd2649c22,Unsupervised Learning by Predicting Noise.,"Instance-level classification considers each image in a dataset as its own class [4, 15, 49].",other,reporting prior findings on instance-level classification
422,5e5e18b693d709897ce29a22,53a77e8f73f2ca422d6e38fa9ecc490231ac044c,Neural Text Generation with Unlikelihood Training,5bdc31b417c44a1f58a0b483,Adaptive Input Representations for Neural Language Modeling.,"Training As in Baevski and Auli (2019); Radford et al. (2019), we train on ﬁxed-length contiguous sequences, in our case of length 1,536, which was selected based on GPU memory constraints.###We follow standard pre-processing and perform experiments at the word level as in Baevski and Auli (2019).###We follow a standard language modeling setup from Baevski and Auli (2019), which we detail along with our sequence completion protocol below.###We emphasize that our proposed method is architecture ag-nostic; we choose this one as a representative of recent large-scale language models, e.g. (Baevski and Auli, 2019; Radford et al., 2019).###Baseline The baseline model trained with maximum likelihood ( L MLE ) achieved 24.52 validation perplexity (25.71 test), comparable to a current state-of-the-art system (Baevski and Auli, 2019) (23.87 valid, 24.92 test).###Thus we adopt a 16-layer Transformer architecture, with 8 attention heads, an embedding dimension of 1024, and a fully-connected dimension of 4096 ; the architecture is based on (Baevski and Auli, 2019) but with standard embedding and softmax layers.###As a document-level dataset, Wikitext-103 is an open-source representative of recent datasets used for large-scale language modeling (Baevski and Auli, 2019; Radford et al., 2019).",impact-revealing,describing the training process and model architecture
2740,5e3940c73a55ace46ed436d2,845b4941d8c016aa5f8967da2f86d38ef6c18fa3,a survey on knowledge graphs representation acquisition and applications,573698636e3b12023e7296e0,Representation Learning Of Knowledge Graphs With Entity Descriptions,"Aforementioned KRL methods (e.g., TransE [16], TransH [20], TransR [17], HolE [21], and R-GCN [60]) and joint learning methods like DKRL [65] with textual information can been used for KGC. Unlike representing inputs and candidates in the uniﬁed embedding space, ProjE [82] proposes a combined embedding by space projection of the known parts of input triples, i.e., ( h, r, ?) or (?###, TransE [12], TransH [15], TransR [13], HolE [16], and R-GCN [53]) and joint learning methods like DKRL [57] with textual information can been used for KGC.###DKRL [57] extends TransE [12] to learn representation directly from entity descriptions by a convolutional encoder.###DKRL [65] extends TransE [16] to learn representation directly from entity descriptions by a convolutional encoder.",other,acknowledge existing KRL methods and their applications
1576,,06186b137e1bec506e99d0f3608a7f8467a1163f,Spatial Resolution and Algorithm Choice as Modifiers of Downslope Flow Computed from Digital Elevation Models,,,"###Lea (1992) linked flow to terrain aspect, and this idea was extended by Costa-Cabral and Burgess (1994) in a set of procedures called DEMON.###…1969; Evans 1980; Moore et al. 1993; Quinn et al. 1991), attention focused on hydrographic feature delineation, extraction, and use in modeling (Costa-Cabral and Burgess 1994; Gallant and Wilson 1996; Jenson and Domingue 1988; Lea 1992; Ritter 1987; Tarboton et al. 1993; Zevenbergen and Throne…",impact-revealing,acknowledge prior work on flow and terrain aspect
2846,5a260c0c17c44a4ba8a1e113,c751ab01aedc2888a7fe6e8b4f77ab1afa94072f,protein interface prediction using graph convolutional networks.,5736986c6e3b12023e7308a0,Convolutional Networks on Graphs for Learning Molecular Fingerprints,"018) – – – Single Weight Matrix (MFN [9]) 0.###[9] proposed a spatial graph convolution approach similar to Equation (1), except that they use a single weight matrix for all nodes in a receptive field and sum the results, whereas we distinguish between the center node and the neighboring nodes, and we average over neighbors rather than sum over them.",other,describing a methodological distinction in graph convolution approaches
3393,5e09a76bdf1a9c0c41677a7b,71bd6b3a2bfa54ee4ea8499be0c4ff478fd735f6,POLAR++: Active One-Shot Personalized Article Recommendation,53e99dccb7602d970268decf,A Keyphrase-Based Paper Recommender System.,"Most approaches use plain words as features, although some use n-grams [20], topics [9], [21], and cita-###Content-based filtering [4] is one of the most widely used and researched recommendation method and has been successfully applied in article recommendation [9], [19], [20], [21].",other,acknowledge existing features used in approaches
727,53e9b413b7602d9703f0894b,c40d176e8c55399c45ac18f90d254438a26829e1,dynamic data dependence tracking and its application to branch prediction,53e9ba11b7602d9704614c21,Improving branch predictors by correlating on data values,A 3-bit performance counter based on Heil’s design [17] tracks the effectiveness of each entry and is used to select which entry to replace when a new entry is added.###Heil [17] proposed another approach that correlates on the differences between branch source operand values.,impact-revealing,reporting prior findings on performance counter design
2614,5c5ce4fd17c44a400fc38abb,d524f10d653ba09e36456475da0aed92d244f795,Image Super-Resolution As A Defense Against Adversarial Attacks,595b91a10cf267f13d9b1818,Deep Learning in Medical Imaging: General Overview.,"a pivotal role in designing many critical real-world systems, including self-driving cars [12] and models for disease diagnosis [13], which necessitates their robustness in such situations.",other,highlighting the importance of robustness in critical real-world systems
1643,,1a1f336475c01c7f84fea6d278df330925267404,What we inherit and what we create. Making the case for an interpretive approach to societal cultures,,,"###It is inspired by social identity theories including the work by Tajfel and Turner (2001).###Tajfel and Turner (2001) posit three theoretical principles: individuals seek to maintain or acquire a positive social identity; a positive social identity is determined to a large extent by social comparisons favoring the ingroup over an outgroup; when their social identity is judged unsatisfying,…",impact-revealing,Highlighting theoretical principles of social identity theories
3548,5e6cae3493d709897ccff2f9,804f23ac1a4a56b8dc5bb7201dab7b8cece76a70,classifying memory access patterns for prefetching,5550451945ce0a409eb53a23,A Case for Resource Efficient Prefetching in Multicores,"The performance benefits provided by static approaches are limited, as only simple structures such as Singly-Nested Loop Nests (SNLNs) [50] or regular strides [29, 35, 52] can be learned.",other,highlighting limitations of static approaches in learning complex structures
381,5d4d46fb3a55acff992fdbc4,728bd558a45fb5faf907e3de0e3f7942e3e2f81a,CNN-Based Chinese NER with Lexicon Rethinking,5a260c8417c44a4ba8a3153b,Learning with Rethinking: Recurrently Improving Convolutional Neural Networks through Feedback.,"Previous attempts to use a rethinking mechanism in neural networks have been made in image classification to tackle issues of occlusion and noise [Li et al., 2018].###The second issue is addressed by the use of a rethinking mechanism [Li et al., 2018].",impact-revealing,acknowledging prior attempts in neural networks
2605,5ebbc75d91e0119bc4e43623,407f1d16ba4eb3cb4851429cae46c97d723a35a5,invertible image rescaling,5a260c3517c44a4ba8a254a7,How will Deep Learning Change Internet Video Delivery?,", upscaling the downscaled image to a higher resolution or its original size [19,47,58,59].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2728,5da2f8aa3a55ac3402d8c2e1,17f2f3f7e58b916175d495109bc74b2757ef952a,Barrage of Random Transforms for Adversarially Robust Defense,599c7980601a182cd2645249,Feature Squeezing: Detecting Adversarial Examples in Deep Neural   Networks,"Prior works have used the median filter [12], wavelet [15, 17], and non-local mean [27] as defenses, but all have since been defeated.###Color Precision Reduction Reducing bit-resolution of color was originally proposed as a defense by Xu, Evans, and Qi [27] and later reduced to 0% effectiveness [4].",other,highlighting the limitations of previous defenses against adversarial examples
3570,5cede0e8da562983788c741f,b56e5fb4f367a8d54614f1047bd4f9a2d58b9973,Multi-Interest Network with Dynamic Routing for Recommendation at Tmall,5aed14d617c44a4438158fd3,A Capsule Network-based Embedding Model for Search Personalization.,[19] uses capsule network to model relationship triples for knowledge graph completion and search personalization.,other,reporting prior findings on knowledge graph completion
840,5dbebb7447c8f766462c22a6,e1fd81af050dbdc4232ff8b1ab71cf0973d530b6,graph convolutional networks with motif-based attention,5aed148b17c44a4438154efb,Higher-order Network Representation Learning.,"Rossi et al. [33] proposed the notion of higher-order network embeddings and demonstrated that one can learn better embeddings when various motif-based matrix formulations are considered.###While the K -step motif-based adjacencies defined here share some similarity to that of Rossi et al. [33] we would like to point out that there is an important distinction with our formulation.###Specifically, in contrast to [28, 33, 34, 46], we propose a new class of higher-order network embedding methods which utilizes a novel motif-based attention for the task of semi-supervised node classification.###The weighted adjacency matrices are computed using various network motifs [33].###If we classify node i correctly then Our work differs from previous approaches [28, 33, 34, 36, 46] in several key points.###However, in many cases, it has been shown that it may be beneficial to consider the higher-order structure in graphs [6, 25, 33, 35, 46].###Multiple work have demonstrated that it is useful to account for higher-order structures in different graph-based ML tasks [3, 28, 33, 46].###P = D � 1 W Given a function Ψ , we can obtain motif-based matrices ˜ = Ψ , for t = 1 , , T . a motif-based matrix formulation as a function Ψ : R × → R × N over a motif adjacency A t ∈ A similar to [33].###In this work, we introduce a general class of graph convolution networks which utilize weighted multi-hop motif adjacency matrices [33] to capture higher-order neighborhoods in graphs.",impact-revealing,highlighting the distinction and contribution of the proposed method in higher-order network embeddings
2561,5dcd263a3a55ac58039516c5,add2f205338d70e10ce5e686df4a690e2851bdfc,Momentum contrast for unsupervised visual representation learning,5e63725891e011ae97a69ae0,Rethinking Imagenet Pre-Training,"In green are the gaps of at least + 0.5 point. ity of features, so our experiments are on controlled schedules, e.g ., the 1 ( ∼ 12 epochs) or 2 schedules [22] for COCO, in contrast to 6 ∼ 9 in [31].###If the fine-tuning schedule is long enough, training detectors from random initialization can be strong baselines, and can match the ImageNet supervised counterpart on COCO [31].###As prerequisites, we discuss two important issues involved [31]: normalization and schedules.###Table 5 shows the results on COCO with the FPN (Table 5a, b) and C4 (Table 5c, d) backbones.###COCO dense pose estimation [1]: MoCo substantially outperforms supervised pre-training, e.g ., by 3.7 points in AP dp 75 , in this highly localization-sensitive task.###Next we compare MoCo with ImageNet supervised pre-training, transferred to various tasks including PASCAL VOC [18], COCO [42], etc .###, the 1× (∼12 epochs) or 2× schedules [22] for COCO, in contrast to 6×∼9× in [31].###with ImageNet supervised pre-training: COCO keypoint detection : supervised pre-training has no clear advantage over random initialization, whereas MoCo outperforms in all metrics.###On smaller datasets like VOC, training longer may not catch up [31].###%) and the more stringent metrics of COCO-style AP and AP 75 .###If the ﬁne-tuning schedule is long enough, training detectors from random initialization can be strong baselines, and can match the ImageNet supervised counter-part on COCO [31].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3754,5db92a0d47c8f766461ff307,9014c4e7bec425a117b514e41d9cdf9dec8bd6c1,meta relational learning for few-shot link prediction in knowledge graphs,57d063c3ac44367354290599,Representation Learning of Knowledge Graphs with Hierarchical Types.,"There are also some others like ConvE (Dettmers et al., 2018) using convolutional structure to score triples and models using additional information such as entity types (Xie et al., 2016) and relation paths (Lin et al., 2015a).###, 2018) using convolutional structure to score triples and models using additional information such as entity types (Xie et al., 2016) and relation paths (Lin et al.",other,acknowledge variations in existing models
3727,5feb068b91e011f5d3420813,e339c5d31ffc7029c1f72d567ac07b4606701c72,ALP-KD: Attention-Based Layer Projection for Knowledge Distillation,599c7978601a182cd2641b24,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,"• MNLI: A multi-genre natural language inference corpus including sentence pairs with textual entailment annotations (Williams, Nangia, and Bowman 2018).",other,reporting on a specific dataset used for natural language inference
3496,5f00587b9fced0a24b1fbbf1,1803c317dbd25d64210026fc4181faa31e521719,USMPep: universal sequence models for major histocompatibility complex binding affinity prediction,55a5ec3965cead59c82fc2d4,"HLA class I alleles are associated with peptide-binding repertoires of different size, affinity, and immunogenicity.","Second, the overall performance measure implicitly assumes that prediction scores are directly comparable across different alleles, which seems slightly questionable in the light of the discussion of allele-dependent binding thresholds [19].###As discussed in the previous section, there exist commonly applied threshold values for the datasets under consideration but the simplicity of this procedure neglects a possible allele dependence of these threshold values [19].",other,highlighting potential issues with performance measures in allele binding
3377,5f7d893591e011346ad27d16,ff8a988d88ebe5d5ca116340baa34ae00ce011e8,PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation,5b8c9f5317c44af36f8b745b,Retrieve and Refine: Improved Sequence Generation Models For Dialogue,"Our proposed strategy substantially differs from prior solutions that rely on in-place word substitutions (Novak et al., 2016; Xia et al., 2017; Weston et al., 2018), as we leverage the seq2seq architecture to offer more ﬂexible edits.",other,highlighting the differences and advantages of the proposed strategy over prior solutions
2805,5d245bb5da56295a28fcca5f,4efb9a950f252138a30eeb942ed02663a3ea29d1,MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing,5550401245ce0a409eb3205c,Dropout: a simple way to prevent neural networks from overfitting,"In practice, one would stack multiple layers and interleave them with standard neural operators such as BatchNorm (Ioffe & Szegedy, 2015), element-wise activation, and Dropout (Srivastava et al., 2014).###In practice, one would stack multiple layers and interleave them with standard neural operators such as BatchNorm (Ioffe & Szegedy, 2015), element-wise activation, and Dropout (Sri-vastava et al., 2014).",other,providing context on neural network layer stacking and operations
2935,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,5aed148b17c44a4438154eb5,VERSE: Versatile Graph Embeddings from Similarity Measures.,"Graph embeddings [23, 47, 58] can be thought of as (very restricted) unsupervised GNNs with an identity feature matrix, meaning each node learns its own positional representation [67].",other,providing context on graph embeddings and their relation to GNNs
3470,5db1765a3a55ac101c887e97,6c4b76232bb72897685d19b3d264c6ee3005bc2b,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,5ca600ae6558b90bfa4d76e3,An efficient framework for learning sentence representations,"…transfer learning for text classification tasks advocated fine-tuning only the parameters of a small classifier that was fed sentence embeddings produced by a fixed pre-trained model [Subramanian et al., 2018; Kiros et al., 2015; Logeswaran and Lee, 2018; Hill et al., 2016; Conneau et al., 2017].###Early results on transfer learning for text classification tasks advocated fine-tuning only the parameters of a small classifier that was fed sentence embeddings produced by a fixed pre-trained model [Subramanian et al., 2018; Kiros et al., 2015; Logeswaran and Lee, 2018; Hill et al., 2016; Conneau et al., 2017].",other,reporting prior findings on transfer learning for text classification
3802,5736977f6e3b12023e666356,a188bc7c5b74cf4fa1d719e3955f9a9b81e0a146,Study on the Relationship between Profile Images and User Behaviors on Twitter,53e9adb6b7602d97037b8d3e,Investigating topic models for social media user recommendation.,Pennacchiotti and Gurumurthy used LDA to associate Twitter users to a number of topics and based on the results recommended users with similar interests on the Twitter network to the target user [9].,other,reporting prior findings on user recommendation based on LDA
2803,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",53e9979bb7602d9701f638a2,Diffusion wavelets,"Coming from another angle, motivated by processing data collected by sensor networks where sensors are irregularly placed, different authors develop regression algorithms [62], wavelet decompositions [63], [64], [65], [61], [66], filter banks on graphs [67], [68], de-noising [69], and compression###The filterbanks developed in [66] were not critically sampled, unlike [61] or [101].###Pioneering contributions [100] and [61], provided early examples of designs based on vertex domain and spectral domain characteristics, respectively.###Conversely, diffusion wavelets [61] are defined in the spectral domain, but do not guarantee exact vertex domain localization (only energy decay properties).###References [59], [60], [61] choose discrete approximations to other continuous operators, for example, a conjugate to an elliptic Schrödinger-type operator, and obtain other spectral bases for the characterization of the geometry of the manifold underlying the data.",other,acknowledge various approaches in sensor network data processing
2849,5fd0a7f691e01147f1d1e367,d4e9bf1d5fab52698a3aa48c65b098ea327deea6,CTRLsum: Towards Generic Controllable Text Summarization,5b67b4b417c44aac1c86756a,The Natural Language Decathlon: Multitask Learning as Question Answering.,", 2016) and also to demonstrate the multi-task ability present in large pretrained models (McCann et al., 2018; Radford et al., 2019; Keskar et al., 2019; Brown et al., 2020).",other,acknowledge existing research on multi-task ability in large pretrained models
1276,,7e39a4948448b5681aa52534e631b7f5d6a54bec,Rational selection of experimental readout and intervention sites for reducing uncertainties in computational model predictions,,,"###with δα being the α quantile of the χ2 distribution with df = 1 (pointwise) or df = nθ (simultaneous) degrees of freedom [9].###As has been shown by several authors, including [9,12], FI may not be well suited for nonlinear models.###As a result, predictions on internal states - states that are not directly observed in experiments - become highly uncertain [9].###Following [9], the profile likelihood of a parameter θi is given by χ2 PL(θi) = min θj =i χ2(θ), (4)###In the following we build on results of [4,9], who already proposed to use the set of parameters along the profile likelihood for analyzing the impact of parameter uncertainties on model states or more generally model predictions p.###Profile likelihood estimation has been proven to be a valuable tool for parameter identifiability analysis [9].",impact-revealing,providing context on parameter estimation methods
294,555048d345ce0a409eb71be1,eba36ac75bf22edf9a1bfd33244d459c75b98305,Recurrent convolutional neural networks for text classification,53e9ad7cb7602d970376c0c5,Recurrent Convolutional Neural Networks for Discourse Compositionality.,"However, previous studies on CNNs tends to use simple convolutional kernels such as a ﬁxed window (Collobert et al. 2011; Kalchbrenner and Blunsom 2013).###Kalchbrenner and Blunsom (2013) proposed a novel recurrent network for dialogue act classiﬁcation.",impact-revealing,acknowledge limitations in previous CNN studies
4017,5f44e5bd91e011872f85ed90,9c160a71d3265eedaf7645c39be073c966f10433,A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"ask. Firstly, instead of feeding grayscale images concatenated channel-wise as in the original model, we feed color images. Secondly, our model is significantly deeper, with residual skip connections [15]. Thirdly, inspired by this public implementation2, we use a different loss function: cosine-similarity with binary cross-entropy loss. That is, we compute a dot product between the ReLU-activated vid",other,describing modifications to the original model
569,5e7345fd91e011a051ebf85f,9c6dccf7e17221adc3b02bfc202a0e0e061fe28a,deliberation model based two-pass end-to-end speech recognition,5ce3abaaced107d4c659d34c,Towards End-To-End Speech-To-Text Translation With Two-Pass Decoding,"The deliberation model has been used in state-of-the-art machine translation [17], or generating intermediate representation in speech-to-text translation [18].",impact-revealing,highlighting the application of the deliberation model in machine translation and speech-to-text translation
1517,,e20b341364afda76780c7c31b76a5b761ee91be7,Treatment of small‐cell lung cancer in elderly patients,,,"###The North American Intergroup Trial 0096 demonstrated that hyperfractionated (twice daily) accelerated radiotherapy offers a survival benefit compared with conventional daily fractionation (both radiotherapy arms in combination with 4 cycles of combined cisplatin and etoposide), although at a cost of increased toxicity, especially esophagitis.(19) The issue of increased toxicity has raised concerns about the feasibility of this approach in elderly SCLC patients.",impact-revealing,highlighting the findings and implications of a clinical trial on radiotherapy
2344,5db6c73a3a55acec0731cd68,73a5605ce482bd639078ebbb19baac7b903017e2,A Unified MRC Framework for Named Entity Recognition,5736960c6e3b12023e51f0a0,End-To-End Sequence Labeling Via Bi-Directional Lstm-Cnns-Crf,"model (Chiu and Nichols, 2016; Ma and Hovy, 2016; Devlin et al., 2018) is trained to assign a tagging class to each unit within a sequence of tokens.###English CoNLL 2003 Model P R F BiLSTM-CRF (Ma and Hovy, 2016) - - 91.###We followed data processing protocols in Ma and Hovy (2016).###• BiLSTM-CRF from Ma and Hovy (2016).",other,reporting prior findings and methods in sequence tagging
1612,,a3913d61d5cb4e800e4cdad4dd1771bc859cb033,Online Auctions: A study of Bidder Satisfaction,,,"###Instruments that assess both general UIS (e.g., Ives et al. 1983, Bailey and Pearson 1983), and application-specific UIS, or end-user computing satisfaction EUCS (Doll and Torkzedah 1988), have been widely used by researchers (Gelderman 1998; Igbaria 1990; Somers et al. 2003).",impact-revealing,acknowledge the use of various instruments for assessing user information satisfaction
437,5e09a701df1a9c0c4167614c,3caf34532597683c980134579b156cd0d7db2f40,Universal Adversarial Triggers for Attacking and Analyzing NLP,5b67b46b17c44aac1c861f68,Hotflip: White-Box Adversarial Examples For Text Classification,"For instance, Ebrahimi et al. (2018b) use gradients to attack text classiﬁers.###Ebrahimi et al. (2018a) ﬁnd similar for attacking neural machine translation.###Token Replacement Strategy Our HotFlip-inspired token replacement strategy is based on reviews).###Instead, we build upon HotFlip (Ebrahimi et al., 2018b), a method that approximates the effect of replacing a token using its gradient.",impact-revealing,reporting prior findings on text classifier attacks
235,5f03f3b611dc83056223205d,639206a9a32d91386924f1c94e9760dfb43df72e,Towards Deeper Graph Neural Networks,5e5e193793d709897ce5cdee,Measuring And Relieving The Over-Smoothing Problem For Graph Neural Networks From The Topological View,"Several studies [3, 15] attribute this performance degradation phenomenon to the over-smoothing issue.###Several recent works attribute this performance degradation to the oversmoothing issue [3, 15, 33], which states that representations from different classes become inseparable due to repeated propagation.###Another recent work [3] verify that smoothing is the nature of most typical graph convolutions.###A smoothness regularizer term and adaptive edge optimization are proposed in [3] to relieve the over-smoothing problem.",impact-revealing,highlighting the performance degradation phenomenon in graph convolutions
227,573695fd6e3b12023e511373,e49ff72d420c8d72e62a9353e3abc053445e59bd,Deep convolutional networks on graph-structured data,53e9a841b7602d970318da4e,Spectral Networks and Locally Connected Networks on Graphs.,"Whereas some recognition tasks in non-Euclidean domains, such as those considered in [2] or [12], might have a prior knowledge of the graph structure of the input data, many other real-world applications do not have such knowledge.###When the graph structure of the input is known, [2] introduced a model to generalize ConvNets using low learning complexity similar to that of a ConvNet, and which was demonstrated on simple low-dimensional graphs.###Our main contributions can be summarized as follows: • We extend the ideas from [2] to large-scale classiﬁcation problems, speciﬁcally Imagenet Object Recognition, text categorization and bioinformatics.###In [2] it was suggested to use the same principle in a general graph, by considering a smoothing kernel K ∈ R N × N 0 , such as splines, and searching for spectral multipliers of the form w g = K ˜ w g .###Our work builds upon [2] which introduced spectral networks.###Recently, [2] proposed a generalization of convolutions to graphs via the Graph Laplacian.",impact-revealing,highlighting contributions and extensions of existing models to new applications
3282,599c7982601a182cd2645d4e,88d346374f17189cebef7394ae5d39492443df89,Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution,53e9b195b7602d9703c1dd78,Image super-resolution as sparse representation of raw image patches,"Numerous SR methods learn the LR-HR mapping with image pairs collected from external databases using supervised learning algorithms, such as nearest neighbor [10], manifold embedding [2, 5], kernel ridge regression [19], and sparse representation [37, 38, 39].###Numerous learning algorithms have been applied to learn such a mapping, including dictionary learning [37, 38], local linear regression [30, 36], and random forest [26].",other,reporting various learning algorithms used for SR methods
665,5f50ba4291e01182e69239cb,20454697ea082975db2503a52418efb8f65b8ae6,clocs: camera-lidar object candidates fusion for 3d object detection,58d82fcbd649053542fd640a,Multi-view 3D Object Detection Network for Autonomous Driving,"MV3D [11] and AVOD [12] project the raw point cloud into bird’s eye view (BEV) to form a multi-channel BEV image.###While early and deep fusion have greatest potential to leverage cross modality information, they suffer from sensitivity to data alignment, often involve complicated architectures [11], [12],",impact-revealing,acknowledge existing methods in point cloud processing
1179,,9f3d304588324a0d294239de335d65d0f1096bb6,Competitive Two Team Target Search Game with Communication Symmetry and Asymmetry,,,"###Our world model shares many of the same assumptions as [5].###Using sweep patterns for single agent coverage is studied by [5], while [30] extends these ideas to a single multiagent team searching for a moving and possibly evading target.###Differences include our extensions to higher dimensional spaces, which themselves build on coverage methods that use lawn-mower sweep patterns [5].",impact-revealing,highlighting similarities and differences with existing world models
259,5b67b45517c44aac1c860823,7f01c6fe27f57ee6191b51efa18b9199baf7b82a,Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System,599c7ceb601a182cd27e832c,Collaborative Metric Learning.,"To get the student predicted rank for this document, we apply Weston et al [36]’s sequential sampling, and do it in a parallel manner [16].",impact-revealing,describing the method for obtaining student predicted rank
1529,,54b35c1ccc3efc32ee7706b42f73149b86a56073,Exploration of Racial Enactments in an Interracial Therapeutic Dyad to Foster the Strengthening of Voice and Identity in African American Male Adolescents,,,"###If stereotypical biased views are activated in
the classroom then this understanding is lost and A. becomes reduced to the stereotype of the African American student who goofs off, disrupts others, and does not take himself seriously as a student (Sue et al., 2007; Vaughans, 2016).###These transference and countertransference dynamics can prevent the therapy from dealing with the effects of racial discrimination on the client’s functioning and as Sue et al. (2007) emphasize, can often lead to premature termination.###The typology of racial microaggressions proposed by Sue et al. (2007) is useful in operationalizing racial enactments in cross-racial therapy dyads and assists the white therapist in identifying unconscious bias and making stereotypical thinking more explicitly conscious.###Sue et al. (2007) define racial microinvalidation as often unconscious communications that negate a person of color’s subjective experience, restrict or even exclude their psychological thoughts and feelings, rendering experience-near dialogue impossible.###Racial microassaults are typically conscious and deliberate acts defined by Sue et al. (2007) as more explicit racial verbal and nonverbal attacks intended to be hurtful through the use of name-calling, avoidant behavior, or purposeful discrimination.###Supervisory support is necessary to help identify deeply held and often disavowed racist beliefs and feelings which if unexamined can lead to the plethora of premature terminations that occur in cross-racial therapeutic dyads (Miller & Garran, 2017; Sue et al., 2007).###The conscious belief that we live in a post-racial, colorblind society (Miller & Garran, 2017; Sue et al., 2007) could be understood as motivated by an unconscious defense on the therapist’s part, designed to ward off an emerging awareness of one’s own racist beliefs, attitudes, and assumptions.###The important terms presented by Sue et al. (2007) that capture implicit and explicit racist processes include racial microaggressions, microassaults, microinvalidations, and environmental microaggressions.###Racial microaggressions are defined by Sue et al. (2007) as, everyday interpersonal exchanges that communicate denigrating messages to people of color solely because of their membership in a racial minority group.###Sue et al. (2007) use the concept of implicit bias to underscore a subtle form of racism that has emerged in recent history that can be more damaging and pernicious because of its unconscious, vague and hard to name qualities.###Racial environmental microaggressions occur on a macro level defined by Sue et al. (2007) as the accumulation of racial assaults, insults, and invalidations, which affect the individual person of color in a global way.###This inquiry, in and of itself, is a form of validation (Miller & Garran, 2017; Sue et al., 2007) of the client’s experience and represents a new kind of therapeutic encounter with a white authority figure, that opens up the therapeutic space and creates the possibility for a meaningful dialogue,…###…white professionals place an inordinate emphasis on the student of color’s behavioral problems and due to unexamined unconscious bias may be less optimistic about the student’s capacity to gain self-awareness, exert selfcontrol, and display resilience (Sue et al., 2007; Vaughans, 2016).",impact-revealing,Highlighting the significance of racial microaggressions and their impact in therapeutic settings
26,5d9ed30647c8f76646f7f04c,f160c69c428122e8fa7ba96f220b4ded5f8761f4,ARNOR: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification,57d063b9ac4436735428eaae,Attention-Based Bidirectional Long Short-Term Memory Networks For Relation Classification,"For RC task, various models are recently proposed based on different neural architectures, such as convolutional neural networks (Zeng et al., 2014, 2015) and recurrent neural network (Zhang et al., 2015; Zhou et al., 2016).###In order to capture the key feature words for identifying relations, we apply an attention mechanism over a BiLSTM Encoder, which is ﬁrst introduced in (Zhou et al., 2016) for RC.###BiLSTM+ATT (Zhou et al., 2016) adds an attention mechanism into BiLSTM to capture the most important features for identifying relations.",impact-revealing,acknowledge recent models in relation classification
2622,5c04967517c44a2c7470927f,b9015d4f1e591eba7ea21c3566c919f80f7c2afe,attentive long short-term preference modeling for personalized product search,5390b71120f70186a0f1f063,The impact of images on user clicks in product search,"However, the user clicks only indicate that the user may be interested in an item [12].###Approaches in [12, 21] analyze the impact of images on user clicks.",other,acknowledging the limitations of user clicks as indicators of interest
2443,5eede0b791e0116a23aafe75,150f95f9c73820e0a0fa1546140e9f2bdfd25954,temporal graph networks for deep learning on dynamic graphs,5bdc315017c44a1f58a05aaa,Combining Temporal Aspects of Dynamic Networks with Node2Vec for a more Efficient Dynamic Link Prediction.,"Another line of work encodes DTDGs by ﬁrst performing random walks on an initial snapshot and then modifying the walk behaviour for subsequent snapshots [40, 14, 64, 13, 71].",other,acknowledge existing methods for encoding DTDGs
2785,5c7a561ff56def9798e6a297,abe4e92d6e4868115aef57a4eba97ec4d48ac124,language-adversarial transfer learning for low-resource speech recognition,53e9ad33b7602d9703713f8e,Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers,"The shared layers of the SHL-Model are transferable to an unseen target language [30].###propose to use DNN based SHL-Model [30] to transfer model parameters for unseen languages.###One kind of the SHL-Models only uses the softmax layer to learn language specific features [30].###A good representation for cross-lingual knowledge transfer is one for which an algorithm can not learn to identify the language origin of the input observation [30], [33].",other,discussing the transferability of model layers in cross-lingual settings
926,53e99c2fb7602d97024dc163,fa4cb30bc910cf0d128c9d863ad9f0705b03850c,A Predictive Model for Dynamic Microarchitectural Adaptivity Control,53e9b6dcb7602d970426f0ba,Efficiency trends and limits from comprehensive microarchitectural adaptivity,"PRIOR WORK ON MICROARCHITECTURAL ADAPTIVITY Recently, Lee and Brooks [1] showed that it is possible to significantly increase processor energy efficiency by adapting it as a program is running.###Recently, Lee and Brooks [1] showed that it is possible to significantly increase processor energy efficiency by adapting it as a program is running.###[5], Lee and Brooks [7] and Joseph et al. [6] proposed predictive
modelling (i.e., machine learning) for architectural design space exploration.###Although previous work has quantified the theoretical benefits of high adaptivity [1], predicting and delivering this adaptation is still an open and challenging problem.###It represents the design space of a high-performance out-of-order superscalar processor and is similar to spaces that other researchers have considered [1].",impact-revealing,highlighting the significance of prior work on microarchitectural adaptivity and its challenges
1906,,f1ae7c168a6470ee2cf482bc25cd1364d04e340a,Validating an Approach to Examining Cognitive Engagement Within Online Groups,,,"###Table 3 shows our proposed alignments of cognitive presence (Garrison, Anderson, and Archer 2000, 2001) in Oriogun’s (2003b) SQUAD approach by adopting the TAT model (Fahy 2002) coding categories based on the TAT mapping articulated earlier.###…we adopt the theoretical framework of two recently developed tools, commonly used for analyzing students’ cognitive elements online (Fahy 2002; Garrison, Anderson, and Archer 2000, 2001) at the individual level to validate at the group level the cognitive engagement of groups of students…",impact-revealing,describing the theoretical framework for cognitive engagement analysis
3571,5db92aec47c8f76646216865,146128184cd585b5a1298d3e75e15fb8fbbbd7a5,Improved low-resource Somali speech recognition by semi-supervised acoustic and language model training,5b8c9f5317c44af36f8b779d,Automatic Speech Recognition for Humanitarian Applications in Somali,"In comparison with our previous ASR system [9], the improvement afforded by TDNN-F is clear (rows 1 and 2).###While the Somali news text and Facebook posts were carefully manually cleaned and filtered, the Facebook comments consist of raw, unfiltered text [9].###The language model used in [9] was used as the baseline (LMbase).###The multilingual Somali acoustic model described in [9] uses a hybrid neural network that contains several million parameters.###To start the process, we used our best previouslyavailable Somali acoustic model [9] labelled ASR1 in Figure 2.###In our previous work, we found multilingual training to improve ASR performance substantially [9].",other,highlighting improvements in ASR system performance
1472,,a9df0ec8dafbc56709cf696e6389929c40edc85c,Unsupervised Blind SNR Region Estimation Using Prototype-Based Multi-Stage Deep Neural Network,,,###Another modification made to the DNN architecture was the use of exponential linear unit (ELU) activation layers in place of the popular rectified linear unit (ReLU) layers [9].###The choice of ELU was motivated by its ability to smoothly approximate the activation function by alowing negative values [9].,impact-revealing,describing a modification in DNN architecture and its rationale
2055,,891fb060f4e41830f3e420e22742540088868f1a,Meningeal hemangiopericytomas: long-term outcome and biological behavior.,,,"###Galanis et al [11] and Guthrie et al [13] emphasized the importance of complete removal at the first operation to prolong the time to recurrence and extend survival.###One of the major factors accounting for this wide range is the varying time intervals from diagnosis when the given analysis is performed [4,13,18,19].###The probability increases steadily with time, so long-term follow-up of this tumor is necessary [13,18,19].###Meningeal hemangiopericytoma (M-HPC) is a rare vascular tumor which is most commonly diagnosed in the early fifth decade of life [2,4,12,13,19] and accounts for 1% of all central nervous system (CNS) tumors [2,12].###Guthrie et al [13] demonstrated increasing metastases from time of treatment, reporting a 5-, 10-, and 15-year probability of developing metastasis of 13%, 33%, and 64%, respectively (the average time to extraneural metastasis after the first operation: 99 months).###Guthrie et al [13] also showed that lung and bone were the most common metastatic sites.###Guthrie et al [13] showed the recurrence rate at 10 years after surgery was 76%, and Goellner et al [12] also noted that the recurrence rate at 15 years was 76%.###Considering our results and the current literature, it seems that complete excision favorably affects recurrence and survival, as opposed to incomplete excision [2,9,13,14,15,18].###Many authors have reported that recurrence is a late event [2,4,12,13,15].",impact-revealing,highlighting the significance of complete excision in improving recurrence and survival rates
1633,,1802da1f30f2f2b8105d6a3e8ac4e1c747f1290c,Multiple Pyogenic Liver Abscesses Caused by Eggerthella lenta Treated with Ertapenem: A Case Report,,,"###Because the 16S RNA gene is highly conserved within species and amongst members of a particular genus, this sequencing technology has been instrumental in reclassification and identification of novel genera and species [1].###We report here a case of multiple liver abscesses, in a patient without underlying gastrointestinal disease, caused by Eggerthella lenta that was identified via 16S rRNA sequencing, an increasingly used technology that has improved detection of these less commonly seen anaerobic organisms [1].###However, with the availability of PCR, automated RNA sequencing technology, as well as Clinical and Laboratory Standards Institute (CLSI) DNA target sequencing guidelines [10], the 16S rRNA sequencing has become a more reliable and widely used identification method for difficult microorganisms in a matter of 1-2 days [1].###They are fastidious, slow growing, and phenotypically labor intensive to speciate [9]; sometimes it may take 2 to 6 weeks for identification [1].",impact-revealing,highlighting the significance and reliability of 16S rRNA sequencing in microbial identification
3325,5d3ed25a275ded87f97dea9b,0ddd7c9a1955c76eb831a230ef20a68b662886bb,Mining Algorithm Roadmap in Scientific Publications,5550415945ce0a409eb3a867,INRIASAC: Simple Hypernym Extraction Methods.,"Sent_cooccur: A method similar to co-occurrence method used in hypernymdetection [10].###Evaluated methods can be divided to unsupervised methods including co-occurrence based methods [10], word-similarity based methods [20], and supervised relation extraction methods [40].",other,providing context on hypernym detection methods
940,5c20b1fcda5629702063aff8,482f0510d8f64fe972ec7a05ae5989c25e98ea48,STRAIGHT: Hazardless Processor Architecture Without Register Renaming,573697796e3b12023e6605b7,Exploring the potential of heterogeneous von neumann/dataflow execution models,A hybrid approach has been introduced to increase its viability [32].,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3625,5e3be3c33a55ac29c4ae7e18,6dbdc34000b034b75b8ff70872fc7c35549e273a,Interpretable & Time-Budget-Constrained Contextualization For Re-Ranking,53e9b873b7602d97044403fa,The Greedy Miser: Learning under Test-time Budgets.,"In traditional learning-to-rank the trade-off between effectiveness and efficiency has been thoroughly studied [34, 35, 37, 4].",other,acknowledging prior research on learning-to-rank methods
641,53e99822b7602d9702043cce,7a9b2b108e4b29251c7bf2d641f67359b4fa82eb,software trace cache,53e9a3fbb7602d9702d12b6c,Trace cache: a low latency approach to high bandwidth instruction fetching,"We implement the basic trace cache model described in [13].###The combination of these characteristics makes supplying a high bandwidth of useful instructions a di cult task, even in the presence of aiding devices like a hardware trace cache (HTC) [13, 4].###1 The Core Fetch Unit In order to evaluate the actual increase in instructions per cycle obtained using the Software Trace Cache, we simulated an aggressive sequential fetch unit similar to that described in [13].",impact-revealing,describing the implementation of a basic model
414,5a4aef9e17c44a2190f7a8b8,ff772950f66ac6a57f4201ce1f02f0013ccdc1bb,Receptive Field Block Net for Accurate and Fast Object Detection,599c7949601a182cd262c13a,Deformable Convolutional Networks,"Deformable CNN [4] attempts to adaptively adjust the spatial distribution of RFs according to the scale and shape of the object.###Block architecture: We also compare our RFB to Deformable CNN [4], Inception [34], Dilated Convolution [3], ResNet [13], ResNext [40], and several RFB-like modules with special settings.###Actually, there exist several studies that discuss RFs in CNN, and the most related ones are GoogleNet and its variants [34, 35, 33], Dilated Convolution [3], and Deformable CNN [4].###0 R-FCN w Deformable CNN [4] ResNet-101 trainval 125 ms* 34.###6 40 R-FCN w Deformable CNN [4] ResNet-101 07+12 82.",impact-revealing,acknowledge existing studies on deformable CNN and related architectures
865,5edb32399e795ec54fd81737,6c6c265c6d1de08f03fed0da0604bef5307fbcec,adagcn: adaboosting graph convolutional networks into deep models.,5ce2d032ced107d4c635260c,Predict then Propagate: Graph Neural Networks meet Personalized PageRank,"“(ours)” denotes the results based on our implementation, which are slight lower than numbers above from original literature [17].###In Section 4.2, we employ the same baselines as [17]: V.GCN (vanilla GCN) [16] and GCN with our early stopping, N-GCN (network of GCN) [1], GAT (Graph Attention Networks) [27], BT.FP (bootstrapped feature propagation) [4] and JK (jumping knowledge networks with concatenation) [28].###Moreover, APPNP (Approximate Personalized Propagation of Neural Predictions, [17]) leverages the relationship between GCN and personalized PageRank to derive an improved global propagation scheme.###We also established a strong connection between AdaGCN and the state-of-the-art APPNP [17] method that leverages personalized pagerank to reconstruct graph convolutions in order to use information from a large and adjustable neighborhood.###We refer to the result of baselines from [17] and the implementation of AdaGCN is adapted from APPNP with best setting.###[17], we test all approaches on multiple random splits and initialization to conduct a rigorous study.###We apply the same early stopping mechanism across all the methods as [17] for fair comparison.",impact-revealing,providing context for the implementation and comparison of methods
1989,,0e83d1aa809764d7b6735df7fff012f312825f7e,Balancing multiple ecosystem services in conservation priority setting,,,"###…conservation efforts have had a measure of success in conserving species and habitats (Rodrigues 2006; Wilcove 2008), the sustained high rate of habitat degradation and species loss reveals deficiencies in this approach (Pimm et al. 1995, 2014; Wilcove et al. 1998; MA 2005; Butchart et al. 2010).###Increasing human activities and the resulting changes to the natural environment are bringing many species to the brink of extinction, with extinction rates of up to 1,000 times higher than pre-human levels being observed and future extinction rates poised to increase (Pimm et al. 1995, 2014).",impact-revealing,highlighting the deficiencies in current conservation efforts and the urgent need for improved strategies
3522,5aed147c17c44a4438153a60,5245d411b9dd97ffafe07320981d1282e8f32764,"dCat: dynamic cache management for efficient, performance-sensitive infrastructure-as-a-service",53e99dd4b7602d9702696a12,CQoS: a framework for enabling QoS in shared caches of CMP platforms.,"Fine-grained cache partitioning on chip Some researchers have noticed the performance interference caused by shared LLC and tried to provide a series of chip-level cache allocation mechanisms according to the workloads behaviors [22, 23, 27, 36, 37, 41, 42].",other,highlighting research on cache partitioning mechanisms
1512,,1dd7143e60b32d70d5e69c0b38b6d7f40a1ba536,Risk factors for paravalvular leak after transcatheter aortic valve implantation,,,"###In current clinical practice, two-dimensional TEE is the method most commonly used for annular assessment, but this technique has numerous limitations [5, 14].",impact-revealing,highlighting limitations of the commonly used method in clinical practice
2346,5e2d653a3a55acc8374367eb,f4be875eb05424e7606b03686750a7fb41684579,Privacy-Aware Recommendation with Private-Attribute Protection using Adversarial Learning,57a4e91dac44365e35c9813f,You are Who You Know and How You Behave: Attribute Inference Attacks via   Users' Social Friends and Behaviors,"The third group of works exploits both friend and behavioral information [16, 17, 25].###[16, 17] make a social-behavior-attribute network in which all users’ behavioral and friendship information is integrated in a unified framework.",other,acknowledge existing approaches in social-behavioral networks
1677,,4cf07c2253b37118739c909a70031be5f5ce509d,A Social Identity Model of Pro-Environmental Action (SIMPEA),,,"###It does not consider the human capacity to incorporate collectives or social ingroups in the self, as outlined in the social identity approach (Fielding, Terry, Masser, & Hogg, 2008; Hogg, 2010; Reicher et al., 2010; Tajfel & Turner, 1979; Terry & Hogg, 1996).###Social identity theory (Tajfel & Turner, 1979) and self-categorization theory (Turner et al., 1987), collectively known as the social identity approach (Reicher et al., 2010), provide the ground on which a host of research has developed.###What is largely missing in the models of environmental appraisal and action is the collective dimension or the dynamics of social identity (Reicher et al., 2010; Tajfel & Turner, 1979).###This state of “We” thinking is described in the social identity approach (Reicher, Spears, & Haslam, 2010; Tajfel & Turner, 1979; Turner, Hogg, Oakes, Reicher, & Wetherell, 1987): According to this approach, people define their self either in terms of their idiosyncratic person or as…###…of social identity, such as intergroup comparisons or social identity management, are determined by people’s desire for (a) self-esteem (Rubin & Hewstone, 1998; Tajfel & Turner, 1979), leading to group-based (e.g., ingroup bias) or personal (e.g., leaving the group) responses to low ingroup status.",impact-revealing,highlighting the limitations of existing models in considering social identity
3567,5e3940c73a55ace46ed436d2,845b4941d8c016aa5f8967da2f86d38ef6c18fa3,a survey on knowledge graphs representation acquisition and applications,5d9edc8b47c8f76646043fb7,End-To-End Relation Extraction Using Lstms On Sequences And Tree Structures,"Inspired by the diachronic word embedding, Goel et al. [182] CNN + max pooling position embedding Multi CNN [137] Multi-window convolution + max pooling position embedding PCNN [138] CNN + piecewise max pooling position embedding MIMLCNN [139] CNN + piecewise and cross-sentence max pooling position embedding Ye et al. [140] CNN/PCNN + pairwise ranking position embedding, class ties Zeng et al. [141] CNN + max pooling position embedding, relation path RNNs SDP-LSTM [142] Multichannel LSTM + dropout dependency tree, POS, GR, hypernyms LSTM-RNN [143] Bi-LSTM + Bi-TreeLSTM POS, dependency tree BRCNN [144] Two-channel LSTM + CNN + max pooling dependency tree, POS, NER Attention Attention-CNN [145] CNN + word-level attention + max pooling POS, position embedding Lin et al. [146] CNN/PCNN + selective attention + max pooling position embedding Att-BLSTM [80] Bi-LSTM + word-level attention position indicator APCNN [147] PCNN + sentence-level attention entity descriptions HATT [148] CNN/PCNN + hierarchical attention position embedding, relation hierarchy GCNs C-GCN [150] LSTM + GCN + path-centric pruning dependency tree KATT [152] Pre-training + GCN + CNN + attention position embedding, relation hierarchy AGGCN [151] GCN + multi-head attention + dense layers dependency tree Adversarial Wu et al. [153] AT + PCNN/RNN + selective attention indicator encoding DSGAN [154] GAN + PCNN/CNN + attention position embedding RL Qin et al. [155] Policy gradient + CNN + performance change reward position embedding Zeng et al. [156] Policy gradient + CNN + +1/-1 bag-result reward position embedding Feng et al. [157] Policy gradient + CNN + predictive probability reward position embedding HRL [158] Hierarchical policy learning + Bi-LSTM + MLP relation indicator took an entity and timestamp as the input of entity embedding function to preserve the temporal-aware characteristics of entities at any time point.###RNNs SDP-LSTM [114] Multichannel LSTM + dropout dependency tree, POS, GR, hypernyms LSTM-RNN [115] Bi-LSTM + Bi-TreeLSTM POS, dependency tree BRCNN [116] Two-channel LSTM + CNN + max pooling dependency tree, POS, NER###[115] stacks sequential and tree-structure LSTMs based on dependency tree.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1052,,6273f4ac2088cf1ae48c2cd023b2a54590af7046,Scalable Network Management with Merlin,,,"###The Merlin policy language is inspired by previous work on streaming query languages [1, 4] and is designed to strike a balance between expressiveness and simplicity.",impact-revealing,highlighting the inspiration behind the Merlin policy language
3662,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e9a877b7602d97031c5c85,Integrating Opponent Models with Monte-Carlo Tree Search in Poker.,Nijssen and Winands [155] also describe a multi-player version of their MCTS-Solver (4.5).,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2978,5cf48a34da56291d58296d51,3c64b7a74c749d43b1a4b96dd1a00620ba613ee0,Representation Learning for Attributed Multiplex Heterogeneous Network,5cd7fa07ced107d4c65bf2e7,NetSMF: Large-Scale Network Embedding as Sparse Matrix Factorization,"NetMF [29] gives a theoretical analysis of equivalence for the different network embedding algorithms, and later NetSMF [28] gives a scalable solution via sparsification.###Single Single / LINE [35] node2vec [10] NetMF [29] NetSMF [28]",other,reporting theoretical analysis and scalable solutions in network embedding
2800,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",573696586e3b12023e564f44,Translation on Graphs: An Isometric Shift Operator,"To overcome challenges associated to existing shift operators, one solution, first proposed by [129], is to introduce alternative graph shift operators (see also [52]) or localization operators that have both a spectral interpretation and vertex domain localization [130], [131].###Other choices have been proposed, including the Laplacians [3], or variations of these matrices [52], [53].###Subsequently, authors have proposed other shifts obtained from the adjacency matrix of the graph [52], [53] that attempt to preserve isometry of the shift, but in some cases lose the locality of the adjacency",other,acknowledge existing solutions and alternatives in graph shift operators
2320,5db9295f47c8f766461f5135,2c1006c856fefdbd6cd710e840e57153f2d6cd04,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,5c8c6f864895d9cbc603ee08,One pixel attack for fooling deep neural networks.,"Many variants of attacks have been developed later [41, 8, 54, 62, 7, 6].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1365,,477494763b3571684fe67f11cc180239ded68f33,"Cyanophycin‐degrading bacteria in digestive tracts of mammals, birds and fish and consequences for possible applications of cyanophycin and its dipeptides in nutrition and therapy",,,"###474 Journal compilation ª 2009 The Society for Applied Microbiology, Journal of Applied Microbiology 107 (2009) 474–484 ª 2009 The Authors
arginine and aspartate arranged in the form of a poly(aspartic acid) (PAA) backbone, with arginine moieties linked to the b-carboxyl group of each aspartic acid by its a-amino group (Simon and Weathers 1976).###PAA is also used as a substitute of nonbiodegradable poly(acrylic acid) for which many technical applications are described.###Previous studies on CGP were motivated by CGP as a potential source for biodegradable PAA (Mooibroek et al. 2007; Scott et al. 2007).",impact-revealing,highlighting the potential of CGP as a source for biodegradable PAA
3430,5c7a561ff56def9798e6a297,abe4e92d6e4868115aef57a4eba97ec4d48ac124,language-adversarial transfer learning for low-resource speech recognition,5a73cbc317c44a0b3035e6bd,Multitask Learning for Phone Recognition of Underresourced Languages Using Mismatched Transcription.,"The transfer learning methods can be roughly classiﬁed into two categories: transferring bottleneck features [17], [23]–[26] and transferring model parameters [10], [11], [27].",other,providing context on transfer learning methods
2441,5aed147c17c44a4438153ea5,665c0dde22c2f8598869d690d59c9b6d84b07c01,domino temporal data prefetcher,53e99d51b7602d970260624e,Speculative Precomputation: Long-Range Prefetching Of Delinquent Loads,"Thread-based prefetching techniques [12], [45], [46], [47], [48], [49], [50], [51] exploit idle thread contexts to execute threads that prefetch for the main program thread.",other,providing context on thread-based prefetching techniques
1310,,3579ed9fe85835fc10eb58a5d419c0e63d384749,Dependability and Computer Engineering : Concepts for Software-Intensive Systems,,,"###Dependent threat diagrams are inspired by assumption-guarantee reasoning, which has been suggested as a means to facilitate modular system development (Jones, 1981; Misra and Chandy, 1981; Abadi and Lamport, 1995).",impact-revealing,providing context for dependent threat diagrams
3410,5fef22c691e0113b265a0289,b5b006dc558cb7fbd532d67e989173b536e8ac80,MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers,53e9a6dfb7602d97030135a5,The Third PASCAL Recognizing Textual Entailment Challenge.,"GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classiﬁcation tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017) and QQP), and four inference tasks (MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007 et al., 2009) and WNLI (Levesque et al., 2012)).###, 2016), RTE (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) and WNLI (Levesque et al.",other,providing context about the GLUE benchmark and its tasks
1687,,79236a1ab386ee3419c32ab4b9daf2c823a24e82,Identity- and contact-related determinants of reciprocal intergroup relations in ethno-culturally diverse societies,,,"###Building on the frameworks of the social identity theory (SIT; Tajfel & Turner, 1979), the theory of acculturation (Berry, 1997) and Allport’s (1954) contact hypothesis, with a particular focus on the concepts of cultural discordance (Piontkowski, Rohmann, & Florack, 2002), the secondary transfer…###Another milestone contributing to the intensification of research on intergroup relations was the development of SIT (Tajfel & Turner, 1979), which changed the approach to examining outgroup attitudes.###Such clear ingroup-outgroup distinction, according to SIT (Tajfel & Turner, 1979), fosters more negative attitudes towards the outgroup.###Tutkimuksen teoreettinen viitekehys pohjautuu sosiaalisen identiteetin teoriaan (SIT; Tajfel & Turner, 1979), akkulturaatioteoriaan (Berry, 1997) sekä kontaktihypoteesiin (Allport, 1954).###This theoretical framework had dominated the research on intergroup relations until the late 1970s, when Tajfel and Turner (1979) introduced SIT, revolutionising the approach to intergroup attitudes and prejudice.###The theoretical framework builds on the social identity theory (SIT; Tajfel & Turner, 1979), the theory of acculturation (Berry, 1997) and Allport’s (1954) contact hypothesis, with a particular focus on the concepts of cultural discordance (Piontkowski, Rohmann, & Florack, 2002), the secondary…###Such conceptualisation of intergroup differences by the multicultural ideology is in opposition to how these differences are viewed in, for instance, SIT (Tajfel & Turner, 1979), which emphasises their fundamental role in the development of intergroup bias.",impact-revealing,highlighting the significance of social identity theory in intergroup relations research
1595,,cb9a015e6ac4454c318b0af4a33b11dba5bb8188,Assessment of myocardial regional strain and strain rate by tissue tracking in B-mode echocardiograms.,,,"###Elastography is commonly used as the method to describe the regional cardiac function, by assessing the regional mechanical properties (Konofagou et al. 2002).",impact-revealing,providing context on the use of elastography in cardiac function assessment
732,5ee7495191e01198a507f7ae,09bda461aa4911d0513e8e46dd39a4113947e450,Ansor : Generating High-Performance Tensor Programs for Deep Learning,5bdc316717c44a1f58a071ff,TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.,"It cannot outperform TVM on operators like conv2d and matmul [11,48].###The common optimizations at graph level include layout optimizations [32], operator fusion [11, 38, 60], constant folding [42], auto-batching [33], automatic generation of graph substitution [29] and so forth.###Given the importance of DNNs’ performance, researchers and industry practitioners have turned to search-based compilation [2, 11, 32, 49, 59] for automated generation of tensor programs, i.###TVM [11] utilizes a similar scheduling language and includes a template-guided search framework AutoTVM [12].###, TVM [11], Halide [41], Tensor Comprehensions [49]).###Typically, the compiler partitions the large computational graph of a DNN into several small subgraphs [11, 42].###TensorComprehensions can search for GPU code automatically, but it is not yet meant to be used for compute-bounded problems [11].",impact-revealing,discussing common optimizations in deep neural network compilation
3444,5ce2d0b2ced107d4c63a8bff,f02420c1d814ed6bdeae1a1c0923ee9d8eeec8dd,Hierarchical Reinforcement Learning for Course Recommendation in MOOCs,5b3d98cc17c44a510f801cd1,Joint Training Of Candidate Extraction And Answer Selection For Reading Comprehension,"…algorithm to solve many kinds of problems, such as relation classification (Feng et al. 2018), text classification (Zhang, Huang, and Zhao 2018), information extraction (Narasimhan, Yala, and Barzilay 2016), question answering (Wang et al. 2018b) and treatment recommendation (Wang et al. 2018a).###2018), text classification (Zhang, Huang, and Zhao 2018), information extraction (Narasimhan, Yala, and Barzilay 2016), question answering (Wang et al. 2018b) and treatment recommendation (Wang et al.",other,acknowledge various applications of the algorithm
2268,5c234870da562935fc1d4da6,94bd59e507ba8496b36605be0f6740e5731e91d5,CounterMiner: Mining Big Performance Data from Hardware Counters,558c7bb484ae6766fdf3585f,Non-determinism and overcount on modern hardware performance counter implementations,"Error Reduction The measurement errors caused by MLPX have been observed for nearly two decades [29]–[31], [33], [34], [38], [54]–[56] and several approaches were proposed to reduce them.",other,reporting on the historical observation of measurement errors and approaches to mitigate them
550,5aed14d117c44a4438158af2,30f86d38f0660af5ea2e16d996434c72eee8c5ee,espnet: end-to-end speech processing toolkit,5550443b45ce0a409eb4c3b9,Towards end-to-end speech recognition with recurrent neural networks,"ESPnet fully utilizes benefits of two major end-to-end ASR implementations based on both connectionist temporal classification (CTC) [10, 11, 12] and attention-based encoder-decoder",impact-revealing,reporting on the benefits of ASR implementations in ESPnet
1202,,1e3e05c16a07e1f87ea49a011735014deb6256d3,Equivalence of Linear Complementarity Problems: Theory and Application to Nonsmooth Bifurcations,,,"###It uses a notion of regularity that is closely related to that proposed in [47], and it is inspired by the notion of smooth equilibrium bifurcation used in [33].",impact-revealing,drawing inspiration from existing concepts in regularity and equilibrium bifurcation
3058,5e09a801df1a9c0c41680233,1438d8c68b4495947fd5de001b87e6ef5a263a3e,Learning to Jointly Generate and Separate Reflections,5a260c8417c44a4ba8a31302,A Generic Deep Architecture for Single Image Reflection Removal and   Image Smoothing,"The comparison methods include Wan18 [24], Zhang18 [29], CycleGAN [30], and FY17 [5].###Existing method [5] can be treated as a special instance of our method when the generator is sim-pliﬁed as a linear function.###In contrast with previous methods [24, 13, 5], that heavily rely on the simpliﬁed model in Equation 1 and regard the image generation and separation as two independent stages, the proposed model leverages the mutual beneﬁts of the image generation and separation in a joint learning manner to…###Instead of the one-to-one mapping in previous methods, our generator learn the mapping as G : ( B , R ) → M , where the non-linear mappings can produces more realistic reﬂection appearances (ﬁrst to third columns in Figure 4 1 ) than previous linear functions [5, 26, 17, 1] with ﬁxed coefﬁcients.###Recently, deep learning based reﬂection removal meth-ods [24, 5] with better generalization ability have been proposed to address the limitations arising from the hand-crafted image priors.###In contrast to the conventional pipelines [5, 29, 24] that treat the image generation and separation as two independent stages, we come up with a uniﬁed model, such that the mutual effects between two stages can beneﬁt the robustness.###Here, α and β are the mixing coefﬁcients [5, 27, 26].###Moreover, we introduce the gradient constraints [5, 26] to make the model learning more effective, in which the edge map estimation is ele-gantly dealt with as an auxiliary task via a multi-task learning structure.###Instead of one-to-one framework in previous methods [5, 29], our separator learns the mapping function as S : M → ( B , R , E ) , where the multi-task learning framework models the image separation process in a more reasonable way, especially the auxiliary task of edge map estimation, that provides…###The framework introduced in [5] exploited the edge information when training the whole network to better preserve the image details.",other,describing the proposed model's advantages over previous methods
379,573696106e3b12023e5227c8,f5a7da72496e2ca8edcd9f9123773012c010cfc6,Neural Architectures for Named Entity Recognition,573695fe6e3b12023e511e25,Bidirectional LSTM-CRF Models for Sequence Tagging,"More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features.###This architecture is similar to the ones presented by Collobert et al. (2011) and Huang et al. (2015).",impact-revealing,acknowledging similarities in model architectures
520,5d8b3b1d3a55acc418bda58c,40c6fadf7b08fbcd5aedfc8bebf99ccbdb52d945,Portuguese Named Entity Recognition using BERT-CRF,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Following the approach of Devlin et al. (2018) on the SQuAD dataset, examples longer than S tokens are broken into spans of length up to S using a stride of D tokens.###Following the approach of Devlin et al. (2018) on the SQuAD dataset, examples larger than S tokens are broken into spans of length up to S using a stride of D tokens.###Examples of such models are ELMo (Peters et al., 2018), OpenAI GPT (Rad-ford et al., 2018), BERT (Devlin et al., 2018), XL-Net (Yang et al., 2019), RoBERTa (Liu et al., 2019), Albert (Lan et al., 2019) and T5 (Raffel et al., 2019).###Following Devlin et al. (2018), we compute predictions and losses only for the ﬁrst sub-token of each token.###We train Portuguese BERT models for the two model sizes deﬁned in Devlin et al. (2018): BERT Base and BERT Large.###Instead of using only the last hidden representation layer of BERT, we sum the last 4 layers, following Devlin et al. (2018). The resulting architecture resembles the LSTM-CRF model Lample et al.###As described in Devlin et al. (2018), WordPiece tokenization requires predictions and losses to be computed only for the first sub-token of each to-###Since it is a bigger model with longer training time, we follow the instructions of Devlin et al. (2018) and use sequences of 128 tokens in batches of size 256 for the ﬁrst 900,000 steps and then sequences of 512 tokens and batch size 128 for the last 100,000 steps.###We use the customized Adam optimizer of Devlin et al. (2018).###Instead of using only the last hidden representation layer of BERT, we sum the last 4 layers, following Devlin et al. (2018). The resulting architecture resembles the LSTM-CRF model Lample et al. (2016) replacing its embedding techniques by BERT.###We use a batch of size 16 and the customized Adam optimizer of Devlin et al. (2018) with weight decay of 0 .",impact-revealing,providing context on the methodology used in the study
2423,53e99845b7602d9702072224,00ae1c36f7eb925873322e548073a64d6795787e,multiple stream prediction,53e9bb8cb7602d97047d477d,A low-complexity fetch architecture for high-performance superscalar processors,"The stream fetch engine [11,16] model is shown in Figure 2.###Our approach to achieve high fetch bandwidth, while maintaining the complexity under control, is the stream fetch engine [11,16].###In addition, data are shown for the original single-stream predictor, described in [11,16], and a 2-stream multiple predictor.###The next stream predictor [11,16], which is shown in Figure 4.###All hysteresis counters behave like the counter used by the original stream predictor to decide whether a stream should be replaced from the prediction table [16].###To avoid this increase in the fetch engine complexity, we propose to use long instruction streams [11,16] as basic prediction unit, which makes it possible to hide the prediction table access delay.###Since previous work [16] has shown that code layout optimizations are able to enlarge instruction streams, we present data for both a baseline and an optimized code layout.###Code layout optimizations have a beneficial effect on the length of instruction streams [16].###This makes it possible for the stream fetch engine to provide high fetch bandwidth while requiring low implementation cost [11,16].###A stream prediction contains enough instructions to feed the execution engine during multiple cycles [16].",other,describing the stream fetch engine model and its optimizations
531,5bdc31b417c44a1f58a0ba6c,62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9,How Powerful are Graph Neural Networks,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"Recently, there has been a surge of interest in Graph Neural Network (GNN) approaches for representation learning of graphs (Li et al., 2016; Hamilton et al., 2017a; Kipf & Welling, 2017; Velickovic et al., 2018; Xu et al., 2018).###There are other non-standard and complicated neighbor aggregation schemes that we do not cover in this paper, e.g. , Graph Attention Networks (Velickovic et al., 2018) and LSTM pooling (Hamilton et al., 2017a; Murphy et al., 2018).###The COMBINE step could be a concatenation followed by a linear mapping W · h as in GraphSAGE.###What happens if we replace the sum in h ( X ) = x ∈ X f ( x ) with mean or max-pooling as in GCN and GraphSAGE?###Learning with graph structured data, such as molecules, social, biological, and ﬁnancial networks, requires effective representation of their graph structure (Hamilton et al., 2017b).###…and graph-level pooling schemes have been proposed (Scarselli et al., 2009b; Battaglia et al., 2016; Defferrard et al., 2016; Duvenaud et al., 2015; Hamilton et al., 2017a; Kearnes et al., 2016; Kipf & Welling, 2017; Li et al., 2016; Velickovic et al., 2018; Santoro et al., 2017; Xu et al., 2018;…###Next we study GNNs that do not satisfy the conditions in Theorem 3, including GCN (Kipf & Welling, 2017) and GraphSAGE (Hamilton et al., 2017a).###In the pooling variant of GraphSAGE (Hamilton et al., 2017a), AGGREGATE has been formulated as Figure 1: Subtree structures at the blue nodes in Weisfeiler-Lehman graph isomorphism test.###3) We identify graph structures that cannot be distinguished by popular GNN variants, such as GCN (Kipf & Welling, 2017) and GraphSAGE (Hamilton et al., 2017a), and we precisely characterize the kinds of graph structures such GNN-based models can capture.",impact-revealing,highlighting the growing interest in Graph Neural Network approaches for representation learning
608,59ae3be32bbe271c4c71b8e0,2b166bb181428aa3e6837d2081500208af7c70ed,Weak Memory Models: Balancing Definitional Simplicity and Implementation Flexibility,53e9b2b1b7602d9703d59dce,Weak ordering—a new definition,Release Consistency (RC) are often mixed with the concept of “SC for data-race-free (DRF) programs” [28].,impact-revealing,providing context on the concept of Release Consistency
2932,5c0f87a5da562944ac95a190,4d157bec76125b1121a999ab2b79730540ab7aad,"Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases",53e9bc3bb7602d97048b28e2,Network pharmacology: the next paradigm in drug discovery,"However, it is known that most of the bioactive compounds act on multiple targets (which causes these off target effects); in fact, the cases where a compound interacts with only a one-target protein are considered as exceptional [25, 26].",other,highlighting the complexity of bioactive compounds and their interactions
1091,,d8315d15257b42f7a1cd8b3c79ee2f67783680e6,A Symbolic Representation and Classification of Fruits,,,"###, [13] proposed a novel technique for classification of fruit images by making use of multi-class kernel support vector machine and after extraction of features, principal component analysis is adopted for decreasing the dimensions of feature space.",impact-revealing,reporting a novel technique for fruit image classification
1285,,b8093f36aa4bdc597c6bd57163f0015ccc2502f8,Classification of single‐voxel 1H spectra of childhood cerebellar tumors using lcmodel and whole tissue representations,,,"###We have previously presented a novel method for the classification of (1)H MRS brain tumor spectra using the widely used analysis tool LCModel (14,15) to fit in vivo adult tumor spectra with a linear combination of mean brain tumor spectra (16).",impact-revealing,reporting a previously presented method for brain tumor classification
556,5c04967517c44a2c747089d5,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,leveraging weakly supervised data to improve end-to-end speech-to-text translation,5a9cb65d17c44a376ffb826e,End-to-End Automatic Speech Translation of Audiobooks,"Recently explored techniques to mitigate this issue include multi-task learning [9, 10] and pre-trained components [11] in order to utilize weakly supervised data, i.###We then adopt pretraining and multi-task learning as proposed in previous literature [9, 10, 11, 15] in order to improve its performance.###[11] conducts experiments on a larger 236 hour English-to-French dataset and pre-trains the encoder and decoder prior to multi-task learning, which further improves performance.",impact-revealing,highlighting recent techniques to improve performance in weakly supervised data
822,5e5794b791e011545375102b,38bccac4f05585ec595c7bb7c0e747561dcad886,DNN-Chip Predictor: An Analytical Performance Predictor for DNN Accelerators with Various Dataflows and Hardware Architectures,5cc2e401ced107d4c67aceba,Timeloop: A Systematic Approach to DNN Accelerator Evaluation,Timeloop [21] and Eyeriss [22] use for and parallel-for to describe the temporal and spatial mapping of DNN accelerators.,impact-revealing,reporting existing methods for DNN accelerators
3219,5edcbb9a91e0110f1d6db1c2,c99e2877d4b1f171746a3aa73d5fd5fd680c982d,Hierarchical Bipartite Graph Neural Networks: Towards Large-Scale E-commerce Applications,5b67b45517c44aac1c8607fe,TaxoGen: Unsupervised Topic Taxonomy Construction by Adaptive Term Embedding and Clustering.,"To make the topic more interpretive, we follow the similar strategy described in [37] to ﬁnd the most representative query as the description for a speciﬁc topic.",other,describing a method for topic interpretation
3849,5d8dded23a55acd1b54967df,1ecbaf7a2cd3059e07261e72a1195a7c70b3d664,GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning,53e99822b7602d9702042f69,Semi-Supervised Learning,"…learning algorithms (Laine & Aila, 2016; Miyato et al., 2018; Tarvainen & Valpola, 2017; Verma et al., 2019b) are based on the cluster assumption (Chapelle et al., 2010), which posits that the class boundary should pass through the low-density regions of the marginal data distribution.",other,acknowledge foundational concepts in learning algorithms
2278,58d82fc8d649053542fd5862,2a94c84383ee3de5e6211d43d16e7de387f68878,Feature Pyramid Networks For Object Detection,57a4e91aac44365e35c98023,Training Region-Based Object Detectors with Online Hard Example Mining,"Our pyramid representation greatly improves RPN’s robustness to object scale variation.###For these reasons, Fast and Faster R-CNN [11, 29] opt to not use fea-turized image pyramids under default settings.",other,highlighting the improvement in robustness of RPN with pyramid representation
3409,5bdc31b417c44a1f58a0b240,6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da,Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks,5c8b107c4895d9cbc6493324,Deriving Neural Architectures from Sequence and Graph Kernels.,"…theoretical work on GNNs—with the notable exceptions of Li et al.’s (Li, Han, and Wu 2018) work connecting GNNs to a special form Laplacian smoothing and Lei et al.’s (Lei et al. 2017) work showing that the feature maps generated by GNNs lie in the same Hilbert space as some popular graph kernels.###Up to this point (and despite their empirical success) there has been very little theoretical work on GNNs—with the notable exceptions of Li et al.’s (Li, Han, and Wu 2018) work connecting GNNs to a special form Laplacian smoothing and Lei et al.’s (Lei et al. 2017) work showing that the feature maps generated by GNNs lie in the same Hilbert space as some popular graph kernels.###’s (Lei et al. 2017) work showing that the feature maps generated by GNNs lie in the same Hilbert space as some popular graph kernels.",other,highlighting the lack of theoretical work on GNNs despite their empirical success
616,5f33bcf791e011861cfa0fd7,21066f476b04b602b40e73f494b577f7eb660e75,Woodpecker-DL: Accelerating Deep Neural Networks via Hardware-Aware Multifaceted Optimizations,5bdc316717c44a1f58a071ff,TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.,"Typical works include XLA [9] (applicable to training as well), TVM [10], Glow [11], Tensor Comprehensions [12], nGraph [13], OpenVINO [14], and TensorRT [15].###TVM [10] is an end-to-end compiler framework with Halide at the core, which ﬁrst optimizes a computational graph, then converts the optimized graph into intermediate representations and ﬁnally compiles to executable codes on a speciﬁc target device.",impact-revealing,acknowledge existing frameworks and tools in computational optimization
20,5f7d8a8391e011346ad27d2b,3bb1e24eb3429f807397833105d1e137d9927767,SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup,5a260c8417c44a4ba8a31511,mixup: Beyond Empirical Risk Minimization.,"We propose SeqMix, a data augmentation method for generating sub-sequences along with their labels based on mixup (Zhang et al., 2018).###Interpolation-based Regularizations Mixup implements interpolation in the input space to regularize models (Zhang et al., 2018).###Mixup (Zhang et al., 2018) is a data augmentation method that implements linear interpolation in the input space.",impact-revealing,describing a data augmentation method and its implementation
2801,5eede0b091e0116a23aafcd3,9898b3d600b3881bde162b7e4b668a3c063cba10,sequential graph convolutional network for active learning,53e9a508b7602d9702e2bcf5,Rectified Linear Units Improve Restricted Boltzmann Machines,A rectified linear unit activation [24] is applied after the first layer to maximise feature contribution.,other,providing context for activation function application
436,5e09a701df1a9c0c4167614c,3caf34532597683c980134579b156cd0d7db2f40,Universal Adversarial Triggers for Attacking and Analyzing NLP,5a73cbcc17c44a0b3035f602,Adversarial Patch.,"The adversarial threat is higher if an attack is universal : using the exact same attack for any input (Moosavi-Dezfooli et al., 2017; Brown et al., 2017).",impact-revealing,highlighting the significance of universal adversarial attacks
1756,,90b91f274febb634e3e07c8bb276f238df5dabde,An Energy-Efficient Deep Neural Network Accelerator Design,,,"###In addition, sparsity controller is commonly used for efficient computation, and compressions are adopted for energy-efficient memory access such as run-length compression, Huffman compression, and compressed sparse format [7], [8], [24].###DNN ASICs [8]-[10] were proposed to efficiently accelerate the massive computation.###First, many processors [7], [8], [17], [24] employ the zero-skipping technique.###Reference [25] and [26] utilized systolic array architecture and [8]-[10] designed SIMD arrays with dedicated data scheduler and controller to maximize data reuse.###A CNN ASIC [8] contained 12 × 14 PE arrays for CONV acceleration with Network-on-Chip for data broadcasting.###Reference [8] detected zero-activation and prevented from weight read and CONV operations by gating.",impact-revealing,describing methods for efficient computation in DNNs
3003,5db929b747c8f766461fa94f,2a6d160b529272964ce1a6707adf52f3d6ba4861,Diffusion Improves Graph Learning,57a4e91aac44365e35c97ad6,Block Models And Personalized Pagerank,[35] and Ragain [64] showed that PPR is optimal in recovering the SBM and DCSBM clusters in the space of landing probabilities under the mean field assumption.,other,reporting prior findings on PPR and clustering
2874,5b67b4b917c44aac1c867dbc,d18b48f77eb5c517a6d2c1fa434d2952a1b0a825,hierarchical graph representation learning with differentiable pooling,573696ce6e3b12023e5ce95a,Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift,Batch normalization [18] is applied after every layer of GRAPHSAGE.,other,providing context on the application of batch normalization in GRAPHSAGE
638,5c234870da562935fc1d4db3,0b6a531754e67379518a946f3a3cf685f59358cc,CritICs Critiquing Criticality in Mobile Apps,53e9a3abb7602d9702cbe7eb,Focusing processor policies via critical-path prediction,"These proposals identify high-fanout loads to mark them as critical to issue prefetch [18] and prioritize the critical instructions for ALU resource allocations [32], [33].###Without loss in generality, we have taken two representative, well-studied and well-proven criticality optimizations in prioritizing two important resources - one for memory which issues prefetches for critical loads [18] and another for ALU resources in instruction scheduling [4], [32], [33].###…for those instructions, we additionally consider the following conﬁgurations: • BackendPrio [33]: This platform implements the prioritization hardware for the back-end resources proposed in [33], using the tracking hardware proposed in [32], which requires 1.5KB SRAM for maintaining the tokens.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3809,5eede0b091e0116a23aafbd3,91fb815361fdbf80ff15ce4d783a41846bd99232,GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training,5b1642388fbcbf6e5a9b5488,An End-to-End Deep Learning Architecture for Graph Classification.,"ication layer to predict the label of the graph. We compare GCC with several recent developed graph classification models, including Deep Graph Kernel (DGK) [61], graph2vec [33], InfoGraph [46], DGCNN[67]andGIN[60].Amongthesebaselines,DGK,graph2vec 4 In node classification tasks, we follow Struc2vec to use logistic regression. For graph classification tasks, we follow DGK [61] and GIN [60] to use SVM ###[12], graph2vec[33] and InfoGraph [46]. GCC with freezing setting belongs to this category. In the second category, the models are optimized in an end-to-end supervised manner. Examples include DGCNN [67] and GIN [60]. GCC with the full fine-tuning setting belongs to this category. For a fair comparison, we fix the representation dimension of all models to be 64 except graph2vec and InfoGraph5. The de",other,comparing graph classification models
152,5d9edbfc47c8f7664602eba5,dde65325dc7600d02983a76bd54693f0050946a4,integrating both visual and audio cues for enhanced video caption,58d82fced649053542fd68c9,Multimodal Memory Modelling for Video Captioning.,"We compare our feature fusion models with several video caption models, including M(3) (Wang et al. 2016), Visual model (our basic video caption model), Audio model (our basic video caption model with audio features instead of visual features) respectively.###2015) and M(3) (multimodal memory modeling) model (Wang et al. 2016), which is shown in Figure 1.###Evaluation of the Generated Sentences of Dynamic Multimodal Feature Fusion Framework Figure 6 presents some sentences generated by GA (models with only generated audio features), M(3) (Wang et al. 2016), V-ShaWei-GA models and human-annotated ground truth based on the test set of MSVD.###We compare our feature fusion models with several video caption models, including M 3 (Wang et al. 2016), Visual model (our basic video caption model), Audio model (our basic video caption model with audio features instead of visual features) respectively.###Our basic video caption framework is extended from S2VT (sequence to sequence: video to text) model (Venu-gopalan et al. 2015) and M 3 (multimodal memory modeling) model (Wang et al. 2016), which is shown in Figure 1.###Evaluation of the Generated Sentences of Dynamic Multimodal Feature Fusion Framework Figure 6 presents some sentences generated by GA (models with only generated audio features), M 3 (Wang et al. 2016), V-ShaWei-GA models and human-annotated ground truth based on the test set of MSVD.",impact-revealing,acknowledge existing video caption models for comparison
37,5f75aa6a9fced0a24b64599d,69fb130409d48478fab0c4545dc2ff102a2630ef,The forward slice core microarchitecture,53e999ffb7602d970224d68c,Complexity-effective superscalar processors,Palacharla et al. [13] propose the complexity-effective superscalar processor (CESP) architecture which steers chains of dependent instructions into generic in-order queues.###Palacharla et al. [13] pro-pose the complexity-effective superscalar processors (CESP) architecture which steers chains of dependent instructions to in-order queues.,impact-revealing,reporting prior findings on processor architecture
3137,5550424345ce0a409eb411c6,334da45ee9a6325cd8ca0142586b820f88de6a77,crema-d: crowd-sourced emotional multimodal actors dataset,55a387b165ce5cd7b3adc75c,Cross-Cultural Recognition Of Basic Emotions Through Nonverbal Emotional Vocalizations,"Parallel research exists in vocal expression [4], [5].",other,acknowledge related research in vocal expression
1433,,b15fa9e57fb791899154a0f6c321eb703f1c0b09,Can we generate shellcodes via natural language? An empirical study,,,"###However, since we are interested in the specific problem of code generation, we focus on NMT that has shown superior performance on public benchmarks [9], and that it is widely recognized as the premier method for the translation of different languages [83].###In general, NMT translates between different languages (including natural and programming languages), using Natural Language Processing (NLP) and Deep Learning (DL) techniques [31,7,83,9], in order to learn the typical idioms of a target programming language from datasets of annotated programs.",impact-revealing,highlighting the significance and application of NMT in code generation
1345,,9c8244a5c3a14bd836c16c9e03634bdc2a39a3ba,Practical Image Quality Metric Applied to Image Coding,,,"###In this study, five typical metrics are compared whose codes are publicly available: 1) MSE and DCTune [23], [30], which are commonly used for perceptual coding and watermarking; 2) MSSIM (the multi-scale version of SSIM [18]) and VIF [31], which exhibit an outstanding performance in previous study [32]; 3) PSNR-HVS-M [33] and VSNR [24], which are designed based on the databases [4] and [5], respectively, and performs quite well on them.###From the view point of practicality, we compare MSE, DCTune, MSSIM, PSNR-HVS-M, VSNR, VIF, and the proposed metrics on the distortion subsets below.###But if MSSIM or VIF guides the RD optimization, optimizing each parameter needs to perform inverse DCT (IDCT) and compute the entire image distortion, and such operations needs to be repeated by times for coding an image.###are publicly available: 1) MSE and DCTune [23], [30], which are commonly used for perceptual coding and watermarking; 2) MSSIM (the multi-scale version of SSIM [18]) and VIF [31], which exhibit an outstanding performance in previous study###To the best of our knowledge, no literature has discussed how to optimize MSSIM and VIF.###The metrics that measure the signal difference between the reference and the distorted image often cannot tolerate such distortions VIF, which evaluates the statistical difference of image variance, can tackle “contrast change”.",impact-revealing,comparing typical metrics for image distortion evaluation
1402,,79f923d6575bd8253e2f0b70813caa61a870ccee,Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning,,,"###Offline reinforcement learning (RL), also known as batch RL (Lange et al., 2012) focuses on learning optimal policies from a previously collected dataset without further active interactions with the environment (Levine et al., 2020).###Furthermore, our comparison includes CQL and (Kumar et al., 2020) IQL (Kostrikov et al., 2021), known for its conservative Q-value updates and substituting the max operator with expectile regression.###Under review. havior policy (Fujimoto et al., 2019; Fujimoto & Gu, 2021) or by making conservative updates for Q-networks (Kumar et al., 2020; Kostrikov et al., 2021).###Another line of work addresses the overestimation problem by enforcing the Q-values to be more pessimistic (Kumar et al., 2020; Jin et al., 2021).###Note that we use a single critic network and remove the max Q-backup (Kumar et al., 2020) trick in both training and inference for a fair comparison.###In contrast, the standard diffusion policy without any tricks (e.g., max Q-backup and Q-ensemble) can hardly learn good policies on unbalanced datasets, resulting in a terrible performance drop on the Antmaze task.###Inspired by this, uncertainty-driven RL algorithms employ an ensemble of Q-networks to provide different Q-value predictions for the same state-action pairs (An et al., 2021; Bai et al., 2022).###In the offline setting, two primary challenges are frequently encountered: over-conservatism and a limited capacity to effectively utilize diversified datasets (Levine et al., 2020).###Similarly, in model-free offline RL methods, EDAC (An et al., 2021) and MSG (Ghasemipour et al., 2022), the uncertainty is modeled by an ensemble of Q-networks, and then pessimistic value estimations are obtained to guide the policy.###However, its performance is limited by pre-collected datasets (or behavior policies) and the learning suffers severe overestimation of Q-value functions on unseen state-action samples (Levine et al., 2020).###In addition, we keep all the hyperparameters the same inside each domain task, except for ’medium’ and ’large’ datasets of AntMaze which we use max Q-backup following Wang et al. (2022) and Kumar et al. (2020).###Update Q-networks { Q ψ m } Mm =1 by (16).###We use max Q backup (Kumar et al., 2020) for complete tasks.",impact-revealing,describing the characteristics and challenges of offline reinforcement learning
3131,599c797a601a182cd2642797,668db48c6a79826456341680ee1175dfc4cced71,Get To The Point: Summarization with Pointer-Generator Networks,5c8775174895d9cbc61e9917,Efficient Summarization with Read-Again and Copy Mechanism,"The pointer network has been used to create hybrid approaches for NMT (Gul-cehre et al., 2016), language modeling (Merity et al., 2016), and summarization (Gu et al., 2016; Gulcehre et al., 2016; Miao and Blunsom, 2016; Nallapati et al., 2016; Zeng et al., 2016).###However, the recent success of sequence-to-sequence models (Sutskever et al., 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016).",other,acknowledge the application of pointer networks in various tasks and the impact of sequence-to-sequence models on summarization
2217,5da1a6d447c8f766460688bc,9570147aee646e15affffc30a6b18be0568cfa82,SWQUE: A Mode Switching Issue Queue with Priority-Correcting Circular Queue,53e9b4cab7602d9703fe7aa3,Select-free instruction scheduling logic,"Brown et al. proposed issuing instructions without selection [7], where the select operation is eliminated from the critical path.",other,reporting prior findings
1772,,9b3912ed58d62f9ccfed77d4335a20e33fa9897c,A Salient Missing Link in RFID Security Protocols,,,"###Unfortunately, most of these, like [1–11], failed to address the requirements to a satisfactory extent partially because of not having a common adversary and system deﬁnitions.###In most lightweight RFID authentication protocols, including [1–3, 5, 20–29] (Weis et al.’s the randomized access control scheme [20] performs an exhaustive search in identiﬁcation of the tag), the server could need to query its entire database in order to authenticate responder tag in Round 2.",impact-revealing,highlighting limitations in existing RFID authentication protocols
1134,,8ba81705848dc1d32b446012c655f148a5a7f1c4,Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS,,,"###A prominent example is the denoising diffusion implicit model (DDIM) [16], which builds upon the structure of the DDPM [5] to improve data generation speed.###The demand for faster synthesis is prevalent not only in computer vision but also in speech synthesis, spurring ongoing research to enhance the generation speed of diffusion models [16], [17].",impact-revealing,highlighting the significance of DDIM in improving data generation speed
343,599c7982601a182cd2645d4e,88d346374f17189cebef7394ae5d39492443df89,Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution,5736986b6e3b12023e72ff32,Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks.,"propose a generative model based on a Laplacian pyramid framework (LAPGAN) to generate realistic images in [6], which is the most related to our work.",impact-revealing,drawing a connection to a related generative model for image generation
2147,,07fb6493958ddbe838be21eaebe4636d3c5521f1,"Pro-Environmental Behavior and Green Information Systems Research - Review, Synthesis and Directions for Future Research",,,"###…research examined the role of organizations in influencing the natural environment thoroughly, but widely neglected the potential of IS, which can improve environmental and economic performance by enabling more sustainable organizational practices and processes (Melville 2010; Watson et al. 2010).###We also conducted a forward search with three seminal papers in the Green IS field (Elliot 2011; Melville 2010; Watson et al. 2010) that are most frequently cited by the IS community.###Green IS address issues related to their usage by individuals, groups, organizations, and society to help eco-sustainable practices to emerge and diffuse (Watson et al. 2010).",impact-revealing,highlighting the gap in research on the role of information systems in sustainability
508,5ecfae0d9e795eb20a615048,fde4e53ba166567f3b9b977a866020f10a996c02,LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition,5c8ce3904895d9cbc628ae7f,Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions,"of hours several hours × unpaired speech (single-speaker, high-quality) ✓ dozens of hours × × unpaired speech (multi-speaker, low-quality) ✓ ✓ dozens of hours ✓ unpaired text ✓ ✓ ✓ ✓ Related Work TTS [22, 28, 30, 35] [2, 12, 23, 31] Our Work / ASR [6, 10, 11] [16, 32, 33, 39] [8, 24, 45] Table 1: The data resource to build TTS and ASR systems and the corresponding related works in rich-resource, low-resource, ext###tual Event, CA, USA © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-7998-4/20/08...$15.00 https://doi.org/10.1145/3394486.3403331 1 INTRODUCTION Speech synthesis (text to speech, TTS) [28, 30, 35, 41] and speech recognition (automatic speech recognition, ASR) [6, 10, 11] are two key tasks in speech domain, and attract a lot of attention in both the research and industry community. However, popular",impact-revealing,reporting data resources for TTS and ASR systems
41,5e09a76bdf1a9c0c41677a7b,71bd6b3a2bfa54ee4ea8499be0c4ff478fd735f6,POLAR++: Active One-Shot Personalized Article Recommendation,53e9b732b7602d97042cceb6,Active Learning for Ranking through Expected Loss Optimization,"Expected Loss Optimization [17] selects the instance that maximizes the expected loss based on Bayesian decision theory.###This conﬁrms our idea in Section 4.2.2: Expected Loss Optimization is a better active learning strategy for the ranking problem than Uncertainty Sampling, because it considers not only the prediction uncertainty but also the relative ranking in the list.###Combining the proposedmodel with density-weighted Expected Loss Optimization [17], we introduce active###The active learning metric we choose is Expected Loss Optimization [17].###• Combining the proposed model with density-weighted Expected Loss Optimization [17], we introduce active learning into POLAR [16], an attention-based CNN combined with one-shot learning for personalized article recommendation to utilize extremely sparse implicit user feedback.###• POLAR++DWELO DWELO (Density-Weighted Expected Loss Optimization) is the complete active leraning algorithm which combines ELO and density, as described in Algorithm 1.",impact-revealing,highlighting the advantages of Expected Loss Optimization for active learning
3222,5736982b6e3b12023e6fd21d,0344f9ac92006155f6fa464cab19925ac481dea4,Microarchitectural implications of event-driven server-side web applications,53e99ae2b7602d97023691f2,Temporal Instruction Fetch Streaming,We propose to use the Temporal Instruction Fetch Streaming (TIFS) prefetcher [51] to prefetch recurring missing instructions.###We classify instruction cache misses into three categories as originally defined in [51].,other,reporting a proposed method for instruction prefetching
766,5aed14d617c44a4438158d78,7d89abfe87ed7d1b40391d37364560656d208117,learning memory access patterns,58437725ac44360f1082fa5e,WaveNet: A Generative Model for Raw Audio.,"In (Oord et al., 2016b), they predict 16-bit integer values from an acoustic signal.###We take inspiration from recent works in image and audio generation that discretize the space, namely PixelRNN and Wavenet (Oord et al., 2016a;b).###This idea of treating the output prediction problem as one of classiﬁcation instead of regression has been successfully used in image (Oord et al., 2016a) and audio generation (Oord et al., 2016b).",impact-revealing,Drawing inspiration from successful methods in image and audio generation
2292,5a9cb60d17c44a376ffb3c6d,75a927501749c2cbc0e19a58f798f04de59df64a,Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding,57d063b4ac4436735428dc80,Multi-Rate Deep Learning For Temporal Recommendation,"A related but di erent problem is temporal recommendation [26, 31, 34].",other,acknowledging a related problem in the field
2839,5def6ca63a55ac6095fe0607,2a6f2656c716082e34f9b86d9e589390842e0853,DistTC: High Performance Distributed Triangle Counting,53e9b37ab7602d9703e59a45,Parallel Triangle Counting in Massive Streaming Graphs,"Triangle counting on distributed CPUs and GPUs: Suri et al. [21] implement triangle counting using MapReduce.###Each node in the cluster has 2 Intel Broadwell E5-2683 v4 CPUs with 16 cores per CPU.###However, our distributed algorithm is platform-agnostic as described in Section IV: one can use our partitioning policy to distribute computation across multiple platforms and aggregate the independent counts at the end to arrive at a correct solution, regardless of whether the compute units are CPUs, GPUs or FPGAs.###Triangle counting has been implemented on various platforms including shared-memory CPUs [7], clusters [8], [9], [10], [11] and GPUs [12], [13].###Optimizations that apply to CPUs may not necessarily be useful for GPUs.###We also use the CPUs to do graph partitioning and graph preprocessing.###In addition, the platform is not a factor: CPUs, GPUs, and/or FPGAs can all be used as long as they are able to aggregate triangle counts across all platforms at the end of the local computation.###[7] present parallel algorithms for streaming graphs on the shared memory CPUs.###Triangle counting on shared memory CPUs: Shun et al. [14] detail a cache-oblivious parallel triangle counting on shared-memory multicore CPUs.###Tangwongsan et al. [7] present parallel algorithms for streaming graphs on the shared memory CPUs.###For example, binary search of neighbor lists may not give faster performance on CPUs, but it does give better performance on GPUs due to coalesced memory accesses [15], [16].",other,describing the implementation of triangle counting across various platforms
1871,,9cbea71e43cc5320c9c400ba8bdf1c1727a049fa,Meta-integration for synthesizing data in a systematic mixed studies review: insights from research on autism spectrum disorder,,,"###Despite the Whittemore and Knafl (2005) description of stages in data analysis, namely, reduction, display, comparison, conclusion and verification, this work provides more concrete descriptions.###Whittemore’s paper explains that the ‘data analysis stage’ of an integrative review requires, that ‘‘data from primary sources are ordered, coded, categorized, and summarized into a unified and integrated conclusion about the research problem’’ (Whittemore and Knafl 2005, p. 550).###paper was inspired by Miles and Huberman ‘Qualitative Data Analysis’ (Miles and Huberman 1994) the stages of data analysis in an integrative review consists of data reduction, data display, data comparison, conclusion drawing, and verification (Whittemore and Knafl 2005).###In 2005 Whittemore and Knafl published a paper presenting the concept of integrative reviews that brought together the combination of diverse methodologies in one review (Whittemore and Knafl 2005).###…in the Whittemore et al. paper was inspired by Miles and Huberman ‘Qualitative Data Analysis’ (Miles and Huberman 1994) the stages of data analysis in an integrative review consists of data reduction, data display, data comparison, conclusion drawing, and verification (Whittemore and Knafl 2005).###Even though the integrative review allows inclusion of experimental and non-experimental research and may combine theoretical as well as empirical literature (Whittemore and Knafl 2005), no specific designs for integration of QUAN and QUAL methods were presented.###Whittemore and Knafl (2005) utilize five stages: (1) problem identification, (2) literature search, (3) data evaluation, (4) data analysis and (5) presentation.###Despite the pioneering work of Whittemore and Knafl (2005), Sandelowski et al. (2006), Heyvaert et al. (2013) and Pluye and Hong (2014) exploring procedures for systematic reviews when there are QUAL, QUAN and MM papers, a key gap in the literature has been the lack of practical methodological…###The work of Whittemore and Knafl (2005) contributed to the idea of combining varied perspectives on a phenomenon and was related to previous work from Cooper and Cooper (1998) and Kirkevold (1997).###Despite the additional literature on specific steps, Whittemore and Knafl (2005), Sandelowski et al. (2006), Heyvaert et al. (2013) and Pluye and Hong (2014) cover the full spectrum of procedures.###Still, the method of data analysis gave a hint about elements of relevant synthesis procedures as ‘‘synthesis of important elements or conclusions of each subgroup into an integrated summation of the topic or phenomenon’’ was emphasized (Whittemore and Knafl 2005).###In addition, Whittemore and Knafl (2005) did not refer to any specific designs for integration of QUAN and QUAL papers and used the nomenclature of methodologies instead of a nomenclature related to MM.###In short, Whittemore and Knafl (2005) highlights the advantage of comprehensive understanding of a phenomenon, Sandelowski et al. (2006) provide detailed information about differences in QUAN and QUAL synthesis procedures while Heyvaert et al. (2013), Pluye and Hong (2014) and Cochrane (Higgins and…",impact-revealing,highlighting the contributions and limitations of previous work on data analysis stages
1559,,ef4c9c6181c328d2c6dca1fd826ecaa8df9e0c8a,Some exact results on Lindley process with Laplace jumps,,,"###Such distribution is solution of the Lindley integral equation [26].###Using the strong law of large numbers, in 1952 Lindley [26] showed that the process admits a limit distribution as n diverges if and only if E [ Z ]   0, i.e. if the customer’s arrival rate is slower than the service one.###It was introduced in [26] to describe waiting times experienced by customers in a queue over time.###Inthis paper we focus on a particular constrained random walk: the Lindley process [26].###Many contributions concerning properties of the Lindley process are motivated by their important role in applications [2, 4, 10, 16, 26].",impact-revealing,highlighting the significance and applications of the Lindley process
1740,,16b0ad00f59c462cafa4c2bec451d2b5ed044a64,An Efficient Sensitivity Analysis Method for Large Cloud Simulations,,,"###Previously, we demonstrated our methods in the problem domain of Internet simulation [2], which provided the original challenge identified by Paxson and Floyd.###Paxson and Floyd [1] describe many difficult problems that impede simulation of large data communication networks, which typically require hundreds of parameters that can each take on millions of values and that can also record hundreds of response variables, which might represent aspects of fewer…###Our work on methods to improve simulation modeling of large systems was inspired by Paxson and Floyd [1], who describe many difficult problems that impede simulation of the Internet, and recommend two main coping strategies: search for invariants and carefully explore the parameter space.###Paxson and Floyd [1] describe many difficult problems that impede simulation of large data communication networks, which typically require hundreds of parameters that can each take on millions of values and that can also record hundreds of response variables, which might represent aspects of fewer significant underlying model behaviors.###While providing sound advice on "" what "" to do, Paxson and Floyd did not show "" how "" to accomplish such search and exploration.###This issue remains open over a decade since Paxson and Floyd wrote.",impact-revealing,highlighting the significance of prior work in simulation modeling and its challenges
3373,5c8c52bc4895d9cbc6ddad8d,76e4d56d712d64ec2f77fd5b2fcb504888c07eab,Island loss for learning discriminative features in facial expression recognition,58437785ac44360f10843261,Emotion Recognition In The Wild From Videos Using Images,"Most recently, deep features learned by deep convolutional neural networks (CNNs) have achieved promising results on facial expression recognition especially in more challenging settings [18], [53], [32], [9], [51], [3].###Benefiting from the advance in feature learning, features can be learned either unsupervised by sparse coding [25], [59], [35], [29] or supervised by deep learning [36], [44], [24], [5], [8], [30], [21], [15], [22], [58], [9], [51], [3], [49],###Among them, deep CNNs have achieved promising recognition performance under real-world conditions as demonstrated in the recent EmotiW2015 [18], [53], [32], [52], [43], [60] and EmotiW2016 challenge [9], [51], [3].",other,highlighting the effectiveness of deep CNNs in facial expression recognition
2557,5b67b47917c44aac1c8637c6,5aea95e1ae78a66474051a330ded374e199b658c,Representation Learning on Graphs with Jumping Knowledge Networks,599c7983601a182cd2646c6a,Understanding Black-box Predictions via Influence Functions.,"Related to ideas of sensitivity analysis and inﬂuence functions in statistics (Koh & Liang, 2017) that measure the inﬂuence of a training point on parameters, we study the range of nodes whose features affect a given node’s representation.",other,drawing parallels between statistical concepts and the current study
1183,,4198c80589896573b748cb1634d22da644a997f8,AnoDe: A Log-based Self-Supervised Framework to Detect Scrubber Failures in SRAM-FPGA,,,"###…and the underlying data (distribution, nature, scale etc) for example, K-means assumes spherical clusters, Support vector machine is dependent on the kernel selection and is not suitable for large-scale data, isolation forest doesn’t perform well for high-dimensional data etc [31]–[33].",impact-revealing,highlighting limitations of various data clustering methods
2673,53e9b9fbb7602d97045fabff,f89facea7ae5a3f51af96b549f04f3dbe8c884b3,SHIFT: Shared history instruction fetch for lean-core server processors,53e9ab69b7602d970350814d,Vantage: Scalable And Efficient Fine-Grain Cache Partitioning,", Vantage [31]) can easily guarantee the required cache partition for the history buffer.###As an alternative, 
a cache partitioning scheme (e.g., Van­tage [31]) can easily guarantee the required cache partition for 
the history buffer.###Vantage: Scalable and efficientfine-grain cache partitioning.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3711,5d04eeba8607575390f83f3a,b67fd0612e3f72faee0fed9b1e930b69ed7ee98d,sparse reram engine: joint exploration of activation and weight sparsity in compressed neural networks,5c88d688e1cd8e62e5220dbc,A 1Mb Multibit ReRAM Computing-In-Memory Macro with 14.6ns Parallel MAC Computing Time for CNN Based AI Edge Processors.,"In a practical ReRAM-based DNN accelerator, only a limited number of wordlines and bitlines in a crossbar array can be activated in a single cycle [6, 42, 47].###Since oxide-based ReRAM has good electronic properties (high density, low switching energy, and high endurance) [14, 46] and thus is commonly deployed in latest ReRAM-based DNN accelerator studies [6, 42, 47], we use one of the oxide-based ReRAM, WOx ReRAM [22], for our evaluation.###Compared to the trivial RC delay (around 10ps for a 100×100 crossbar array [35]), ADC sensing time brings a much larger impact on the operating speed of a ReRAM crossbar array [47].",other,highlighting the significance of oxide-based ReRAM in DNN accelerators
3437,5b3d98cc17c44a510f80212a,cb91c2f8d3cac0b655a39be318b603334eb18987,learning to optimize tensor programs,573696cd6e3b12023e5ce1c8,Scalable Bayesian Optimization Using Deep Neural Networks,"Bayesian optimization methods [34, 33, 35, 17] use acquisition functions other than the mean when an uncertainty estimate of f̂ is available.###This problem formalization is similar to that of traditional hyper-parameter optimization problems [34, 33, 35, 13, 17, 25] but with several key differentiating characteristics: Relatively Low Experiment Cost.",other,highlighting the characteristics of Bayesian optimization methods
3525,5fd0a7f691e01147f1d1e367,d4e9bf1d5fab52698a3aa48c65b098ea327deea6,CTRLsum: Towards Generic Controllable Text Summarization,599c7959601a182cd263387c,Convolutional Sequence to Sequence Learning,"We train CTRLsum with another two architectures in addition to BART: (1) convolutional seq2seq (Gehring et al., 2017) with the same hyperparameters as in (Fan et al., 2018), and (2) transformer seq2seq with the same hyperparameters as the base model in (Vaswani et al., 2017).###Gehrmann et al. (2018) utilize copying words at test time to mask copying operations in a summarization task.",other,reporting on training architectures for CTRLsum
583,5dee1b4b3a55ac3d409adcbb,a84689ef8eaf38344eb3de24850ec0720c815605,synchronous transformers for end-to-end speech recognition,5ce3aa43ced107d4c6583051,Triggered Attention For End-To-End Speech Recognition,"Triggered attention [9] utilizes the spikes produced by connectionist temporal classification (CTC) model to split the sequence into many state chunks, and then the decoder predicts the output sequence in a chunkwise way.",impact-revealing,providing context on triggered attention mechanism
2883,5f92ba1691e011edb3573ba0,268d347e8a55b5eb82fb5e7d2f800e33c75ab18a,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"We base ViT conﬁgurations on those used for BERT (Devlin et al., 2019), as summarized in Table 1.###Similar to BERT’s [class] token, we prepend a learnable embedding to the sequence of embedded patches ( z 00 = x class ), whose state at the output of the Transformer encoder ( z 0 L ) serves as the image representation y (Eq.###The dominant approach is to pre-train on a large text corpus and then ﬁne-tune on a smaller task-speciﬁc dataset (Devlin et al., 2019).###However, much of their success stems not only from their excellent scalability but also from large scale self-supervised pre-training (Devlin et al., 2019; Radford et al., 2018).###This setup is very similar to the one used for language by Devlin et al. (2019).###Large Transformer-based models are often pre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown…###We also experimented with 15% corruption rate as used by Devlin et al. (2019) but results were also slightly worse on our few-shot metrics.###The “Base” and “Large” models are directly adopted from BERT and we add the larger “Huge” model.###Large Transformer-based models are often pre-trained on large corpora and then ﬁne-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of work uses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).###We also perform a preliminary exploration on masked patch prediction for self-supervision, mimicking the masked language modeling task used in BERT.",other,describing the configuration and training approach for ViT based on BERT
3421,5736973b6e3b12023e62b0a8,97e3bb4af723f43927317e9b9f2d794a9e398e8e,PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks,53e99915b7602d970215077b,Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis.,"This is because document-level word co-occurrences suffer from the sparsity in short documents, with similar results observed in statistical topic models [26].",other,highlighting the limitations of document-level word co-occurrences
3669,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e9af46b7602d97039855fd,Multi-agent Monte Carlo Go,"The tree selection allows the algorithm to favour more promising nodes (without allowing the selection probability of the other nodes to converge to zero), leading to an asymmetric tree over time.###Kloetzer [115] demonstrates the use of MCTS for generating opening books for the game of Amazons (7.3).",other,reporting prior findings on MCTS application
389,5ec49a639fced0a24b4de82e,724d182e2b0217ce10ff05e3ad0b2f548fb39b9f,Iterative Edit-Based Unsupervised Sentence Simplification,5eb3dc4191e011ce31f27f53,Discrete Optimization for Unsupervised Sentence Summarization with  Word-Level Extraction,"Concurrent work further shows the success of search-based unsupervised text generation for paraphrasing (Liu et al., 2020) and summarization (Schumann et al., 2020).",impact-revealing,highlighting the success of search-based unsupervised text generation methods
1266,,a4633aeb054ca721b076e6d5dacd348a8643dab7,Validation of a Spanish Version of the Marwit–Meuser Caregiver Grief Inventory Short Form in a Puerto Rican Sample,,,"###A Spanish version of the short form of this instrument (Shiekh & Yesavage, 1986), translated and validated by
Martı́nez de la Iglesia et al. (2002), was used to measure reported clinical depression symptoms.",impact-revealing,reporting the use of a validated instrument for measuring clinical depression symptoms
214,5ecbc8eb9fced0a24b52a39b,2b9514be4679f68f97bbcf9671053ac2f03df2e4,Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning,5d70dfa83a55acf39f3e7fd6,Seeing Is Not Believing: Camouflage Attacks On Image Scaling Algorithms,[35] have shown that scaling algorithms are vulnerable to attacks and can be misused to fool machine learning systems.###[35] have demonstrated that data preprocessing used in machine learning can also suffer from vulnerabilities.###[35] for a description how to calculate L and R.###[35] is not applicable to this scaling algorithm.,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2824,599c7959601a182cd2633b3e,c0c0990b84a350d5efde8d3b2cb2636b6b57c21c,On Sampling Strategies for Neural Network-based Collaborative Filtering,5736977f6e3b12023e66632b,LINE: Large-scale Information Network Embedding,", LINE [29]) tasks, negative sampling is utilized.###Many existing algorithms with graph-based loss functions [1, 22, 29] adopt the “Negative Sampling” strategy, in which k negative samples are drawn whenever a positive example is drawn.###links # tf # tд # ti pointwise pairwise IID [3] b bk b (1 + k ) b (1 + k ) b (1 + k ) vec ✓ × Negative [1, 21, 29] b bk b b (1 + k ) b (1 + k ) vec ✓ ✓ Stratified (by Items) b bk b (1 + k ) s b (1 + k ) vec ✓ × Negative Sharing b b (b − 1) b b b × b mat ✓ ✓###In the setting where negative examples are overwhelming, such as in word embedding (e.g., Word2Vec [22]) and network embedding (e.g., LINE [29]) tasks, negative sampling is utilized.###Compared to traditional feature detectors, such as SIFT and n-grams, DNNs and other embedding methods [5, 6, 29] can automatically extract better features that produce higher performance in various tasks.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2548,5dde4b463a55ac4c42972afc,43a8b2fd651c3783723f4265d7641f9601a5a6f4,One Man’s Trash Is Another Man’s Treasure: Resisting Adversarial Examples by Adversarial Examples,5a73cbc317c44a0b3035eb01,Towards Robust Neural Networks via Random Self-ensemble,"But those works either report degraded robustness under BPDA attacks [23, 31] or neglected the evaluation against BPDA attacks [17, 42, 22].###Since then, a few other gradient obfuscation based defenses have been proposed [23, 31, 17, 42, 22].",other,highlighting limitations in existing works on robustness under BPDA attacks
3041,5aed146117c44a44381527f9,0aef7a464840621c384380ac8877ae53b73d23a8,Blasting through the Front-End Bottleneck with Shotgun,5736982b6e3b12023e6fd28c,Confluence: Unified Instruction Supply For Scale-Out Servers,"Confluence: Confluence is the state-of-the-art temporal streaming prefetcher that uses unified metadata to prefetch into both L1-I and BTB [10].###The second is using one set of unified metadata for both instruction cache and BTB prefetching, thus avoiding the cost and complexity of maintaining two separate control flow histories [10].###For filling the BTBs, Shotgun takes a hybrid approach by incorporating the features from both Boomerang [13] and Confluence [10].###In addition to using the spatial footprint to prefetch instructions into the L1-I, Shotgun exploits control flow commonality [10] to prefetch into the C-BTB as well.###By exploiting prior observations on control flow commonality in instruction and BTB working sets [10], Shotgun prefetches into the conditional branch BTB by predecoding cache lines brought into the L1-I through the use of spatial footprints.###We model Confluence as SHIFT augmented with a 16K-entry BTB, which was shown to provide a generous upper bound on Confluence’s performance [10].###Recent temporal streaming research has focused on lowering the storage costs [9, 10, 12]; however, even with optimizations, for a many-core CMP running several consolidated workloads, the total prefetcher storage requirements can reach into megabytes.###To provide high L1-I and BTB miss coverage, Confluence requires at least a 32K-entry instruction history and an 8Kentry index table, resulting in high storage overhead.###The state-of-the-art in temporal streaming combines the two ideas into a unified front-end prefetcher called Confluence [10].",other,describing the state-of-the-art in temporal streaming prefetching
677,53e99cb5b7602d970256b3bb,b8048c62a985dcb0f7827db470f148e4db58535f,mamba: a scalable communication centric multi-threaded processor architecture,53e99b1bb7602d97023b3110,Specifying Concurrent Program Modules,"For MIPS64 this has been implemented exactly as [8], for Mamba a simple modification was made.###A simple FIFO queue implementation was written based upon [8].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3499,5e09cab43a55ac662f721ac6,5c109db04998e623b794b269494f44b7e5006af1,Category-Level Articulated Object Pose Estimation,5736960e6e3b12023e520c52,Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images,"For each part, we evaluate rotation error measured in degrees, translation error in normalized part coordinate space, and 3D intersection over union (IoU) [20] of the predicted amodal bounding box.",other,describing evaluation metrics for the method
21,58d82fcbd649053542fd669e,a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c,Deep Variational Information Bottleneck.,53e99a48b7602d97022a8346,Auto-Encoding Variational Bayes.,"By using the reparameterization trick (Kingma & Welling, 2014), we can use Monte Carlo sampling to get an unbiased estimate of the gradient, and hence we can optimize the objective using stochastic gradient descent.###Putting these two bounds together we have that our unsupervised information bottleneck objective takes the form And this takes the form of a variational autoencoder (Kingma & Welling, 2014), except with the second KL divergence term having an arbitrary weight β .###In the unsupervised learning literature, our work is closely related to the work in Kingma & Welling (2014) on variational autoencoders.###) Then we can use the reparameterization trick (Kingma & Welling, 2014 to write p ( z | x ) dz = p ( (cid:15) ) d(cid:15) , where z = f ( x, (cid:15) ) is a deterministic function of x and the Gaussian random variable (cid:15) .###…of an analytic Kullback-Leibler divergence, we can put everything together to get the following objective function, which we try to minimize: As in Kingma & Welling (2014), this formulation allows us to directly backpropagate through a single sample of our stochastic code and ensure that our…",impact-revealing,reporting on the use of the reparameterization trick in variational autoencoders
1290,,6792bdda833d2c7cd5a94664515b00d4d04896a6,In vivo quantitative 1H MRS of cerebellum and evaluation of quantitation reproducibility by simulation of different levels of noise and spectral resolution.,,,"###1-0, under Linux platform [9] in the spectral window 4.###Indeed, the fitting line shape parameters account for the increased transversal relaxation rates from in vitro to in vivo conditions, and a modification of the expected T2 is only required when metabolites experience differential T2 variation [9].###To date, the software packages more commonly used for quantitation are AMARES [7], a nonlinear least square fitting algorithm operating in the time domain as implemented in the MRUI package [8], and LCModel [9], which analyzes an in vivo spectrum as a linear combination of a basis set of in vitro spectra from individual metabolite solutions, thus exploiting the full spectroscopic information of each metabolite and not isolated resonances.",impact-revealing,reporting software packages used for quantitation in spectroscopy
3566,5b1643998fbcbf6e5a9bc25e,080e1bb6bbebeb78f822b3998b7ed898ab6457aa,End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction,55503f5645ce0a409eb2d86b,On Training Targets for Supervised Speech Separation.,"Sigmoidal units are dominantly used in the output layer of deep learning based T-F masking [36, 35], partly because they can model well data with bi-modal distribution [37], such as the IRM [38] and its variants [36].###On the publicly-available wsj0-2mix corpus, our algorithm reaches 12.6 dB scale-invariant SDR, which surpasses the previous best by a large margin and is comparable to the oracle 12.7 dB result obtained using the so-called ideal ratio mask (IRM).###To obtain clean magnitudes, the oracle mask should be |Sc|/|X| (also known as the FFT mask in [38] or the ideal amplitude mask in [21]).",other,Highlighting the effectiveness of the proposed algorithm in surpassing previous results
3563,5d1eb9ecda562961f0b261db,1bfad6fd818bd64db381791efd9252e0313dc100,Certifiable Robustness and Robust Training for Graph Convolutional Networks,5ce2d032ced107d4c635260c,Predict then Propagate: Graph Neural Networks meet Personalized PageRank,"While there exist many classical approaches to node classification [2, 15], recently graph neural networks (GNNs), also called graph convolutional networks, have gained much attention and improved the state of the art in node classification [5, 7, 12, 13].",other,highlighting the rise and significance of graph neural networks in node classification
284,5f00587b9fced0a24b1fbbf1,1803c317dbd25d64210026fc4181faa31e521719,USMPep: universal sequence models for major histocompatibility complex binding affinity prediction,5e16fa4bdf1a9c0c41713de9,UDSMProt: Universal Deep Sequence Models for Protein Classification.,"Similar to [12], the training procedure included 1-cycle learning rate scheduling [15] and discriminative learning rates [13] during finetuning.###The setup closely follows that used in [12], where protein properties were predicted.###The approach builds on the UDSMProt-framework [12] and related work in natural language processing [13].###A. USMPep: Universal Sequence Models for Peptide Binding Prediction The approach builds on the UDSMProt-framework [12] and related work in natural language processing [13].###These results stress that further efforts might be required to truly leverage the potential of unlabeled peptide data in order to observe similar improvements as seen for proteins [12] in particular for small datasets.",impact-revealing,acknowledging the training procedure and its relation to prior work
3119,5ec7a32791e0118397f3ee40,2523aea79e9776dc34020f3d150dd49211aa144d,Leveraging Text Data Using Hybrid Transformer-LSTM Based End-to-End ASR in Transfer Learning,5db92b4647c8f76646222fc8,Building the Singapore English National Speech Corpus,"For the source language, which is English, we use two sub-sets of the National Speech Corpus (NSC) [20] to train source models.###For the source language, which is English, we use two subsets of the National Speech Corpus (NSC) [20] to train source models.",other,reporting data sources used for training
3646,5aed14d617c44a4438158cff,b77b179522ac01b6903c2719d9b5d29c1efa652e,Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba,5550416845ce0a409eb3b00b,Collaborative Deep Learning for Recommender Systems,", collaborative filtering (CF) [9, 11, 16], content-based methods [2], and deep learning based methods [5, 6, 22], the problems facing these methods become more severe in Taobao because of the billion-scale of users and items.",other,highlighting challenges in recommendation methods due to scale
2552,5fb24ee191e01186d3f5decc,050fecf6e3e21faeeb7629f179ba839c4dfd1ead,EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints,53e9b0b2b7602d9703b235d4,Automatic evaluation of translation quality for distant language pairs,"Results conﬁrm that enforcing hard constraints increases CPR but degrades translation quality compared to the same model using soft constraints: for LevT, it degrades BLEU by 2.2–3.9 and RIBES by 5.0–6.6.###Evaluation We evaluate translation quality via case-sensitive tokenized BLEU (as in Gu et al. (2019)) 9 and RIBES (Isozaki et al., 2010), which is more sensitive to word order differences.###Finally, results conﬁrm the beneﬁts of EDI-TOR’s reposition operation over LevT: decoding with EDITOR is 6–7% faster than LevT on Ro-En and En-De, and 33% faster on En-Ja – a more distant language pair which requires more reordering but no inﬂection changes on reordered words – with no statistically signiﬁcant difference in BLEU nor RIBES, except for En-Ja, where ED-10 ITOR signiﬁcantly outperforms LevT on RIBES.###For EDITOR, it degrades BLEU by 1.6–4.3 and RIBES by 3.1–4.4 (Table 3).###The RIBES trends are more surprising: both NAR models signiﬁcantly outperform the AR models (Sockeye) on RIBES, except for En-Ja, where EDITOR and the AR models sig-niﬁcantly outperforms LevT.",other,highlighting the trade-offs between hard and soft constraints in translation quality
1311,,1efcdd79ac384cdbb3f42e37fd4dac111cc3dcc4,Using Model-Driven Risk Analysis in Component-Based Development,,,"###Dependent threat diagrams are inspired by assumption-guarantee reasoning, which has been suggested as a means to facilitate modular system development (Jones, 1981; Misra and Chandy, 1981; Abadi and Lamport, 1995).",impact-revealing,highlighting the inspiration behind dependent threat diagrams
3117,5db9298647c8f766461f8ecd,ce177672b00ddf46e4906157a7e997ca9338b8b9,Attention is not not Explanation,55a6bcf965ce054aad739806,Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features.,"Citing Ross et al. (2017), Jain and Wallace’s requisite for attention distributions to be used as explanation is that there must only exist one or a few closely-related correct explanations for a model prediction.###…4http://www.di.unipi.it/˜gulli/AG_
corpus_of_news_articles.html 5https://github.com/successar/ AttentionExplanation 6We do not include the Twitter Adverse Drug Reactions (ADR) (Nikfarjam et al., 2015) dataset as the source tweets are no longer all available.###AttentionExplanation We do not include the Twitter Adverse Drug Reactions (ADR) (Nikfarjam et al., 2015) dataset as the source tweets are no longer all available.###We report F1 scores on the positive class, and apply the same metrics they use for model comparison, namely Total Variation
3http://qwone.com/˜jason/20Newsgroups/ 4http://www.di.unipi.it/˜gulli/AG_
corpus_of_news_articles.html 5https://github.com/successar/ AttentionExplanation 6We do not include the Twitter Adverse Drug Reactions (ADR) (Nikfarjam et al., 2015) dataset as the source tweets are no longer all available.###tion of the decision-making process, and Riedl (2019) classifies explainable rationales as valuable in that they mimic what we as humans do when we rationalize past actions: we invent a story that plausibly justifies our actions, even if it is not an entirely accurate reconstruction of the neural processes that produced our behavior at the time.",other,highlighting the importance of attention distributions in model explanations
1683,,584a08d30f598e294188ba357563545c17ed986d,When Organizational Identification Elicits Moral Decision-Making: A Matter of the Right Climate,,,"###Most of this research builds on Social Identity Theory (Abrams and Hogg 1990, 2010; Tajfel and Turner 1979) and the closely related Self-Categorization Theory (Turner et al. 1987)—see Abrams and Hogg (2010) for a recent overview.",impact-revealing,acknowledge foundational theories in social identity research
2213,5db1765a3a55ac101c887e97,6c4b76232bb72897685d19b3d264c6ee3005bc2b,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,599c7965601a182cd2638cad,A Deep Reinforced Model for Abstractive Summarization.,"It has been shown that improvements to the ROUGE score do not necessarily correspond to more coherent summaries [Paulus et al., 2017].",other,highlighting the relationship between ROUGE scores and summary coherence
1768,,59310c4925fc6ef18320f40dfb24c2deb988d9cd,PII: S0925-4773(01)00556-1,,,"###Anti-
sense and sense digoxigenin-labeled RNA probes were
generated for Tcf1 (1.8 kb), Tcf3 (1.8 kb), Tcf4 (0.5 kb),
Lef1 (1.0 kb) (Oosterwegel et al., 1993; Korinek et al.,
1998), sFrp1 (2.5 kb), sFrp2 (2.0 kb), sFrp3 (2.5 kb) and
sFrp4 (1.8 kb) (Rattner et al., 1997).###…and Wnt signaling related genes (Wnt2, Wnt11, Tcf1, Lef1, sFrp1, sFrp2 and
sFrp4) in the embryonic and adult mouse lung (Levay-
Young and Navre, 1992; Oosterwegel et al., 1993; Rattner
et al., 1997; Lako et al., 1998) did not address the temporal
and spatial localization of transcripts and…###Previous studies of expression of Wnts and Wnt signaling related genes (Wnt2, Wnt11, Tcf1, Lef1, sFrp1, sFrp2 and sFrp4) in the embryonic and adult mouse lung (LevayYoung and Navre, 1992; Oosterwegel et al., 1993; Rattner et al., 1997; Lako et al., 1998) did not address the temporal and spatial localization of transcripts and proteins in relation to morphogenesis and differentiation of the murine lung.",impact-revealing,highlighting the limitations of previous studies on Wnt signaling related genes
3065,5f0bde8e9e795ea206ff8ef5,0feea94f89d395436bf41bd10c797447eecbc128,Unsupervised data augmentation for consistency training,53e9a806b7602d970314bc39,Learning Word Vectors for Sentiment Analysis.,"For language, we rely on six text classification benchmark datasets, including IMDb, Yelp-2, Yelp-5, Amazon-2 and Amazon5 sentiment classification and DBPedia topic classification (Maas et al., 2011; Zhang et al., 2015).",other,reporting benchmark datasets used for text classification
1528,,b305e1d4c99afacd7e645c850f91a642fb696ee4,All Power to the Peers: Black Women Graduate Students’ Peer Relationship Typologies in PWIs,,,"###…discriminatory interactions, and microaggressions: brief and commonplace daily verbal, behavioral, and environmental indignities, whether intentional or unintentional, that communicate hostile, derogatory, or negative racial slights and insults to the target person or group (Sue et al., 2007).###Nikki emphasized the importance of her sistah-gurls acting as emotional buffers against perceived racism, discriminatory interactions, and microaggressions: brief and commonplace daily verbal, behavioral, and environmental indignities, whether intentional or unintentional, that communicate hostile, derogatory, or negative racial slights and insults to the target person or group (Sue et al., 2007).",impact-revealing,providing context for the concept of microaggressions
3781,5ee8986891e011e66831c3bc,965652c0e426c5b42d7218d7429025be7ac542bf,DeeperGCN: All You Need to Train Deeper GCNs,5ca5dbb6e1cd8e17e1d7137c,An Experimental Study of Neural Networks for Variable Graphs.,"The methods include GCN [Kipf and Welling, 2016], GraphSAGE [Hamilton et al., 2017], GIN [Xu et al., 2019b], GIN with virtual nodes, GaAN [Zhang et al., 2018], and GatedGCN [Bresson and Laurent, 2018].",other,acknowledge various graph neural network methods
435,5e09a701df1a9c0c4167614c,3caf34532597683c980134579b156cd0d7db2f40,Universal Adversarial Triggers for Attacking and Analyzing NLP,58437722ac44360f1082f55f,Universal Adversarial Perturbations,"Triggers are a new form of universal adversarial perturbation (Moosavi-Dezfooli et al., 2017) adapted to discrete textual inputs.###Why Universal? The adversarial threat is higher if an attack is universal: using the exact same attack for any input (Moosavi-Dezfooli et al., 2017; Brown et al., 2017).###The adversarial threat is higher if an attack is universal : using the exact same attack for any input (Moosavi-Dezfooli et al., 2017; Brown et al., 2017).###Moreover, universal attacks often transfer across models (Moosavi-Dezfooli et al., 2017), which further decreases attack requirements: the adversary does not need white-box (gradient) access to the target model.",impact-revealing,highlighting the significance of universal adversarial perturbations in text
2037,,fff11fa12a5fcf858f037f3cb7e632bff050f3ed,Serum- and glucocorticoid-induced kinase drives hepatic insulin resistance by directly inhibiting AMP-activated protein kinase,,,"###Sgk3 deficiency mitigates HFD-induced insulin resistance but not glucose intolerance.###Although unlikely to be evident in isolated hepatocytes, at the whole-animal level, it remains a distinct possibility that the global Sgk2 and Sgk3 knockouts used affect insulin sensitivity by action in other tissues.###We demonstrate that SGK1 is dominant among SGK family kinases in regulation of insulin sensitivity, as Sgk1, Sgk2, and Sgk3 triple-knockout mice have similar increases in hepatic insulin sensitivity.###Double Sgk2;Sgk3 global knockout mice were generated by C57BL/6N zygotic injection of small guide RNAs targeting Sgk2 and Sgk3 and S.p.Cas9 mRNA at the Genome Modification Facility, Harvard University.###Mice entirely lacking all three Sgk genes in liver (liver-specific Sgk1 knockout in the background of whole-body Sgk2 and Sgk3 knockouts [SgkLtko]) do not exhibit further improvement in insulin sensitivity, suggesting that Sgk1 is dominant among the Sgk family members in governance of hepatic insulin action.###Two independent cohorts of Sgk3-knockout mice show mild improvement in HFD-induced insulin resistance but not glucose intolerance (Figures S6I–S6L).###Mammalian genomes harbor three genes encoding SGK family members: Sgk1, Sgk2, and Sgk3 (Kobayashi et al., 1999).###SgkLtko mice were generated by crossing Sgk1Lko mice to Sgk2;Sgk3 global knockout mice.###To study the function of all SGK family members in liver, we first generated Sgk2 and Sgk3 global knockout mice (Sgk2−/− and Sgk3−/−) using CRISPR-Cas9 zygotic injection in C57BL/6N embryos.###To investigate whether there is functional redundancy among SGK1, SGK2, and SGK3 in the liver, we crossed Sgk2−/−;Sgk3−/− global knockout mice with Sgk1Lko mice to generate mice lacking hepatic expression of all Sgk family members (SgkLtko), confirmed by western blotting in primary hepatocytes (Figure 7B).###After four backcrosses to C57BL/6J mice, glucose and insulin tolerance in male Sgk2−/− and Sgk3−/− mice fed a normal chow diet are not significantly different versus wild-type mice (Figures
Cell Rep. Author manuscript; available in PMC 2021 November 09.###Cg-SgkLtko This study N/A
Oligonucleotides
Sgk1 sgRNA targeting sequence: TAAGCAGCCGTATGACCGGA This paper N/A
Sgk2 sgRNA targeting sequence: CGGAGCCTTCTACGCCGTGA This paper N/A
Sgk3 sgRNA targeting sequence: CAAGGCACTGGCGATCTCCG This paper N/A
Actin QPCR primer, Forward: CTAAGGCCAACCGTGAAAAG This paper N/A
Cell Rep. Author manuscript; available in PMC 2021 November 09.###(B) Hepatic knockout of Sgk1, Sgk2, and Sgk3 validated by western blotting.###Guide RNA sequences used are as follows:
Sgk2: CGGAGCCTTCTACGCCGTGA;
Sgk3: CAAGGCACTGGCGATCTCCG.###However, as glucose tolerance is reflective of insulin production and insulin action, the lack of difference in glucose tolerance in Sgk3−/− mice could be due to the combined impact of a mild improvement in insulin sensitivity and a previously described defect in insulin secretion (Yao et al., 2011).",impact-revealing,reporting findings on the role of SGK family kinases in insulin sensitivity
836,5e5e19c093d709897ce87ab9,9b240a87b11d085641d6640f73cc3cc2d678e305,automatically scheduling halide image processing pipelines,53e9b0b2b7602d9703b20db9,"Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines","Eight of the benchmarks are drawn from public literature [Ragan-Kelley et al. 2012; Ragan-Kelley et al. 2013; RaganKelley et al. 2015] and the Halide open source community.###In recent years, the Halide image processing language [Ragan-Kelley et al. 2012; Ragan-Kelley et al. 2013] has proven to be an effective system for authoring high-performance image processing code, and it is now used to synthesize production code used in datacenters and on hundreds of millions of…###We assume familiarity with the Halide system, and refer the reader to [Ragan-Kelley et al. 2012; Ragan-Kelley et al. 2013] for a comprehensive description of the language and its features.###In recent years, the Halide image processing language [Ragan-Kelley et al. 2012; Ragan-Kelley et al. 2013] has proven to be an effective system for authoring high-performance image processing code, and it is now used to synthesize production code used in datacenters and on hundreds of millions of smartphones.",impact-revealing,highlighting the effectiveness and widespread use of the Halide image processing language
3618,5f06e5e591e0117f54657c19,f6d32ed0eee5fb3f6ac518f3aebc8ceff2aae397,NVAE: A Deep Hierarchical Variational Autoencoder,573696ce6e3b12023e5cecfc,DRAW: A Recurrent Neural Network For Image Generation,"In deep hierarchical VAEs [5, 9, 4, 42, 43], to increase the expressiveness of both the approximate posterior and prior, the latent variables are partitioned into disjoint groups, zzz = { zzz 1 ,zzz 1 , . . . ,zzz L } , where L is the number of groups.###…VAEs [1, 2] is dedicated to the statistical challenges, such as reducing the gap between approximate and true posterior distributions [3, 4, 5, 6, 7, 8, 9, 10], formulating tighter bounds [11, 12, 13, 14], reducing the gradient noise [15, 16], extending VAEs to discrete variables [17, 18,…",other,providing context on deep hierarchical VAEs and their statistical challenges
1689,,749df5dd2dc2ad34a76288ef8ba270058a61caf4,Conflict and segregation in networks: An experiment on the interplay between individual preferences and social influence,,,"###Our theoretical framework builds on two lines of work examining how relationships and behaviour emerge from actors’ individual preferences and the influence from those around them: identity theory (originating from Psychology but recently increasingly adopted in Economics, see [4]) and strategic interaction in networks (from Economics).###The interaction of identity considerations and individual incentives had not been directly addressed theoretically or experimentally, in Psychology, leaving an important gap to be developed.###One of the most prominent theoretical tools to study the effect individual preferences have on the way people behave is identity theory [38, 1].###[34] B. Mullen, R. Brown and C. Smith, Ingroup bias as a function of salience, relevance, and
status: An integration, European Journal of Social Psychology, 22 (1992), 103–122.###Research on the theory of identities was initiated in Psychology [38, 40], mainly focusing on the effects that the social context has on group processes and inter-group relations.",impact-revealing,Highlighting the theoretical gap in identity theory and its application in Economics
3130,573696026e3b12023e515eec,2c03df8b48bf3fa39054345bafabfeff15bfd11d,Deep residual learning for image recognition,53e9a186b7602d9702a772f5,Fisher Kernels On Visual Vocabularies For Image Categorization,"In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD.",other,providing context on image recognition representations
3753,5e09caba3a55ac662f721afe,36ad06e6f9b39192e7668634eadd6fcf9593e922,Efficient Adversarial Training With Transferable Adversarial Examples,5c04967517c44a2c74708e33,Universal Adversarial Training,"Several works [2,4,10,15,18,23,24,28,29,32] focus on enhancing either the efficiency or effectiveness of the adversarial training that was first proposed in [13].",other,acknowledge existing works on adversarial training
2616,5ecc763e9e795e81e9307559,270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,mpnet: masked and permuted pre-training for language understanding,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"One of the most successful models is BERT [2], which mainly adopts masked language modeling (MLM) for pre-training1.###For language understanding, masked language modeling (MLM) in BERT [2] and permuted language modeling (PLM) in XLNet [5] are two representative objectives.###BERT (Devlin et al., 2019) 80.8 88.5 RoBERTa (Liu et al., 2019a) single model without any data augmentation for fair comparisons.###We assume all the three objectives mask and predict the same amount of tokens ( 15% ), following the common practice in BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) 5 .###Pre-training language models [1, 2, 3, 4, 5, 6, 7, 8] have greatly boosted the accuracy of NLP tasks in the past years.###On the dev set of GLUE tasks, MPNet outperforms BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019a) by 4.6, 3.2, 1.3 points on average.###The key of pre-training methods [1, 2, 4, 5, 10] is the design of self-supervised tasks/objectives for model training to exploit large language corpora for language understanding and generation.###Pre-training models (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019b; Song et al., 2019; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019a; Raffel et al., 2019a) have greatly boosted the accuracy of NLP tasks in the past years.###One of the most successful models is BERT (Devlin et al., 2019), which mainly adopts masked language modeling (MLM) for pre-training 2 .###For language understanding, masked language modeling (MLM) in BERT (Devlin et al., 2019) and permuted language modeling (PLM) in XLNet (Yang et al., 2019) are two representative objectives.###All of the listed re-sults are reported in BERT BASE setting and from MNLI QNLI QQP RTE SST MRPC CoLA STS Avg Single model on dev set BERT (Devlin et al., 2019) 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 83.1 XLNet (Yang et al., 2019) 86.8 91.7 91.4 74.0 94.7 88.2 60.2 89.5 84.5 RoBERTa (Liu et al.,…###1 Experimental Setup We conduct experiments under the BERT base setting (BERTBASE) [2], where the model consists of 12 transformer layers, with 768 hidden size, 12 attention heads, and 110M model parameters in total.###For the pre-training objective of MPNet, we randomly permute the sentence following PLM [5]5, choose the rightmost 15% tokens as the predicted tokens, and prepare mask tokens following the same 8:1:1 replacement strategy in BERT [2].###We conduct experiments under the BERT base setting (BERT BASE ) (Devlin et al., 2019), where the model consists of 12 transformer layers, with 768 hidden size, 12 attention heads as 12, and 110M model parameters in total.###We assume all the three objectives mask and predict the same amount of tokens (15%), following the common practice in BERT [2] and XLNet [5]3.###We use a sub-word dictionary with 30K BPE codes in BERT [2] to tokenize the sentences.###MLM in BERT BERT [2] is one of the most successful pre-training models for natural language understanding.###For the pre-training objective of MPNet, we randomly permute the sentence following PLM (Yang et al., 2019) 7 , choose the rightmost 15% tokens as the predicted tokens, and prepare mask tokens following the same 8:1:1 replacement strategy in BERT (Devlin et al., 2019).###For the non-predicted part (xz<=c ,Mz>c), we use bidirectional modeling [2] to extract the representations, which is illustrated as the light grey lines in Figure 2a.###The key of pre-training methods (Radford et al., 2018; Devlin et al., 2019; Song et al., 2019; Yang et al., 2019; Clark et al., 2020) is the design of self-supervised tasks/objectives for model training to exploit large language corpora for language understanding and generation.###MLM in BERT BERT (Devlin et al., 2019) is one of the most successful pre-training models for natural language understanding.",other,highlighting the success and impact of pre-training models in NLP
1254,,441dfd8e9bab7253637cbd7956eb03ce0e4781ba,Nutritional interventions: the evidence,,,"###Advice to change the nature of dietary fat rests on a solid research foundation: it is backed up by large-scale prospective epidemiology (Hu et al. 1997), randomized clinical trials (Sacks & Katan, 2000), and mechanistic insights (Spady & Dietschy, 1985).",impact-revealing,highlighting the strong research foundation supporting dietary fat recommendations
3858,5efb0d5691e011063336d27d,2b364917b0c51e91fcf2ab9c1d66a14ed4b44c03,BERTology Meets Biology: Interpreting Attention in Protein Language Models,58d83051d649053542fe9ba7,Diagnostic Classifiers Revealing how Neural Networks Process Hierarchical Structure.,"So-called diagnostic classiﬁers are used to interpret the outputs from BERT’s layers [81].###Besides analyzing attention, we also use a diagnostic classiﬁer [1, 16, 81] to probe the layer outputs to determine what information they contain about the properties described above.",other,providing context on the use of diagnostic classifiers in BERT
1476,,23d49bb5d3c798064f7677ac6e48258f267a419c,A sound-based crowd activity recognition with neural network based regression models,,,###ReLU is a non-linear function which is widely used as the activation function in neural network [6].,impact-revealing,providing context about a widely used activation function in neural networks
2190,5dbebb7447c8f766462c21c0,3d9baf7e87ec43f0ad486e2077824a346a58118e,Emotion-aware Chat Machine: Automatic Emotional Response Generation for Human-like Emotional Interaction,599c7987601a182cd2648444,A Structured Self-attentive Sentence Embedding.,"Furthermore, self-attention mechanism proposed by [18, 38] is proved effective for machine translation, which can yield large gains in terms of BLEU [24] as compared to the state-of-the-art methods.###To enhance the representation power of the hidden states, we utilize self-attention mechanism [18] to enable the encoders to attend to emotion-rich words in the post, and then obtain the emotional hidden state he by calculating:",other,highlighting the effectiveness of self-attention mechanisms in machine translation
3967,5dcbd5da3a55ac789b0dbbdd,d6b414487787d0b6efd735a3236a690ad13aae70,TENER: Adapting Transformer Encoder for Named Entity Recognition,573696106e3b12023e5227c8,Neural Architectures for Named Entity Recognition.,"Compared to BiLSTM based character encoder (Lample et al., 2016; Ghaddar and Langlais, 2018), CNN is more efﬁcient.###…viewed as a sequence labeling task, the neural models usually contain three components: word embedding layer, context encoder layer, and decoder layer (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Chiu and Nichols, 2016; Chen et al., 2019; Zhang et al., 2018; Gui et al., 2019b).###Since (Collobert et al., 2011), various neural models have been introduced to avoid hand-crafted features (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).###Owing to BiLSTM’s high power to learn the contextual representation of words, it has been adopted by the majority of NER models as the encoder (Ma and Hovy, 2016; Lample et al., 2016; Zhang et al., 2018; Gui et al., 2019b).###Compared to BiLSTM based character encoder (Lample et al., 2016; Ghaddar and Langlais, 2018), CNN is more efficient.###The previous work has proved that character encoder is necessary to capture the character-level features and alleviate the out-of-vocabulary (OOV) problem (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Xin et al., 2018).###, 2011), various neural models have been introduced to avoid hand-crafted features (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).###Since the word shape information, such as the capitalization and n-gram, is important in recognizing named entities, CNN and BiLSTM have been used to extract character-level information (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016; Strubell et al., 2017; Chen et al., 2019).",other,comparing efficiency of CNN and BiLSTM in character encoding for NER tasks
141,5f3e44b791e011c0de1c29bc,61325245e98920a69b40e18c069fda0c1cf00f21,MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings for Sequential Recommendation,5cede10dda562983788ed645,BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer,"BERT4Rec [19] improved SASRec by adopting Transformer [22] and Cloze-task based training method.###Multi-head self-attention proposed in Transformer [22] shows excellent performance in recommendation tasks [19].###Pos-embedding is analogous to the learnable positional embedding used in [2, 19], where each position has a corresponding embedding inMP ∈ RN×h .###Traditionally, self-attention based models [2, 10, 19] employ a simple absolute positional encoding by feeding the sum of EI tem and EPos as input as shown in Figure 2(a).###Our model is based on a fixed-length model with length N as in previous works [10, 19].###Note that while some works [19, 22] apply LayerNorm at the very end, we empirically chose to apply LayerNorm in the front.###As in previous works [10, 19, 22], we apply a residual connection for each sublayer to facilitate training:###Many algorithms have been proposed to better understand the sequential history of users [5, 8, 10, 18–21, 36].###After self-attention, MEANTIME operates similar to [2, 19].###Several self-attention based recommendation models have been proposed recently [10, 19].",impact-revealing,highlighting improvements in recommendation tasks using BERT4Rec
1087,,5b351d111ce70795a178104746f5f6adb088f4a0,Enabling Trust with Behavior Metamodels,,,"###…& Laird 2003), we examined a particular type of metamodel called a hierarchical behavioral representation that is inspired by AND/OR trees, HTN planning (Erol, Hendler, & Nau 1994) and GOMS modeling (John & Kieras 1996) to encode the variety of ways in which particular tasks can be accomplished.",impact-revealing,describing a specific type of metamodel and its inspirations
39,5da1a6d447c8f7664606888d,91a4a5a1184a12821e7f5ddf5372b259ded96feb,Directed Statistical Warming through Time Traveling,53e9b054b7602d9703ab5b01,SMARTS: accelerating microarchitecture simulation via rigorous statistical sampling,", evaluate (a) small region(s) of the execution in detail and then extrapolate to the entire execution [28, 34].###Traditionally, functional warming warms up microarchitecture state using all memory references between two consecutive detailed regions, which is very slow [34].###De-Lorean improves simulation speed by 96 × on average compared to SMARTS.###Functional fast-forwarding [34] leverages functional simulation to get to the next representative region, which is slow.###Functional Warming (FW) on the other hand does not incur any storage overhead and is transferable across both hardware and software changes [26, 34].###Figure 5 reports simulation speed normalized to SMARTS.###, 30,000) prior to the detailed region [34].###3MIPS for SMARTS.###These so-called Flex points eliminate warming overhead in SMARTS.###The key observation is that DeLorean tracks the reference curves obtained using SMARTS well.###• SMARTS: Functional Warming (FW) is used to keep the caches warm using functional simulation in-between detailed regions as done in SMARTS [34].###We consider the following sampling strategies in the evaluation: • SMARTS: Functional Warming (FW) is used to keep the caches warm using functional simulation in-between detailed regions as done in SMARTS [34].###Prior research shows that the highest accuracy is achieved for small detailed regions [34]; larger detailed regions will likely make DeLorean even more accurate since small regions make the penalty for mispredicting the outcome of a single key access high.###Functional warming (FW) [26, 34] does not incur any storage overhead, allows for software changes, but is slow.###Note that SMARTS is our reference here because of the full cache warming done in-between detailed regions.###Sampling is a widely used technique to speed up workload analysis and computer architecture performance evaluation by considering a select number of representative detailed regions that are evaluated in detail to then extrapolate from [34].###DeLorean tracks the reference (SMARTS) accurately and accurately predicts performance sensitivity to LLC size.",impact-revealing,highlighting the improvements in simulation speed and accuracy with DeLorean compared to SMARTS
2280,5d3ed2653a55ac61d998598b,077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,roberta: a robustly optimized bert pretraining approach,5a260c8417c44a4ba8a314e3,Mixed Precision Training.,"We train with mixed precision ﬂoating point arithmetic on DGX-1 machines, each with 8 × 32GB Nvidia V100 GPUs interconnected by In-ﬁniband (Micikevicius et al., 2018).",other,reporting hardware and training method used
3110,5e5e18ad93d709897ce2654c,19605cad33f79d3070b7c4b24aa49653ab7c90da,Inductive Matrix Completion Based on Graph Neural Networks,53e9b976b7602d9704566994,A Comprehensive Survey Of Neighborhood-Based Recommendation Methods,"In fact, many neighborhood-based recommender systems (Desrosiers & Karypis, 2011) rely on similar heuristics.",other,acknowledge similarities in neighborhood-based recommender systems
2447,5d9edc8347c8f76646042a37,7e71eedb078181873a56f2adcfef9dddaeb95602,simplifying graph convolutional networks,5c5ce4fd17c44a400fc38983,LanczosNet: Multi-Scale Deep Graph Convolutional Networks.,"Similarly, on QM8 quantum chemistry dataset (Ramakrishnan et al., 2015), more advanced AdaLNet and LNet (Liao et al., 2019) get 0.01 MAE on QM8, outperforming SGC’s 0.03 MAE by a large margin.###Also, both LNet and AdaLNet are unstable on citation networks.###For citation networks, we compare against GCN (Kipf & Welling, 2017) GAT (Velickovic et al., 2018) FastGCN (Chen et al., 2018) LNet, AdaLNet (Liao et al., 2019) and DGI (Velikovi et al., 2019) using the publicly released implementations.###Atwood & Towsley (2016), Abu-El-Haija et al. (2018), and Liao et al. (2019) exploit multi-scale information by raising S to higher order.###, 2018) LNet, AdaLNet (Liao et al., 2019) and DGI (Velikovi et al.###, 2018), applied chemistry (Liao et al., 2019), natural language processing (Yao et al.###…application areas, including but not limited to citation networks (Kipf & Welling, 2017), social networks (Chen et al., 2018), applied chemistry (Liao et al., 2019), natural language processing (Yao et al., 2019; Han et al., 2012; Zhang et al., 2018c), and computer vision (Wang et al., 2018;…###, 2015), more advanced AdaLNet and LNet (Liao et al., 2019) get 0.",other,reporting performance comparisons of various models on the QM8 dataset
477,599c7945601a182cd262a009,6727f574ad8b1c3763be8d58eeaf82c551aa33ef,Generative and Discriminative Text Classification with Recurrent Neural Networks,58d82fc8d649053542fd5c5a,PathNet: Evolution Channels Gradient Descent in Super Neural Networks.,"Discriminative models are known to suffer from catastrophic forgetting when learning sequentially from examples from a single class at a time, and specialized techniques are actively being developed to minimize this problem (Rusu et al., 2016; Kirkpatrick et al., 2017; Fernando et al., 2017).",impact-revealing,highlighting the challenges faced by discriminative models and ongoing efforts to address them
1696,,6613ba76fc35bfac23258382c8e81d30e2ff6341,Predicting transactive memory systems in multidisciplinary teams : The interplay between team and professional identities,,,"###According to the social identity approach, social identities are definitions of the self based on group memberships that are cued by contextual factors (Tajfel & Turner, 1979; Turner, 1985).###Contents lists available at ScienceDirect
Journal of Business Research
aspects of the self are derived from memberships to social groups, the extent to which people identify and internalize group memberships can shape and influence coordinated group efforts (Tajfel & Turner, 1979; Turner, 1985).###In accord with this argument, we draw upon social identity/self-categorization theories (Tajfel, 1978; Tajfel & Turner, 1979; Turner, Hogg, Oakes, Reicher, & Wetherell, 1987) to build on prior work showing the predictive role of communication on TMS.3
Multidisciplinary teams are, by definition,…",impact-revealing,providing context for social identity theory
1880,,b8079759d3075b1b793e0d2023dff70bfeb4d1be,Acute nutritional ketosis: implications for exercise performance and metabolism,,,"###In a series of experiments in the 1960s, Cahill demonstrated the importance of cerebral ketone body oxidation in starvation, where up to 60% of the brain energy needs are derived from ketones, replacing glucose as its primary fuel [26-28].",impact-revealing,highlighting historical findings on cerebral ketone body oxidation
1234,,d3f7c04d604c80590b9611e7d142697099a9b6e2,Cardiovascular Effects Of Chronic Nitric Oxide Synthase Inhibition In Genetically Hypertensive Rats,,,"###N-Nitro-L-arginine methyl ester causes constriction of arteries and vascular beds and induces a hypertensive effect in various species, including humans, effects that are reversible by L-arginine, the substrate for NO production.(3,8) The status of NO in experimental hypertension is unclear because there is conflicting evidence between, and sometimes within, the different hypertensive models.###Nitric oxide (NO) is a potent vasodilator responsible for maintaining a reduced arterial tone in the systemic, pulmonary and coronary circulation of humans and other species.1–3 In addition, NO regulates blood fluidity and local cell growth.4 It is not surprising that NO and endothelial dysfunction have been implicated in a number of different cardiovascular diseases, including hypertension, atherosclerosis and hypercholesterolaemia.5,6 Essential hypertension in humans is characterized by a decrease in both basal and stimulated release of NO, as evidenced by a blunted response to NO synthase (NOS) inhibitors and acetylcholine (ACh) in the forearm vasculature of essential hypertensives compared with normotensive controls.1,5 Acetylcholine is commonly used as a test for endothelial function because its vasorelaxant action on vascular smooth muscle is mediated via the release of endothelial-derived factors, such as NO.7 Nv-Nitro-L-arginine methyl ester (L-NAME), an L-arginine analogue, is a non-selective inhibitor of the different isoforms of NOS.8 NG-Nitro-L-arginine methyl ester causes constriction of arteries and vascular beds and induces a hypertensive effect in various species, including humans,3,9–11 effects that are reversible by L-arginine, the substrate for NO production.3,8 The status of NO in experimental hypertension is unclear because there is conflicting evidence between, and sometimes within, the different hypertensive models.12 Evidence regarding the status of the L-arginine/NO system in the New Zealand genetically hypertensive (GH) rat model is limited.###N-Nitro-L-arginine methyl ester (L-NAME), an L-arginine analogue, is a non-selective inhibitor of the different isoforms of NOS.(8) N-Nitro-L-arginine methyl ester causes constriction of arteries and vascular beds and induces a hypertensive effect in various species, including humans, effects that are reversible by L-arginine, the substrate for NO production.",impact-revealing,discussing the role of nitric oxide in hypertension and related cardiovascular diseases
3089,5ec49a639fced0a24b4de922,0d965ed237a3b4592ecefdb618c29f63adedff76,Towards Debiasing Sentence Representations,5c4880dd7301396d1ffc813b,Law and Word Order: NLP in Legal Tech.,"Machine learning tools for learning from language are increasingly deployed in real-world scenarios such as healthcare [27], legal systems [7], and computational social science [2].",other,highlighting the application of machine learning tools in various real-world scenarios
2313,5e5e18e493d709897ce3a0f2,7a064df1aeada7e69e5173f7d4c8606f4470365b,albert: a lite bert for self-supervised learning of language representations,5d0b00668607575390fc2add,Efficient Training of BERT by Progressively Stacking,"Networks with 3 or more layers are trained by fine-tuning using the parameters from the depth before (e.g., the 12-layer network parameters are fine-tuned from the checkpoint of the 6-layer network parameters).5 Similar technique has been used in Gong et al. (2019).###We choose to use the same E for all word pieces because they are much more evenly distributed across documents compared to whole-word embedding, where having different embedding size (Grave et al. (2017); Baevski & Auli (2018); Dai et al.",other,acknowledge prior work on fine-tuning techniques
2870,5f1022a091e01168a7d6fc4f,04e234c7e4f300b4e8a16370728d875f9f484b39,learning visual context by comparison,5a4aef9e17c44a2190f7a308,Thoracic Disease Identification and Localization with Limited   Supervision,"Some works observe that although getting annotations for chest X-rays is costly, it is still helpful to leverage both a small number of location annotations and a large number of class-level labels to improve both localization and classification performances [23].###It is used as a benchmark dataset in previous studies [41,12,29,6,25,23,32,10].###With the recent presence of large-scale chest X-ray datasets [37,18,19,3], there has been a long line of works that find thoracic diseases from chest X-rays using deep learning [41,12,23,29].###[23] ResNet-v2-50 BCE 299 80.",other,highlighting the importance of leveraging annotations for improving performance in chest X-ray analysis
450,5c04966a17c44a2c74708401,51203e9d5620abdcdf6c9be93b1e221e79cda67d,Transfer Learning of Language-independent End-to-end ASR with Language Model Fusion,57d063e8ac44367354295042,"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",",hT ′) (T ′ ≤ T ), interleaved with subsampling layers to reduce the computational complexity [26].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1920,,e10587ebb50129adb5874110630404d8d1fe7e36,Efficient 3D TOF PET Reconstruction Using View-Grouped Histo-Images: DIRECT - Direct Image Reconstruction for TOF,,,"###reconstruction approaches are list-mode approaches [ 6 ], [7], that accurately (geometrically and statistically) treat each TOF event one by one.###Several aspects of the proposed approach were inspired by TOF works from early 80’s, such as placing events directly into image space rather than into projection space (e.g., [2], [8]), merging/grouping events into set of views and tilts (e.g., [9]), using maximum-likelihood (ML) based statistical reconstruction (e.g., [ 6 ], [8], [10]).",impact-revealing,acknowledge inspiration from prior TOF works
2662,5ea2b8c391e01167f5a89e2d,38643c2926b10f6f74f122a7037e2cd20d77c0f1,Supervised Contrastive Learning,53e9b844b7602d970440513c,Imagenet: A Large-Scale Hierarchical Image Database,"y of poor margins [14,30], leading to reduced generalization performance. In practice, however, most proposed alternatives do not seem to have worked better for large-scale datasets, such as ImageNet [11], as evidenced by the continued use of cross-entropy to achieve state of the art results [9,10,51,26]. Many proposed improvements to regular crossentropy in fact involve a loosening of the deﬁnition o###sed contrastive loss by measuring classiﬁcation accuracy on ImageNet and robustness to common image corruptions [22]. After training the embedding network with supervised contrastive loss on ImageNet [11], we replace the projection head of the network with a a new randomly initialized linear dense (fully connected) layer. This linear layer is trained with standard cross entropy while the parameters of",other,highlighting the limitations of proposed alternatives in large-scale datasets
624,56d91a37dabfae2eee74e343,cacc42beffa7e259b60f26dcd886320215ae63b2,Selective cache ways: on-demand cache resource allocation,53e9ada5b7602d97037a06e3,Architectural and compiler support for energy reduction in the memory hierarchy of high performance microprocessors,The L-Cache [13] similarly reduces switching activity by holding loop-nested basic blocks designated by the compiler and providing these to the pipeline in place of the larger L1 Icache.,impact-revealing,reporting on a method for reducing switching activity in cache systems
2866,53e9a232b7602d9702b3a1a9,327722247ffc70a0d51f5c2246bc9a53c0e7daa3,Accurate branch prediction for short threads,5c7804144895d9cbc672cf7b,A Minimal Dual-Core Speculative Multi-Threading Architecture,"Wrong-path prediction of speculative threads has been presented in [42], but we do not assume that hardware support here.",other,acknowledge prior work on wrong-path prediction
4050,5e3d353b3a55ac4de4104f40,3024f58826a5bce3378af94f677e8fb90cbb49e0,LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation,5aed148b17c44a4438154ee8,Latent Relational Metric Learning via Memory-based Attention for Collaborative Ranking.,"The recent neural recommender models like NCF [19] and LRML [34] use the same embedding component, while enhance the interaction modeling with neural networks.",other,highlighting advancements in neural recommender models
3232,5de4e0b73a55ac2224ba53a6,a75649771901a4881b44c0ceafa469fcc6e6f968,how can we know what language models know?,5a9cb66717c44a376ffb8ac1,Deep contextualized word representations.,"KnowBert underperforms BERT on LAMA, which is opposite to the observation made in Peters et al. (2019). This is probably because that multi token subjects/objects are used to evaluate KnowBert in Peters et al.###…task for feature extractors , where the hidden vectors learned through a language modeling objective are then used in ∗ The first two authors contributed equally. down-stream language understanding systems (Dai and Le, 2015; Melamud et al., 2016; Peters et al., 2018; Devlin et al., 2019).###down-stream language understanding systems (Dai and Le, 2015; Melamud et al., 2016; Peters et al., 2018; Devlin et al., 2019).",other,highlighting the performance comparison between KnowBert and BERT
1423,,034695810afa6fafdf696f2030d4ed1fb8e42b86,Addressing Distribution Shift in Online Reinforcement Learning with Offline Datasets,,,"###Our work builds on a conservative Q-learning method [18], so as to allow fast, unconstrained policy updates once fine-tuning starts.###On the other hand, a more recent state-of-the-art offline RL algorithm, conservative Q-learning (CQL) [18], does not require explicit behavior modeling, and one might expect CQL to be amenable to fine-tuning.###[18], we trained offline RL agents for 1000 epochs without early stopping.###CQL [18] is an offline RL algorithm that learns a lower bound of the Q-function Qθ(s, a), in order to prevent extrapolation error – value overestimation caused by bootstrapping from out-of-distribution actions.###Accordingly, various offline RL methods have been developed, some of which are often capable of training agents that are more performant than the best data-generating policy [1, 6, 14, 17, 18, 27, 31, 32].###To address this problem, numerous approaches have been proposed, including policy constraint methods that constrain the policy to stay close to the modeled dataset behavior [6, 17, 27, 31], and conservative Q-learning methods that train the Q-function to be pessimistic in the unseen regime [14, 18, 32].",impact-revealing,discussing advancements and methodologies in offline reinforcement learning
2891,5ecb57199e795ec6f2ba59cc,512f34906ddaefe885af2e5eec9b2b3b50ffd377,deep entity matching with pre-trained language models,5687e6400cf2afdcd165ff31,Semantic-aware blocking for entity resolution,"EM solutions have tackled the blocking problem [2, 6, 14, 28, 45] and the matching problem with rules [9, 13, 38, 44], crowdsourcing [16, 18, 43], or machine learning [37, 8, 3, 16, 20].",other,acknowledge existing solutions to blocking and matching problems
3095,5e15adcb3a55ac47ab5b0b8c,aa63ac11aa9dcaa9edd4c88db18bec87e0834328,Graph Transformer Networks,53e99796b7602d9701f617db,Graph Kernels,"Conventionally, hand-crafted features have been used such as simple graph statistics [2], graph kernel [34], and engineered features from a local neighbor structure [23].",other,acknowledge traditional methods in feature engineering
3899,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",55a4a68365ceb7cb02d5432b,The economy of brain network organization,"The network links (edge weights) are considered to be known a priori and represent the structural connectivity or the functional coherence between brain regions [170], [171].",other,providing context for network connectivity in brain regions
3854,5fd8acf991e0119b22c1f38d,5b9d8bcc46b766b47389c912a8e026f81b91b0d8,Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting,5c7571aaf56def9798796f18,Dilated Convolutions for Modeling Long-Distance Genomic Dependencies,"s the input’s time dimension sharply, seeing the n-heads weights matrix (overlapping red squares) of Attention blocks in Fig.(3). Inspired by the dilated convolution (Yu, Koltun, and Funkhouser 2017; Gupta and Rush 2017), our “distilling” procedure forwards from j-th layer into (j+ 1)-th layer as Xt j+1 = MaxPool ELU( Conv1d( [Xt j] AB ) ) ; (5) where [] AB contains the Multi-head ProbSparse selfattention and the ess###Inspired by the dilated convolution (Yu, Koltun, and Funkhouser 2017; Gupta and Rush 2017), our “distilling” procedure forwards from j -th layer into ( j + 1) -th layer as where [ · ] AB contains the Multi-head ProbSparse self-attention and the essential operations in attention block, and Conv1d (…",other,providing context for the proposed method
3645,5fb24ee191e01186d3f5decc,050fecf6e3e21faeeb7629f179ba839c4dfd1ead,EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints,5bbacb9e17c44aecc4eaffeb,An Operation Sequence Model for Explainable Neural Machine Translation.,", 2007), or the Operation Sequence Model (Durrani et al., 2015; Stahlberg et al., 2018), which views translation as a sequence of translation and reordering operations over bilingual minimal units.###This view is reﬂected in architectures ranging from the word-based IBM models (Brown et al., 1990), sentence-level models that generate a bag of target words that is reordered to construct a target sentence (Bangalore et al., 2007), or the Operation Sequence Model (Durrani et al., 2015; Stahlberg et al., 2018), which views translation as a sequence of translation and reordering operations over bilingual minimal units.",other,acknowledge existing translation models and approaches
1359,,928a68c36ec90626f638114f6b4ffeca6616d411,Developing a framework for understanding information literacy in the 21st century: a review of literature,,,"###Technological proficiency is the knowledge and ability to use technology flexibly and creatively for particular purposes (Eisenberg, 2008).###When individuals produce and process information in communities, there is usually a consensus on how to interpret information as a community (Elmborg, 2006). Harris (2008) refers to two types of communities for information literacy: communities of practice and learning communities.###Technological proficiency is the knowledge and ability to use technology flexibly and creatively for particular purposes (Eisenberg, 2008). The new understanding of technological proficiency led some researchers to explore the potentials or affordances of technology for specific purposes. Shand, Winstead, and Kottler (2012) organize digital tools into five categories: communication, collaboration, presentation, organization, and critical thinking.###Technological Capabilities and the Big Six Model (Eisenberg, 2008)###Eisenberg (2008), however, integrates the use of digital tools specifically into information literacy.###Looking at digital tools from the perspectives of the potentials they offer allows us to move from isolated computer skills to integrated information and technology skills, where isolated digital tools become powerful information tools (Eisenberg, 2008).###Technological proficiency is not just learning how to use a particular digital tool, but recognizing how it contributes to accomplishing a task and fulfilling a purpose in information literacy (Eisenberg, 2008).###Information literacy has been widely recognized as one of the essential life, learning and workplace skills (Eisenberg, 2008), and according to UNESCO, as a ""basic human right in a digital world"" (Alexandria Proclamation, 2005).###The I-LEARN model, proposed by Neuman (2011), is similar to the Big Six (Eisenberg & Berkowitz , 1990) and ISP (Kuhlthau, 1991) models in that it provides a set of skills or processes to describe information literacy. However, Neuman (2011) emphasizes the concept of learning.###Several key researchers have developed information literacy models (Eisenberg & Berkowitz, 1990; Kuhlthau, 1991; Neuman, 2011). This review will examine the models that have been used and referred to the most in educational contexts: Eisenberg & Berkowitz’ (1990) “Big Six Model,” Kuhlthau's (1991) “Information Search Model” (ISP), and Neuman's (2011) “I-LEARN model.",impact-revealing,reviewing and summarizing various information literacy models and their significance in education
2667,573697636e3b12023e64a731,793dd71c59e0f2222d60fc5a43871cf919eeb1a2,runtime-driven shared last-level cache management for task-parallel programs,53e9aaa3b7602d97034208a8,Why nothing matters: the impact of zeroing,"Researchers have explored compiler and profile-based techniques to improve cache utilization through these hints [8, 10, 33, 34, 38, 39].###explored the benefits of using non-temporal accesses to reduce the cache pollution effects of zero-intializtions in virtual machine-managed applications [39].",other,acknowledge existing techniques for improving cache utilization
2054,,a0ccbbaa9a01d851cb586acfeeb3691a1daae89d,Recurrence in meningeal hemangiopericytomas.,,,"###[4], the average time before the first recurrence was 47 months, the average time to extraneural metastasis was 99 months, and the average survival period was 84 months.###Hematoxylin and eosin staining in our cases was almost the same as that reported by Guthrie et al [4], who noted that the histologic diagnosis of the tumor was not related to survival or recurrence and did not change with recurrence.###Guthrie et al [4] emphasized the importance of complete removal at the first operation and subsequent radiation therapy to prolong the time to recurrence and extend survival.###Clinically, meningeal hemangiopericytomas are much more aggressive than typical meningiomas, with a high rate of recurrence and distant metastases [4,7,15].###Meningeal hemangiopericytoma is a rare tumor and, in contrast to ordinary meningiomas, has a propensity for both local recurrence and extraneural metastasis [4,7,15].###Some series note a relationship between the prognosis and histologic characteristics, but others show little or none [4,15].",impact-revealing,highlighting the importance of complete removal and subsequent radiation therapy in treatment outcomes
3765,5c234870da562935fc1d4db3,0b6a531754e67379518a946f3a3cf685f59358cc,CritICs Critiquing Criticality in Mobile Apps,5a260c2817c44a4ba8a23ad7,Effective Optimization Of Branch Predictors Through Lightweight Simulation,"…have been extensively addressed in high end processors through numerous techniques - smart i-cache management (e.g. [ 63]–[65], [97]–[99]) prefetching (e.g. [ 70]– [74]), branch prediction (e.g [66]–[69]), instruction compression [100] SIMD [38], [39], VLIW [40], vector processing [41], etc.###One may note that numerous prior hardware enhancements proposed to address the Fetch stage problems, including larger and more intelligently managed i-caches [63]–[65], better branch predictors [66]–[69], and/or instruction prefetchers [70]– [74].",other,acknowledge existing techniques in high-end processors
3037,5f03f3b611dc83056223206d,75c8466a0c1c3b9fe595efc83671984ef95bd679,XGNN: Towards Model-Level Explanations of Graph Neural Networks,5bdc31b417c44a1f58a0ba6c,How Powerful are Graph Neural Networks?,"ODUCTION Graph Neural Networks (GNNs) have shown their effectiveness and obtained the state-of-the-art performance on different graph tasks, such as node classification [11, 37], graph classification [39, 47], and link prediction [46]. In addition, extensive efforts have been made towards different graph operations, such as graph convolution [13, 16, 19], graph pooling [20, 44], and graph attention [10, 3###es based on these matrices. Even though there are several variants of GNNs, such as graph convolution networks (GCNs) [19], graph attention networks (GATs) [37], and graph isomorphism networks (GINs) [39], they share a similar feature learning strategy. For each node, GNNs update its node features by aggregating the features from its neighbors and combining them with its own features. We take GCNs as ",other,acknowledge effectiveness and state-of-the-art performance of Graph Neural Networks
2751,5b67b45517c44aac1c86078b,e62ddf27659bc131968d2dcc3e2bd59de98c6917,"Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop.",573696016e3b12023e514a63,Ethnicity sensitive author disambiguation using semi-supervised learning,[17] use a classi er to learn pairwise similarity and perform semi-supervised hierarchical clustering to generate results.###[17]: This method rst trains a pairwise distance function base on a set of carefully designed similarity features.,other,reporting prior findings on pairwise similarity and clustering methods
1600,,d520d3e210da76dcc7732751a1d2b3e4b39060de,Breaking the HISCO Barrier: Automatic Occupational Standardization with OccCANINE,,,###15 This approach (although here in a much simpler implementation) is inspired by TextAttack by Morris et al. (2020).,impact-revealing,drawing inspiration from a prior work
285,5f8cf5159e795ea21aee7f07,0ee0801ba010a441403f9ed666ef9bf006b3aa07,Adaptive Universal Generalized PageRank Graph Neural Network,5b8c9f4a17c44af36f8b71a9,Contextual Stochastic Block Models.,"Theorem A.7 (Informal main result in Deshpande et al. (2018)).###Due to space limitation we refer the interested reader to (Deshpande et al., 2018) for a review of all formal theoretical results and only outline the cSBM properties needed for our analysis.###The information-theoretic limits of reconstruction for the cSBM are characterized in Deshpande et al. (2018).###…to optimally learn on both homophilic and heterophilic graph should have similar performances for φ and −φ. Due to space limitation we refer the interested reader to (Deshpande et al., 2018) for a review of all formal theoretical results and only outline the cSBM properties needed for our analysis.###In order to test the ability of label learning of GNNs on graphs with arbitrary levels of homophily and heterophily, we propose to use cSBMs (Deshpande et al., 2018) to generate synthetic graphs.###One reason for using the cSBM to generate synthetic data is that the information-theoretic limit of the model is already characterized in Deshpande et al. (2018).###To test the performance of GPR-GNN on homophilic and heterophilic node label patterns and determine the trade-off between node and topological feature exploration, we first describe the recently proposed contextual stochastic block model (cSBM) (Deshpande et al., 2018).",impact-revealing,referring to prior theoretical results and methods for analysis
3752,5ecbc8eb9fced0a24b52a39b,2b9514be4679f68f97bbcf9671053ac2f03df2e4,Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning,5b8c9ed917c44af36f8ae7d5,Trojaning Attack on Neural Networks.,"Prominent examples of these attacks are methods for crafting adversarial examples [6, 32], backdooring neural networks [10, 15], and inferring properties from learning models [9, 27].",other,providing context on prominent adversarial attack methods
1259,,5b66a00ab0c2b737a4704e27fbca37accb3febeb,The quality of parent–teacher relationships in inclusive preschools,,,"###…of the main factors that affect children, parent and teacher outcomes (Elicker et al., 2013; Serpell & Mashburn, 2012; Swartz & Easterbrooks, 2014; Webster-Stratton et al., 2001), (b) almost all preschool teachers emphasized the importance of an effective relationship between parents and…###To summarize, considering the facts that (a) the parent–teacher relationship is one of the main factors that affect children, parent and teacher outcomes (Elicker et al., 2013; Serpell & Mashburn, 2012; Swartz & Easterbrooks, 2014; Webster-Stratton et al., 2001), (b) almost all preschool teachers emphasized the importance of an effective relationship between parents and teachers of young CWD (Akalın et al.",impact-revealing,highlighting the importance of parent-teacher relationships in educational outcomes
2326,58d82fd2d649053542fd75bc,e0b207e96351671453aa8bf05b7225c8a340a0b2,towards end-to-end speech recognition with deep convolutional neural networks,5550465645ce0a409eb5d85a,Improving deep neural network acoustic models using generalized maxout networks,"Another type of activation function which has been shown to improve the results for the task of speech recognition [16, 24, 25, 26] is the maxout function [27].",other,highlighting the significance of the maxout function in speech recognition
834,53e99822b7602d9702041bcd,5e4c84225338263e74c0a41eac4938f9a18fedb5,spatial memory streaming,53e99ce5b7602d9702599579,Accurate and Complexity-Effective Spatial Pattern Prediction,"patterns with the code and/or data address that initiates the pattern [4,17].###Whereas existing spatial pattern prefetching designs are effective for desktop/engineering applications [4], the only practical implementation evaluated on server workloads provides less than 20% miss rate reduction [17].###We show that the cache-coupled structures used in previous work ([4,17]) are suboptimal for observing spatial correlation.###We formalize our notion of spatial correlation similar to prior studies of spatial footprints [4,17].###For SPEC CPU 2000 applications, PC+address indexing can be approximated by combining the PC with a spatial region offset [4,17].###Prior studies of spatial predictors [4,17] advocate predictor indices that include address information.###Past predictors [4,17] couple the predictor training structure to a sectored (i.###To mitigate this disadvantage, the spatial footprint predictor [17] employed a decoupled sectored cache [22], whereas the spatial pattern predictor [4] provided a logical sectored-cache tag array alongside a traditional cache.###For scientific applications, we corroborate the conclusions of prior work [4] that indicate PC+offset indexing generally approaches the peak coverage achieved by the PC+address indexing scheme.",impact-revealing,highlighting limitations in existing spatial pattern prefetching designs
1477,,e16e546d53bac1b2f20340fd59c683b82209678e,L*ReLU: Piece-wise Linear Activation Functions for Deep Fine-grained Visual Categorization,,,"###Therefore, AFs such as ReLU , ELU , SELU , and Swish are well suited for CGVC tasks.###Similarly, Parametric ELU [43] evades the vanishing gradient problem and allows for precisely controlling the bias shift by learning parameters from the data.###Similar also applies to ELU .###Modeling the presence of features is already realized very well by existing AFs using linear ( ReLU , ELU ) or quasi-linear ( Swish ) functions for the positive domain.###However, both ELU and SELU are bounded in the negative part, which is not a desired property for the FGVC task.###In these cases, this is achieved via the positive part of the function ( i.e ., for x > 0 ) being either a linear function ( ReLU , LReLU , ELU ) or a ”quasi-linear” function ( Swish ).###Bothshortcomings of ReLU can be avoided by using Exponential Linear Unit ( ELU ) [7], which is robust to noise and eliminates the bias shift in the succeeding layers by pushing the mean activation value towards zero.###The idea was later extended by introducing Scaled Exponential Linear Unit SELU [6], showing that the proposed self-normalizing network converges towards a normal distribution with zero mean and unit variance.###In this way, f(x) codes the degree of presence or absence of a feature in the input [7]: a feature is present if f(x) > 0 and absent if f(x) ≤ 0.###Both shortcomings of ReLU can be avoided by using Exponential Linear Unit (ELU) [7], which is robust to noise and eliminates the bias shift in the succeeding layers by pushing the mean activation value towards zero.###By returning a bounded exponential value for negative inputs ELU is saturated at a predeﬁned threshold.###In this paper, we address the FGCV problem by using a proper AF, modeling the degrees of presence and absence of features [7].###We compared our approach to existing AFs, known to yield good results in practice, namely ReLU [15], ELU [7], SELU [6], and Swish [11].",impact-revealing,discussing the suitability of activation functions for CGVC tasks
2179,,af1b5755b49bce47f5fd03c5319150b3b3d73c57,Soft error considerations for computer web servers,,,###Cloud computing builds on the established trends to reduce the cost of the delivery of services while increasing the speed and agility with which services are deployed [21].,impact-revealing,highlighting the benefits of cloud computing in service delivery
2913,5550446645ce0a409eb4d54a,06e122f475a21d92dba137609c40f35690217475,Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification,53e9a946b7602d9703298fd9,Thumbs up?: sentiment classification using machine learning techniques,"The features used in traditional learning-based methods (Pang et al., 2002; Nakagawa et al., 2010) are independent to the targets, hence the results are computed despite what the targets are.",other,highlighting limitations in traditional learning-based methods
1261,,0777990cc1f38abfa1fa730f1f3f72f6933b670c,Discounting Time and Time Discounting: Subjective Time Perception and Intertemporal Preferences,,,"###Because of the potential importance of these findings, we briefly present three follow-up experiments that test the robustness of our findings (for further information, see the Web Appendix at http://www.marketingpower.com/jmraug09).###For this purpose, we defined two subjective time perception functions, using objective time and log-transformed time, consistent with the Weber– Fechner law:
5When using the BIC, a difference greater than 10 indicates strong evidence that one model fits better than the other (Raftery 1995).###However, unresolved issues are whether estimation of prospective duration is similarly biased and what the implications of such subjective judgments are for intertemporal preferences.###For simplicity, we used the logarithmic function associated with the Weber–Fechner law instead of Stevens’s power law (and the power function associated with it).###Weber’s law states that the threshold of discriminating two stimuli, such as brightness, loudness, or duration, increases monotonically as the intensity of stimuli increases, and the Weber–Fechner law depicts the relationship between physical stimulus and the corresponding human sensation as a logarithmic function (Dehaene 2003; Grondin 2001).2
Our perspective builds on these ideas and is psychologically distinct from previous explanations because it separates discounting the outcome itself from the perception of the time interval relevant to that decision.###The first part included the priming task, which we implemented by asking participants to estimate the duration of seven activities for the duration-priming condition and the number of calories contained in seven food items in the control condition (for detailed information, see Section 4 in the Web Appendix at http://www.marketingpower.com/ jmraug09).###If his
2For simplicity, we used the logarithmic function associated with the Weber–Fechner law rather than the power function associated with Stevens’s (1957) power law.###Later, when introducing Herrnstein’s matching law, Gibbon (1977) suggested that “this law simply represents the Weber–Fechner law” (cited in Ainslie and Haslam 1992, p. 72).###Our view, though developed independently, is consistent with recent theoretical notes published in Medical Hypotheses that argue that error in time estimation following the Weber–Fechner law can explain both subadditive discounting (Takahashi 2006) and hyperbolic discounting
(Takahashi 2005).###This design enables us to directly examine the pattern of hyperbolic discounting with respect to objective and subjective time, to further eliminate alternative accounts for our explanation, and to directly examine whether subjective time perception indeed follows the nonlinear logarithmic function implied by the Weber–Fechner law.###Keywords: hyperbolic discounting, present bias, time perception, Weber–Fechner law
Discounting Time and Time Discounting: Subjective Time Perception and Intertemporal Preferences
Many consumer decisions involve trading off costs and benefits over time.",impact-revealing,highlighting the importance of follow-up experiments to test findings
67,5ee9f15b91e01152af022ce0,bf2174c69f84f4e57813e0bed4571c6dbff123ed,Automatic Validation of Textual Attribute Values in E-commerce Catalog by Learning with Limited Labeled Data,53e9b60eb7602d9704165d84,Deep Generative Stochastic Networks Trainable by Backprop.,"−µj s c )) 2σ2 j x s c ) − 1 2 ! , (11) where d is the dimension of z. Thus, we only need to calculate category loss term. To enable distribution qϕ c (z|xq c )differentiable, we follow previous work [2, 3, 19] to use reparameterization trick to parameterize z. Reparameterization Trick Instead of directly sampling from a complex distribution, we can reparametrize the random variable as a deterministic trans",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1231,,af9a97279f569ed2e4185320769cd890a2b99c89,Amiodarone: Reevaluation of an Old Drug,,,"###Although amiodarone is also effective for preventing atrioventricular nodal reentrant tachycardia, radiofrequency ablation is a useful approach that is widely used as a first-line therapy (49, 50).",impact-revealing,highlighting the effectiveness of radiofrequency ablation as a first-line therapy
896,5b8c9f4a17c44af36f8b6ef6,97cc458d7dcb0c60c81f4de07ccca5c5838c9071,A Dynamic Service Migration Mechanism in Edge Cognitive Computing,5a9cb65217c44a376ffb7419,"Connecting the Edges: A Universal, Mobile-Centric, and Opportunistic Communications Architecture.",The work in [1] introduced a system that can pervasively operate in any networking environment and allows for the development of innovative applications by pushing services near to the edge of the network.###The massive proliferation of personal computing devices is opening up new human-centered designs that blur the boundaries between humans and machines [1].,impact-revealing,highlighting the significance of a system that operates in diverse networking environments
1061,,a8df8e82b8af6316cd993a67d32ea2b31a466af7,Analysis of Genomic Islands and Other Features in Draft Versus Complete Bacterial Genomes,,,"###In particular, HGT has been problematic for the production of a single, universal tree of life (Doolittle & Bapteste, 2007).",impact-revealing,highlighting challenges in creating a universal tree of life
985,,be42f6884f4c4dba3a6291cedfa7dddd8d722db0,"A qualitative description of community service, business, and organization perspectives on mental illness and inclusion",,,"###Stigmatization is highly prevalent internationally, with the majority of populations in many countries demonstrating moderate to high levels of stigmatizing views (Krajewski, Burazeri, & Brand, 2013; Pescosolido, 2013).###Unlike previous work that has emphasized specific labels or types of mental illness in determining response (Pescosolido, 2013), these narratives emphasized the importance of where responsibility for the illness and its outcomes were located.###types of mental illness in determining response (Pescosolido, 2013), these narratives emphasized",impact-revealing,highlighting the prevalence of stigmatization and its implications
679,53e9bafbb7602d9704734d1d,99a56abcea69a05eaa8effdfe1be01c283b08f4a,branch history matching: branch predictor warmup for sampled simulation,53e9bd24b7602d97049b0567,Accelerated warmup for sampled microarchitecture simulation,"The second paper mentioning branch predictor warmup is by Haskins and Conte [17,20] in which they propose memory reference reuse latency (MRRL).###The first paper dealing with branch predictor warmup was by Conte et al. [1].###(Note that the MRRL approach [17,20] corresponds to a zero BHM history length.",impact-revealing,reporting prior findings on branch predictor warmup
2196,5d9c5e4d3a55ac916a95fbd8,038b2d276e3c04e4a6ef0c04ace98e9621edf5bc,Negative Sampling in Variational Autoencoders,5c8d29934895d9cbc6412a52,Distribution Matching in Variational Inference,"Following [1], for grayscale images, we use the encoder architecture described in [14].###Also, as in [14], all of the models are trained with the RMSProp optimizer with learning rate set to 10−4.",other,reporting methodology and model training details
3995,5eede0b091e0116a23aafbd3,91fb815361fdbf80ff15ce4d783a41846bd99232,GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training,5c0f85b2da562944ac91331b,A Scalable Permutation Approach Reveals Replication and Preservation Patterns of Network Modules in Large Datasets.,"As for social graphs, we collect Facebook and IMDB datasets from NetRep [44], as well as a LiveJournal dataset from SNAP [3].###As for academic graphs, we collect the Academia dataset from NetRep [44] as well as two DBLP datasets from SNAP [62] and NetRep [44], respectively.",other,reporting data sources for social and academic graphs
211,5ac1829d17c44a1fda917e29,ef2ec69e7c94b4194ba01719ac76d4595e6b4bdf,L2-Nonexpansive Neural Networks,599c7954601a182cd2631079,Parseval Networks: Improving Robustness to Adversarial Examples.,"For example, Parseval networks (Cisse et al., 2017) bound the Lipschitz constant by requiring each linear or convolution layer be composed of orthonormal ﬁlters.###To be fair, the scheme of Cisse et al. (2017) has an advantage of being nonexpansive with respect to any L p -norm.###Because splitting is not modiﬁed in Cisse et al. (2017), their scheme may seem approximately equivalent to ours if a common t parameter is used for (9) and (10).###The above is where our linear and convolution layers differ from those in Cisse et al. (2017): they requireWW to be an identity matrix, and it is straightforward to see that their scheme is only one special case that makes b (W ) equal to 1.###Parseval networks (Cisse et al., 2017) can be viewed as models without L c term, norm-pooling or two-sided ReLU, and with a more restrictive scheme for weight matrix regularization.###For example, Parseval networks (Cisse et al., 2017) bound the Lipschitz constant by requiring each linear or convolution layer be composed of orthonormal filters.###Instead we use an upper bound: 4 The above is where our linear and convolution layers differ from those in Cisse et al. (2017): they require WW T to be an identity matrix, and it is straightforward to see that their scheme is only one special case that makes b ( W ) equal to 1.###ResNet-like reconvergence is referred to as aggregation layers in Cisse et al. (2017) and a different formula was used: ) where α ∈ [0 , 1] is a trainable parameter.###The work on Parseval networks (Cisse et al., 2017) shows that it is possible to control Lipschitz constants of neural networks through regularization.###The reported robustness results of Cisse et al. (2017), however, are much weaker than those by adversarial training in Madry et al. (2017).###Parseval networks (Cisse et al., 2017) can be viewed as models without Lc term, norm-pooling or two-sided ReLU, and with a more restrictive scheme for weight matrix regularization.###There are previous works that achieve the first condition (Cisse et al., 2017; Hein & Andriushchenko, 2017) or bound responses to input perturbations by other means (Kolter & Wong, 2017; Raghunathan et al.###from a set of new techniques: our weight regularization, which is key in enforcing the first condition, allows greater degrees of freedom in parameter training than the scheme in Cisse et al. (2017); a new loss function is specially designed for the second condition; we adapt various layers in new ways for the third condition, for example norm-pooling and two-sided ReLU, which will be presented later.###There are previous works that achieve the ﬁrst condition (Cisse et al., 2017; Hein & Andriushchenko, 2017) or bound responses to input perturbations by other means (Kolter & Wong, 2017; Raghunathan et al., 2018; Haber & Ruthotto, 2017).###…that are not part of f ( x 2 ) and therefore better preserve distances. because In contrast, splitting is not modiﬁed, at reconvergence the scheme of Cisse et al. (2017) must apply the shrinking factor of 1 − α on all outputs of convolution layers, regardless of whether a channel is part of…###…our weight regularization, which is key in enforcing the ﬁrst condition, allows greater degrees of freedom in parameter training than the scheme in Cisse et al. (2017); a new loss function is specially designed for the second condition; we adapt various layers in new ways for the third…",impact-revealing,comparing and contrasting the proposed method with Parseval networks
3438,5eede0b791e0116a23aafe75,150f95f9c73820e0a0fa1546140e9f2bdfd25954,temporal graph networks for deep learning on dynamic graphs,53e9a0fbb7602d97029e6222,A particle-and-density based evolutionary clustering method for dynamic networks,"In the latter case, the embeddings are either aggregated by taking a weighted sum [67, 74], ﬁt to time series models [30, 24, 11, 44], used as components in RNNs [55, 45, 41, 69, 8, 54, 48], or learned by imposing a smoothness constraint over time [33, 25, 67, 75, 73, 57, 22, 17, 50].",other,acknowledge variations in embedding aggregation methods
1827,,75853996fe17aa87c261680cff7ba9aa6d51faad,Clinical correlates of augmentation/combination treatment strategies in major depressive disorder,,,"###Probably, the use of lithium in the clinical practice is limited by the need of continuous plasma level measurements to ensure the achievement of the therapeutic window and due to the anticipation of adverse effects (19, 20).",impact-revealing,highlighting limitations in the clinical use of lithium
1241,,3f1bfcfd29fcf8620026f7f0ab9ec658bc50fb62,Effects of random whole-body vibration on postural control in Parkinson's disease,,,"###In general it occurrs in the late and most advanced stages of the disease (Marchese, Bove, and Abbruzzese 2003), but falls in the relatively early course of PD also have been reported (Bloem, Grimbergen, Cramer, et al. 2001).###Research in Sports Medicine, 13: 243–256, 2005 Copyright © Taylor & Francis Inc. ISSN 1543-8627 print / 1543-8635 online DOI: 10.1080/15438620500222588
GSPM1543-862735Research in Sports Medicine, Vol. 13, No. 03, July 2005: pp. 1–21###A succesfull treatemt of PD, however, is difficult due to its multifactorial pathophysiology (Bronte-Stewart et al. 2002; Horak et al. 1992; Marchese et al. 2003).###But the value of this test is limited by the lack of normative data, the lack of analysing postural control in medial–lateral direction, and difficulties in standardisation across different subjects (Bloem et al. 1998; Marchese et al. 2003).###In several studies postural control is examined by the centre of pressure (COP) in a static condition; that is, the platform is not movable and there are no disturbances during standing.###A special feature in Parkinsonism is the fact that a simultaneous execution of a cognitive or motor task induced a significant worsening of PI (Marchese et al. 2003; Morris, Iansek, Smithson, et al. 2000).",impact-revealing,highlighting challenges in treating Parkinson's disease due to its multifactorial nature
2559,5b8c9f5317c44af36f8b778e,16ba65426ed5e1e3367eff5bd507dcf6d99bd7c2,Wide Activation for Efficient and Accurate Image Super-Resolution,573696026e3b12023e5163e1,The Power of Depth for Feedforward Neural Networks,"The increasing of depth brings beneﬁts to representation power [2, 5, 18, 28] but meanwhile under-use the feature information from shallow layers (usually represent low-level features).",other,highlighting the trade-off between depth and feature utilization in model architecture
3681,57d063d3ac44367354292258,db645d8a227d024121dc95a1c0eeec1b15c7fe53,A multi-player Markov stopping game for delay-tolerant and opportunistic resource sharing networks,558b368a84ae84d265c21d46,A Distributed Multi-Service Resource Allocation Algorithm in Heterogeneous Wireless Access Medium,…feature makes the resource access problem considered in this work distinctive from the conventional distributed resource allocation problems [6] [7] [8] [9] in which the luxury of resource selection over time is usually not available. user cases and build their results on the classic optimal…,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2892,5d0b003a8607575390fb4f6a,43d74cd04fb22bbe61d650861766528e369e08cc,An Encoding Strategy Based Word-Character LSTM for Chinese NER,573695fe6e3b12023e511e25,Bidirectional LSTM-CRF Models for Sequence Tagging,"Huang et al. (2015) propose a BiLSTM-CRF model for NER and achieves good performance.###Red labels(without underline) denote predicted labels, and blue labels(with underline) denote gold labels. works (Huang et al., 2015; Lample et al., 2016; Habibi et al., 2017) have been introduced to NER task.",other,reporting prior findings on NER models
1239,,6d18c4b12efdf60b8f9930800ede6a7267a03eec,Lost in dissociation: The main paradigms in unconscious cognition,,,"###These principles are motivated by the deliberation-without-attention effect, stated as the hypothesis that better effects ensue from non-attentive (i.e., unconscious) deliberation.###These results in social psychology are largely corroborated by recent findings in cognitive psychology (e.g., Dehaene et al., 2006; Martens & Kiefer, 2009; Martens et al., 2011; Kiefer, 2007, 2012); in particular, cognitive factors such as attention are now believed to be implicated in both conscious and unconscious processing modes, albeit in different ways (e.g., VanRullen & Koch, 2005).###These authors verified that congruent trials (positive prime / positive target) elicited faster evaluative decisions in comparison with incongruent trials (e.g., negative prime followed by positive target).###These same properties are also often invoked to account for the bottom-up (vs. top-down) character of unconscious cognition (Section 3.3).###Augusto - Lost in dissociation: The main paradigms in unconscious cognition ble to threshold-vision, spared islands of conscious vision, or even just an effect of scattered light? These (Campion et al., 1983) and further challenges (e.###These implicate the social agent in a crucial way, and the purported unconstrained character of the activation of stereotypes and attitudes clashes with our notions of agency, free will, and social responsibility.###These (Campion et al., 1983) and further challenges (e.g., Fendrich et al., 1992) have not yet been definitely addressed.###These dissociating thresholds call for dissociating measures defined in relation
with the two levels of awareness (see Table 1).###These and similar effects elicited by priming in the context of the automaticity (vs. control) of social constructs, as well as of social evaluations (see Section 3.5.2 below), have been abundantly reported (e.g., Aarts et al., 2005;
Luis M. Augusto - Lost in dissociation: The main paradigms in unconscious cognition
Abbate et al., 2013; Blair & Banaji, 1996; Spruyt & Tibboel, 2015; Steele & Ambadi, 2006).###(c) Subjects are incapable of reporting how and what they are acquir-
ing/retrieving by means of free report or by any other verbal means (the reportability criterion; an all too diffuse criterion);
These criteria in turn require the establishing of thresholds of awareness (Cheesman & Merikle, 1984; 1986).###These need not be separate, as one can know a fact and at the same time be capable of recalling the event in which it was learned (where, when, etc.), but clinical studies show that they can indeed be independent (e.g., Schmolck et al., 2002).###These findings have been reported in tasks using sequence learning, artificial grammars, and complex simulated systems in the context of learning, and, now in the context of memory, in tasks of priming and skill performance (for reviews, see, e.g., Augusto, 2010; Cleeremans, 1997; Schacter, 1992; Shanks, 2005).###These assumptions motivate dissociation criteria and thresholds, as well as measures and tests of dissociation.###These are known in the literature as the exhaustiveness and exclusiveness assumptions, respectively (Reingold & Merikle, 1988; 1990); see also Schmidt & Vorberg, 2006).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3756,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,5bdc31b417c44a1f58a0b4e9,Deep Graph Infomax.,"We learn unsupervised node representations with DGI and run k-means on them.###• k-means(DGI) [60] demonstrates the need of joint learning of clusters and representations.###We compare against challenging baselines that baselines that optimize structure (SBM), features (kmeans), or both (DGI+k-means), in addition to a recently proposed state-of-the-art pooling method (MinCutPool).###The learning process in graph embeddings is often done in a similar way to DGI through noise contrastive estimation [24].###Deep Graph Infomax (DGI) [60] adapted the mutual informatonbased learning from Deep InfoMax [26], learning unsupervised representations for nodes in attributed graphs.###DMoN achieves better clustering performance than its neural counterparts on every single dataset and metric besides losing twice to DGI+k-means on Cora and Citeseer in terms of NMI.###We also notice that while the kmeans(DGI) baseline offers some improvements over using features or the graph structure alone, it never surpasses the strongest signal provider in the graph, never being better than the best one between k-means(features) and SBM. Finally, we provide an illustration of the how MinCutPool collapsing into a single cluster while DMoN succeeds in Figure 5.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3607,558c9cd6e4b0cfb70a1eab8d,b456871277de554e768c287533983135586c7521,Achieving Non-Inclusive Cache Performance with Inclusive Caches: Temporal Locality Aware (TLA) Cache Management Policies,53e9b790b7602d9704338380,Pipp: Promotion/Insertion Pseudo-Partitioning Of Multi-Core Shared Caches,This methodology is similar to existing work on shared cache management [15] [21] [25].,other,acknowledge similarities with existing methodologies
2107,,e08a72fafaf04734d1017f1fad9433b5dc534e9b,A Secure Triple-Key Management Scheme for Wireless Sensor Networks,,,"###Our research is motivated by many efforts in key management in wireless sensor networks such as master key based key predistribution scheme, random and extended random key predistribution scheme[4,5], multiple space key predistribution scheme, key management using deployment knowledge[6,7].",impact-revealing,motivating research in key management for wireless sensor networks
1664,,51e1793c445cb80399db780743bbaa015fbc72ed,Immigration and Redistribution,,,"###This idea has been reinforced by social identity theory (Tajfel and Turner 1979; Tajfel 1981), which posits that people categorise others into us and them, often also referred to as in-group and out-group.",impact-revealing,providing context for social identity theory
3565,5c5ce4fd17c44a400fc38abb,d524f10d653ba09e36456475da0aed92d244f795,Image Super-Resolution As A Defense Against Adversarial Attacks,5a260c8b17c44a4ba8a3287e,Mitigating adversarial effects through randomization.,"The effectiveness of our proposed defense is then demonstrated through extensive experiments against state-of-the art adversarial attacks [15], [16], [34]–[37] and comparison with other recently proposed model-agnostic defenses [19], [27], [39], [40] (see Section IV).###In the single-step attacks category (e.g., FGSM-10), our defense model outperforms Random Resizing + Padding and PD by a large margin of 26.7% and 21.0%, respectively.###For the single-step attack categories, Random Resizing + Padding fails to defend.###[19], which performs image transformations by randomly re-sizing and padding an image before passing it through a CNN classifier.###For the recently proposed strong attack (MDI2FGSM), all defense techniques (JPEG compression, Random Resizing + Padding, Quilting + TVM and PD) largely fail, recovering only 1.3%, 5.8%, 1.7% and 21.9% of the images, respectively.###These include JPEG Compression [39], Random Resizing and Padding [19], Image quilting + total variance minimization [40] and Pixel Deflection (PD) [27].###For the iterative attacks (C&W and DeepFool), both Random Resizing + Padding and PD achieve similar performance, successfully recovering about 90% of the images.###Examples of such techniques include JPEG compression [31], [32], foveation-based methods, which crop the image background [33], random pixel deflection [27] and random image padding & re-sizing [19].###3) Hyper-Parameters Selection: Unlike many existing defense schemes, which require computationally expensive model re-training and parameter optimization [19]–[21], [28], our proposed defense is training-free and does not require tuning a large set of hyper-parameters.",other,demonstrating the effectiveness of the proposed defense against adversarial attacks
2419,5f64211c9e795e0286c313a2,f5316f15c665e5a5f89b8b70de13438892e21207,ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks,599c7945601a182cd2629f8a,Learning from Noisy Labels with Distillation,"existing approaches for solving label noise: (1) Loss correction, in which we are given or we need to estimate a noisetransition matrix, which defines the distribution of noise labels [26, 8, 41, 44, 52, 12, 32, 48].###Here, a clean dataset is used only for validation, which is generally necessary for any method and differs from the methods [44, 45, 19, 36, 26, 16, 53, 60] which use a clean dataset to train a network’s learning parameters.",other,describing existing approaches to label noise
730,53e9ad3bb7602d970372109a,8686368908956c506f6e3bbcaa2810adfda14914,NoC-sprinting: Interconnect for fine-grained sprinting in the dark silicon era,53e997a2b7602d9701f73f21,Computational sprinting,"We evaluate NoC-sprinting with multi-threaded workloads from PARSEC [2] by assuming the chip can sustain computational sprinting for one second in the worst case, which is consistent with [17].###Instead of shrinking the chip or sacrificing transistor density, computational sprinting [17] embraces dark silicon by leveraging the extra transistors transiently when performance really counts.###[17] proposed computational sprinting, in which a chip improves its responsiveness to short-burst of computations through temporarily exceeding its sustainable thermal design power (TDP) budget.",impact-revealing,describing the concept of computational sprinting and its evaluation
1132,,a34d2a44cc52dbcd10b07c93b2d31cec5e466c4d,Prior-free Guided TTS: An Improved and Efficient Diffusion-based Text-Guided Speech Synthesis,,,###We adopted the accelerated sampling algorithm of [6] that was motivated by [18].,impact-revealing,reporting the use of a specific algorithm
753,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,55a3e8f2c91b587b09666dae,Finding community structure in networks using the eigenvectors of matrices,"Maximizing the modularity is proven to be NP-hard [5], however, a spectral relaxation of the problem can be solved efficiently [37].###One can then obtain clusters by means of spectral bisection [37] with iterative refinement akin to",impact-revealing,providing context on clustering methods
2554,573695fd6e3b12023e510ff5,06c06885fd53b2cbd407704cf14f658842ed48e5,deeply-recursive convolutional network for image super-resolution,53e9b221b7602d9703cb8ae4,Understanding Deep Architectures using a Recursive Convolutional Network.,"In Eigen et al. [6], recursive layers have the same input and output dimension, but recursive convolutions resulted in worse performances than a single convolution due to over-ﬁtting.",other,reporting findings on recursive layers and their performance
1282,,c3e52d0b6ddd74ce810cbb398c9ec20b353969d2,Intact metabolite spectrum mining by deep learning in proton magnetic resonance spectroscopy of the brain,,,"###2,6 To this end, development of a means of robust metabolite quantification from the brain spectra that are severely degraded by low SNR, broadened linewidth, and unknown spectral baseline has long been a major technical issue in 1 H‐MRS.###2 Although data postprocessing is routinely performed for improved SNR and/or linewidth by applying filters on the free induction decay (FID) signal in the time domain, their performance is limited by different FID signal decay rates for different spin systems.###2,37 The GSH as well as GABA can be detected only upon the use of spectral editing meth-ods.###1,2 However, even with the FIGURE 7 Comparison of the relative metabolite concentrations estimated by the CNN (red symbols; left) and the LCModel analysis (blue symbols; right) from the in vivo brain spectra (NSA = 64) of the five healthy subjects (#1–#5).",impact-revealing,highlighting the technical challenges in metabolite quantification from brain spectra
518,55a6bae665ce054aad73115b,340f48901f72278f6bf78a04ee5b01df208cc508,Human-level control through deep reinforcement learning,53e99d9db7602d9702657558,Deep Auto-Encoder Neural Networks In Reinforcement Learning,"Because Q maps history–action pairs to scalar estimates of their Q-value, the history andtheactionhave beenusedasinputstothe neuralnetwork by some previous approaches 24,26 .###Incontrasttopreviouswork 24,26 , our approach incorporates ‘end-to-end’ reinforcement learning that uses reward to continuously shape representations within the convolutional network towards salientfeaturesoftheenvironmentthatfacilitatevalueestimation.",impact-revealing,highlighting the differences between previous approaches and the proposed method
535,5f058d15dfae54570ec57ea1,fb93ca1e004cbdcb93c8ffc57357189fa4eb6770,Resnest: Split-attention networks,5a260c8117c44a4ba8a30771,Squeeze-and-Excitation Networks.,"Figure 1 shows an overall comparison with SE-Net and SK-Net blocks.###SE-Net [29] introduces a channel-attention mechanism by adaptively recalibrating the channel feature responses.###Recent ResNet implementations usually apply the strided convolution at the 3× 3 layer instead of the previous 1× 1 layer to better preserve such information [25, 27].###Comparing our ResNeSt block with SE-Net [27] and SK-Net [35].###First introduced in SE-Net [29], the idea of squeeze-and-attention (called excitation in the original paper) is to employ a global context to predict channel-wise attention factors.###1: Comparing our ResNeSt block with SE-Net [30] and SK-Net [38].###Following [27, 35], a combined representation for each cardinal group can be obtained by fusing via an element-wise summation across multiple splits.###With radix = 1, our Split-Attention block is applying a squeeze-and-attention operation to each cardinal group, while the SE-Net operates on top of the entire block regardless of multiple groups.",impact-revealing,comparing different network blocks and their mechanisms
2737,53e99ab2b7602d970232a35e,1f4400f0441e1cc3c8eeab6b7c8accac59c378a2,Late-binding: enabling unordered load-store queues,53e9b8c1b7602d97044a1fd6,Orion: A Power-Performance Simulator For Interconnection Networks,[29] show a 20% increase in power for a,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2089,,60d39826d43489058e5c448c6f75a57fff5c9516,Data-Driven Non-Intrusive Speech Intelligibility Prediction Using Speech Presence Probability,,,"###Inspired by the AI, the Speech Intelligibility Index (SII) [17] and Extended SII (ESII) [18] are based on linear combinations of contributions to SI from separate frequency bands.###This idea of working on short time segments is common in the ﬁeld of SI prediction, and is inspired by existing SI predictors such as ESII [18] and ESTOI [21].",impact-revealing,acknowledging existing methods in speech intelligibility prediction
2876,5f6f14c49fced0a24bb647ec,c980c5ab5944260c95891fbc8c1e556171333442,DPDDI: a deep predictor for drug-drug interactions,5e807d589fced0a24b30b594,A Comprehensive Survey on Graph Neural Networks,"Inspired by the traditional convolutional neural networks (CNNs) operating on regular Euclidean data like images (2D grid) and text (1D sequence) [18], GCN formulates convolution on an irregular graph in non-Euclidean domains, then aggregates information about each node ’ s neighborhood to distill…",other,providing context on the application of GCN in non-Euclidean domains
64,5ebe685391e0117693a52241,99314a532a3358cb86064fc8917ed2c283227539,NAT: Noise-Aware Training for Robust Neural Sequence Labeling,5ed3971a9e795ee582134389,Toward robust neural machine translation for noisy input sequences,"Moreover, we plan to evaluate NAT on other real noise distributions (e.g., from ASR) and other sequence labeling tasks to support our claims further.###Sequence labeling systems are generally trained on clean text, although in real-world scenarios, they often follow an error-prone upstream component, such as Optical Character Recognition (OCR; Neudecker, 2016) or Automatic Speech Recognition (ASR; Parada et al., 2011).###The latter technique was successfully applied in other domains, including computer vision (Krizhevsky et al., 2012) and speech recognition (Sperber et al., 2017).###The impact of noisy input data In the context of ASR, Parada et al. (2011) observed that named entities are often OOV tokens, and therefore they cause more recognition errors.###Inspired by recent research in computer vision (Zheng et al., 2016), Neural Machine Translation (NMT; Cheng et al., 2018), and ASR (Sperber et al., 2017), we propose two Noise-Aware Training (NAT) objectives that improve the accuracy of sequence labeling performed on noisy input without reducing…###Inspired by recent research in computer vision (Zheng et al., 2016), Neural Machine Translation (NMT; Cheng et al., 2018), and ASR (Sperber et al., 2017), we propose two Noise-Aware Training (NAT) objectives that improve the accuracy of sequence labeling performed on noisy input without reducing efﬁciency on the original data.###, 2018), and ASR (Sperber et al., 2017), we propose two Noise-Aware Training (NAT) objectives that improve the accuracy of sequence labeling performed on noisy input without reducing efficiency on the original data.###Noise can also be introduced in an upstream task, like OCR (Alex and Burns, 2014) or ASR (Chen et al., 2017), causing the errors to be propagated downstream.###Consequently, they exhibit degraded performance in real-world scenarios where the transcriptions are produced by the previous up-stream component, such as OCR or ASR ( § 2.2), which results in a detrimental mismatch between the training and the test conditions.",impact-revealing,highlighting the need for noise-aware training in sequence labeling tasks
1721,,76c57043e5a837680af7af7d5a3354a32b865169,Cooperative advertising and pricing decisions in a manufacturer-retailer supply chain; a game-theoretic approach,,,"###In Xie and Neyret [25] , both retail price and relationship between and are roughly estimated based on results of two games and extended for others.###A number of recent studies have been undertaken aimed at developing a model that comprises most of the above mentioned factors in order to examine the manufacturerretailer relationships in a supply chain [1, 7, 20, 23, 25, 29].###The work of Xie and Neyret [25] is extended by considering a general price demand function and relaxing the assumption of equal margins in order to understand pricing impacts on channel members’ profits.###An approach similar to Xie and Neyret [25] but more precise is adopted here to present the numerical simulations and to illustrate the previous results.###The current study is not only closely connected to the three papers cited above, namely Xie and Neyret [25], SeyedEsfahani, Biazaran [1], and Aust and Buscher [20], but also extends beyond by generating a number of insights.###Following Xie and Neyret [25], we will employ an appropriate change of variables to handle the problem in an equivalent but more convenient way shown in Table 3.###A similar approach was adopted by Xie and Neyret [25].###Xie and Neyret [25] Assumed p α β − (A ) Ba q γ δ − − −###In contrast, we address the advertising-sales response function by utilizing the model proposed by Huang and Li [18] which is very popular in the literature [7, 23-25, 28-30].###The static models study the co-op advertising in a single period; examples include Berger [5]; Dant and Berger [6]; Huang and Li [18]; Huang, Li [23]; Li, Huang [24]; and Xie and Neyret [25].",impact-revealing,acknowledge and extend previous work on pricing impacts in supply chains
188,5736986b6e3b12023e730129,424561d8585ff8ebce7d5d07de8dbf7aae5e7270,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,556f622a2401b4b38c23635c,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,", Selective Search [4] object detectors, RCNN [5], and Fast R-CNN [2]).###The R-CNNmethod [5] trains CNNs end-to-end to classify the proposal regions into object categories or background.###izations of the four coordinates following [5]:###Although region-based CNNs were computationally expensive as originally developed in [5], their cost has been drastically reduced thanks to sharing convolutions across proposals [1], [2].###These class-agnostic boxes are used as proposals for R-CNN [5].###, [4]) and region-based convolutional neural networks (R-CNNs) [5].###, the shared convolutional layers) are initialized by pre-training a model for ImageNet classification [36], as is standard practice [5].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3134,5f7d8a8391e011346ad27d2b,3bb1e24eb3429f807397833105d1e137d9927767,SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup,5a9cb66717c44a376ffb8ac1,Deep contextualized word representations.,"Different neural architectures has been proposed (Huang et al., 2015; Lample et al., 2016; Peters et al., 2018; Akbik et al., 2018) in recent years, which have achieved state-of-the-art performance in a number of sequence labeling tasks.",other,acknowledge advancements in neural architectures for sequence labeling
85,5e2d653a3a55acc8374367fd,36ff7927f9049d37b314c2a114769b517b3f5f7a,Joint Recognition of Names and Publications in Academic Homepages,5bdc315017c44a1f58a05beb,PubSE: A Hierarchical Model for Publication Extraction from Academic Homepages.,"We perform a manual inspection of recognition results of state-of-the-art models for the two tasks (i.e., PubSE [31] and CogNN [1]) and our proposed PAM model on 50 randomly selected homepages.###Usually, the plain text of an academic homepage is saved first, then the recognition tasks are conducted on text [1, 31].###For example, state-of-the-art techniques for publication recognition [31] and for person names recognition [1] use Bi-LSTM-CRF based models to recognise information from the plain text of the homepages.###Following state-of-the-art methods [1, 31], we use GloVe [19] to learn word embeddings on an academic homepage dataset (detailed in Section 4.1.1), although other pre-training methods may be used here without loss of generality.###There have been extensive research interests in the extraction and mining of such information from academic homepages [3, 8, 16, 20, 28, 31].###The state-of-the-art for publication recognition [31] trains webpage-level and line-level models together to capture the position information of academic homepage, whereas our model captures position information by integrating them into the memory updating process.###• HomePub dataset [31] contains the plain text of 2,087 home-pages from different universities and research institutes with 12,796 publications annotated.###We use the same datasets used by the state-of-the-art for publication recognition [31] and person name recognition [1].###PAM also outperforms the hierarchical PubSE [31] model, which can capture the positional diversity, by 3.64% in F1 score.###The state-of-the-art for publication recognition [31] uses a Bi-LSTM-CRF based model to learn the page-level and line-level structure.",impact-revealing,reporting on the performance and methods of state-of-the-art models in recognition tasks
904,53e9b79fb7602d970434793d,45ce4be870f0a5be7b45b064726696dacd83c786,Improving memory Bank-Level Parallelism in the presence of prefetching,53e99b2cb7602d97023c4503,Prefetch-Aware DRAM Controllers,"Prefetch bits are already used in many proposals [7, 29, 23, 12] to indicate whether or not a cache line (or request) was brought in (or made) by the prefetcher.###800MHz DRAM bus cycle, DDR3 1600MHz [14], 8 to 1 core to DRAM bus frequency ratio; DRAM and bus 8B-wide data bus per channel, BL = 8; 1 rank, 8 banks per channel, 8KB row buffer per bank; On-chip, open-row, demand-first [12] FR-FCFS [20] DRAM controllers 1, 2, 4 channels for 1, 4, 8-core CMPs; 64-entry (8 × 8 banks) for single-core processor DRAM request 256 and 512-entry (16 × 8 banks per channel) buffers for 4 and 8-core CMPs###To implement this, we need to measure the run-time accuracy of the prefetcher [23, 12, 5].###Figure 13 shows the performance of PADC alone and PADC combined with our mechanisms for the 4- core workloads.###PADC also delays and drops useless prefetches to reduce waste in on-chip buffer resources and DRAM bandwidth.###A number of DRAM scheduling policies [20, 28, 17, 15, 12] have been proposed.###PADC aims to minimize DRAM latency of useful requests by prioritizing useful row-hit requests over others to the same bank.###We conclude that our mech-
anisms complement PADC and thus significantly improve system performance.###PADC significantly improves WS and HS by 14.1% and 16.3% respectively compared to the baseline.###Prefetch-Aware DRAM Controllers (PADC) [12] was proposed to maximize DRAM row buffer hits for useful requests (demands and useful prefetches).###When combined with PADC, BAPI and BPMRI improve WS and HS by 20.6% and 22.5%.###5, our mechanisms are complementary to prefetch-aware DRAM controllers [12] which employ an adaptive prefetch handling technique that is reported to outperform feedback-directed prefetching [23].###We use a stream prefetcher [23, 12] similar to the one in IBM’s POWER 4 [25] for most of our experiments.###In other words, the main goal of PADC is to exploit row buffer locality in each bank in a useful manner.###Adaptive prefetch handling techniques [10, 23, 12, 5] aim to reduce the interference between prefetch and demand requests in the memory system.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2992,5a4aef9e17c44a2190f7a6fb,af03709f0893a7ff1c2656b73249d60030bab996,NISP: Pruning Networks Using Neuron Importance Score Propagation,573696f46e3b12023e5f0cc9,Infinite Feature Selection,"1Details of the method are introduced in [31] and its codes taken from###Inf-FS utilizes properties of the power series of matrices to efficiently compute the importance of a feature with respect to all the other features, i.e., it is able to integrate the importance of a feature over all paths in the affinity graph1.###To study the effects of different criteria to determine neuron importance, we conduct experiments by fixing other parts of NISP and only comparing the pruning results with different measurements of importance: 1) using feature selection method in [31] (NISPFS) and 2) considering only magnitude of weights (NISPMag).###We employ the recently introduced filtering method Inf-FS [31] because of its efficiency and effectiveness on CNN feature selection.###We employ the feature ranking method proposed in [31] in NISP.###, [31]), then we propagate the importance of neurons backwards from the FRL to earlier layers.###We employ the recently introduced filtering method Inf-FS [25] because of its efficiency and effectiveness on CNN feature selection.###The hyper-parameter of Inf-FS is a loading coefficient α ∈ [0, 1], which controls the influence of variance and correlation when measuring the importance.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
403,5aed14e217c44a4438159a0f,8f684080d2b81d3178d681d6917cb077c082a9e1,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,5a260c8117c44a4ba8a30771,Squeeze-and-Excitation Networks.,"[9] experimentally demonstrate that adaptively recalibrating channel-wise features responses can improve the representational power of a network.###Different from the approach in [9], we divide feature maps into two parts.",impact-revealing,reporting prior findings on feature recalibration
3404,5f7fdd328de39f08283980ba,2fce1ef37391cd685fc5459e1cbfcb8490b85242,Graph information bottleneck,5e5e189a93d709897ce1e760,InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization,"can be further formulated as a mutual information maximization term that aims to maximize the mutual information between representations of nodes and their neighbor patches [37], between representations of sub-structures and the hidden feature vectors [38], between representations of graphs and their sub-structures [39].",other,providing context for mutual information maximization in graph representations
938,5ac1829d17c44a1fda917eab,86aeec4d48d949190b3a0c2bf32c101fc23f13a3,Crepe: A Convolutional Representation for Pitch Estimation,5c8cd48f4895d9cbc624832c,An Analysis/Synthesis Framework for Automatic F0 Annotation of Multitrack Datasets.,"This is particularly problematic for tasks that require a flawless f0 estimation, such as using the output of a pitch tracker to generate reference annotations for melody and multi-f0 estimation [18, 19].###To evaluate the algorithms under more realistic (but still controlled) conditions, the second dataset we use is a collection of 230 monophonic stems taken from MedleyDB and re-synthesized using the methodology presented in [18], which uses an analysis/synthesis approach to generate a synthesized track with a perfect f0 annotation that maintains the timbre and dynamics of the original track.",impact-revealing,highlighting the challenges in f0 estimation and the methodology for evaluation
501,5f9be24691e011dcf482d8d6,842bd2d15d53e3083c110e0af55ffd7447ad03e4,Prediction-Based Power Oversubscription in Cloud Platforms,5a260c2e17c44a4ba8a24027,Resource Central: Understanding and Predicting Workloads for Improved Resource Management in Large Cloud Platforms.,", high during the day, low at night), the problem reduces to identifying VMs whose time series of CPU utilizations exhibit 24-hour periods [9].###For example, [9] assumes a workload is user-facing if the FFT indicates a 24-hour period.###As in prior work [9], we consider a workload critical if it is user-facing, i.###Prior work [9] associated a diurnal utilization pattern with user interactivity and the critical need for high performance.",impact-revealing,acknowledging prior work on workload patterns
2910,5ca600ae6558b90bfa4d76e9,7aa38b85fa8cba64d6a4010543f6695dbf5f1386,Towards Deep Learning Models Resistant to Adversarial Attacks,573695fe6e3b12023e5121fc,DeepFool: a simple and accurate method to fool deep neural networks,"Computer vision presents a particularly striking challenge: very small changes to the input image can fool state-of-the-art neural networks with high probability (Szegedy et al., 2013; Goodfellow et al., 2014; Nguyen et al., 2015; Sharif et al., 2016; Moosavi-Dezfooli et al., 2016).###Unfortunately, ERM often does not yield models that are robust to adversarially crafted examples (Goodfellow et al., 2014; Kurakin et al., 2016; Moosavi-Dezfooli et al., 2016; Tramèr et al., 2017b).",other,highlighting the vulnerability of neural networks to adversarial examples in computer vision
81,5fdb2e1691e0118a02c4f566,16913a534b1630d33770b392767bb316f4fdb11e,Clinical Temporal Relation Extraction with Probabilistic Soft Logic Regularization and Global Inference,573696136e3b12023e52549d,Hinge-Loss Markov Random Fields and Probabilistic Soft Logic,"Besides, globally inferring the relations at the document level would also be intractable for them due to the high complexity and low scalability (Bach et al. 2017).###2016) have explored Probabilistic Soft Logic (PSL) (Bach et al. 2017) to tackle the structured prediction problem.###Recently, some researchers (Deng and Wiebe 2015; Chen et al. 2019; Hu et al. 2016) have explored Probabilistic Soft Logic (PSL) (Bach et al. 2017) to tackle the structured prediction problem.",impact-revealing,highlighting the challenges of document-level relation inference due to complexity and scalability issues
1533,,1a2887fd3a26940cf5d36e66c0486dd6303bb303,Exploring racial ideology and coping as moderators of the association between experiencing racial microaggressions and psychological outcomes in black college student,,,"###It seems reasonable to interpret this finding as indicating that the added attributional ambiguity inherent in dealing with microaggressions likely explains this difference in responding (Sue et al., 2007).###Unfortunately, the assertion that racism is no longer a significant problem in the United States fails to consider the extensive empirical literature that denies this claim (Sue et al., 2007; Harrell, 2000; Williams & Williams-Morris, 2000; Kessler et al., 1999; Gibbons et al., 2004).###Microinvalidations are exchanges that exclude, negate, or nullify the psychological thoughts, feelings, or experiential reality of a person of color (Sue et al., 2007, p. 274).###Microassaults are explicit racial derogations characterized by a verbal or nonverbal attack meant to hurt the intended victim through name-calling, avoidant behavior, or purposeful discriminatory actions (Sue et al., 2007, p. 274).###Microinsults are communications that convey rudeness and insensitivity and demean a person’s racial heritage or identity (Sue et al., 2007, p. 274).###In addition, themes such as color blindness, denial of individual racism, and the myth of meritocracy apply broadly to the experience of all minority racial groups (for a more thorough elaboration of these themes, see Sue et al., 2007 and Sue,
2010).###The term microaggressions is increasingly being used to refer to the subtle, nebulous behavioral and environmental manifestations of aversive racism, which communicate derogatory
racial messages on a daily basis (Sue et al., 2007).",impact-revealing,highlighting the significance of microaggressions and their impact on racial dynamics
2549,555045d745ce0a409eb59fe4,460d131e081486491af532604351a0691b52b11f,User-level psychological stress detection from social media using deep neural network,53e99d8eb7602d970264adbf,You Are What You Tweet: Analyzing Twitter for Public Health.,[10] apply the Ailment Topic Aspect Model to over 1.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1586,,e39b549dd63460fed0dfef840a38b0ba8f157521,Layout Optimization of Planar Braced Frames Using Modified Dolphin Monitoring Operator,,,"###Ezzati system search algorithm (CSS) that uses the electric laws of physics and the Newtonian laws of mechanics to guide charged particles to explore location of the optimum [9]; Dolphin Echolocation Optimization (DEO) which mimics strategies used by dolphins for their Locating and hunting process [10]; Colliding bodies optimization (CBO) and Enhanced colliding bodies optimization (ECBO) which are inspired by a collision between two objects in one dimension [11, 12]; Grey wolf optimizer (GWO) algorithm mimics the leadership hierarchy and hunting mechanism of grey wolves [13]; Vibrating particle system (VPS) and Enhanced vibrating particle system (EVPS) simulating the free vibration of single degree of freedom systems with viscous damping [14]; Ant lion optimizer (ALO) mathematically models the interaction of ants and ant lions [15], Grasshopper optimization algorithm (GOA) mathematically models and mimics the behavior of grasshopper swarms [16].",impact-revealing,reporting various optimization algorithms inspired by natural phenomena
478,599c7945601a182cd262a009,6727f574ad8b1c3763be8d58eeaf82c551aa33ef,Generative and Discriminative Text Classification with Recurrent Neural Networks,5c873b4d4895d9cbc6f504ad,Overcoming Catastrophic Forgetting In Neural Networks,"Discriminative models are known to suffer from catastrophic forgetting when learning sequentially from examples from a single class at a time, and specialized techniques are actively being developed to minimize this problem (Rusu et al., 2016; Kirkpatrick et al., 2017; Fernando et al., 2017).",impact-revealing,highlighting the challenges faced by discriminative models in sequential learning
517,573696026e3b12023e516718,f8e79ac0ea341056ef20f2616628b3e964764cfd,"You Only Look Once: Unified, Real-Time Object Detection",556f622a2401b4b38c23635c,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,"Then, classifiers [35, 21, 13, 10] or localizers [1, 31] are used to identify objects in the feature space.###After classification, post-processing is used to refine the bounding boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene [13].",impact-revealing,describing the process of object identification and post-processing
3352,53e9ac5bb7602d970362bd46,9e7cfbf2083432ea013685eeeea2670c87ea4c55,comparing program phase detection techniques,53e99aacb7602d97023277b3,Adaptive Optimization In The Jalapeno Jvm,"Similarly, on the software side, dynamic code optimization [13][14] is gaining importance with the wide spread acceptance of run-time environments such as Java [15] and .",other,acknowledge the growing importance of dynamic code optimization in software development
297,5c2c7a9217c44a4e7cf317b4,61946177ea5d44ad18ff09de7929556d7b34cd8b,simd-x: programming and processing of graph algorithms on gpus.,5c8ba4e94895d9cbc6a04fde,TriCore: parallel triangle counting on GPUs.,"The advent of big data [40, 27, 35, 36, 37, 5, 25, 26, 28, 14, 83] exacerbates the need of extracting useful knowledge within an acceptable time envelope.",impact-revealing,highlighting the urgency of knowledge extraction in the context of big data
1598,,0b206770bbe770bbf4a7e3c295573f4d896a84c2,"Olivine Weathering in Soil, and Its Effects on Growth and Nutrient Uptake in Ryegrass (Lolium perenne L.): A Pot Experiment",,,"###Soils in their natural state retain substantial amounts of organic carbon for longer time periods [2,3], but building-up soil organic carbon stocks is difficult, and is limited by saturation levels that depend on local conditions such as soil type, drainage, temperature and rainfall [3].",impact-revealing,highlighting the challenges in building up soil organic carbon stocks
3502,599c7945601a182cd2629f72,000178cd12c8a6e5da8215b6365fae03c20fd18d,End-to-End Representation Learning for Correlation Filter Based Tracking,5550415945ce0a409eb3a820,ImageNet Large Scale Visual Recognition Challenge,"The ImageNet+CF variant employs features taken from a network trained to solve the ImageNet classification challenge [27].###We use all the 3862 training videos of ImageNet Video [27], containing more than 1 million annotated frames, with multiple objects per frame.###We use all the 3862 training videos of ImageNet Video [28], containing more than 1 million annotated frames, with multiple objects per frame.",other,reporting data sources and methods used
503,5fd8acf991e0119b22c1f38d,5b9d8bcc46b766b47389c912a8e026f81b91b0d8,Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting,599c7987601a182cd2648373,Attention Is All You Need.,"Thus, in this paper, we seek to answer the question: can we improve Transformer models to be computation, memory, and architecture efficient, as well as maintaining higher prediction capacity? Vanilla Transformer (Vaswani et al. 2017) has three significant limitations when solving the LSTF problem: 1.###The canonical self-attention in (Vaswani et al. 2017) is defined based on the tuple inputs, i.###Vanilla Transformer (Vaswani et al. 2017) has three signiﬁcant limitations when solving LSTF: 1.###The canonical self-attention in (Vaswani et al. 2017) is deﬁned on receiving the tuple input (query, key, value) and performs the scaled dot-product as and d is the input dimension.###The vanilla trans-former (Vaswani et al. 2017; Devlin et al. 2018) uses point-wise self-attention mechanism and the time stamps serve as local positional context.###Decoder: Generating long sequential outputs through one forward procedure We use a standard decoder structure (Vaswani et al. 2017) in Fig.###We use a standard decoder structure (Vaswani et al. 2017) in Fig.###The popular self-attention based Transformer (Vaswani et al. 2017) has recently been proposed as new thinking of sequence modeling and has achieved great success, especially in the NLP ﬁeld.",impact-revealing,highlighting limitations of the vanilla Transformer model
598,5cb06564ced107d4c6006f1c,19351711295bddc627f761d59a1ef58ab2fa7e2c,Identifying SDC-Causing Instructions Based on Random Forests Algorithm,5ea014b69fced0a24ba10ffc,Configurable Detection of SDC-causing Errors in Programs,"Recently, a lot of works [10-16] try to improve the static injection framework.###Faults in the instruction opcode are also not considered, since it always causes illegal opcode exception rather than SDC. Finally, as in work [16], we assume that at most one fault occurs during a program’s execution.###The main idea of work [16] is predicting the SDC proneness of a program’s data firstly, then selectively protects the most SDC-prone instructions of the program for the user-specified overhead bound.###We also compare our results with the work [15] and SDCAuto presented in work [16].###The work [16] presents a selective protection technique that allows users to selectively protect these SDC-prone data.###The work [16] first compiles the source code into LLVM IR, and extracts instruction features based on LLVM IR file.###We extract features of instructions according to our analysis and prior work [12, 13, 16, 17 and 18].###2 illustrates the diagram of the work [16].###The work [16] introduces a prediction model named SDCAuto to predict the SDC proneness of a program’s instructions.",impact-revealing,acknowledging prior work on selective protection techniques
937,53e9ac54b7602d9703622ebc,0d8524a1eca5e41ee755acd30a0c28a782d05331,The sharing architecture: sub-core configurability for IaaS clouds,53e9a034b7602d9702919136,Core Fusion: Accommodating Software Diversity in Chip Multiprocessors,"The Sharing Architecture is inspired by Core Fusion [24], WiDGET [64], the Distributed ILP mechanisms in the TRIPS architecture [11, 17, 49, 50, 53], and how ILP is mapped across multiple cores in the Raw [63] and Tilera [65] architectures.###The Sharing Architecture leverages many ideas from Core Fusion [24] on how to distribute resources across cores, but unlike Core Fusion, the Sharing Architecture is designed to scale to larger numbers of Slices (cores).###Unlike the Sharing Architecture, most banked-by-address LSQs, such as those in clustered processors [8] and Core Fusion [24] require LSQ bank prediction because a memory operation’s effective address is unknown until the execution stage.###Similar to Core Fusion [24], a distributed branch predictor is used, thus the effective branch predictor capacity grows with number of Slices.###We leverage the approach used by Core Fusion[24] which uses a pre-commit pointer to guarantee that all ROBs are up to date several cycles before true commit.",impact-revealing,describing the design and inspiration behind the Sharing Architecture
2966,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,5c794e6b4895d9cbc638244f,Structure and inference in annotated networks,"[39] shows that using features allows their estimation-based approach to overcome the spectral “detectability” limit for graphs [35], a result that we replicate in this study.###Specifically, existing combinatorial [65] and bayesian [39] methods for attributed graph clustering use graph features to improve clustering accuracy.",other,reporting prior findings and methods in attributed graph clustering
250,5bdc31b417c44a1f58a0b240,6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da,Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks,5a4aef4b17c44a2190f76aa6,Glocalized Weisfeiler-Lehman Graph Kernels: Global-Local Feature Maps of Graphs,"Prominent examples of this trend include kernels based on graphlet counting (Shervashidze et al. 2009), and, most notably, the Weisfeiler-Lehman subtree kernel (Shervashidze et al. 2011) as well as its higher-order variants (Morris, Kersting, and Mutzel 2017).###Note that we can scale our method to larger datasets by using sampling strategies introduced in, e.g., (Morris, Kersting, and Mutzel 2017; Hamilton, Ying, and Leskovec 2017a).###…(Borgwardt and Kriegel 2005), the Weisfeiler-Lehman subtree kernel (WL) (Sher-vashidze et al. 2011), the Weisfeiler-Lehman Optimal Assignment kernel (WL-OA) (Kriege, Giscard, and Wilson 2016), and the global-local k -WL (Morris, Kersting, and Mutzel 2017) with k in { 2 , 3 } as kernel baselines.",impact-revealing,acknowledge significant methods in graph-based learning
599,5ac1827b17c44a1fda915855,f96a5a9cfa1dbb01df6df749e093c0bad20240a8,Control Flow Checking at Virtual Edges.,53e9bb60b7602d970479d9d1,Graph-tree-based software control flow checking for COTS processors on pico-satellites,"These techniques are suggested in literature that would fall into two general classes, hardware [10, 11, and 12] or software [13, 14, 15, 16, and 17] redundancy.###This solution is inspired by [13, 14, 15, 16, and 17] and incorporates their advantages.###A variety of defense mechanisms are proposed to detect and correct control flow errors (e.g., [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, and 20]).",impact-revealing,acknowledge existing techniques and their advantages
3979,5d0b00ed8607575390fedd4e,493d5f344eea1468260946b29a80dc81b2be409c,HopSkipJumpAttack: A Query-Efficient Decision-Based Attack,55465e660cf2939c2feeaeac,Optimal Rates for Zero-Order Convex Optimization: The Power of Two Function Evaluations,Duchi et al. [21] established optimal rates of convex zeroth-order optimization via mirror descent with two-point gradient estimates.,other,reporting prior findings on convex zeroth-order optimization
622,5e8ef2ae91e011679da0f1a3,07d85a6c8a31f43ee6e128f7ef0a1bf1494567cd,Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence,5c756cd0f56def97984c9a3a,Autoencoding Variational Inference For Topic Models,"Several topic models use neural networks (Larochelle and Lauly, 2012; Salakhutdinov and Hinton, 2009; Gupta et al., 2020) or neural variational inference (Miao et al., 2016; Mnih and Gregor, 2014; Srivastava and Sutton, 2017; Miao et al., 2017; Ding et al., 2018).###Concretely, we extend Neural ProdLDA (Product-of-Experts LDA) (Srivastava and Sutton, 2017), a state-of-the-art topic model that implements black-box variational inference (Ranganath et al., 2014), to include contextualized representations.###This result has also also been conﬁrmed by Srivastava and Sutton (2017).###To show this, we compare our approach to ProdLDA (Srivastava and Sutton, 2017, the model we extend) 7 , and the following models: (ii) Neural Variational Document Model (NVDM) (Miao et al., 2016); (iii) the very recent ETM (Dieng et al., 2020), MetaLDA (MLDA) (Zhao et al., 2017) and (iv) LDA (Blei…###Refer to (Srivastava and Sutton, 2017) for more details on the architecture we extend.###Concretely, we extend Neural ProdLDA (Product-of-Experts LDA) (Srivastava and Sutton, 2017), a state-of-the-art topic model that implements black-box variational inference (Ranganath et al.###Srivastava and Sutton (2017) pro-pose a neural variational framework that explicitly approximates the Dirichlet prior using a Gaussian distribution.###, 2020) or neural variational inference (Miao et al., 2016; Mnih and Gregor, 2014; Srivastava and Sutton, 2017; Miao et al., 2017; Ding et al., 2018).###We follow (Srivastava and Sutton, 2017) for the choice of the parameters.###Our model is built around two main components: (i) the neural topic model ProdLDA (Srivastava and Sutton, 2017) and (ii) the SBERT embedded representations (Reimers and Gurevych, 2019).",impact-revealing,describing the extension of a state-of-the-art topic model
3731,5e5e18ca93d709897ce315f0,68f86237dadcf2f570f0cd5b5e56161693619a74,Residual Energy-Based Models for Text Generation,53e9abb9b7602d970356a42c,Discriminative Reranking For Machine Translation,"Our approach appears similar to discriminative reranking approaches used in the parsing and machine translation community (Shen et al., 2004).",other,drawing parallels between approaches in different fields
705,53e99fe3b7602d97028bddfb,ecf5fd423c117ffb87730d75a473bc05beaae2b8,self-optimizing memory controllers: a reinforcement learning approach,53e99842b7602d970206bc26,Memory access scheduling,"Rixner et al. [37] show that none of the ﬁxed policies studied provide the best performance for all workloads and under all circumstances.###…bandwidth of an example parallel application (SCALPARC [20]) with both a realistic, contemporary controller design (using the FR-FCFS scheduling policy [37, 49]), and an optimistic (and unrealizable) design that is able to sustain 100% of the controller’s peak band-width, provided enough demand.###…it to three diﬀerent schedulers: (1) Rixner et al.’s FR-FCFS scheduling policy, which was shown to be the best-performing policy on average [36, 37], (2) a conventional in-order memory controller [37], and (3) an optimistic (i.e., ideally eﬃcient) scheduler that can sustain 100% of the peak…###However, the FR-FCFS (ﬁrst-ready ﬁrst-come ﬁrst-serve) policy [36, 37] provides the best average performance.###RL’s performance improvement is greater than 5% for all applications, with a maximum of 33% for SCALPARC and a minimum of 7% for FFT. Note that the conventional in-order controller signiﬁcantly un-derperforms the baseline FR-FCFS controller, in line with previous research results [37].###Interested readers can ﬁnd more detailed descriptions in [11, 12, 37] on DRAM systems and in [6, 28, 42] on reinforcement learning.###…FR-FCFS scheduling policy, which was shown to be the best-performing policy on average [36, 37], (2) a conventional in-order memory controller [37], and (3) an optimistic (i.e., ideally eﬃcient) scheduler that can sustain 100% of the peak DRAM throughput if there is enough demand (this…###When confronted with this challenge, existing memory controllers tend to sustain only a small fraction of the peak bandwidth [27, 37].###Most memory controller proposals [17, 25, 30, 36, 37, 38, 48, 49], including ours, aim at increasing overall system performance by delivering higher DRAM throughput and/or lower access latency.###Diﬀerent orderings and inter-leavings of DRAM commands result in diﬀerent levels of DRAM throughput and latency [37].",impact-revealing,highlighting the performance of different scheduling policies in memory controllers
1927,,de1114656f88b59ba09d4eb73ee049d87c58947f,Nanotechnology as Emerging Tool for Enhancing Solubility of Poorly Water-Soluble Drugs,,,"###Therefore, at present, polyeletrolyte complexs are widely used as carrier of drugs [99, 102-105], nonviral vectors of transferred genes [92, 106-109], biospecific sorbents [110, 111], films [112-114], and gels [115-119].",impact-revealing,reporting various applications of polyelectrolyte complexes
2571,599c7987601a182cd2648373,204e3073870fae3d05bcbc2f6a8e263d9b72e776,Attention Is All You Need,53e99bffb7602d97024ac0e6,Generating Sequences With Recurrent Neural Networks.,"At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next.",other,providing context for the model's functioning
1901,,39040ddcfa985c7535ca80bd475518abd2d43728,A Comparison of Online and Hybrid Professional Development for CS Principles Teachers,,,"###This approach to online learning is based on the Community of Inquiry Model or CoI [7], which reinforces the idea of establishing strong learning communities that include collaboration and reflection for improved understanding of course content.###Garrison et al. [7], note that social presence allows participants in the online environment to have a sense of self and belonging and to present themselves as ""real people.""",impact-revealing,providing context for online learning approaches
1101,,60deedaffacfdcc72d5a1111d487932f19f61e08,A General Framework for Deep Supervised Discrete Hashing,,,"###– NINH (Lai et al. 2015) NINH learns bitwise hash codes for images via a carefully designed one stage deep neural network.###Similar to Lai et al. (2015), we adopt the widely used evaluation metrics to evaluate the image retrieval quality: Mean Average Precision (MAP) for different number of bits, precision curves within Hamming distance 2, precision curves with different number of top returned samples, precision within…###We follow the previous experimental setting in Lai et al. (2015), Li et al. (2016b) and Wang et al. (2016 Deep hashing methods usually need many training images to learn the hash function.###Network In Network Hashing (NINH) (Lai et al. 2015) presents a triplet ranking loss to capture the relative similarities of images.###4 we can see that DHN is slightly better than NINH and CNNH due to the joint optimization of pair-wise cross entropy loss and quantization loss.###Lai et al. (2015) propose an end to end method for learning hashing codes, which uses triplet ranking loss to capture the relative similarities of different images.###For example, the average MAP result of SDH is 0.603, while the average MAP result of NINH is 0.700.###The average MAP result of NINH is 0.708, which is better than CNNH (0.659).###Our method is inspired by recent advances in deep hashing methods (Xia et al. 2014; Lai et al. 2015; Li et al. 2016b; Wang et al. 2016) and discrete hashing methods (Kang et al. 2016; Shen et al. 2015; Gui et al. 2016).###Our method is inspired by recent advances in deep hashing methods (Xia et al. 2014; Lai et al. 2015; Li et al. 2016b; Wang et al. 2016) and discrete hashing methods (Kang et al.###The deep hashing methods include CNNH (Xia et al. 2014), NINH (Lai et al. 2015), DSRH (Zhao et al. 2015), SSDH (Zhang and Peng 2017), DRCSH (Zhang et al. 2015), DSCH (Zhang et al. 2015), DHN (Zhu et al. 2016), DPSH (Li et al. 2016b), DTSH (Wang et al. 2016).",impact-revealing,reporting prior findings and methods in image retrieval
398,5e8da0c991e011f2de583820,a24aad407737645bf2ef0148462b2b23f657fab1,Let's Agree to Degree: Comparing Graph Convolutional Networks in the Message-Passing Framework,5bdc31b417c44a1f58a0ba6c,How Powerful are Graph Neural Networks?,"Indeed, as we will shortly see, it follows from two independent works [Xu et al., 2019, Morris et al., 2019] that the distinguishing power of aMPNNs can be linked to the distinguishing power of the WL algorithm.###What follows is in fact an adaptation of Lemma 5 from [Xu et al., 2019] itself based on [Zaheer et al., 2017, Theorem 2].###It remains to argue that M anon is weaker than M WL . al., The proof is a trivial adaptation of the proofs of Lemma 2 in [Xu et al., 2019] and Theorem 5 in [Morris et 2019].###This result can be seen as a slight generalisation of the results in [Xu et al., 2019, Morris et al., 2019].###For anonymous MPNNs related to GNNs [Xu et al., 2019, Morris et al., 2019] and degree-aware MPNNs related to GCNs [Kipf and Welling, 2017], our main results are the following (see Theorems 5.5 and 5.7, and Propositions 6.9 and 6.10): (iv) On a ﬁxed input graph, the WL algorithm can be simulated,…###In two independent studies [Xu et al., 2019, Morris et al., 2019] the distinguishing power of GNNs is linked to the distinguishing power of the classical Weisfeiler-Lehman (WL) algorithm.###We remark that we cannot use the results in [Xu et al., 2019] and [Morris et al., 2019] as a black box because the class M anon is more general than the class considered in those papers.###Indeed, it sufﬁces to observe, just as we did in Example 3.2, that the aggregation functions f (cid:0) ) , based on Lemma 5 from [Xu et al., 2019].###Proposition 5.2 (Based on [Xu et al., 2019, Morris et al., 2019]) .###The former class of MPNNs covers the GNNs studied in [Xu et al., 2019, Morris et al., 2019], the latter class covers the GCNs [Kipf and Welling, 2017], among others.###in GINs Xu et al. (2019): to ensure injectivity. Since Xu et al. (2019) work over rational numbers in their proofs it suffices to choose irrational epsilons.###The proofs in [Xu et al., 2019] and [Morris et al., 2019] relate to graph neural networks which, in round t ≥ 1 , compute for each vertex v a label ℓℓℓ where comb and f aggr are general (computable) combination and aggregation functions which we assume to assign labels in A s t .",impact-revealing,linking the distinguishing power of aMPNNs to the WL algorithm through independent studies
92,5a73cb4d17c44a0b3035672d,7cd5d7f8295b219b029a4231ae5cffb261e00ebe,Early Active Learning with Pairwise Constraint for Person Re-identification,53e99be3b7602d970248a737,Active learning via transductive experimental design,"In [27], an early active learning via a Transduction Experimental Design algorithm (TED) is proposed with the aim of finding the subset V ⊂ X and a project matrix A that minimizes the least squared reconstruction error:###TED [27] Active learning via Transduction Experimental Design is an algorithm that selects a subset of informative samples from a candidate dataset.###The table also confirms that our algorithms are better than the RRSS and TED method by around 5% on rank one matching accuracy.###RRSS and TED have a similar optimization target to our algorithm but without pairwise constraint.###In [27], an early active learning via a Transduction Experimental Design algorithm (TED) is proposed with the aim of finding the subset V ⊂ X and a project matrix A that minimizes the least squared reconstruction error:
min V,A n∑ i=1 (‖xi −Vai‖22 + α‖ai‖22)
s.t.###In the category of early active learning, there are clustering-based methods [19,16] and transductive experimental design methods [27].###(2) is an NP-hard problem to solve, thus an approximate solution by a sequential optimization problem is proposed in [27].",impact-revealing,reporting on an early active learning algorithm and its comparison to other methods
1858,,cf60ad16e8b45648f8bef26cd134cf37f5b36216,Thyroid function tests in persons with occupational exposure to fipronil.,,,###Fipronil is a pesticide of the phenylpyrazole family widely used as a crop protection product and as an insecticide in pets (3).,impact-revealing,providing context about a specific pesticide
3417,5dd6604a3a55ac78684acf68,af54c890ffce96bae303310182be2ca301f2f97e,On Using SpecAugment for End-to-End Speech Translation,557c9120f66765fbb46b7043,RASR/NN: The RWTH neural network toolkit for speech recognition,We apply 40-dimensional Gammatone features [26] using the RASR feature extractor [27].,other,reporting method used for feature extraction
146,5cf48a40da56291d582a2f8e,44842bba66366522de782f537d9bc61d8868bf08,Revisiting Graph Neural Networks: All We Have is Low-Pass Filters,5a9cb66717c44a376ffb8c0f,Deeper insights into graph convolutional networks for semi-supervised learning,[16] empirically analyzed GCN models with many layers under the limited labeled data setting and stated that GCN would not do well with little labeled data or too many stacked layers.,impact-revealing,reporting findings on GCN models performance
2098,,a969b4563f2c9d19412f5d3add352d68ac88252e,Optimizing Airbnb Search Journey with Multi-task Learning,,,"###Journey Ranker is inspired by these previous works, especially [12] in our design of Base Module in Section 4.###Motivated by [12], we also model the six positive guest search milestones as sequential guest actions that are linked together using the chain rule of probability.###Such a strategy is similar to previous industry use cases that leverage multi-task modeling to model multi-step conversions, such as in e-commerce [12] and in customer acquisition [18].",impact-revealing,drawing inspiration from previous works for model design
1381,,c723dd3ed220ae0816387d50d1363e78a7307ed8,On the Closure Properties of the Class of Full G-models of a Deductive System,,,"###The class of protoalgebraic deductive systems was introduced with this name and thoroughly studied in [3], although essentially the same notion was used earlier in [15].###We conclude that the proposal of AlgS as the algebraic counterpart of a deductive system is coherent with previous literature in Abstract Algebraic Logic, such as [3, 4, 12, 13, 53, 59].###12, because it is well known [3, 15] that any protoalgebraic deductive system that satisfies the multiterm 265###This property has been thoroughly studied by its relations with other topics in Abstract Algebraic Logic, see [3, 6, 14, 15, 16].###There is a universal-algebraic-style theory of g-matrices parallel to, and inspired by, that of matrices, see [3, 7, 33, 59].###Using reduced matrices as the “canonical” models for a deductive system has been extraordinarily successful for a very wide class of deductive systems, now called protoalgebraic after [3] (more details are given in Section 1.###9 of [3] any deductive system with the multiterm DD theorem is protoalgebraic; and obviously it has theorems.###We build on the classical theories of deductive systems or sentential logics, of logical matrices, of abstract logics, and of protoalgebraic and algebraizable logics, as developed in [3, 4, 7, 8, 10, 17, 18, 33, 36, 59].",impact-revealing,acknowledge the introduction and study of protoalgebraic deductive systems in previous literature
1787,,ff9ad694b2ee40ae62bc5dcdc8082082a75ef107,Embedding Comparator: Visualizing Differences in Global Structure and Local Neighborhoods via Small Multiples,,,"###In contrast, our design is motivated by insights from visualization recommender systems [75, 76], which have promoted breadth-first exploration of data by adapting Edward Tufte’s maxim of prioritizing data variation over design variation [66].",impact-revealing,drawing inspiration from visualization recommender systems for design motivation
434,53e9b0deb7602d9703b5a742,3fd5a5f29c4b956ee6272855764ed400a84649a7,Japanese NER Post-Processing Based on Improved TBL Method,53e9acd3b7602d97036b29a9,Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging,Transformation Based Error-Driven Learning is an effective learning algorithm which was proposed by Eric Brill in 1992([14]).,impact-revealing,reporting prior findings on an effective learning algorithm
2539,5eccb534e06a4c1b26a83514,a9682a89b2fef793507c365a577f1521745db96c,Boosting the Transferability of Adversarial Samples via Attention,5d67a2a03a55ac09fb007e92,Deep Validation: Toward Detecting Real-World Corner Cases for Deep Neural Networks,"Failed attempts include pre-processing the input images to diminish malicious noises [11, 1], defensive distillation to mask gradients [26, 6], and feature squeezing to detect adversarial samples [40, 14, 38].",other,acknowledging previous unsuccessful methods in adversarial sample detection
794,5736982b6e3b12023e6fd099,684a466028785c39911770b96fb0c814e75b5b6d,DynaMOS: Dynamic schedule migration for heterogeneous cores,53e9bbf5b7602d9704851d5f,Single-ISA Heterogeneous Multi-Core Architectures for Multithreaded Workload Performance,"To address this disparity, researchers have designed heterogeneous multi-core processors [2] in which an application is mapped to the most efficient core that meets its performance requirements.###Higher switching costs of coarse-grained heterogeneous systems [1, 2] enforce switching granularities of the order of milli-seconds.",impact-revealing,highlighting the design of heterogeneous multi-core processors to improve performance
1040,,7409f0225461b451c13e154c32400746d0071ca0,The streaming k-mismatch problem,,,"###sketches developed in [15] for the offline k-mismatch with wildcards problem and combines it with the classic Karp–Rabin fingerprints for exact pattern matching [31]; see Fact 2.###This approach (the Karp–Rabin algorithm [31]) takes O(log n) bits of working space if random access to T is allowed, but it is not suitable for the streaming model because it essentially requires storing T [i−n+ 1 .###The standard scheme is based on polynomial hashes known as Karp–Rabin fingerprints [31].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1244,,7eb7788aaf1026a263b0d496e6ab12541b30f0a7,Functional Genomic and Proteomic Analysis Reveals Disruption of Myelin-Related Genes and Translation in a Mouse Model of Early Life Neglect,,,"###This was motivated by the fact that functional imaging methods have demonstrated medial prefrontal hypoactivity in patients with trauma histories that is strongly correlated with the severity of symptoms (Shin et al., 2006).",impact-revealing,highlighting the correlation between medial prefrontal hypoactivity and trauma symptoms
1886,,101aaef2b56c6a6c3f0b02dcca0f1720e8459e66,THE COMMUNITY OF INQUIRY FRAMEWORK IN ONLINE ENGLISH LEARNING: INSIGHTS FROM INDONESIAN CULTURAL CONTEXTS,,,"###This concept encapsulates learners’ capacity to express themselves socially and emotionally, fundamental for forming connections, interpersonal relationships, and establishing trust (Castellanos-Reyes, 2020; Garrison, 2017; Garrison et al., 2000).###Originally conceptualized by Garrison et al. (2000) and further explored by researchers such as Castellanos-Reyes (2020), the CoI framework has been instrumental in guiding the development of rich, interactive online learning environments.###The coding scheme employed to address the second research question on the manifestation of the CoI framework in collaborative learning activities was referenced from Garrison et al. (2000).###Central to this exploration is the Community of Inquiry (CoI) Framework, a theoretical model introduced by Garrison et al. (2000).###It is central to the learners’ ability to individually and collaboratively construct and confirm meaning through sustained reflection and discourse (Garrison et al., 2000, 2001).###Drawing from this perspective, this section analyzes the roles and practices of instructors in online English courses, focusing on how they embody the three critical functions of teaching presence: instructional management, building understanding, and direct instruction (Garrison et al., 2000).###The use of appropriate technologies in virtual learning environments further enhances this presence, ensuring that learning processes are both engaging and effective (Cleveland-Innes et al., 2019; Garrison, 2017; Garrison et al., 2000).###From this perspective, the study’s findings can be interpreted in light of the three presences within the CoI framework that emphasize cognitive, social, and teaching presences (Castellanos-Reyes, 2020; Garrison et al., 2000).",impact-revealing,highlighting the significance of the Community of Inquiry framework in online learning environments
2391,53e9b0e6b7602d9703b63850,b0bca59d8cf1e0332caa111f4b91756cac61040f,dynamically managed multithreaded reconfigurable architectures for chip multiprocessors,53e9ad2cb7602d970370cef1,Learning-Based Smt Processor Resource Distribution Via Hill-Climbing,"[10] S. Choi and D. Yeung.###Numerous SMT resource management techniques exist that aim to either directly [7, 10] or indirectly [15, 42] control the amount of processor resources that any thread consumes.###Previous work by Choi and Yeung [10] investigated hill climbing for SMT resource allocation 
among concurrently running threads.###Previous work by Choi and Yeung [10] investigated hill climbing for SMT resource allocation among concurrently running threads.",other,acknowledge prior work on SMT resource management techniques
2053,,ab929295d6843833159ab9b4d980bdf91bfc62a8,Nervous System Hemangiopericytoma,,,"###Still, microsurgical resection has been shown to provide poor long-term control of intracranial disease when employed solely, despite being considered as the treatment of choice.(2,3,10,28) In many cases, tumor location and neuroanatomical features may not allow a GTR.###4% of meningeal lesions.(3,7,10,11) The first report of an intracranial HPC can be dated to 1954, by Begg and Garret,(12)###reported that the addition of adjuvant EBRT after resection increased PFS and OS from 34 and 62 months to 75 and 92 months, respectively.(3) Adjuvant EBRT or SRS after a GTR was not shown to improve OS significantly, yet still improved local tumor control rates.###%) patients with WHO-III HPCs.104
Jeon et al.129 recently reported on the efficacy of adjuvant RT in a cohort of 49 patients with intracranial HPC.###In Part I, we focus our discussion on the challenges of intracranial HPC.###Angiogenic pathway studies paved the way to initial testing of bevacizumab, a monoclonal anti-VEGFR antibody commonly used for the treatment of colorectal cancer and recurrent glioblastoma in intracranial HPC.112 Initial studies show activity against HPCs when administered alone29,113–115 or in combination with temozolomide.116 Tyrosine kinase inhibitors targeting either EphA2 or EphB4 are another potential therapeutic venue.117
Preoperative embolization of tumor-related feeding vessels (similar to common practice in meningioma surgery, embolizing ECA vessels) can prove helpful in controlling operative bleeding.###Surgical morbidity for a small osseous, vertebral body-centered ED-HPC is very different clinically and surgically from a large ID-IM HPC. Preoperative electrophysiological evaluation and functional imaging studies as well as intra-operative electrophysiological monitoring are crucial and indispensable.###Achieving a GTR at the first operation was shown to be strongly associated with the prolongation of overall survival (OS) and progression-free survival (PFS) in intracranial HPC.3,10,17,25,29,46,101,102 Still, microsurgical resection has been shown to provide poor long-term control of intracranial disease when employed solely, despite being considered as the treatment of choice.2,3,10,28 In many cases, tumor location and neuroanatomical features may not allow a GTR. HPC of the skull-base not allowing dural resection, lesions involving the cavernous sinus or other dural venous sinuses not allowing tumor GTR with margins, HPC encroaching or casting critical neurovascular structures such as cranial nerves, major arterial branches are a few such limitations.###Achieving a GTR at the first operation was shown to be strongly associated with the prolongation of overall survival (OS) and progression-free survival (PFS) in intracranial HPC.(3,10,17,25,29,46,101,102) Still, microsurgical resection has been shown to provide poor long-term control of intracranial disease when employed solely, despite being considered as the treatment of choice.###Li et al.97 recently reported a cohort of 94 patients operated for spinal HPC.###The patient-friendly design and supporting science and data of SRS makes this a valuable tool in our armamentarium, and future studies are required to directly prove its role in the management of spinal HPC.###SRT (a hybrid technique halfway between EBRT and SRS discussed later) can also serve as a potent salvage strategy for the treatment of recurrent intracranial HPCs.(3,25,47)",impact-revealing,highlighting the challenges and limitations in the treatment of intracranial HPC
1232,,ff4398e15dbd46145c0cf5869c8408374651f9cd,"Meta‐analyses, Metrics and Motivation: Mixed Messages in the Fish Passage Debate",,,"###The Williams and Katopodis commentary raises concerns about the incorrect use of some data by Bunt et al. (2012), which serves to highlight risks that can be inherent in meta-analyses more generally (Greco et al. 2013).###(2012), which serves to highlight risks that can be inherent in meta-analyses more generally (Greco et al. 2013).###However, the results of metaanalyses can also be controversial, with several difficulties that can arise from using meta-data identified (see Greco et al. 2013 for an interesting discussion related to health).",impact-revealing,highlighting risks and controversies in meta-analyses
1730,,83a307315f3f72bdf5f8f936c9fbd2f44efcf93c,Towards serializable replication with snapshot isolation,,,###More recent work has been motivated by the realization that schemes based on distributed two-phase locking do not provide adequate performance [11].,impact-revealing,highlighting limitations in existing distributed locking schemes
619,59ec02da0cf22f5df7319dc3,c27db32efa8137cbf654902f8f728f338e55cd1c,mastering the game of go without human knowledge,56ab70cd0cf2c98bf5bc717a,Mastering the game of Go with deep neural networks and tree search.,"Our program, AlphaGo Zero, differs from AlphaGo Fan and AlphaGo Lee 12 in several important aspects.###In our evaluation, all programs were allowed 5 s of thinking time per move; AlphaGo Zero and AlphaGo Master each played on a single machine with 4 TPUs; AlphaGo Fan and AlphaGo Lee were distributed over 176 GPUs and 48 TPUs, respectively.###We follow the formalism of alter­ nating Markov games described in previous work 12 , noting that algorithms based on value or policy iteration extend naturally to this setting 39 .###AlphaGo Zero and AlphaGo Master played on a single machine on the Google Cloud; AlphaGo Fan and AlphaGo Lee were distributed over many machines.###…data, we trained a second neural network (using the same architecture) to predict expert moves in the KGS Server data­ set; this achieved state­of­the­art prediction accuracy compared to pre­ vious work 12,30–33 (see Extended Data Tables 1 and 2 for current and previous results, respectively).###The published version 12 , which we refer to as AlphaGo Fan, defeated the European champion Fan Hui in October 2015.###This neural network combines the roles of both policy network and value network 12 into a single architecture.###Each simulation starts from the root state and iteratively selects moves that maximize an upper confidence bound Q ( s , a ) + U ( s , a ), where U ( s , a ) ∝ P ( s , a ) / (1 + N ( s , a )) (refs 12, 24), until a leaf node s ′ is encountered.",impact-revealing,highlighting the differences and improvements in AlphaGo Zero compared to previous versions
3897,5c3ff3ecdf5b8c0b3cd013d4,8e68fdceb150d1c93ec9923ffcf766454201fbd3,Advanced recurrent network-based hybrid acoustic models for low resource speech recognition,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"The residual network approach [31] was successfully applied to train more than 100 convolutional layers for image classification and detection.###In recent years, some novel architectures, like residual net [31] and highway networks [32] have introduced an additional spatial shortcut path from lower layers for efficient training of deep networks with multiple layers.",other,acknowledge advancements in deep learning architectures for image classification
2602,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,53e99d74b7602d9702630cf5,Peeling Away Timing Error in NetFlow Data.,"[132] characterized, quantified, and corrected timing errors, which are consequence of Cisco NetFlow version 9 protocol design that estimates the true base time from derived base time information.",other,reporting prior findings on timing errors in Cisco NetFlow protocol
2145,,82064ee08ddb289aa79bf81fffd705bc4660f798,The Role of a Sustainability Informatics Framework in Transportation Systems,,,"###Keeping in line with the 3BL, this paper builds on the Energy Informatics (Watson et al., 2010) and Belief-Action-Outcome (Melville, 2010) frameworks to develop the Sustainability Informatics Framework (SIF).###A background section in four parts introduces the concept of sustainable transportation systems, reviews the Energy Informatics framework, reviews the BeliefAction-Outcome framework, and summarizes the background as a basis for developing the Sustainability Informatics Framework.###We connect energy informatics with sustainability informatics to then extend the Energy Informatics framework to include sustainability.###The Energy Informatics framework is a practical supply-and-demand
schematic whereas the Belief-Action-Outcome framework is explanatory, delving into the interplay between society and individuals.###DOI: 10.4018/978-1-4666-8473-7.ch023
This article develops a Sustainability Informatics Framework, a framework that connects Information Technology with sustainability and is based on the Belief-Action-Outcome and Energy Informatics frameworks.",impact-revealing,developing a new framework based on existing theories
3555,5fb24ee191e01186d3f5decc,050fecf6e3e21faeeb7629f179ba839c4dfd1ead,EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints,59ae3bf12bbe271c4c71c09a,Controlling Linguistic Style Aspects in Neural Language Generation.,"…content or style constraints (Abu Sheikha and Inkpen, 2011; Mei et al., 2016), or via segment-level “side-constraints” (Sennrich et al., 2016a; Ficler and Goldberg, 2017; Scarton and Spe-cia, 2018), which condition generation on users’ stylistic preferences, but do not offer ﬁne-grained…",other,acknowledge existing methods for content or style constraints
838,5cf48a30da56291d58292a2f,ed45bc75ca3406866e1bb9e95ad251536e9b985c,few-shot adversarial learning of realistic neural talking head models,5bdc31b817c44a1f58a0c572,Large Scale GAN Training for High Fidelity Natural Image Synthesis.,"[19], but replace downsampling and upsampling layers with residual blocks similarly to [2] (with batch normalization [15] replaced by instance normalization [36]).###Our meta-learning stage uses the adaptive instance normalization mechanism [14], which was shown to be useful in large-scale conditional generation tasks [2, 34].###We also use self-attention blocks, following [2] and [42].",impact-revealing,describing modifications to existing methods
3307,5eccb534e06a4c1b26a838ac,2709167f1c3a03fa5b970a665ea48ed243aab582,Designing Network Design Spaces,57a4e91aac44365e35c9811d,Wide Residual Networks.,"Our ﬁnal quantized linear parameterization shares similarity with previous work, e.g . how stage widths are set [26, 7, 32, 11, 9].###A common practice is to apply network scaling rules, such as varying network depth [8], width [32], resolution [9], or all three jointly [29].",other,acknowledge similarities with previous work on parameterization
4012,5e524da993d709897cb69fde,09e48adf1a3f012ec56fbb4f92acae975ee43d87,a deep learning approach to antibiotic discovery,5c756cd3f56def97984cb801,"Biology of Acinetobacter baumannii: Pathogenesis, Antibiotic Resistance Mechanisms, and Prospective Treatment Options","Of note, the World Health Organization has designated A. baumannii as one of the highest priority pathogens against which new antibiotics are urgently required (Lee et al., 2017; Perez et al., 2007).###After ranking the candidates according to the model’s predicted score, we selected a list of promising candidates.",other,highlighting the urgency of addressing A. baumannii as a priority pathogen
803,5ed12ca69e795e8ab1c11568,b3b60a15502abca31ae7b20ce16bf50049183c5a,knowledge enhanced personalized search,5550441845ce0a409eb4b2dd,Entity query feature expansion using knowledge base links,"Some take entities contained in the query or document as a kind of relevance ranking features, such as term weight in queries according to entity descriptions [8, 12].###Using such noisy query entities in ranking often requires manual annotations [12] or soft linking/diversification [42].",impact-revealing,providing context on relevance ranking features
216,5e09caba3a55ac662f721afe,36ad06e6f9b39192e7668634eadd6fcf9593e922,Efficient Adversarial Training With Transferable Adversarial Examples,5ca600ae6558b90bfa4d76e9,Towards Deep Learning Models Resistant to Adversarial Attacks.,"To generate strong adversarial examples, iterative attacks [13], which use multiple attack iterations to generate adversarial examples, are widely adopted in various adversarial training methods [4,18,28,34].###For the baseline, we use the author implementation of MAT 4 [21], TRADES 5 [39], YOPO 6 [39], and Free 7 [27] with the hyper-parameters recommended in their works, and we select 1 /λ as 6 for TRADES (both ATTA and PGD).###With a higher value of k (more attack iterations), PGDk can generate adversarial examples with higher loss [18].###In Section 3, which analyzes the transferability between training epochs, we use MAT with PGD-10 to train models and PGD-20 to calculate loss value and error rate.###For MAT, compared to PGD-10 , ATTA-1 achieves 3 .###08% MAT loss 85 .###Since adversarial perturbations are usually bounded by a constrained space S and attack perturbations outside S need to be projected back to S , k-step projected gradient descent method [13,18] (PGDk) has been widely adopted to generate adversarial examples.###We select four state-of-the-art adversarial training methods as baselines: MAT [21], TRADES [39], YOPO [37] and Free [27].###Recently, lots of works [2,10,11,18,22,33,34] focus on analyzing and improving adversarial machine learning.###For CIFAR10, compared to MAT whose training time is more than one day, our method achieves comparable adversarial accuracy in about 2 hours.###[18] first formulate adversarial training as a min-max optimization problem:###As shown in Table 6, for both MAT (ATTA-1 ) and TRADES (ATTA-1 ), models trained with inverse data augmentation achieve about 6% higher accuracy, which means that inverse data augmentation does help improve the transferability between training epochs.###Design of robust models, which correctly classifies adversarial examples, is an active research area [5, 9, 14, 21, 30], with adversarial training [18] being one of the most effective methods.###Trained with ATTA, the adversarial accuracy of MAT can be improved up to 7 .###We apply our technique on Madry’s Adversarial Training method (MAT) [18] and TRADES [34] and evaluate the performance on both MNIST and CIFAR10 dataset.###By comparing the experiment results with YOPO and Free, for MAT, our method is 3 .###In this section, we integrate ATTA with two popular adversarial training methods: Madry’s Adversarial Training (MAT) [21] and TRADES [39].###We apply our technique on Madry’s Adversarial Training method (MAT) [21] and TRADES [39] and evaluate the performance on both MNIST and CIFAR10 dataset.###Since adversarial perturbations are usually bounded by the allowed perturbation space S , PGD-k (k-step projected gradient descent [13]) is adopted to conduct iterative attack [18, 23, 32, 34].###Typically, using more attack iterations (higher value of k) produces stronger adversarial examples [18].###To compare the attack strength of two attacks, we use Madry’s method [18] to adversarially train two models on MNIST and CIFAR10 and evaluate the loss value L(fn(x ∗ n), y) of adversarial examples generated by two attacks.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
799,5cf48a3cda56291d5829eb69,a0852cd9a026bc90168fa85fa422cb0e48f98394,Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss,599c797d601a182cd26438f7,Deep Cross-Modal Audio-Visual Generation.,"Modeling the dynamics of a moving human face/body conditioned on another modality is a fundamental problem in computer vision, where applications are ranging from audio-to-video generation [28, 3, 2] to text-to-video generation [23, 19] and to skeleton-to-image/video generation [21, 7].",impact-revealing,highlighting the significance of modeling dynamics in computer vision applications
1137,,92a6c29885f34f35d464464bdaa129a209f45b23,Spatio-temporal Diffusion Point Processes,,,"###Denoising diffusion probabilistic models (DDPM) [19, 47, 48], are a class of deep generative models, which are inspired by nonequilibrium thermodynamics.###Denoising diffusion probabilistic models (DDPM) [19, 49, 50], are a class of deep generative models, which are inspired by non-equilibrium thermodynamics.",impact-revealing,providing context on denoising diffusion probabilistic models
136,5eede0b091e0116a23aafb82,1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af,Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,5b3d98cc17c44a510f801acc,Unsupervised Feature Learning via Non-parametric Instance Discrimination,"In addition, SwAV works with small and large batch sizes and does not need a large memory bank [49] or a momentum encoder [20].###Instance-level classification considers each image in a dataset as its own class [4, 15, 49].###[49] mitigate this issue by replacing the classifier with a memory bank that stores previously-computed representations.###where τ is a temperature parameter [49].###Since computing all the pairwise comparisons on a large dataset is not practical, most implementations approximate the loss by reducing the number of comparisons to random subsets of images during training [9, 20, 49].###A similar comparison appears in contrastive learning where features are compared directly [49].###This solution is inspired by contrastive instance learning [49] as we do not consider the codes as a target, but only enforce consistent mapping between views of the same image.###Among these improvements are the use of stronger data augmentation [9], MLP projection head [9], cosine learning rate schedule [37], temperature parameter [49], memory bank [49], multi-clustering [2], etc.",impact-revealing,providing context on SwAV and its operational characteristics
1095,,23c9b43e1ad804a22ee042df06831c70a1961c02,Growing Bayesian network models of gene networks from seed genes,,,"###These works build upon the algorithm (Friedman et al., 1999) which, in order to scale to high-dimensional data, restricts the search for the parents of each node to a small set of candidate parents that are heuristically selected in advance.",impact-revealing,describing the algorithm's approach to high-dimensional data
2088,,4509a538f6c3718470e58bedfea9e96a2d49b77e,Current Methods for Automated Filtering of Multiple Sequence Alignments Frequently Worsen Single-Gene Phylogenetic Inference,,,"###To investigate the differences with respect to cluster size (and thus number of species per alignment), we defined subsets depending on cluster size with respect to the total number of species ns (=57): “small MSAs” cover fewer than ns/2 species and contain fewer than ns/2 genes; “medium MSAs” cover at least ns/2 species and contain between ns/2 and 3ns/2 genes; “large MSAs” cover at least ns/2 species and contain more than 3ns/2 genes.###The combined set is then aligned and the various filtering methods are applied to the resulting MSAs.###Next, we show that our main findings hold when controlling for numerous potential confounding factors (MSA/tree inference programs, sequence length, sequence divergence, alignment “gappiness”), and when considering the minimum duplication test.###While it is possible to infer phylogenetic trees directly from sequence data (Jun et al. 2009), the most common inference methods build on multiple sequence alignments (MSA) (Felsenstein 1989; Ronquist and Huelsenbeck 2003; Stamatakis 2006; Guindon et al. 2010).###Two sets of 500 30-sequence MSAs were simulated using Artificial Life Framework (ALF) (Dalquen et al. 2012).###Whereas Gblocks and TrimAl are based on sitewise summary statistics of MSAs, other methods are based on mathematical models.###Hence, the accuracy of phylogenetic trees inherently depends on the accuracy of the underlying MSA.###However, computing such an MSA can be challenging.###—Phylogenetic inference is generally performed on the basis of multiple sequence alignments (MSA).###The tests based on Ensembl data use version 66 (February 2012), which contains 19,491 homologous clusters (and their corresponding MSAs/trees) containing 969,577 protein-coding genes from 57 species.###Thus, uniquely among the filtering methods considered here, Guidance requires knowledge of the aligner that was used to produce the MSA to be filtered.###Given a set of sequences, an ideal MSA identifies homologous characters, that is, characters having common ancestry.###These categories are disjoint and collectively cover 97.4% of all the MSAs we could process (Supplementary Table 1 available on Dryad at http://dx.doi.org/10.5061/dryad.pc5j0).###2009), the most common inference methods build on multiple sequence alignments (MSA) (Felsenstein 1989; Ronquist and Huelsenbeck 2003; Stamatakis 2006; Guindon et al. 2010).###Guidance estimates the reliability of alignment columns from their respective frequencies in “resampled” MSAs obtained based on guide tree bootstrap replicates (100 replicates by default).###Subsequently, trees are estimated from the filtered and unfiltered MSAs.###Filtering—that is, removing unreliable columns before tree reconstruction—has been promoted as a way to increase the signal to noise ratio of MSAs (Talavera and Castresana 2007).###Aliscore v.1.0 (Kück et al. 2010) assesses the randomness of a MSA by considering all the induced pairwise alignments separately, using a sliding window.###Based on multiple genome-wide empirical and simulated data sets, we show that the trees obtained from filtered MSAs are on average worse than those obtained from unfiltered MSAs.",impact-revealing,describing the methodology for phylogenetic inference and MSA filtering
2646,599c795d601a182cd2635171,7743150b3fe21b5cc2ebe9e8a67f54031311f7ae,Smart Mining for Deep Metric Learning,573697846e3b12023e66a80e,Learning to Compare Image Patches via Convolutional Neural Networks,"The development of deep metric learning models for the estimation of effective feature embedding [2, 4, 9, 11, 15, 16, 17, 13, 22, 25, 27, 26] is at the core of many recently proposed computer vision methods [3, 14, 19, 24, 28].",other,highlighting the significance of deep metric learning models in computer vision
1252,,70e2a67afea68cd2c318a5433072b1b6d8d30652,Exploratory Tests of Crude Bacteriocins from Autochthonous Lactic Acid Bacteria against Food-Borne Pathogens and Spoilage Bacteria,,,"###aureus through bacteriocins [7], [38].###However, it should be noticed that this is probably opportunist infections [7].###aureus are the two bacterial species commonly used as target strain for screening antimicrobial agent generally, and specifically the bacteriocins produced by lactic acid bacteria in question [7].",impact-revealing,highlighting the common use of bacterial species in antimicrobial agent screening
3779,599c7988601a182cd2648a09,6b7d6e6416343b2a122f8416e69059ce919026ef,Inductive Representation Learning on Large Graphs.,573696096e3b12023e51c873,Discriminative Embeddings of Latent Variable Models for Structured Data,"There are also a number of recent neural network approaches to supervised learning over graph structures [7, 10, 21, 31].",other,acknowledge recent neural network approaches
2154,,7b0b7080f04f074875a559f56a0737476764ad6a,Iteration Theories: The Equational Logic of Iterative Processes,,,"###1 of a partial correctness assertion as an equation was inspired by a section in the book [MA86], where it was shown how to formulate partial correctness by an equation in a partially additive category.",impact-revealing,highlighting inspiration from existing literature for a specific formulation
374,5bbacb9e17c44aecc4eaff64,5b1516c87818084dc5d195cc274e1ee8923210d2,Neural Cross-Lingual Named Entity Recognition with Minimal Resources,573696106e3b12023e5227c8,Neural Architectures for Named Entity Recognition.,"We will first describe the basic hierarchical neural CRF tagging model (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2016), and introduce the self-attention mechanism that we propose to deal with divergence of word order.###…or organizations, has seen the state-of-the-art greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018).###In this paper, we closely follow the architecture proposed by Lample et al. (2016), and use bi-directional LSTMs for both the character level and word level neural networks.###Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018).###We will ﬁrst describe the basic hierarchical neural CRF tagging model (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2016), and introduce the self-attention mechanism that we propose to deal with divergence of word order.",impact-revealing,acknowledge existing methods in named entity recognition
32,5ce2d0feced107d4c63dd498,d07284a6811f1b2745d91bdb06b040b57f226882,Decoupled Weight Decay Regularization,5550415745ce0a409eb3a739,Adam: A Method for Stochastic Optimization.,"Now, let’s turn to adaptive gradient algorithms like the popular optimizer Adam Kingma & Ba (2014), which scale gradients by their historic magnitudes.###Adaptive gradient methods, such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2014) and most recently AMSGrad (Reddi et al., 2018) have become a default method of choice for training feed-forward and recurrent neural networks (Xu et al., 2015; Radford et…",impact-revealing,acknowledge the popularity and evolution of adaptive gradient algorithms
906,5ccef1b26558b90bfac1ef8f,5e193498be869c44d68031106c27658059c0edd2,Predicting adverse drug reactions of combined medication from heterogeneous pharmacologic databases,58d83051d649053542fea321,Discovering Drug-Drug Interactions And Associated Adverse Drug Reactions With Triad Prediction In Heterogeneous Healthcare Networks,"For example, it is estimated that up to 82% Americans take one or more drugs, and 29% take more than four drugs together [3].###org [23], a popular online health community [3].###A recent survey shows that 72% of Internet users went online to seek health information [3, 21].",impact-revealing,providing statistics on drug usage and health information seeking behavior
1943,,d61ac2e0a8fe1db98a662b216f1e48447e2ec158,"Gender Diversity Programs, Perceived Potential for Advancement, and Organizational Attractiveness",,,"###…we build on and extend existing research that has utilized a signaling logic to examine individual reactions to diversity management (e.g., Allen et al., 2007; Erhart & Ziegert, 2005; Martins & Parsons, 2007; Rynes, 1991; Rynes et al., 1991; Smith et al., 2004; Turban & Greening, 1996;…###Our findings provide empirical support for claims that signaling theory provides an explanation of individuals’ perceptions of organizations (e.g., Allen et al., 2007; Martins & Parsons, 2007; Rynes et al., 1991; Smith et al., 2004; Turban & Greening, 1996; Walker et al., 2012, 2007).",impact-revealing,building on existing research to support claims about signaling theory in diversity management
1925,,1931cf3984151c92db5017572752a499226ae976,FEATURE EXTRACTION FOR BIOMETRIC RECOGNITION USING MILLIMETRE-WAVE IMAGES,,,"###This is inspired by previous works, which show that recognition through the shape and boundary of traits such as hand or signature are fairly reliable [Jain et al., 2002; Yoruk et al., 2006].",impact-revealing,highlighting the reliability of recognition methods based on shape and boundary
1070,,efd306713626a422089c78be78ea39fc515f9d6e,Mining and classifying customer reviews: a survey,,,"###Fan and Chang (2011) used IG, PMI, and Chi-square for feature selection.###Then, PMI was used to calculate the semantic orientation.###They proposed a novel algorithm, PMI–TFIDF, which used both the association of features and domain entities.###However, Chi-square is better than PMI to represent subjective and vague opinions with its normalized value.###PMI is good for collection extraction and shows significant results with normalized PMI (Bouma 2009) and the variations of PMI by incorporating significant co-occurrence (Damani 2013) as semantic of opinions are captured.###The popular and widely used methods in opinion mining applications include Term Frequency Inverse Document Frequency(TF-IDF) (Zheng et al. 2009; Li and Tsai 2013; Moraes et al. 2013; Khairnar and Kinikar 2013; Basari et al. 2013; Martineau et al. 2009), Point-wise Mutual Information (PMI) (Cover and Thomas 2012; Khairnar and Kini-kar 2013), Chi-Square (Khairnar and Kinikar 2013; Fan and Chang 2011; Hagenau et al. 2013), Information Gain (IG) (Moraes et al. 2013), Best Matching 25 (BM25) (Vechto-mova 2010), Uniformity (Uni) (Li and Tsai 2013), Inverted Conformity Frequency (ICF) (Li and Tsai 2013) and Latent Dirichlet Allocation(LDA).",impact-revealing,reporting various feature selection methods and their applications in opinion mining
2655,5736982b6e3b12023e6fd332,d0b8c5464b582e1a582a4affb270e8b02f8414a2,a scalable architecture for ordered parallelism,53e9bd9eb7602d9704a4a194,Bulk Disambiguation Of Speculative Threads In Multiprocessors,"[11] L. Ceze, J. Tuck, J. Torrellas et al., “Bulk disambiguation of speculative threads in multiprocessors,” in ISCA-33, 2006.###TCC [29] and Bulk [11] are the exception: they do not forward data and only abort later readers when the earlier writer commits.###By contrast, most TLS systems use lazy versioning (buffering speculative data in caches) or more expensive multiversioning [11, 25, 28, 29, 56, 60, 61, 66, 68, 69] to limit the cost of aborts.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3770,5d3c234c3a55acd386d4e112,a9ec03dbe702f6909acd1f1f14a3395d0141043b,generative models for graph-based protein design,55a47b4a65ce31bc877ca18d,Protein 3d Structure Computed From Evolutionary Sequence Variation,"We accomplish the ﬁrst two by leveraging a well-evidenced ﬁnding in protein science, namely that long-range dependencies in sequence are generally short-range in 3D space [10–12].",other,highlighting a foundational principle in protein science
1932,,9030cfe5162483f185a29cf52941b3901b35de46,Complement receptor 1 gene (CR1) intragenic duplication and risk of Alzheimer’s disease,,,"###This has highlighted the importance of the immune response to amyloid plaque formation in Alzheimer’s disease, possibly mediated by microglial cells (Efthymiou and Goate 2017; Naj and Schellenberg 2017; Villegas-Llerena et al. 2016).",impact-revealing,highlighting the significance of immune response in Alzheimer's disease
1027,,963827fdf3a0e9ae7613a5a0a02dfbfb04caee4a,Reducing Cross-Cultural Comparability Bias Obtained with Measurement Invariance Analysis by Means of Anchoring Vignettes in Heterogeneous Refugee Samples,,,"###; https://doi.org/10.1101/2023.02.17.23286077doi: medRxiv preprint
10
certain levels of measurement invariance are necessary in order to make bias free statistical comparisons (Millsap, 2011; Meredith 1993; Wu et al., 2007).###…the results often point to data that is not suitable for the comparisons under investigations, which in turn is associated with data not supporting measurement invariance between different countries and languages (e.g., Davidov et al. 2018; Lee et al., 2020; Zercher et al., 2015; Wu et al., 2007).###, 2022), but the results often point to data that is not suitable for the comparisons under investigations, which in turn is associated with data not supporting measurement invariance between different countries and languages (e.g., Davidov et al. 2018; Lee et al., 2020; Zercher et al., 2015; Wu et al., 2007).###10 certain levels of measurement invariance are necessary in order to make bias free statistical comparisons (Millsap, 2011; Meredith 1993; Wu et al., 2007).###…to measurement invariance in cross-cultural studies, researchers often
fail to support strong or even weak invariance in their data, as shown by Wu et al. (2007) for the Trends in International Mathematics and Science Study (TIMMS), by Davidov and colleagues (Davidov et al., 2008, 2014, 2018;…",impact-revealing,highlighting the importance of measurement invariance in cross-cultural studies
1960,,48c8a6f110c4d7c3c99084729c3fd9ea80456f6a,Analysis and Design of Electrodes for Deep Brain Stimulation,,,"###[13, 38], but access to this charge is limited by the rate of electron and ion transport within the coating [39].###and pacing thresholds [40], but diffusion limitations prevent accessing the full surface area during short duration stimulation pulses [38, 41].###oxide reduce interface impedance and increase charge capacity for stimulation [13, 38], but charge capacity during rapid pulsing is limited by the rate of electron and ion transport [58].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2841,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,53e9b6d6b7602d9704262abb,Multimodal fusion using learned text concepts for image categorization.,"While performing image categorization, the accuracy of the classification is measured in terms of image category detection rate [156].###4 Multimodal fusion using visual and text cues for image classification based on pair-wise SVM classifier [156] 354 P.###[156] have reported a multimodal fusion framework to classify the",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
866,5edb32399e795ec54fd81737,6c6c265c6d1de08f03fed0da0604bef5307fbcec,adagcn: adaboosting graph convolutional networks into deep models.,5cede0e2da562983788c187b,Multi-Stage Self-Supervised Learning For Graph Convolutional Networks On Graphs With Few Labeled Nodes,"Inspired by the Layer Effect on graphs (Sun et al., 2019), we argue that the increase of layers in AdaGCN can result in more benefits on the efficient propagation of label signals especially on graphs with limited labeled nodes.",impact-revealing,highlighting the potential benefits of increased layers in AdaGCN for label signal propagation
1904,,847828fa6479f3f5312f11dcecde87690e9ec9a0,Students’ Cultural and Personality Factors as Predictors of their Asynchronous Online Discussion Behaviours,,,"###This is important since previous studies have shown (Gunawardena, et al., 1997; Garrison et al., 2000; Pena-Shaff & Nicholls, 2004) that students learn and engage in knowledge creation activities during online discussions as they build on others’ ideas, as well as when they disagree with others.",impact-revealing,highlighting the importance of previous studies on student engagement in online discussions
402,5c8d57d74895d9cbc653a9e9,3afa826a594d90ee0f4fe062c988289bb213a114,Deep Back-Projection Networks For Super-Resolution,599c7982601a182cd2645d4e,Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution,"Significant progress in deep learning for vision [15, 13, 5, 39, 26, 33, 17] has recently been propagating to the field of super-resolution (SR) [19, 29, 6, 12, 20, 21, 24, 42].###The depth analysis of DBPNs compare to other networks (VDSR [21], DRCN [22], DRRN [42], LapSRN [24]) on Set5 dataset for 4× enlargement.###(c) Progressive upsampling was recently proposed in LapSRN [24].###(c) Progressive upsampling uses a Laplacian pyramid network which gradually predicts SR images [24].###A currently typical approach is to construct an HR image by learning non-linear LR-to-HR mapping, implemented as a deep neural network [6, 7, 37, 24, 21, 22, 42].###DRCN [22], DRRN [42], LapSRN [24], and EDSR [30].",impact-revealing,highlighting the influence of deep learning advancements on super-resolution techniques
3464,5cede0e5da562983788c40d8,e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b,Unsupervised Embedding Learning via Invariant and Spreading Instance Feature,573696f46e3b12023e5f0d4d,Delving Deep into Rectifiers: Surpassing Human-Level Performance on   ImageNet Classification,"Deep embedding learning is a fundamental task in computer vision [14], which aims at learning a feature embedding that has the following properties: 1) positive concentrated, the embedding features of samples belonging to the same category are close to each other [32]; 2) negative separated, the embedding features of samples belonging to different categories are separated as much as possible [52].",other,providing context on deep embedding learning in computer vision
3370,5f0bde8e9e795ea206ff8ef5,0feea94f89d395436bf41bd10c797447eecbc128,Unsupervised data augmentation for consistency training,5a260c0917c44a4ba8a1dfd2,Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.,"For example, Mean Teacher [58] maintains a teacher model with parameters being the ensemble of a student model’s parameters and enforces the consistency between the predictions by the teacher model and the student model.###Works in this category include Mean Teacher [58], fast-Stochastic Weight Averaging [1] and Smooth Neighbors on Teacher Graphs [38].###Works in this category include Mean Teacher [5], fast-Stochastic Weight Averaging [33] and Smooth Neighbors on Teacher Graphs [29].###Recently, Athiwaratkun et al. [1] propose fast-Stochastic Weight Averaging that improves Π -Model and Mean Teacher by encouraging the model to explore a diverse set of plausible parameters.###The recent works in SSL are diverse but those that are based on consistency training [2, 3, 4, 5] have shown to work well on many benchmarks.###06 Mean Teacher [5] Shake-Shake 26M 6.###We compare UDA with Pseudo-Label [36], an algorithm based on self-training, Virtual adversarial training (VAT) [44], an algorithm that generates adversarial Gaussian perturbations on input, Π -Model [35], which combines simple input augmentation with hidden state perturbations, Mean Teacher [58], which enforces smoothness on model parameters and MixMatch [3], a concurrent work that uniﬁes several prior works on semi-supervised learning.",other,describing various methods in semi-supervised learning
1632,,e984ffc1a83fc8a7c10811a1ee51e435896e3e29,Molecular approaches for bacterial azoreductases,,,"###Identification of bacteria based on 16S ribosomal RNA gene (16S rRNA) sequences has been used extensively for molecular taxonomic studies as an attractive alternative to the methods following traditional standard references such as Bergey’s Manual of Systematic Bacteriology or the Manual of Clinical Microbiology (Clarridge, 2004; Woo et al., 2008; Anjaneya et al., 2011).###The 16S rRNA gene is now most commonly used for bacterial taxonomic purposes (Tortoli, 2003; Clarridge, 2004; Seesuriyachan et al., 2007; Woo et al., 2008).###Using 16S rRNA sequencing, bacterial identification is more robust, reproducible, accurate and less subjective test results (Clarridge, 2004; Woo et al., 2008).###…has been used extensively for molecular taxonomic studies as an attractive alternative to the methods following traditional standard references such as Bergey’s Manual of Systematic Bacteriology or the Manual of Clinical Microbiology (Clarridge, 2004; Woo et al., 2008; Anjaneya et al., 2011).",impact-revealing,highlighting the advantages of 16S rRNA sequencing for bacterial identification
664,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,56d92474dabfae2eeeb24e63,Setting a Worm Attack Warning by using Machine Learning to Classify NetFlow Data,"entropy, kernel function, mutual information and Hellinger distance [134], or data fusion with other log files such as Snort, DNS related requests [7] (number of DNS requests, response, normals, and anomalies for each host over a certain period of time).###[7] presented a Support Vector Machines (SVM) method based on the fact that a scanning activity or email worm initiates a significant amount of traffic without DNS.###2008 [142] GA-based Derived non-NetFlow DDoS 2010 [141] Kernel Derived Internet Monitoring 2010 [127] SVM Derived Intranet Masquerade 2011 [7] SVM Application non-NetFlow Worm 2011 [140] SVM Derived Ineternet Attacks 2011 [139] SVM Advanced Internet Attacks 2011 [146] SVM Basic and derived non-NetFlow IDS",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2084,,9922f76c2a74578cffcbc8db94c677df25d91156,A Toolkit for Prototyping Tabletop-Centric Cross-Device Interaction,,,"###…a smartphone is commonly used as a remote controller to select and manipulate objects in a distant large display, such as sweep and point & shoot (Ballagas, Rohs, & Borchers, 2005), touch projector (Boring, Baur, Butz, Gustafson, & Baudisch, 2010), SnapAndGrab for content sharing (Maunder,…",impact-revealing,providing examples of smartphone applications in remote control
1573,,488bc6b764041edad6e8c699a052e4a6b9ed926f,Fear and the Defense Cascade: Clinical Implications and Management,,,"###This use of the therapeutic relationship— the dyadic regulation of affect—builds on developmental processes in which the attachment figure acts as a psychobiological regulator, and regulation is a dyadic interpersonal achievement.(141,142) Different models of therapy have different ways of describing this safe, physiologically calm, therapeutic dimension, whether defined spatially or interpersonally:",impact-revealing,providing context for therapeutic relationship models
3403,5cf48a48da56291d582ab75a,c5f5f179d80a3bf9b4f29750283a87eaca42e91b,neural graph collaborative filtering,5cd7fa07ced107d4c65bf273,Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences,"Following mainstream recommender models [1, 14, 26], we describe a user u (an item i) with an embedding vector eu ∈ Rd (ei ∈ Rd ), where d denotes the embedding size.###To implement the assumption, a common paradigm is to parameterize users and items for reconstructing historical interactions, and predict user preference based on the parameters [1, 14].",other,describing the implementation of mainstream recommender models
3879,5e4672c93a55ac14f595d7f3,7784dd6586ce5d4c8bfd020a23e9ad52378889b6,improving deep learning for airbnb search,5d3ed25a275ded87f97dea6f,150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com,For all practical purposes users are in a state of continuous cold start as noted in [2].,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2626,58437722ac44360f1082efeb,36eff562f65125511b5dfab68ce7f7a943c27478,Semi-Supervised Classification with Graph Convolutional Networks,53e9ac18b7602d97035d9131,Collective Classification In Network Data,"In the citation network datasets—Citeseer, Cora and Pubmed (Sen et al., 2008)—nodes are documents and edges are citation links.###…against the iterative classiﬁcation algorithm (ICA) proposed in Lu & Getoor (2003) in conjunction with two logistic regression classiﬁers, one for local node features alone and one for relational classiﬁcation using local features and an aggregation operator as described in Sen et al. (2008).###L2 regularization parameter and aggregation operator ( count vs. prop , see Sen et al. (2008)) are chosen based on validation set performance for each dataset separately.###For the citation network datasets, we optimize hyperparameters on Cora only and use the same set of parameters for Citeseer and Pubmed.###We used the following sets of hyperparameters for Citeseer, Cora and Pubmed: 0.5 (dropout rate), 5 · 10 − 4 (L2 regularization) and 16 (number of hidden units); and for NELL: 0.1 (dropout rate), 1 · 10 − 5 (L2 regularization) and 64 (number of hidden units).###Right: t-SNE (Maaten & Hinton, 2008) visualization of hidden layer activations of a two-layer GCN trained on the Cora dataset (Sen et al., 2008) using 5% of labels.###Citation networks We consider three citation network datasets: Citeseer, Cora and Pubmed (Sen et al., 2008).",other,reporting on citation network datasets and their characteristics
3740,5d3ed25a275ded87f97dea9b,0ddd7c9a1955c76eb831a230ef20a68b662886bb,Mining Algorithm Roadmap in Scientific Publications,53e9aeb7b7602d97038ddc5e,Learning Syntactic Patterns for Automatic Hypernym Discovery,"One step assistingwith this process is taxonomy construction [11, 14, 32, 42], which extracts concepts from a collection of documents and builds a tree structure to describe the hierarchical relation between different concepts.###They either rely on pattern-based methods [14, 32] which extract hierarchical relation leveraging linguistic features, or clustering-based methods [11, 42], which cluster concepts to induce an implicit hierarchy.",other,providing context on taxonomy construction methods
3503,5b1643ba8fbcbf6e5a9bc5b5,8c1b00128e74f1cd92aede3959690615695d5101,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,599c7945601a182cd2629a89,Neural Question Generation from Text: A Preliminary Study.,Zhou et al. (2017) improved the diversity of the SQuAD data by generating more questions.,other,reporting prior findings on improving data diversity
3524,5d3ed25a275ded87f97dea9b,0ddd7c9a1955c76eb831a230ef20a68b662886bb,Mining Algorithm Roadmap in Scientific Publications,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,We add residual connections [12] to both multihead attention and convolutional layers.,other,describing a modification in model architecture
1395,,07cc1ee7666e3c89a531abfe866a21fcec0c85f9,A Kinematic Information-Theoretic Transmission Scheme for Goal-Directed Movements.,,,"###Many theoretic models [4], [5], [39] build on this idea of intermittent control and submovements.###Crossman and Goodeve [4] proposed a discrete-time model where the change of position from one sampling instant to the next is to be proportional to the current position error, implying a geometrical reduction of error (i.###Some other models minimize movement time for a predetermined endpoint accuracy condition [4], [5], [17] but they fail on different accounts.###Although this term has been discussed several times [8], [54], it is actually of little interest — the changes it induces are sensible only for very low values of the ratio D/W , where Fitts’ law is known to be a poor model [4], and where the estimation of C is unreliable anyways.",impact-revealing,acknowledge existing theoretical models and their limitations
3127,5cf48a34da56291d58296d51,3c64b7a74c749d43b1a4b96dd1a00620ba613ee0,Representation Learning for Attributed Multiplex Heterogeneous Network,5b67b46417c44aac1c86132c,Scalable Multiplex Network Embedding.,"The compared methods include PMNE [22], MVE [30], MNE [43].###MNE [43] uses one common embedding and several additional embeddings of each edge type for each node, which are jointly learned by a unified network embedding model.###We chooseMNE [43], a recent representative work for MHEN, as the base model for multiplex heterogeneous networks to discuss the connection between our proposed model and previous work.###Single Multi / MVE [30] MNE [43] mvn2vec [32] GATNE-T Multi Multi Attributed MHEN (AMHEN) GATNE-I Multi Multi Attributed###, MNE [43]).",other,acknowledge existing methods in multiplex heterogeneous networks
3510,599c7968601a182cd2639c4c,1a0912bb76777469295bb2c059faee907e7f3258,Mask R-CNN,53e9a479b7602d9702d98afa,Microsoft COCO: Common Objects in Context,"All instantiations of our model outperform baseline variants of previous state-of-the-art models.###Finally, we showcase the generality of our framework via the task of human pose estimation on the COCO keypoint dataset [28].###Without bells and whistles, Mask R-CNN surpasses all previous state-of-the-art single-model results on the COCO instance segmentation task [28], including the heavilyengineered entries from the 2016 competition winner.",other,highlighting the superiority of the proposed model over previous state-of-the-art models
1582,,b6e83b70ef8965e5327353c00e236ede0080d7d9,An Improved Moth-Flame Optimization Algorithm with Adaptation Mechanism to Solve Numerical and Mechanical Engineering Problems,,,"###Some SI algorithms are inspired by the behavior of aquatic animals such as prey besieging and foraging, which have been modeled in krill herd (KH) [65], dolphin echolocation (DE) [66], and whale optimization algorithm (WOA) [67].",impact-revealing,highlighting inspiration from nature in algorithm design
3427,5f8d00a29e795ea21aee8001,34d6bc3dc0a4811eb262508379fc74f600671687,a collective approach to scholar name disambiguation,57d063b9ac4436735428eb6c,Collective Entity Resolution With Multi-Focal Attention,"There are also some collective entity resolution methods that can be used to solve multiple name disambiguation problem [7], [14], [27].###base (KB) [14], [27], which is unavailable in most cases.",other,acknowledge methods for name disambiguation
2429,5ee9f15b91e01152af022ce0,bf2174c69f84f4e57813e0bed4571c6dbff123ed,Automatic Validation of Textual Attribute Values in E-commerce Catalog by Learning with Limited Labeled Data,5aed14d617c44a4438158f52,Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection,"The deep learning anomaly detection (DAD) approaches [7, 22, 32] model the log data as a natural language sequence and apply RNN and CNN to detect anomalies.",other,reporting prior findings on deep learning anomaly detection approaches
3298,5f350c4191e011d4254d01da,d592e8ca7ad6096a7c0d590f0602ebee8b70a197,Transfer Learning Approaches for Streaming End-to-End Speech Recognition System,5a73cbcc17c44a0b3035f29b,Improving the Performance of Online Neural Transducer Models,"They replace the acoustic model (AM), language model (LM) and pronunciation model with a single neural network [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11].",other,describing a method for model replacement in speech recognition
3015,5eccb534e06a4c1b26a83a1b,afdac86a934df38748d6ded69a5ff48b06a40053,Defending and Harnessing the Bit-Flip Based Adversarial Weight Attack,573696026e3b12023e516748,XNOR-Net: ImageNet Classification Using Binary Convolutional Neural   Networks,"binarization-aware training is originally proposed as an extreme low bit-width model compression technique, which converts the weights from 32-bit ﬂoating-point to { -1,+1 } binary format encoded by 1-bit [18].",other,reporting prior findings on binarization-aware training
2596,5bdc317017c44a1f58a08086,6062d8f5cc50014e5ff0f9aa467e1b044d33c051,Learning Cloud Dynamics to Optimize Spot,573697146e3b12023e60b41f,Online Auctions in IaaS Clouds: Welfare and Profit Maximization with Server Costs,"Many works [10], [23], [24] assume that the CP’s objective is to maximize its own profit under some capacity constraint, which we show is inconsistent with empirical spot price distributions in Section III-A.",other,highlighting inconsistencies in existing assumptions about CP's objectives
3265,5dc9327d3a55acc1042498de,2cf3bd0cc1382f35384e259d99e4f9744eeaed28,Blockwise Self-Attention for Long Document Understanding,5c5ce4fd17c44a400fc3886c,Passage Re-ranking with BERT.,", 2019) and text retrieval (Lee et al., 2019; Nogueira and Cho, 2019), to name a few.###As a result, downstream tasks have to either truncate their sequences to leading tokens (Nogueira and Cho, 2019) or split their sequences with a sliding window (Joshi et al.",other,acknowledge existing methods in text retrieval
105,5ea2b8bf91e01167f5a89d89,993377a3fc8334558463b82053904e3d684f29c0,SIGN: Scalable Inception Graph Neural Networks,5550417d45ce0a409eb3bc08,Going Deeper With Convolutions,"This model is analogous to the popular Inception module [56] for classic CNN architectures: it consists of convolutional filters of different sizes determined by the parameter r, where r = 0 corresponds to 1 × 1 convolutions in the inception module (amounting to linear transformations of the features in each node without diffusion across nodes).###Note that the model in equation (4) is analogous to the popular Inception module Szegedy et al. (2015) for classic CNN architectures (Figure 1): it consists of convolutional ﬁlters of different sizes determined by the parameter r , where r = 0 corresponds to 1 × 1 convolutions in the inception…###We propose simple scalable graph neural network architecture (SIGN) inspired by the inception module [56, 30].###Our architecture is analogous to the inception module Szegedy et al. (2015); Kazi et al. (2019) and combines graph convolutional ﬁlters of different size that are amenable to efﬁcient precomputation, allowing extremely fast training and inference.",impact-revealing,drawing parallels between proposed model and established CNN architectures
1271,,663dc434fabc1cf1d3e85fff3f7ddcd313035d18,LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series Forecasters,,,"###…widely recognized as prime candidates for end-to-end time-series analysis [Xu et al. , 2021; Liu et al. , 2021; Nie et al. , 2023 ] , CNN-based [ Yue et al. , 2022 ] or RNN-based [Tonekaboni et al. , 2021] backbones consistently stand out as the preferred architecture in time-series…###For self-supervised learning, we choose PatchTST, BTSF [Yang and Hong, 2022], TS2Vec [Yue et al. , 2022], TNC [Tonekaboni et al. , 2021], and TS-TCC [Eldele et al. , 2021].",impact-revealing,acknowledge preferred architectures in time-series analysis
1086,,9a147c089effe4701052d5f52299a67b26facb2c,Behavior Bounding: An Efficient Method for High-Level Behavior Comparison,,,"###The model used in behavior bounding, however, allows us to divide the space of possible behaviors more efficiently and into more refined regions without enumerating their contents.###Behavior bounding’s hierarchical behavior representation is inspired by the hierarchical models used in And/Or trees, HTN planning (Erol, Hendler, & Nau, 1994) and GOMS modeling (John & Kieras, 1996) to encode the variety of ways in which particular tasks can be accomplished.",impact-revealing,describing the efficiency and inspiration behind behavior bounding model
142,5f3e44b791e011c0de1c29bc,61325245e98920a69b40e18c069fda0c1cf00f21,MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings for Sequential Recommendation,5e2d653a3a55acc837436820,Time Interval Aware Self-Attention for Sequential Recommendation.,"17 0.5201 0.5085 0.5590 0.6100 9.12% NDCG@5 0.1893 0.2735 0.2735 0.2970 0.3446 16.03% NDCG@10 0.2334 0.3147 0.3133 0.3415 0.3882 13.67% to form a sequence for each user. Following the custom practice [5, 10, 14, 19], we discard users and items with less than five interactions to ensure the quality of the dataset. 4.1.2 Evaluation. To evaluate the performance of each model, we adopt the widely used leave-one-out ###odel on four real-word datasets with various domains and sparsity: MovieLens 1M [3], MovieLens 20M [3], Amazon Beauty [16] and Amazon Game [16]. We follow the common data preprocessing procedure from [5, 10, 14, 19]. We convert each dataset into an implicit dataset by treating each numerical rating or review as a presence of a user-item interaction. We group the interactions by user ids and then sort them by tim###. First, they don’t utilize timestamp values 2 MEANTIME RecSys ’20, September 22–26, 2020, Virtual Event, Brazil Fig. 1. Model Architecture which hold important contextual information. While TiSASRec [14] successfully addressed this issue, they also used a simple embedding scheme for temporal values. This can possibly lead to an information bottleneck since the same embedding has to represent all poss###ully employed self-attention mechanism, which was a huge success in NLP areas [1, 2, 22, 29]. BERT4Rec [19] improved SASRec by adopting Transformer [22] and Cloze-task based training method. TiSASRec [14] enhanced SASRec by merging timestamp information into self-attention operations. Our work seeks to further expand this trend by suggesting a novel model architecture that uses multiple types of tempo###tter understand the sequential history of users [5, 8, 10, 18–21, 36]. Despite their excellent performance, most of them ignore the interactions’ timestamp values. While recent works such as TiSASRec [14] successfully incorporated time information, their usage of time was also limited to a single embedding scheme. Since the timestamp values are rife with information, it would be beneficial to explore ",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2752,5f8ebbb99fced0a24b4e1966,d299c78121f39c3a4cd09e0994e47ec8cd0c20d6,Diversifying Search Results using Self-Attention Network,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Researchers have used self-attention networks, e.g. GPT [17], BERT [18] and ERNIE [19], as alternatives to RNNs and CNNs in many NLP tasks.###GPT [17], BERT [18] and ERNIE [19], as alternatives to RNNs and CNNs in many NLP tasks.###In the future work we will try to import several deep-learning based technologies for feature extraction and document representation e.g. K-NRM [27] or BERT [18].",other,acknowledge existing alternatives in NLP tasks
3643,573696cd6e3b12023e5ce382,e2820bffe5b42cb7d88b7f65c12171c62ab4aae2,Gradient-based Hyperparameter Optimization through Reversible Learning,56d8155ddabfae2eee6de24a,Markov Chain Monte Carlo and Variational Inference: Bridging the Gap,Gradients with respect to Markov chain parameters Salimans et al. (2014) tune the step-size and mass-matrix parameters of Hamiltonian Monte Carlo by chaining gradients from a lower bound on the marginal likelihood through several iterations of leapfrog dynamics.,other,reporting prior findings on Hamiltonian Monte Carlo tuning methods
2242,58437722ac44360f1082f160,8aa3358a34a17abd0a65622aad8c85317b851af4,very deep convolutional networks for end-to-end speech recognition,53e9b850b7602d9704411f4e,Improvements To Deep Convolutional Neural Networks For Lvcsr,"Recently, very deep CNNs architectures [15] have also been shown to be successful in ASR [16, 17], using more non-linearities, but fewer parameters.###As a result, a single end-to-end model can jointly accomplish the ASR task within one single large neural network.###In this subsection, we extend on Section 3.2 and describe experiments in which we build deeper encoders by stacking convolutional layers and residual blocks in the acoustic encoder before the BLSTM. Unlike computer vision applications or truncated BPTT training in ASR, seq2seq models need to handle very long utterances (i.e., > 2000 frames).###We experimented with the Wall Street Journal (WSJ) ASR task.###In a hybrid system, convolutions require the addition of context window for each frame, or a way to treat the full utterance as a single sample [16].###The sequence-to-sequence (seq2seq) model with attention [1] has recently demonstrated a promising new direction for ASR that entirely sidesteps the complicated machinery developed for classical ASR [2, 3, 4, 5, 6].###CNNs have shown improvement over traditional fully-connected deep neural networks on many ASR tasks [14, 12], we investigate the effect of convolutional layers in seq2seq models.###While very deep CNNs have been successfully applied to ASR, recently there have been several advancements in the computer vision community on very deep CNNs [15, 18] that have not been 1.###On the WSJ ASR task, we obtained 10 .###Recently, very deep CNNs architectures [14] have also been shown to be successful in ASR [15, 16, 17], using more non-linearities, but fewer parameters.###Convolutional Neural Networks (CNNs) [9] have been successfully applied to many ASR tasks [10, 11, 12].",other,highlighting advancements in ASR using deep CNN architectures
1380,,26bec0310240b76a7a39370080574740843449cb,Implicational (semilinear) logics III: completeness properties,,,"###Proposition 2 [12, Theorem 1.4.1] Let L be a ﬁnitary protoalgebraic logic.###The papers [8,11] presented a new approach to abstract algebraic logic (AAL) based on implication, instead of the usual equivalence-based setting leading to the well-known Leibniz hierarchy of protoalgebraic logics (see [12,16]).###Proposition 1 [12, Corollary 0.3.10, Lemma 0.6.8, Theorem 1.4.3] Let L be a logic.###The next proposition is proved for protoalgebraic logics in [12, Theorem 1.1.8] and for the strongly p-disjunctional ones it follows from [10, Lemma 4.13] and the previous proposition.###For the second goal, we build upon results from [12] for the semantics of ﬁnite matrices and we generalize results from [9] that allow us to characterize completeness with respect to densely ordered matrices in terms of an extension property and a metarule inspired in proof-theoretic studies of…###Another crucial precedent of our work is the general study of propositional (mainly protoalgebraic) logics carried out by Czelakowski [12] and in previous papers.###The second claim of the previous theorem is related to [12, Theorem 1.4.8], where Czelakowski proved (using a different method) that, for protoalgebraic logics, the S K C implies MOD ∗ ( L ) = IS ∗ P σ - f ( K ) .###(Note that in [12] the symbol S Ω is used instead.)###For background knowledge on abstract algebraic logic and its methods we refer to [12,16– 18].###For the ﬁrst goal, we consider the preceding results for the S K C and the K C from [12], give alternative proofs that allow to drop some hypotheses, turn some into characterizations, add corresponding results for the FS K C, and enrich them with characterizations in terms of embeddings…",impact-revealing,acknowledging foundational work in abstract algebraic logic
1080,,da7e84e50dd06f48104066b0de7d93eab25bcf92,Information visualization techniques for presenting Qur'an histories and atlas,,,"###The KLM is claimed as the simplest technique ( John and Kieras, 1996) and it is suitable with the simple actions required.###The KLM is claimed as the simplest technique ( John and Kieras, 1996) and it is suitable with the simple actions required. Apart from the simple actions, Kieras (2001) describes that this technique is also effective for estimating execution time or the task time.###GOMS is widely used as the analysis technique and it is rather significant in the field of human computer interaction (John and Kieras, 1996).###GOMS is widely used as the analysis technique and it is rather significant in the field of human computer interaction (John and Kieras, 1996). According to John and Kieras (1996), the four GOMS components are:###It is also limited to keystroke actions (John and Kieras, 1996) whereby only one activity is done at a time until completion (Kurniawan, 2001).",impact-revealing,reporting on the significance and application of the KLM technique in human-computer interaction
3192,5c45b4d03a55ac25e7f0f55c,e5badcfa663c30a983da24dd682288141d00fcc3,GraphSAR: A Sparsity-Aware Processing-in-Memory Architecture for Large-Scale Graph Processing on ReRAMs,53e99d6cb7602d9702624f0c,Understanding The Trade-Offs In Multi-Level Cell Reram Memory Design,"For multi-level ReRAM cells, we modify NVSim according to the parallel sensing scheme proposed in [20], which enables the ‘Write-and-verify’ process of accessing multi-level ReRAM cells.###Moreover, because of heavy overheads of ‘Write-and-verify’ operations [20], Twrite is usually larger than Tr ead in ReRAM crossbars, thus pef f ect ive is around 1.###The parameter of ReRAM cell model is from the same source [20] in GraphR [1] (read/write energy consumption: 1.",other,providing context for modifications in ReRAM cell modeling
3256,5ebe685391e0117693a52241,99314a532a3358cb86064fc8917ed2c283227539,NAT: Noise-Aware Training for Robust Neural Sequence Labeling,5d47ffba3a55acfc82ca0fb0,A Benchmark Corpus Of English Misspellings And A Minimally-Supervised Model For Spelling Correction,"Most popular post-correction techniques include correction candidates ranking (Fivez et al., 2017; Flor et al., 2019), noisy channel modeling (Brill and Moore, 2000; Duan and Hsu, 2011), voting (Wemhoener et al., 2013), sequence to sequence models (Aﬂi et al., 2016; Schmaltz et al., 2017) and…###Most popular post-correction techniques include correction candidates ranking (Fivez et al., 2017; Flor et al., 2019), noisy channel modeling (Brill and Moore, 2000; Duan and Hsu, 2011), voting (Wemhoener et al.",other,acknowledge existing post-correction techniques
3342,5d245bb5da56295a28fcca5f,4efb9a950f252138a30eeb942ed02663a3ea29d1,MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing,5e8d92929fced0a24b618153,MorphNet: Fast & Simple Resource-Constrained Structure Learning of Deep   Networks,"To address this limitation, we propose using a lasso regularization to automatically learn an architecture for our model (Gordon et al., 2018).",other,highlighting a proposed solution to a limitation in model architecture
2573,5736986b6e3b12023e72fc2d,51a55df1f023571a7e07e338ee45a3e3d66ef73e,Character-level convolutional networks for text classification,5550432045ce0a409eb46029,"DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia.","82 39.30 28.80 40.45 4.93 Sm. Conv. Th. 14.80 - 1.85 6.49 40.16 29.84 40.43 5.67 DBPedia ontology dataset. DBpedia is a crowd-sourced community effort to extract structured information from Wikipedia [19]. The DBpedia ontology dataset is constructed by picking 14 nonoverlapping classes from DBpedia 2014. From each of these 14 ontology classes, we randomly choose 40,000 training samples and 5,000 testi",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2307,53e99ab2b7602d970232a35e,1f4400f0441e1cc3c8eeab6b7c8accac59c378a2,Late-binding: enabling unordered load-store queues,53e9b90ab7602d97044ecfcb,Distributed Microarchitectural Protocols In The Trips Prototype Processor,We implemented these flow control mechanisms on a simulator that closely models the TRIPS prototype processor [ 21 ] which has been validated to be within 11% of the RTL for the TRIPS prototype processor.###Additional details about the TRIPS microarchitecture can be found in [ 21 ].,other,providing context for the implementation of flow control mechanisms
1609,,7f6eeebe1b6ccf2251887a17078ab1e2b1e86afd,Development of a high performance Wireless Mesh Network for video streaming,,,"###…streaming
I. INTRODUCTION Wireless Mesh Network (WMN) as a self-organized, self-managed and reliable wireless communication network has been widely applied as a community network to provide wireless broadband Internet access, community healthcare, emergency recovery and entertainments [1], [2].",impact-revealing,providing context on the applications of Wireless Mesh Networks
3005,5f4f6ec291e0111f07b30a2b,3259c9ab1714a4cfdf6439cca6bdc5f78d78fda3,NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size,5a260c8417c44a4ba8a31511,mixup: Beyond Empirical Risk Minimization.,", different data augmentation [13], [14], different regularization [11], different scheduler [15], and different selections of hyper-parameters [16], [17].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
918,573697636e3b12023e64a731,793dd71c59e0f2222d60fc5a43871cf919eeb1a2,runtime-driven shared last-level cache management for task-parallel programs,53e9a374b7602d9702c7fca2,Quality of service shared cache management in chip multiprocessor architecture,"Partitioning techniques for mul-tiprogramming workloads have focussed on managing contention among applications through explicit partitioning of the cache among co-running applications for throughput or fairness improvement [16, 18, 22, 32, 36].",impact-revealing,acknowledge existing techniques in multiprogramming workloads
3208,5ce3af25ced107d4c65f15d4,abb8c7535d62e5cfaa7332df2479312779988fb4,using deep learning to annotate the protein universe,53e9b9f5b7602d97045f190e,Hmmer Web Server: Interactive Sequence Similarity Searching,"BLASTp [3], or profile hidden Markov models built from aligned sequence families such as those provided by Pfam [4, 5].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
480,5db929ff47c8f766461fd7e4,a73051e08af289a50ef8ed53e69f91c189dd01e5,Induction Networks for Few-Shot Text Classification,599c794e601a182cd262e8f1,Prototypical Networks for Few-shot Learning.,"Matching Networks (Vinyals et al., 2016) 65.73 Prototypical Networks (Snell et al., 2017) 68.17 Graph Network (Garcia and Bruna, 2017) 82.61 Relation Network (Sung et al., 2018) 83.07 SNAIL (Mishra et al., 2018) 82.57 ROBUSTTC-FSL (Yu et al., 2018) 83.12 Induction Networks (ours) 85.63 1.###Evaluation Methods We evaluate the performance by few-shot classification accuracy following previous studies in few-shot learning (Snell et al., 2017; Sung et al., 2018).###Few-shot classiﬁcation (Vinyals et al., 2016; Snell et al., 2017) is a task in which a classiﬁer must be adapted to accommodate new classes not seen in training, given only a few examples of each of these new classes.###• Prototypical Networks: a deep metric-based method using sample average as class prototypes (Snell et al., 2017).###Evaluation Methods We evaluate the performance by few-shot classiﬁcation accuracy following previous studies in few-shot learning (Snell et al., 2017; Sung et al., 2018).###The approaches proposed by Snell et al. (2017) and Sung et al. (2018), which combine non-parametric methods and metric learning, provide potential solutions to some of those problems.###Few-shot classification (Vinyals et al., 2016; Snell et al., 2017) is a task in which a classifier must be adapted to accommodate new classes not seen in training, given only a few examples of each of these new classes.###In the distance metric learning models (Matching Networks, Prototypical Networks, Graph Network and Relation Network), all the learning occurs in representing features and measuring distances at the sample-wise level.",impact-revealing,reporting evaluation methods in few-shot classification
720,5ef0816891e0112aee042b88,5c126ae3421f05768d8edd97ecd44b1364e2c99a,denoising diffusion probabilistic models,573696cd6e3b12023e5ce334,Deep unsupervised learning using nonequilibrium thermodynamics,"We set T = 1000 for all experiments so that the number of neural network evaluations needed during sampling matches previous work [50, 52].###…in part by the choice of Gaussian conditionals in p θ ( x t 1 | x t ) , because both processes have the same functional form when β t are small [50]. property − A notable of the forward process is that it admits sampling x t at an arbitrary timestep t in closed form: using the notation α t :=…###The ﬁrst choice is optimal for x 0 ∼ N ( 0 , I ) , and the second is optimal for x 0 deterministically set to one point. two These are the extreme choices corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance [50].###This paper presents progress in diffusion probabilistic models [50].###This material is from Sohl-Dickstein et al. [50]; we include it here only for completeness.###These algorithms serve as a compression interpretation of the variational bound (5) of Sohl-Dickstein et al. [50], not yet as a practical compression system.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1537,,2a3531987423e82c0ab6ab6a95e0171d5b432988,"Coping with Homonegative Experiences Among Gay Men: Impacts on Mental Health, Psychological Well-being, and Identity Growth",,,"###Homonegative messages or acts may be
11
purposefully perpetrated, or may be unconscious, accidental, or out of the immediate awareness of the perpetrator (Sue et al., 2007).###Gay men also face microaggressions, or subtle, commonplace implications or insults that communicate negativity towards gay people (Sue et al., 2007).###Within these environments, perpetrators may be strangers, family members, or peers (D’Augelli, 2006; Sue et al., 2007) or the source may be vague, such as through an###Within these environments, perpetrators may be strangers, family members, or
peers (D’Augelli, 2006; Sue et al., 2007) or the source may be vague, such as through an institution, social ideology, or systemic policy (Brown, 2008).###purposefully perpetrated, or may be unconscious, accidental, or out of the immediate awareness of the perpetrator (Sue et al., 2007).###Homonegativity occurs across many environments and components of a gay person’s life, ranging from individual events in specific homonegative locations, to underpinning societal messages across a social environment (Kantor, 2010; Sue et al., 2007).###…can be verbal, non-verbal, or environmental and can be one of three specific forms: Microinsults (unconscious demeaning insensitivities), microassaults (conscious purposeful attacks or avoidance), and microinvalidations (negating ones thoughts and experiences) (Sue et al., 2007; Sue, 2010).###These microaggressions can be verbal, non-verbal, or environmental and can be one of three specific forms: Microinsults (unconscious demeaning insensitivities), microassaults (conscious purposeful attacks or avoidance), and microinvalidations (negating ones thoughts and experiences) (Sue et al., 2007; Sue, 2010).###…going unacknowledged, crimes going unreported, and emotional reactions being minimized because a gay person is unsure if discrimination was real or perceived, or if it was motivated by anti-gay beliefs (Cohler & Galatzer-Levy, 2000; Herek, 2009; Herek,
16
Cogan, & Gillis, 2002; Sue et al., 2007).",impact-revealing,describing the concept of homonegativity and its implications
3656,5ee8986891e011e66831c3b6,38f93092ece8eee9771e61c1edaf11b1293cae1b,Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,57a4e921ac44365e35c98d77,Adversarially Learned Inference.,"Many of these approaches rely either on auto-encoding of images [24, 25, 26] or on adversarial learning [27], jointly modelling data and representation [28, 29, 30, 31].",other,acknowledge existing approaches in image processing
1757,,83121e5e70d58e940bedfd9e99926c606e1858a2,A Parallel Loading Based Accelerator for Convolution Neural Network,,,"###Note that our PE array is also fully used in CONV2-CONV5 and FC1, that is the reason why it has a better performance compare with other method [18].###…structure has the following advantages: 1, data is load into PE in parallel; 2, the proposed structure can fully support FC layer efficiently; Other methods [18] [15] use image batching to improve the efficiency of FC layer which can cause large latency and is not suitable for EDGE application.###This will reduce the complexity of memory access, and we can remove the input/output buffer which is required by other solution for data reformatting [18], [15].###Most neural-network accelerator architectures [18] [15] are designed for kernel reuse and have to use image batching method to improve the efficiency of their design.###Compare with [18], our CONV2 takes only 7.05 ms (70% of [18]).###4, the proposed method can outperform [18], and the hardware resource requirement is minimized.",impact-revealing,highlighting the advantages and performance improvements of the proposed method compared to existing solutions
3799,5db9297247c8f766461f6d13,9ec95c1130a6ac4238ac2e5c7b2b66047511ea92,long and diverse text generation with planning-based hierarchical variational model,59ae3bf12bbe271c4c71bef4,Challenges in Data-to-Document Generation.,"As for diverse text generation, existing methods can be divided into three categories: enriching conditions (Xing et al., 2017), post-processing with beam search and rerank (Li et al., 2016), and designing effective models (Xu et al., 2018).",other,acknowledge existing methods for diverse text generation
2331,5cede104da562983788e3653,05dc5fb3a3bdefdf181aafcc42cd80ff6b7704e7,TuckER: Tensor Factorization for Knowledge Graph Completion,5550401245ce0a409eb3205c,Dropout: a simple way to prevent neural networks from overfitting,"We use batch normalization (Ioffe and Szegedy, 2015) and dropout (Srivastava et al., 2014) to speed up training.",other,reporting methods used for training speed improvement
93,5a73cb4d17c44a0b3035672d,7cd5d7f8295b219b029a4231ae5cffb261e00ebe,Early Active Learning with Pairwise Constraint for Person Re-identification,53e9b145b7602d9703bc5fda,Early active learning via robust representation and structured sparsity,"As analyzed in [18,30], the `2,1-norm can suppress the effect of outlying samples.###The table also confirms that our algorithms are better than the RRSS and TED method by around 5% on rank one matching accuracy.###(8) is sensitive to the outliers [18], which makes the algorithm not robust.###As discussed in [18], active learning methods can be divided into two categories.###K-means We use the K-means algorithm as another baseline algorithm as in [18].###They thus require a certain number of labeled samples to evaluate the uncertainty of the unlabeled data or sampling bias [18] will result.###RRSS and TED have a similar optimization target to our algorithm but without pairwise constraint.###We note that in previous researches [17,18,26], the `2,1-norm is used instead of the `2,0-norm.###RRSS [18] Early active learning via Robust Representation and Structured Sparsity is a early active learning algorithm.###However, RRSS does not consider the pairwise relations in re-id.###Inspired by [18], we relax the problem to the following problem by introducing the `2,0-norm for structure sparsity:###These kinds of active learning algorithms are referred to as early active learning or early stage experimental design [18].###It is shown in [18] that the `2,1-norm is the minimum convex hull of the `2,0-norm when row-sparsity is required.###We also introduce the kernelized RRSS denoted as RRSS_K.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
857,5f0d85c69fced0a24be4f052,0dd3e9f581c617eb826bc0fabac5ae1394f9cef1,Data Compression Accelerator on IBM POWER9 and z15 Processors : Industrial Product,573695f96e3b12023e50d00a,Integrated high-performance data compression in the IBM z13.,"Many hardware designs implement a static table with an assumed LZ symbol distribution which typically degrades compression ratio [2]–[4], [14].###In [4] In [2], [15], the 32KB sliding window data is replicated M times stored directly in M SRAMs organized as hash tables ( M = 16 ).###We describe an area efﬁcient LZ77 encoding hardware that improves state of the art in Section IV, where the related work [2]–[4], [14]–[19] are also discussed.###However, the FPGA cost and limited number of PCIe slots restrict their usage to high-end servers [2] and specialized applications such as storage controllers.###We would need 68 PCIe based compression cards such as [2], with 4GB/s peak throughput to match the NXU performance.###IBM z13 adapter uses a ”pseudo-dynamic” approach where one of 15 different static Huffman tables yielding the smallest output is selected at run time [2].###In contrast, we implemented a true ”Dynamic Huffman” mode in both POWER9 and z15 to achieve highest possible compression ratio, as described in Section V, where related work [2]–[4], [7], [14], [20], [22]– [26] and hardware issues are also discussed.###In the past, IBM z13 and many other systems have used FPGA based PCIe attached compression accelerators [2]–[5].###IBM z13 and POWER systems used PCIe based Deﬂate accelerators [2], [5], [15], [28].",impact-revealing,highlighting advancements in hardware compression techniques and their limitations
2955,599c7959601a182cd2633b3e,c0c0990b84a350d5efde8d3b2cb2636b6b57c21c,On Sampling Strategies for Neural Network-based Collaborative Filtering,57d063c7ac443673542906db,Entity Embedding-Based Anomaly Detection for Heterogeneous Categorical Events.,"Compared to traditional feature detectors, such as SIFT and n-grams, DNNs and other embedding methods [5, 6, 29] can automatically extract better features that produce higher performance in various tasks.",other,highlighting the advantages of DNNs and embedding methods over traditional feature detectors
1205,,f3706e4566ad4e12f1fcffddf04c36a84a1d9a58,Robinson’s implicit function theorem and its extensions,,,"###For now, before getting on with the formulation of Robinson's main theorem from [14], we mention two results which indicate how the classical inverse function theorem can be complemented, or sharpened, in terms of the concepts just reviewed.###The case of (1) where Robinson broke new ground in [14] is the one where Y is the dual Banach space X * and F is the normal cone mapping N C : X → → X * associated with a nonempty closed convex set C ⊂ X, as defined by (3) N C (x) = { y ∈ X * | y, v − x ≤ 0 for all v ∈ C} if x ∈ C, ∅ if x / ∈ C,…###The invertibility property assumed in (b) of Theorem 1.6 is what Robinson in [14] called the "" strong regularity "" of the generalized equation at hand.###We are ready now to state the important result of Robinson from [14].###For stating and proving his result, Robinson was clearly motivated by the problem of how the solutions of the standard nonlinear programming problem depend on parameters, and he pursued this goal in the same paper [14] where he presented his implicit function.",impact-revealing,providing context for Robinson's theorem and its significance
3418,5d9edc8347c8f76646042a37,7e71eedb078181873a56f2adcfef9dddaeb95602,simplifying graph convolutional networks,53e9a727b7602d970305fa6d,Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples,"Graph Laplacian regularization (Zhu et al., 2003; Zhou et al., 2004; Belkin & Niyogi, 2004; Belkin et al., 2006) introduce an additional regularization term based on graph structure which forces nodes to have similar labels to their neighbors and helps the models generalize.",other,reporting on the concept of Graph Laplacian regularization and its benefits
1849,,cebc2f825fc20f1fe946f86e47206660d9c5b558,Indexing Priority of Position: Eben as Response Particle in German,,,"###confirmable for their failure of correct interactional bookkeeping (cf. Brandom, 1998; Schegloff, 1991), i.###…of eben can be used to imply criticism of the producer of the confirmable for their failure of correct interactional bookkeeping (cf. Brandom, 1998; Schegloff, 1991), i.e., for not having added to common ground what the eben speaker had already communicated before and what should have been obvious…###…the eben speaker indexes that she had herself already communicated what was just formulated by the coparticipant—thus orienting to the coparticipant’s responsibility for correct interactional bookkeeping, i.e., to know things that have already been said or conveyed (Schegloff, 1991, p. 164).###Using eben as a response thus is a practice that builds on and indexes the relevance of interactional bookkeeping (Brandom, 1998; Schegloff, 1991), i.###Using eben as a response thus is a practice that builds on and indexes the relevance of interactional bookkeeping (Brandom, 1998; Schegloff, 1991), i.e., the tracking of the emergence of common ground in social interaction (Clark, 1992).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3198,5e5e18a493d709897ce22b32,3345443925cec95381c2cf2f0b029c411945bfef,GraphSAINT: Graph Sampling Based Inductive Learning Method,53e99b16b7602d97023aad98,Sampling from large graphs.,"There are many random walk based samplers in the literature [26, 23, 17, 24].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3541,5d04eeba8607575390f83f4d,9cceaadb580c24d0d5c381fa8e3d4afb32fd88b9,perceptron-based prefetch filtering,5a260c3b17c44a4ba8a2600d,Multiperspective reuse prediction.,"Multiperspective Reuse Prediction [23] improves on Perceptron Learning by contributing many new features.###Beyond branch prediction, perceptron learning has been applied to last-level cache reuse prediction [22, 23].",other,highlighting the contributions of Multiperspective Reuse Prediction to Perceptron Learning
2884,5db929e147c8f766461fc024,e03b5bc5edeb44d4b47d225c0c26ac54088fe528,An Improved Neural Baseline for Temporal Relation Extraction,5a260c2817c44a4ba8a23814,A Structured Learning Approach to Temporal Relation Extraction,"Based on the conﬁdence scores, global inference is performed via integer linear programming (ILP), which is a standard procedure used in many existing works to enforce the transitivity property of time (Chambers and Jurafsky, 2008b; Do et al., 2012; Ning et al., 2017).###…2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Juraf-sky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017).",other,reporting standard procedures used in existing works
3273,5cede0e8da562983788c741f,b56e5fb4f367a8d54614f1047bd4f9a2d58b9973,Multi-Interest Network with Dynamic Routing for Recommendation at Tmall,5736977f6e3b12023e66659a,Ups and Downs: Modeling the Visual Evolution of Fashion Trends with   One-Class Collaborative Filtering,"One is Amazon Books1 provided by [10, 20], representing one of the most widely-used public dataset for e-commerce recommendations.",other,acknowledge a widely-used dataset for e-commerce recommendations
2370,5fe30a2291e01125d4b5b5e3,07fd366a8ebdefe54cdb57d87c81dcd22de25a91,A DISTRIBUTIONAL APPROACH TO CONTROLLED TEXT GENERATION,573696106e3b12023e52394b,Structured Prediction Energy Networks,"Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).",other,acknowledge early research in neural-based sequence labeling
1002,,66db7c58ba34680ce01b95a46c475a1fa56c67fa,Neighborhood social capital and self-rated mental health: Disparities between migrants and native residents in Beijing,,,"###On the one hand, neighborhood social networks, defined as informal social relationships between neighbors developed in their everyday-life encounters, can procure predictable mental health benefits (38, 43).###, trust and support, the negative emotions reinforced by neighborhood interactions may not be ameliorated by neighborhood social capital (38).###While previous studies found the importance of neighborhood social networks in improving mental health outcomes (11, 38, 43), this study suggests that the mental health benefits of neighborhood social networks can be attenuated by particular hukou-based residency status.###Public health scholars have sought to understand the effects of neighborhood social capital on mental health, though conceptualization and measurement of social capital have varied in different studies (9, 38).",impact-revealing,highlighting the complex relationship between neighborhood social networks and mental health outcomes
3458,5eede0b791e0116a23aafe75,150f95f9c73820e0a0fa1546140e9f2bdfd25954,temporal graph networks for deep learning on dynamic graphs,58d82fd2d649053542fd75d8,Geometric deep learning: going beyond Euclidean data.,"In the past few years, graph representation learning (7; 27; 4) has produced a sequence of successes, gaining increasing popularity in machine learning.",other,highlighting the growing popularity and success of graph representation learning in machine learning
2404,57a4e921ac44365e35c99004,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,man is to computer programmer as woman is to homemaker? debiasing word embeddings,5843777eac44360f10841807,Semi-supervised Question Retrieval with Gated Convolutions.,", document ranking [27], sentiment analysis [18], and question retrieval [22]).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
255,5c8fd41a4895d9cbc66534e9,950aae7979a2faa874b7481179c064d3ad151ea8,Objects Segmentation From High-Resolution Aerial,573698016e3b12023e6da477,U-Net: Convolutional Networks for Biomedical Image Segmentation,"To overcome this problem, skip connection methods are used [18], [25].###U-Net [18] is a modified FCN for yielding more precise###original architecture proposed in [18] but a highly calibrated model for the enhanced capability of object segmentation from satellite images [27].###on deep learning techniques such as fully convolutional networks (FCNs) [13]–[17], and U-Net [18] algorithms have been used.",impact-revealing,highlighting the use of skip connection methods in deep learning for object segmentation
140,5cede10dda562983788ed645,690edf44e8739fd80bdfb76f40c9a4a222f3bba8,BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"For representation power, the superior results for deep bidirectional models on text sequence modeling tasks show that it is beneficial to incorporate context from both sides for sequence representations learning [6].###Specifically, inspired by the success of BERT [6] in text understanding, we propose to apply the deep bidirectional self-attention model to sequential recommendation, as illustrated in Figure 1b.###To tackle this problem, we introduce the Cloze task [6, 50] to take the place of the objective in unidirectional models (i.###Previous work has shown that it is beneficial to jointly attend to information from different representation subspaces at different positions [6, 29, 52].###In contrast, Transformer [52] and BERT [6] are built solely on multi-head self-attention and achieve state-of-the-art results on text sequence modeling.###In this work, following OpenAI GPT [38] and BERT [6], we use a smoother GELU [13] activation rather than the standard ReLu activation.###In order to efficiently train our proposed model, we apply a new objective: Cloze task [50] (also known as “Masked Language Model” in [6]) to sequential recommendation.",impact-revealing,highlighting the benefits of deep bidirectional models for sequence representation learning
147,5e09aa66df1a9c0c416bebf6,4e7dd1e79f0f13650b2612325e6ba8d206dc04fb,DeepGCNs: Can GCNs Go As Deep As CNNs?,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,"trained a semi-supervised GCN model for node classification and showed how performance degrades when using more than 3 layers [18].###For example, the aggregation function can be a mean aggregator [18], a max-pooling aggregator [27, 10, 42], an attention aggregator [39] or an LSTM aggregator [25].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
4002,5bdc316717c44a1f58a06ecf,472b54e648effcb5f12b8083766b34f19e44b0ed,MEgo2Vec: Embedding Matched Ego Networks for User Alignment Across Social Networks,5736965d6e3b12023e56a96d,Cross-Platform Identification of Anonymous Identical Users in Multiple Social Media Networks,"In practice, most of the neighbor pairs are unlabeled, which prevents us from directly calculating the metrics like Jac-card’s coefficient or Adamic/Adar based on the shared neighbors as that proposed in [7, 8, 26, 30].###Despite existing studies on user alignment [7–9, 26–28, 30], the problem still poses several unsolved challenges.###Some of them calculated the metrics like Jaccard’s coefficient or Adamic/Adar based on the number of the shared neighbors [7, 30].",other,highlighting challenges in calculating metrics due to unlabeled neighbor pairs
842,5e4672c93a55ac14f595d7f3,7784dd6586ce5d4c8bfd020a23e9ad52378889b6,improving deep learning for airbnb search,5c2c7a9217c44a4e7cf31a1f,Estimating Position Bias without Intrusive Interventions,"While constructing the propensity model typically involves perturbing the search results to collect examples of counterfactuals, [1] describes methods to construct the propensity model without additional interventions.",impact-revealing,reporting methods for constructing propensity models
2066,,1a104315adea3708e81dfe3644a72acb26683231,Comparison of Antimicrobial Susceptibility of Campylobacter Strains Isolated from Food Samples and Patients with Diarrhea.,,,###High resistance to ciprofloxacin in the present study may be due to the fact that fluoroquinolones such as ciprofloxacin are frequently used for treatment of campylobacteriosis because of their broad spectrum of activity against enteric pathogens [32] .,impact-revealing,highlighting the potential reason for ciprofloxacin resistance
52,5c0495ae17c44a2c747018e6,dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4,Mobilenetv2: Inverted Residuals And Linear Bottlenecks,58437722ac44360f1082ed35,Xception: Deep Learning with Depthwise Separable Convolutions,"Depthwise Separable Convolutions are a key building block for many efficient neural network architectures [27, 28, 20] and we use them in the present work as well.",impact-revealing,highlighting the importance of depthwise separable convolutions in neural network architectures
3516,5c5ce50d17c44a400fc38e42,350c5f528b557cde46177866121c40e250201a0f,Goal-based Course Recommendation,56d92794dabfae2eeec55086,Using institutional data to predict student course selections in higher education,"One explanation is that students are trying to match course difficulty with their ability (i.e., ZPD) when making their selections [22] and that, since the B models have higher grade prediction accuracy than the A models (88.05% vs. 75.23%), they are better at filtering out courses that are not a…",other,highlighting the relationship between course difficulty and student ability
2170,,1c6f84833f0939d49c6b8c294454cef7107dcb06,Scientometric Analysis of Cloud Computing,,,"###Since then, everyone is talking about “Cloud Computing” (Vouk, 2008).###Similar type of study carried out by Sivakumaren, K.S., Swaminathan, S., Karthikeyan G. 2012 found that 510 records related to Cloud Computing in “Web of Science’ appeared during the periods 2001-2010.###While as (Vouk, 2008) visioned it as a technology, embraces cyber-infrastructure, and builds on virtualisation, distributed computing, grid computing, utility computing, networking, and Web and software services.###There is a tremendous growth in the publications of Cloud Computing from 55 (2.92###1879
Table 1 reveals the amount of publication on Cloud Computing during 5 years .",impact-revealing,highlighting the growth and significance of Cloud Computing research
3994,5736973b6e3b12023e62b0a8,97e3bb4af723f43927317e9b9f2d794a9e398e8e,PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks,53e9b108b7602d9703b85b88,Distributed Representations of Words and Phrases and their   Compositionality,"Distributed representations of words and documents [18, 10] effectively address this problem through representing words and documents in low-dimensional spaces, in which similar words and documents are embedded closely to each other.###(3) The objective (3) can be optimized with stochastic gradient descent using the techniques of edge sampling [27] and negative sampling [18].###proposed a simple and elegant word embedding model called the Skip-gram [18], which uses the embedding of the target word to predict the embedding of each individual context word in a local window.###1 Distributed Text Embedding Distributed representation of text has proved to be quite effective in many natural language processing tasks such as word analogy [18], POS tagging [6], parsing [6], language modeling [17], and sentiment analysis [16, 10, 5, 8].###, Skip-gram [18]) or at document level (e.###Comparing to other classical approaches that also utilize the distributional similarity of word context, such as the Brown clustering or nearest neighbors, these text embedding approaches have been proved to be quite efficient, scaling up to millions of documents on a single machine [18].",other,highlighting the effectiveness of distributed representations in NLP tasks
673,5e2ac357df1a9c0c41e7fa48,73bc990757d54f2161eceacd330e0aeb189393bf,Building attention and edge message passing neural networks for bioactivity and physical–chemical property prediction,5d7b69f03a55acfa63f08a26,Attention And Edge Memory Convolution For Bioactivity Prediction,"We call this model an Edge Memory Neural Network (EMNN).###When hidden states instead reside in the directed edges as per EMNN, this cannot happen.###The EMNN network shares architectural similarities to the D-MPNN model published by Yang et  al.###The readout function for EMNN is the same as in AMPNN (Eq.###Each coloured arrow is used to represent a respective message pass within the graph—purple represents the transition from one arbitrary direction to the other when the graph branches
edge hidden states taken into account when updating edge (v,w) of the directed graph G = (V ,E) is
In the EMNN, before message passing takes place, the two node features are embedded into an edge feature by feeding a concatenation of the original edge and node feature vectors through a FFNN f embNN ,
At the initial state (t = 0) , evw , h(0)v are the raw bond feature vector and atom feature vector respectively and (,) refers to the concatenation operation.###We introduce a selection of augmentations to known MPNN architectures, which we refer to as Attention MPNN (AMPNN) and Edge Memory Neural Network (EMNN) [34], and evaluate them against published benchmark results with a range of metrics.###The EMNN network was the best performing architecture, beating SVM models and presenting a predictive power on average over four times higher than MoleculeNet original performance, with only a slightly higher variance.###The EMNN had computational cost disadvantages, however, its use may be justified in  situations where it offers significant performance increases: We demonstrate that our algorithms can outperform stateof-the-art models in virtual screening settings, notably demonstrated on sparse multi-task datasets, even without the inclusion of target structural information.###1 The message passing from directed neighbouring edges to another edge in EMNN.###AMPNN
NA h(t+1)v = GRU ( m (t) v , h (t) v )
NA
EMNN evw ′ = f embNN
((
evw , h (0) v , h (0) w
))
h (t+1) vw = GRU
(
m (t) vw , h (t) vw
)
h (K) v = ∑
w∈N(v)
h (K) vw###We show our architectures (SELU-MPNN, AMPNN and EMNN models) for both the unaltered and for the SMD preprocessed data, compared against the literature values for the original datasets to allow for fair benchmarking comparison for both the methods and for the preprocessing approaches.###Due to architectural differences and an increased number of parameters, the optimisation range for the EMNN was slightly tightened.",impact-revealing,describing the performance and architecture of the Edge Memory Neural Network
3038,5ee8986891e011e66831c3b6,38f93092ece8eee9771e61c1edaf11b1293cae1b,Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,58437722ac44360f1082f13a,Temporal Ensembling for Semi-Supervised Learning,"In the semi-supervised setting [55, 56], an unsupervised loss is combined with a classification loss over a handful of labels to ground the training [19, 20, 57, 58, 59, 60, 61, 62].###While previous methods based on bootstrapping have used pseudo-labels [16], cluster indices [17] or a handful of labels [18, 19, 20], we propose to directly bootstrap the representations.",other,describing a proposed method for semi-supervised learning
1167,,2a41ae528eb779ce75239e4cabf096eceab3c1b7,Neighbor Discovery in Low-Duty-Cycle Wireless Sensor Networks with Multipacket Reception,,,"###• By setting k = 1 and m = 1, we get O(n log n) which is the time complexity of birthday protocols [5, 7] which is designed for SPR networks and nodes are always awake.###They are widely adopted by many previous works [5, 7, 9, 12, 13].###first extended the results of [5, 7] to the k-MPR situation.###The milestone of the randomized protocols of ND is the Birthday Protocol proposed in [5] by McGlynn et al.",impact-revealing,acknowledge the foundational work on birthday protocols and their time complexity
1843,,23cfba37185cb8dd158d1be9ea2c58aa0c4db2d5,An executable specification language for planning attacks to security protocols,,,"###In some cases, to correctly model authentication and confidentiality attacks, it is necessary to add an oopsrule, where the intruder gets hold of past confidential data [22, 23].###The SitCalc, with its elegance and close similarity to Paulson’s inductive approach [22], would seem to be the appropriate candidate for planning attack, thus offering a nice complement to Paulson’s approach.###Thus, our method, based on a logical specification for model checking security protocols, offers an alternative to process algebras [13, 14] and state exploration methods [18], and complements Paulson’s inductive approach [22].###close correspondence with protocol traces and the formalization shares many similarity with Paulson’s inductive approach [22].###In the initial proposals by Lowe [12, 13], Schneider [26], or Paulson [22, 23], knowledge is captured by complex operations or indirectly.###These actions are basically present in the inductive theory of traces by Paulson and Bella [22, 3].###See also [3, 22] for a discussion of this modelling choice.###Lowe [12] and Schneider [26] use one relation to model the abilities to compose and decompose messages, Paulson [22] treats these abilities separately with two relations.###7The recursive protocol analyzed in [22, 25] is an exception.###As in Paulson’s inductive approach [22, 3], we describe each protocol run as a trace of send and receive actions.",impact-revealing,discussing the relationship and complementarity of different approaches in security protocol modeling
1144,,35ccd924de9e8483bdcf144cbf2edf09be157b7e,Text-to-image Diffusion Models in Generative AI: A Survey,,,"###To mitigate this problem, Exact Diffusion Inversion via Coupled Transformations (EDICT) [130] proposes to maintain two coupled noise vectors in the diffusion process and achieves higher reconstruction quality than DDIM [125].###deterministic DDIM [125] instead of DDPM reverse process [30].###Another work Null-text Inversion [131] improves image editing with Diffusion Pivotal Inversion and null-text optimization, Inspired by the finding that the accumulated error in DDIM [125] can be neglected in the unconditional diffusion models, but is amplified when applying classifier-guidance with a large guidance scale w in image editing, [131] proposes to take the initial###Instead of DDPM [30], DDIM [125] has been widely applied for its nearly perfect inversion [103].###However, the computation time of EDICT [130] is almost twice of DDIM [125].###However, due to the local linearization assumptions, DDIM [125] may lead to incorrect image reconstruction with the error propagation [130].",impact-revealing,highlighting advancements in diffusion models for image editing
1503,,cc8b5609b98d0d7936d32f74e991ae1865c1d725,Empower Generalizability for Pansharpening Through Text-Modulated Diffusion Model,,,"###Our model builds upon the conditional denoising DPM [6].###Currently, DPMs exhibit extraordinary performance in general imaging inverse problems [24], such as image super-resolution [6] and image restoration [37].###Very recently, diffusion probabilistic models (DPMs) [5] demonstrate remarkable performance in a variety of imaging inverse problems [6].",impact-revealing,highlighting the performance of diffusion probabilistic models in imaging tasks
484,5db929e947c8f766461fd005,11ead744b2edbdabe174258a8dfe4023be8e501d,Hierarchical Attention Prototypical Networks for Few-Shot Text Classification,5bdc315017c44a1f58a05d2c,FewRel: A Large-Scale Supervised Few-shot Relation Classification Dataset with State-of-the-Art Evaluation.,"FewRel Few-Shot Relation Classiﬁcation (Han et al., 2018)###, 2017) has achieved excellent performance in few-shot image classification and few-shot text classification (Han et al., 2018; Gao et al., 2019) tasks respectively, so our model is based on prototypical networks and aims to get promotion.###FewRel Few-Shot Relation Classification (Han et al., 2018) is a new large-scale supervised dataset1.###For ease of comparison, its details are the same as Han et al. (2018) proposed.###For the neural networks based baselines, we use the same hyper parameters as Han et al. (2018) proposed.###…the best weighted combination from a set of metrics obtained from meta-training tasks for a newly seen few-shot task such as intention classiﬁcation, Han et al. (2018) present a relation classiﬁcation dataset - FewRel, and adapt most recent state-of-the-art few-shot learning methods for it, Gao et…###The prototypical networks (Snell et al., 2017) has achieved excellent performance in few-shot image classiﬁcation and few-shot text classiﬁcation (Han et al., 2018; Gao et al., 2019) tasks respectively, so our model is based on prototypical networks and aims to get promotion.",impact-revealing,acknowledge the basis of the model on FewRel and its performance in few-shot classification tasks
3620,5ed7796e91e011e6e91120f0,8ade0f3bbe8d8251c5de9ef39886a13055f463e5,TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding,5bdc316717c44a1f58a06ebb,Heterogeneous Graph Neural Networks for Malicious Account Detection.,"Some other methods such as GEM [22] and SHINE [38] should be capable of handling the dataset at this scale, but they are not releasing their code to the public, and we can not easily guarantee reproduction.###GEM [22] is almost a special case of r-GCN.",other,highlighting limitations in reproducibility of certain methods
3313,5f842b5891e01129be18ffbd,7097137596f6755675f6aafcdd80969a747322ae,Contrastive Learning with Hard Negative Samples,5efdaf7b91e01191d3d28242,Debiased Contrastive Learning,"Our proposed hard negative sampling method conditions on the event { h ( x ) (cid:54) = h ( x − ) } in order to avoid false negatives (termed “debiasing” (Chuang et al., 2020)).###2 DOES AVOIDING FALSE NEGATIVES IMPROVE HARD SAMPLING? Our proposed hard negative sampling method conditions on the event {h(x) 6= h(x−)} in order to avoid false negatives (termed “debiasing” (Chuang et al., 2020)).###2 show consistent improvement over SimCLR ( q = p ) and the particular case of our method with β = 0 proposed in (Chuang et al., 2020) (called debiasing) on STL10 and CIFAR100.###2 show consistent improvement over SimCLR (q = p) and the particular case of our method with β = 0 proposed in (Chuang et al., 2020) (called debiasing) on STL10 and CIFAR100.###baseline in 5 out of 6 cases, the debiased baseline (Chuang et al., 2020) in 4 out of 6, and both in 3 out of 6 cases.###As a special case our our method, when the hardness level is tuned fully down, we obtain the method proposed in (Chuang et al., 2020) that only upholds Principle 1 (approximately) but not Principle 2.###To work towards a practical method, note that we can rewrite this distribution by adopting a PU-learning viewpoint (Elkan & Noto, 2008; Du Plessis et al., 2014; Chuang et al., 2020).###As discussed earlier, the debiasing method of Chuang et al. (2020) can be recovered as a special case: taking β = 0 to obtain the distribution q− 0 .###As discussed earlier, the debiasing method of Chuang et al. (2020) can be recovered as a special case: taking β = 0 to obtain the distribution q − 0 .",other,describing a proposed method for hard negative sampling
1265,,18b38ca5d672825bcb12cbc9a3bcf111ec136d98,Bringing Alive Blurred Moments,,,"###Results on Camera Motion Dataset: For evaluating qualitative performance on videos with camera motion alone, we tested our network’s ability to reconstruct videos from blurred images taken from datasets of [7], [15] and [19], which are commonly used for benchmarking deblurring techniques.###Video generation from images blurred with global camera motion from datasets of [7, 15] and [19].###The dataset [19] consists of both synthetic and real images collected from various conventional prior works on deblurring.",impact-revealing,reporting evaluation methods and datasets used for benchmarking
2759,5e3d353b3a55ac4de4104f40,3024f58826a5bce3378af94f677e8fb90cbb49e0,LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation,5d9edc8347c8f76646042a37,Simplifying Graph Convolutional Networks,"It is worth mentioning that several recent efforts provide deep insights into GNNs [24, 27, 40], which inspire us developing LightGCN.###First we discuss the connection with the Simplified GCN (SGCN) [40], which is a recent linear###The basic idea of GCN is to learning representation for nodes by smoothing features over the graph [23, 40].###[40] argues the unnecessary complexity of GCN, developing a simplified GCN (SGCN) model by removing nonlinearities and collapsing multiple weight matrices into one.###In [40], the authors argue the unnecessary complexity of GCN for node classfication and propose SGCN, which simplifies GCN by removing nonlinearities and collapsing the weight matrices to one weight matrix.",other,acknowledge insights from recent efforts in GNNs and their influence on developing LightGCN
3416,5c8d1d8b4895d9cbc63cdd4f,dec56bd20137a1076751c9d7190b685a01a08885,5G Ubiquitous Sensing: Passive Environmental Perception in Cellular Systems,53e998f0b7602d970212bbab,RF-Sensing of Activities from Non-Cooperative Subjects in Device-Free Recognition Systems Using Ambient and Local Signals,"The recognition of walking speed from RF-ﬂuctuation has been considered from variance in FM radio signal strength [14], custom SDR [16] or RSSI [17].###The recognition of walking speed from RF-fluctuation has been considered from variance in FM radio signal strength [15], custom SDR [17] or RSSI [18].",other,acknowledging existing methods for recognizing walking speed
3722,5f58a1b491e011e46ee73247,435bc42450259a22cfba92b40217b8d26f4a7ed5,Adversarial Attack on Large Scale Graph,5c0495ae17c44a2c74701db3,A Weighted Meta-graph Based Approach for Mobile Application Recommendation on Heterogeneous Information Networks.,"Graph structures are ubiquitous in nature and society, there is a great deal of research interest in studying graph data [5], [6], [7], [8], [9].",other,highlighting the significance of graph structures in research
1966,,a149b332add921da7bb47f05cc91487d09fe67cf,A prospective comparison of acute intestinal toxicity following whole pelvic versus small field intensity-modulated radiotherapy for prostate cancer,,,"###reported that acute bowel complications were higher in patients treated with whole pelvic than in those receiving prostate-only RT,(4,9) but with small insignificant increases in late intestinal toxicity.(1,3) Most of these studies were based on three-dimensional conformal RT.###Another trial, GETUG-01, reported no difference in 5-year progression-free survival between WP and prostate-only RT.(3) However, GETUG-01 study has been criticized for its low superior border, S1/S2 interspace.",impact-revealing,acknowledging findings and limitations in radiation therapy studies
523,5f7fdd328de39f0828397e22,21e33bd0ad95ee1f79d8b778e693fd316cbb72d4,Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs,5b67b47917c44aac1c8637c6,Representation Learning on Graphs with Jumping Knowledge Networks.,"We compare GraphSAGE, GCN-Cheby and GCN to their corresponding variants enhanced with JK connections [38].###Under homophily, the performance with and without JK connections is similar (gaps mostly less than 2%), matching the observations in [38].###This design is introduced in jumping knowledge networks [38] and shown to increase the representation power of GCNs under homophily.###, concatenation, LSTM-attention [38].###While these designs have been utilized separately in some prior works [11, 7, 1, 38], we are the first to discuss their importance under heterophily by providing novel theoretical justifications and an extensive empirical analysis on a variety of datasets.###By concatenating the intermediate representations from two rounds with the embedded ego-representation (following the jumping knowledge framework [38]), GCN’s accuracy increases to 58.###[38] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka.###For GCN+JK, GCN-Cheby+JK and GraphSAGE+JK, we enhanced the corresponding base model with jumping knowledge (JK) connections using JK-Concat [38] without changing the number of layers or other hyperparameters for the base method.",impact-revealing,Highlighting the novelty and significance of combining various model designs under heterophily
1459,,38499c7f9052f80fee24aeeb8130756a6c7766c3,The typicality effect in basic needs,,,"###…to explain typicality effects better than theClassical Theory, among them (1) the exemplar theory (Brooks, 1978; Medin & Schaffer, 1978), (2) the theory theory (e.g., Carey, 1985; Gopnik &Meltzoff, 1997) and (3) the prototype theory (Hampton, 2006; Rosch, 1973, 1975, 1978; Rosch & Mervis, 1975).###…(such as bird) to things that share enough of the sufficientlyweighty—though often not necessary—features of typical instances (such as the characteristics “flies”, “nests in trees”, “sings”, etc.) (e.g., Rips et al., 1973; Hampton, 1979, 1981, 2006; Rosch, 1973, 1975, 1978; Rosch & Mervis, 1975).###Free-listing tasks are an established qualitative method (Quinlan, 2017; Weller & Romney, 1988) that has been commonly used in typicality effect research (e.g., Hampton, 1981; Rosch, 1975; Wang et al., 2016).###…specific question ofwhether the concept of basic needs shows a typicality effect either.
the concept bird, and consequently for this concept’s non-classical internal structure (Rosch, 1975).12
Our third study tested whether the above response time prediction holds for the concept of basic needs.###Some of the most pressing objections against it have been based on a line of psychological research that was conducted initially by Eleanor Rosch1 in the 1970s, documenting a so called “typicality effect” (Rosch, 1973, 1975, 1978; Rosch & Mervis, 1975).2
According to the Classical Theory, all things that fall under a concept ought to count equally as instances of the concept.###For example, Weatherson, one proponent of this “specificity defense”, writes:
Philosophers aren’t particularly interested in terms like ‘weapon’, so these experiments only have philosophical interest if the results can be shown to generalise
1 According to Rosch (1975), her research on typicality (and prototype semantics more generally) was inspired by Wittgenstein’s idea of “family resemblance”, as it figures most prominently in §66–68 of his Philosophical Investigations.###…interested in terms like ‘weapon’, so these experiments only have philosophical interest if the results can be shown to generalise
1 According to Rosch (1975), her research on typicality (and prototype semantics more generally) was inspired by Wittgenstein’s idea of “family resemblance”,…###For example, examining the concept bird, Rosch (1975) found that classifying a sparrow as a bird (typical instance) takes significantly less time than classifying a penguin as a bird (atypical instance).###…based on a line of psychological research that was conducted initially by Eleanor Rosch1 in the 1970s, documenting a so called “typicality effect” (Rosch, 1973, 1975, 1978; Rosch & Mervis, 1975).2
According to the Classical Theory, all things that fall under a concept ought to count equally as…###For example, in some previous studies people were asked how typical they thought robins, sparrows, eagles, penguins, etc. are for the concept of birds; or how typical they thought cars, boats, horses and skis are for the concept of vehicles (Rips et al., 1973; Rosch, 1975; Rosch & Mervis, 1975).###The particular formulation of our instructions was adapted from Rosch (1975) as follows:
It is generally thought that humans have basic needs.###According to the so-called Classical Theory, concepts are mentally represented by individually necessary and jointly sufficient application conditions (Hull, 1920; Katz
B Thomas Pölzler thomas.poelzler@uni-graz.at http://thomaspoelzler.com
Ivar R. Hannikainen ivar@ugr.es
1 Department of Philosophy, University of Graz, Attemsgasse 25/II, 8010 Graz, Austria
2 Department of Philosophy I, University of Granada, Campus de la Cartuja, 18011 Granada, Spain
0123456789().###Moreover, items that are judged to be more typical should be listed more frequently and should rank higher in corresponding free-listing tasks (e.g., Rosch, 1975; Rosch et al., 1976).###…they judge that sparrows are a better example of the concept bird than penguins; if asked to name instances of the concept, they more often name sparrows than penguins; and they are faster in classifying sparrows as birds than penguins (Rips et al., 1973; Rosch, 1973, 1975; Rosch et al., 1976).",impact-revealing,discussing the typicality effect in concept classification
2541,5fef22c691e0113b265a0289,b5b006dc558cb7fbd532d67e989173b536e8ac80,MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers,56d81308dabfae2eee5eff4b,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,"GLUE General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of two single-sentence classiﬁcation tasks (SST-2 (Socher et al., 2013) and CoLA (Warstadt et al., 2018)), three similarity and paraphrase tasks (MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017)…",other,providing context about the GLUE benchmark and its tasks
3072,5aed147c17c44a4438153ea5,665c0dde22c2f8598869d690d59c9b6d84b07c01,domino temporal data prefetcher,573698426e3b12023e70bf13,Best-Offset Hardware Prefetching,"Complex access patterns in the context of hardware prefetching are considered in many pieces of recent work [60], [61], [62], [63], [64], [65].",other,acknowledge recent work on hardware prefetching
206,5da2f8aa3a55ac3402d8c2e1,17f2f3f7e58b916175d495109bc74b2757ef952a,Barrage of Random Transforms for Adversarially Robust Defense,5ca600ae6558b90bfa4d76e9,Towards Deep Learning Models Resistant to Adversarial Attacks.,"[25], which is the computational cost of a threat model.###PGD has been conjectured to be a near-optimal first-order attack [25].###[25] used adversarial training on the cifar dataset, which still has the best empirical robustness to attack [24] and has been repeatedly validated as effective and capable of fully defending against the best known adversaries under the whitebox threat model [4].",impact-revealing,reporting findings on adversarial training effectiveness
2842,5d08be648607575390f908ca,b30b850e726ee41f80a28c2a81da9c640a8541fc,QoSMT: supporting precise performance control for simultaneous multithreading architecture,53e9a4e4b7602d9702e01f67,Hill-climbing SMT processor resource distribution,"There are many studies [7][30][9][10][27] providing solutions on improving overall SMT throughput and fairness, but they did not take performance control into account.###Although there is previous literature [9] [10]on hardware design for guarantee of quality-of-service (QoS) on SMT processors, most of their designs require profiling in-advance.",other,highlighting gaps in existing studies on SMT throughput and fairness
2354,5c04967517c44a2c74708b7e,c18663fea10c8a303d045fd2c1f33cacf9b73ca3,GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism,573696ce6e3b12023e5ce95a,Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift,"If batch normalization [25] is used in the network, the sufﬁcient statistics of inputs during training are computed over each micro-batch , and over replicas if necessary [41].",other,providing context on batch normalization in neural networks
925,57d063e0ac44367354294777,9c62532d78d6eb31f7b0489b6bcb4c2baf517860,CASH: Supporting IaaS Customers with a Sub-core Configurable Architecture,53e9ac54b7602d9703622ebc,The sharing architecture: sub-core configurability for IaaS clouds,"The CASH architecture is inspired by the Sharing Architecture [64], but improves on it with fast reconfiguration, a well defined software-hardware interface which is
needed for the CASH runtime, and the use of an on-chip network to monitor remote cores’ performance.###The CASH architecture extends the Sharing Architecture [64] – a prior configurable core architecture – by (1) innovating in fast reconfiguration, (2) providing a well designed hardware-software interface, and (3) allowing remote cores’ performance to be monitored over an on-chip network.###For instance, configurable cache hierarchies [20, 23,
28, 44, 52] and configurable pipeline architectures (modifiable issue width, instruction window size, number of physical registers, etc.) [15, 41, 57, 64] allow fine grain control over resource scheduling.###We demonstrate the potential of fine-grain configuration by running the x264 video encoder [5] on the CASH Architecture (which extends the Sharing Architecture [64]).###Microservers [4, 37, 59], data center optimized accelerators [19, 31, 43], and fine-grain reconfigurable processors [64] are all examples.###These control approaches measure runtime QoS feedback and compute a control signal indicating
a resource allocation that maintains the desired QoS.###The data networks already exist in the Sharing Architecture, but the CASH Runtime Interface Network is newly added to support fast reconfiguration and performance gathering.###1 and 2 together comprise a standard control system similar to prior approaches, but with some limitations.",impact-revealing,highlighting improvements and innovations in the CASH architecture compared to prior work
2067,,0db3570346c752338b20d1627b8a81eba09d4e51,Short Sleep Duration and Erectile Dysfunction: A Review of the Literature,,,"###Actually, cortisol plays an important role in maintaining the tone of vascular smooth muscles (VSMCs) via increasing vascular responses to vasoconstrictors.78 It is proven that sustained elevation of serum cortisol facilitates the contraction of coronary VSMCs by enhancing RhoA/ROCK signaling pathway.79 Conversely, adrenalectomized animal models show decreased responsiveness to the administration of vasoactive agents, which are similar to Addisonian in humans.80 With regard to sexual function, it has been reported that the levels of serum cortisol are negatively correlated with some domains of the International Index of Erectile Function-5 (IIEF-5) score and males with high levels of serum cortisol might be vulnerable to ED.81 Although this was previously interpreted as a stress-related outcome due to the relationship between the HPA axis and stress, we hold the opinion that the disordered the HPA axis might be an important cog
Nature and Science of Sleep 2022:14 https://doi.org/10.2147/NSS.S375571 DovePress 1951
Powered by TCPDF (www.tcpdf.org)
between sleep disorders and ED (Figure 1).###Received: 5 June 2022 Accepted: 14 October 2022 Published: 27 October 2022
and ED.17,18 Therefore, current data is not enough to interpret the effects of non-apnea sleep disorders with short sleep duration on erectile function.###Even so, several pathophysiological mechanisms have been proposed to link sleep duration and medical conditions in recent years.21,22 Moreover, causal associations of short sleep duration with various diseases seem to dominate in current studies.8,23 Thus, we reviewed available data on sleep and erection (Table 1) and proposed putative mechanisms underlying causal associations of short sleep duration with ED, in order to facilitate further studies to elucidate the causal link between short sleep duration and ED.
https://doi.org/10.2147/NSS.S375571 DovePress Nature and Science of Sleep 2022:14 1946
Powered by TCPDF (www.tcpdf.org)
Nature and Science of Sleep 2022:14 https://doi.org/10.2147/NSS.S375571 DovePress 1947
Powered by TCPDF (www.tcpdf.org)###ROCK deactivates the myosin lightchain phosphatase (MLCP) by phosphorylating myosin phosphatase target subunit 1 (MYPT1), or directly phosphorylates the myosin light chain 2 (MLC2), promoting the smooth muscle contraction and penile flaccidity, which is also known as the calcium-sensitized pathway.47,48
On the other hand, noradrenaline (NA) released from adrenergic nerve binds the adrenergic receptors (ADRs) in corpus cavernosum and leads to a contraction which involves the influx of calcium in CCSMC and calcium-sensitized pathway.49 It has been demonstrated that penile erection in humans was accompanied by a remarkable decrease of NA in cavernous blood.50 Meanwhile, current evidence supports the viewpoint that post-synaptic adrenergic receptor α1 (ADRα1) in smooth muscle may play a leading role in contraction.51 Existing data have demonstrated the potential effect of adenosine in penile erection, priapism, and ED.52 The role of adenosine is played through its binding to specific
Nature and Science of Sleep 2022:14 https://doi.org/10.2147/NSS.S375571 DovePress 1949
Powered by TCPDF (www.tcpdf.org)
G protein-coupled receptors, including A1, A2A, A2B, and A3.###It is demonstrated that ED patients exhibit different heart rate variation (HRV) parameters compared with controls, suggesting that not only sympathetic but also general imbalance of the ANS might be an underlying cause of ED.88
Meanwhile, patients with some diseases that directly lead to collective autonomic dysfunction and neuropathy, such as primary autonomic failure and diabetes, are prone to ED.89 Sometimes autonomic imbalance is implicated in higher levels of both the parasympathetic and sympathetic system, which is inconsistent with the fact that sexual stimulation results in the penile tumescence through release of NO from parasympathetic cholinergic fibers.40 However, it should be noted that parasympathetic activity is different from the single effect of cholinergic transmitters, and other neurotransmitters may be co-released from nervous terminals.90 Actually, co-released transmitters from imbalanced global autonomic function might produce mixed effects on the regulation of erectile function, which might become a risk factor for ED.###Meanwhile, cGMP could be hydrolyzed by phosphodiesterase (PDE) in CCSMC, leading to the increase of calcium in the cytoplasm and a flaccid penis.12,42 There are 13 kinds of PDE in corpus cavernosum of human.43 Among them, the higher expression of PDE5 in corpus cavernosum has been established, and PDE5 inhibitors (PDE5Is) have been widely applied as a therapeutic revolution in the management of ED.44
On the contrary, the flaccidity of the penis is maintained by chronic contraction of CCSMC.45 The molecular mechanism of penile flaccidity is mainly regulated by noradrenaline (NA), endothelin-1 (ET-1), and angiotensin II (AngII) signaling, which triggers an increase in intracellular calcium and thereby results in the contraction of CCSMC via the phosphorylation of myosin light chain (MLC).42,45 However, the contraction of smooth muscle is not paralleled with the levels of intracellular calcium and the phosphorylation of MLC, suggesting the existence of signal pathway which inhibits the smooth muscle relaxation independent of NO.46 Ras homolog gene family member A (RhoA) is a small GTPase that can activate Rho-associated coiled-coil containing kinase (ROCK).###Nature and Science of Sleep 2022:14 https://doi.org/10.2147/NSS.S375571 DovePress 1953
Powered by TCPDF (www.tcpdf.org)
Inflammation Inflammation is considered as an important mechanism connecting the sleep and the development of various conditions.122,123 Actually, there is a lot of evidence that poor sleep contributes to the increases of inflammatory markers.124,125 Among them, shorter sleep duration was reportedly associated with higher levels of CRP.124
Meanwhile, the sleep deprivation of a night might result in significantly increased monocyte production of IL-6 and TNF-α in the morning compared with normal sleep, which may be mediated by the nuclear factor kappa B (NF-κB) inflammatory signaling and hormone response pathways.126 Moreover, an increased level of interferon γ (IFN-γ) has been observed in real-life models after a night of sleep deprivation with unchanged levels of TNF-α, interleukin-2 (IL-2) and interleukin-10 (IL-10).127
Although inconsistent data on inflammatory markers after sleep deprivation are reported in previous studies, several markers, such as IL-6 and CRP, appear to have more consistent results.124,128 On the other hand, longitudinal analyses have indicated that effective intervention, such as recovery sleep, might decrease the levels of baseline inflammatory markers and health risk.129 In the clinical setting, the IL-6 and CRP have been shown to be associated with vascular risk and coronary heart disease.130,131 Meanwhile, IL-6 seems to mediate the vascular contraction through inhibiting the endothelium-dependent NO-cGMP pathway in animal models.132
https://doi.org/10.2147/NSS.S375571 DovePress Nature and Science of Sleep 2022:14 1954
Powered by TCPDF (www.tcpdf.org)
In agreement with cardiovascular diseases, increased circulating levels of inflammatory cytokines are found to be associated with the presence and severity of ED.133 Similar findings have been reported in studies on the molecular mechanisms between chronic pelvic pain syndrome (CPPS) and ED.###Similarly, a cross-sectional designed pilot study using mobile health platforms showed an underlying connection between poor sleep quality and ED.71 In addition, a subgroup analysis of older adults suggested that self-reported sleep duration might be related to sexual dysfunction.11
In recent years, although mixed results were concluded usually, more and more studies provided clues that short sleep duration might be a risk factor for ED.69,72 However, the cross-sectional nature of those studies restrict the establishment of a causal relationship, and impede the development of further research.###Generally speaking, the evaluation of autonomic function, such as HRV analysis, can provide valuable information for the diagnosis of patients with both sleep disorders and ED.###There are 19 Wnt genes in mammals, and mutated Wnt pathway components are thought to be implicated in various diseases.146 The Wnt signaling pathway is commonly classified into β-catenin dependent and βcatenin independent pathways, which have various downstream signaling pathways.147 Among them, RhoA/ROCK signaling plays a key role in β-catenin independent pathway and mediates pathophysiological mechanisms of several
Nature and Science of Sleep 2022:14 https://doi.org/10.2147/NSS.S375571 DovePress 1955
Powered by TCPDF (www.tcpdf.org)
Wnt-related diseases, such as hypertension.148 In diabetic ED models, aberrant expression of Wnts in the corpus cavernosum has been identified, suggesting that Wnt signaling might contribute to the development of ED.149
Meanwhile, the inhibition of Wnt signaling via Dickkopf3 (DKK3) could improve the erectile function in diabetic models through restoring the cavernous vascular integrity and endothelial function.150 Hence, the Wnt pathway may represent a potential target in the management of ED.###Due to the absence of intermediate mechanisms, it is difficult to prove a causal link between short sleep duration and ED.###Among them, the higher expression of PDE5 in corpus cavernosum has been established, and PDE5 inhibitors (PDE5Is) have been widely applied as a therapeutic revolution in the management of ED.(44) On the contrary, the flaccidity of the penis is maintained by chronic contraction of CCSMC.###Meanwhile, neuroendocrine systems, various molecular mechanisms and signaling pathways may participate in the process of ED in patients with short sleep duration, and the improvement sleep quality and habits may be conducive to the treatment of ED.###Autonomic Nervous System The ANS imbalance as a potential risk factor for various diseases has been widely investigated.21 Short sleep duration is found to be involved changes in the ANS characterized by lower levels of parasympathetic tone and global sympathetic overactivity.82 Meanwhile, most data reveal that short sleepers are susceptible to have increased sympathetic activity, which could be carried into the daytime, as well as altered autonomic function following short sleep duration might result in serious repercussions for well-being and health condition.83,84 On the other hand, short sleep duration always cooccurs with the increased levels of circulating blood catecholamines, which might mediate some adverse effects of sleep disorders on erectile function.85,86 It is established that noradrenaline could regulate the influx of Ca2+ into smooth muscle cells and inhibits the penile erection via α-adrenergic receptors.87 Therefore, the autonomic imbalance towards sympathetic activity might take part in the association of short sleep with ED.###As a mystery of biology, the roles of sleep have puzzled and intrigued people for millennia.1 For adults, the recommended sleep duration is 7 to 9 hours, while short sleep duration is defined as habitual sleep time less than 6 hours.2 Sleep disorders are divided into 7 major diagnostic sections based on International Classification of Sleep Disorders-3 (ICSD-3), including insomnia, sleep-related breathing disorders, central disorders of hypersomnolence, circadian rhythm sleep-wake disorders, parasomnias, sleep-related movement disorders and other sleep disorders.3
Short sleep duration, as one of the most notable characteristics of sleep disorders and some psychiatric disorders, was received with concern in modern society.3,4 Meanwhile, it is reported that sleep disorders with short sleep duration appear to have more severe manifestation than those with normal sleep duration.5,6 Furthermore, it is estimated that the prevalence of short sleep duration has gradually increased, in which 29.1% of US adults reported short sleep duration in 2009, including the habitual short sleepers and those with medical conditions.7 Although many issues remain unresolved, the association between short sleep duration and many negative health outcomes, such as cardiovascular diseases, metabolic diseases and inflammatory disorders, has been widely investigated.8,9 Furthermore, several studies have hinted that short sleep may be associated with men’s health and sexual function.10,11
Erectile dysfunction (ED) is defined as the inability to acquire or maintain penile erection for satisfactory sexual performance.12 Although ED is not a fatal condition, it bothers a large proportion of males around the world.13,14 It is suggested that more than 80% of patients with ED have organic etiologies, including vascular, neurological and endocrine factors.12 However, ED still involves the psychological component, especially in younger males.15 Thus, it must be pointed out that previous psychological problems and the disturbance of psychosocial process, such as sleep homeostasis, may be involved in the pathophysiology of ED.",impact-revealing,highlighting the association between short sleep duration and erectile dysfunction
536,5f058d15dfae54570ec57ea1,fb93ca1e004cbdcb93c8ffc57357189fa4eb6770,Resnest: Split-attention networks,5cede0fcda562983788db9a8,Selective Kernel Networks,"26 SKNet-101 [38] 48.###Table 3 shows that our proposed ResNeSt outperforms all ResNet variants with a similar number of network parameters and FLOPS, including: ResNet [23], ResNeXt [60], SENet [29], ResNet-D [26] and SKNet [38].###Global contextual information with embedded channel-wise statistics can be gathered with global average pooling across spatial dimensions s ∈ R [29, 38].###For comparison with ResNet variants [23,26,29,38,60], all networks (including ResNeSt) are trained using a crop size size of 224 × 224, and then evaluated using center crop with sizes 224× 224 as well as 320× 320.###Following [30, 38], a combined representation for each cardinal group can be obtained by fusing via an element-wise summation across multiple splits.###SK-Net [38] brings the feature-map attention across two network branches.###SKNet-50 [38] 27.###1: Comparing our ResNeSt block with SE-Net [30] and SK-Net [38].###Previous models like SK-Net [38] introduced feature attention between two network branches, but their operation is not optimized for training efficiency and scaling to large neural networks.",impact-revealing,comparing performance of ResNeSt with other ResNet variants
1589,,2c0c941447ba83f8d05c4ce4b3da327a0eb9c53c,Overview of Parallel Computing for Meta-Heuristic Algorithms,,,###The Dolphin Echolocation (DE) algorithm [23] was inspired by the unique echolocation of dolphins.,impact-revealing,highlighting the inspiration behind the Dolphin Echolocation algorithm
2086,,1514fbdc7e4fa7d09eec766eb2b54184d532ad94,"De Novo Gene Birth, Horizontal Gene Transfer, and Gene Duplication as Sources of New Gene Families Associated with the Origin of Symbiosis in Amanita",,,"###To identify orthologs in asymbiotic species, we generated a more robust phylogeny using RAxML with the uBLAST results generated for the HGT analysis.###7.2.8 (Stamatakis 2006), using the trimmed protein alignment, and applying gamma rate heterogeneity and the best substitution model (either JTT, LG, or WAG) as determined by AICc values calculated by ProtTest ver.###After building the RAxML gene phylogenies, one gene family was no longer placed with the putative donor lineage and therefore we did not consider it further.###A phylogeny for each gene family was built with RAxML ver.###These data sets were aligned and trimmed again and used to generate new trees using RAxML (Stamatakis 2006) with best evolutionary models identified by ProtTest based on AICc values.###Each constraint tree was used to reconstruct a RAxML phylogeny.###15 were aligned and trimmed again and used to generate new trees using RAxML (Stamatakis 2006) with best evolutionary models identified by ProtTest based on AICc values.###8 (Stamatakis 2006), using the trimmed protein alignment, and applying gamma rate heterogeneity and the best substitution model (either JTT, LG or WAG) as determined by AICc values calculated by ProtTest ver.###RAxML-VI-HPC: maximum likelihood-based phyloge-
netic analyses with thousands of taxa and mixed models.###To generate phylogenies we used MAFFT, trimAl, ProtTest, and RAxML as above.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2520,5e6cae3493d709897ccff2f9,804f23ac1a4a56b8dc5bb7201dab7b8cece76a70,classifying memory access patterns for prefetching,559163420cf2e89307ca980e,Profiling a warehouse-scale computer,"As a result, current caches and prefetchers still leave up to 60% of performance on the table in the data center [4, 28].###However, given that data set sizes are increasing [28] and transistor scaling is slowing down [17, 20], we cannot rely solely on cache capacity scaling.",other,highlighting the limitations of current caches and prefetchers in data centers
2413,5c96086e3cb210d2716c4a08,bb76749fab841ccbe20ace74111a518f65d9870b,make the most out of last level cache in intel processors,599c7ac8601a182cd26e050b,Attack of the killer microseconds.,"Unfortunately, accessing DRAM takes ∼ 60 ns and the performance of the processors is no longer doubling at the earlier rate, making it harder to keep up with the growth in link speeds [4, 58].",other,highlighting challenges in accessing DRAM and processor performance
448,5db92aec47c8f76646216865,146128184cd585b5a1298d3e75e15fb8fbbbd7a5,Improved low-resource Somali speech recognition by semi-supervised acoustic and language model training,58d83008d649053542fe05c0,Purely Sequence-Trained Neural Networks For Asr Based On Lattice-Free Mmi,Our TDNN-F was trained using the lattice-free maximum mutual information objective criterion [22].,impact-revealing,reporting the training method used for TDNN-F
323,5a73cbcc17c44a0b3035f264,afd76be183d34af3bf944debd73db1c77987a8c6,Covariant Compositional Networks For Learning Graphs,573696116e3b12023e52463f,Group Equivariant Convolutional Networks.,"…an active ﬁeld of research, they differ from classical CNNs in a fundamental way: the internal feature representations in CNNs are equivariant to such transformations of the inputs as translation and rotations (Cohen & Welling, 2016a;b), the internal representations in MPNNs are fully invariant.###This phenomenon is called steerability , and has a signiﬁcant literature in both classical signal processing (Freeman & Adelson, 1991; Simoncelli et al., 1992; Perona, 1995; Teo & Hel-Or, 1998; Manduchi et al., 1998) and the neural networks ﬁeld (Cohen & Welling, 2016b).###This is the simplest manifestation of a well studied property of CNNs called equivariance (Cohen & Welling, 2016a; Worrall et al., 2017).",impact-revealing,providing context on the differences between CNNs and MPNNs
1043,,583075c6b6333bf4dd1ae5151fd9f440c6d16ba1,Analysis and Implementation of Long Pattern Matching Approaches,,,"###In this example only SA[8] was inferred, but if there are z occurrences for a pattern, we only conduct two binary searches and infer the remaining z − 2 matches from those.###binary searches, but what is even more important is that every array index between these two denotes correct results, which means that SA[7], SA[8] and SA[9] are the solution of the problem (which refer to S11, S2 and S10, respectively).###To calculate the hash values during construction time, it is possible to use a so-called rolling hash, which is also used by the Rabin-Karp algorithm [8].###Hashing is already widely used for pattern matching, presented in a seminal paper by Rabin and Karp [8], and it is also possible to match multiple pattern by this existing method.",impact-revealing,providing context for pattern matching methods
2265,5f7fdd328de39f0828397c88,edcb65ea0954067d9137599423790fbd331de7b3,how hard is to distinguish graphs with graph neural networks,5ac1824c17c44a1fda913a60,Lower Bounds for Subgraph Detection in the CONGEST Model.,"The analysis stands out from previous relevant works that have studied subcases of isomorphism, such as subgraph freeness [34, 35] or those focused on anonymous networks [3– 6, 17, 19].",other,highlighting the distinctiveness of the current analysis compared to previous works
2562,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",53e9bcc5b7602d970494800f,Graphical models in applied multivariate statistics,"3) Graphical models: The focus in this area is on inference and learning from large datasets, [26], [27], [28], [29], [30].",other,providing context on graphical models in inference and learning
3885,53e99967b7602d97021ac42b,41721de035c15528a7e35d3ab4d79b053633d763,Feedback-directed memory disambiguation through store distance analysis,53e9b682b7602d97041eb99e,Estimating cache misses and locality using stack distances.,"Others have developed efficient reuse distance analysis tools to estimate cache misses [1, 4, 24, 27] and to evaluate the effect of program transformations [1, 2, 6].",other,acknowledge existing tools for reuse distance analysis
2502,5bdc31b417c44a1f58a0b240,6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da,Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks,599c7ef5601a182cd28dca5c,Matching Node Embeddings for Graph Similarity.,"Recent works focus on assignment-based approaches (Kriege, Giscard, and Wilson 2016; Nikolentzos, Meladianos, and Vazirgiannis 2017; Johansson and Dubhashi 2015), spectral approaches (Kondor and Pan 2016), and graph decomposition approaches (Niko-lentzos et al. 2018).",other,acknowledge various approaches in recent works
657,5ee9f15b91e01152af022eaf,a83902f8b3aadfda633968a840ca1738bedef837,modeling graph structure via relative position for text generation from knowledge graphs,61a724c96750f8421870395d,Attention is all you need,"Graformer follows the general multi-layer encoder-decoder pattern known from the original Trans-former (Vaswani et al., 2017).###Our decoder follows closely the standard Trans-former decoder (Vaswani et al., 2017), except for the modiﬁcations suggested by Chen et al. (2018).###…choices are graph neural networks based on message passing between direct neighbors in the graph (Kipf and Welling, 2017; Veliˇckovi´c et al., 2018) or variants of Transformer (Vaswani et al., 2017) that apply self-attention on all nodes together, including those that are not directly connected.###The output of multiple heads is combined as in the original Transformer (Vaswani et al., 2017).###To compute the node representation H ( L ) in the L th layer, we follow Vaswani et al. (2017), i.e., we ﬁrst normalize the input from the previous layer H ( L − 1) via layer normalization LN , followed by multi-head graph self-attention SelfAtt g (see § 3.3 for details), which – after dropout…###Other approaches (Zhu et al., 2019; Cai and Lam, 2020) base their encoder on the Transformer architecture (Vaswani et al., 2017) and thus, in each layer, compute self-attention on all nodes, not only direct neighbors, facilitating the information ﬂow between distant nodes.",impact-revealing,describing the architecture and modifications of the Graformer model
1178,,4b0496664f6e52c145b5897c3f605ef7312ce21c,Path Planning for UAV to Cover Multiple Separated Convex Polygonal Regions,,,"###Typical cellular decomposition methods include the Trapezoidal decomposition [52], Boustrophedon decomposition [53] and Morse decomposition [54].###The back-and-forth pattern [53], [67] is widely adopted by many CPP methods, as it greatly simpliﬁes the path design and is easy to implement.",impact-revealing,providing context on cellular decomposition methods
3949,5c96086e3cb210d2716c4a08,bb76749fab841ccbe20ace74111a518f65d9870b,make the most out of last level cache in intel processors,53e9b87fb7602d970444ba0b,Contention-Aware Application Mapping For Network-On-Chip Communication Architectures,"Some other works focus on software-based strategies, such as data layout optimization [41, 87] or compiler optimization [7, 11, 30, 31], to exploit NUCA characteristics to improve performance.",other,acknowledge existing software-based strategies for performance improvement
2526,5aed14d617c44a4438158f7c,ae1c89817a3a239e5344293138bdd80293983460,Attention U-Net:,5c0495ae17c44a2c7470198c,Recurrent Saliency Transformation Network: Incorporating Multi-Stage   Visual Cues for Small Organ Segmentation,"Recent work [33] proposed an iterative two-stage model that recursively updates local and global predictions, and both models are trained end-to-end.",other,reporting prior findings on iterative two-stage models
2942,5db1765a3a55ac101c887e97,6c4b76232bb72897685d19b3d264c6ee3005bc2b,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,53e99bffb7602d97024ac0e6,Generating Sequences With Recurrent Neural Networks.,"Self-attention is a variant of attention [Graves, 2013; Bahdanau et al., 2015] that processes a sequence by replacing each element by a weighted average of the rest of the sequence.###Language models are typically used for compression or sequence generation [Graves, 2013].",other,providing context on self-attention and its applications
3263,5ecb57199e795ec6f2ba59cc,512f34906ddaefe885af2e5eec9b2b3b50ffd377,deep entity matching with pre-trained language models,5dbebb7447c8f766462c230e,Learning-Based Methods with Human-in-the-Loop for Entity Resolution,"Active learning is a recent trend in EM to train high-quality matching models with limited labeling resources [19, 23, 30, 40].",other,highlighting the trend of active learning in efficient model training
3266,5ee8986891e011e66831c3bc,965652c0e426c5b42d7218d7429025be7ac542bf,DeeperGCN: All You Need to Train Deeper GCNs,5bdc31b417c44a1f58a0ba6c,How Powerful are Graph Neural Networks?,"The methods include GCN [Kipf and Welling, 2016], GraphSAGE [Hamilton et al., 2017], GIN [Xu et al., 2019b], GIN with virtual nodes, GaAN [Zhang et al., 2018], and GatedGCN [Bresson and Laurent, 2018].###Although mean and max aggregators are proven to be less powerful than the WL test in [Xu et al., 2019b], they are found to be effective on the tasks of node classiﬁcation [Kipf and Welling, 2016, Hamilton et al., 2017] and 3D point cloud processing [Qi et al., 2017, Wang et al., 2019].###[Hamilton et al., 2017, Xu et al., 2019b].###To embrace the nice properties of invariance and equivariance (Property 1), many works in the graph learning ﬁeld tend to use simple permutation invariant functions like mean [Kipf and Welling, 2016], max [Hamilton et al., 2017] and sum [Xu et al., 2019b].###This invariance property guarantees the invariance/equivariance to isomorphic graphs [Battaglia et al., 2018, Xu et al., 2019b].###Inspired by the Weisfeiler-Lehman (WL) graph isomorphism test [Weisfeiler and Lehman, 1968], Xu et al. [2019b] propose a theoretical framework and analyze the representational power of GCNs with mean , max and sum aggregators.###Popular choices for aggregation functions include mean [Kipf and Welling, 2016], max [Hamilton et al., 2017], and sum [Xu et al., 2019b].###Furthermore, Xu et al. [2019b] propose a GCN architecture, denoted Graph Isomorphism Network (GIN), with a sum aggregation that is able to have as large discriminative power as the Weisfeiler-Lehman (WL) graph isomorphism test [Weisfeiler and Lehman, 1968].###For et al., 2018, Xu et al., 2019b].###…[Perozzi et al., 2014], Planetoid [Yang et al., 2016], Node2Vec [Grover and Leskovec, 2016], Chebyshev graph CNN [Defferrard et al., 2016], GCN [Kipf and Welling, 2016], MPNN [Gilmer et al., 2017], GraphSage [Hamilton et al., 2017], GAT [Veli ˇ ckovi ´ c et al., 2018] and GIN [Xu et al., 2019b].###…applied to vertex features h ( l ) v of v , its neighbor’s features h l ) guarantees the invariance/equivariance to isomorphic graphs [Battaglia et al., 2018]. ζ ( l ) can be a simply symmetric function such as mean [Kipf and Welling, 2016], max [Hamilton et al., 2017], or sum [Xu et al., 2019b].",other,acknowledge various graph learning methods and their properties
1487,,65e5f2c5fafa70088169bc4d27abc40406f08b1f,Contrastive Learning of Sentence Embeddings from Scratch,,,"###This is inspired by recent successes of prompting large language models (LLMs) to perform various tasks (Chung et al., 2022; Ouyang et al., 2022; OpenAI, 2023), especially the superior performance of LLMs over crowd-workers on text annotation (Gilardi et al., 2023).###Gilardi et al. (2023) used ChatGPT for dataset annotation.###, 2022; OpenAI, 2023), especially the superior performance of LLMs over crowd-workers on text annotation (Gilardi et al., 2023).",impact-revealing,highlighting the influence of recent successes in prompting large language models for various tasks
3407,5c6a37d03a69b1c9e12a9fc4,81b6d24e8f313fd88b0fe5ff6c21dd154fbe32d2,SimGNN: A Neural Network Approach to Fast Graph Similarity Computation,5b1643ba8fbcbf6e5a9bc8bc,Drug Similarity Integration Through Attentive Multi-view Graph   Auto-Encoders,"A great amount of graphbased applications have been tackled by neural network basedmethods, most of which are framed as node-level prediction tasks [27, 37].",other,acknowledge the application of neural networks in graph-based tasks
915,599c782b601a182cd25a765e,ac553579eaaaabd1003bce9a2ad679e901b3ae73,Predicting drug–drug interactions through drug structural similarities and interaction networks incorporating pharmacokinetics and pharmacodynamics knowledge,53e9a52bb7602d9702e4b414,Drug-drug interaction through molecular structure similarity analysis.,"Structural similarity for DDI prediction has been employed based on the idea that if there is a DDI between drug A and drug B, and drug C has a similar structure to drug A, there is likely a DDI between drug C and drug B [8].###predicted DDIs with a matrix transformation approach using structural similarities of drugs with molecular fingerprints [8].###[8], which also used drug structural similarities, two of the top ten drugs predicted by our model (i.",impact-revealing,describing the concept of structural similarity in DDI prediction
699,5ede0553e06a4c1b26a841e6,9a772646ef9ed9c917f45fa592d5f89f7d5f8542,bayesian graph neural networks with adaptive connection sampling,5c2c7a9217c44a4e7cf319c1,Evaluating Bayesian Deep Learning Methods for Semantic Segmentation.,"To evaluate the quality of uncertainty estimates obtained by our model, we use the Patch Accuracy vs Patch Uncertainty (PAvPU) metric introduced in (Mukhoti & Gal, 2018).",impact-revealing,reporting the evaluation metric used for model quality assessment
3241,5c7572b7f56def97988385ce,c342c71cb23199f112d0bc644fcce56a7306bf94,active learning for convolutional neural networks: a core-set approach,5736960c6e3b12023e51ec0c,Semi-Supervised Learning with Ladder Networks,"For weakly-supervised learning, we used Ladder networks (Rasmus et al., 2015) and for all experiments we used VGG-16 (Simonyan & Zisserman, 2014) as the CNN architecture.###One of the early weakly-supervised convolutional neural network algorithms was Ladder networks (Rasmus et al., 2015).",other,reporting prior findings on weakly-supervised learning methods
992,,c5da5370016de29611a455b14d20c496b548471a,Transactive memory directories in small work units,,,"###These questions, designed to measure organizational knowledge creation process, are validated by  Nonaka  et al.  (1994)",impact-revealing,reporting validation of measurement questions by prior work
1716,,8cbe92394f9ac984a90be04f9c43bd192b6cc972,Sticking fewer (or more) pins into a doll? The role of self-compassion in the relations between interpersonal goals and aggression,,,"###In this study, we investigated whether and how two types of interpersonal goals—compassionate goals and self-image goals (Crocker & Canevello, 2008)—would be related to aggressive inclinations toward others.###People who pursue self-image goals want to appeal to their competence and avoid showing their weaknesses to others (Crocker & Canevello, 2008; Niiya & Crocker, 2019) and believe that, when one wins, the other loses (Crocker et al., 2017).###When people are motivated by selfimage goals, their primary goal in relationships is to meet their own needs, such as the validation of the positive self and a positive evaluation by others, at the expense of others (Crocker & Canevello, 2008, 2015; Crocker et al., 2017).###Empirical research shows that people who pursue compassionate goals generally create good relationships with others (Canevello & Crocker, 2010; Crocker & Canevello, 2008, 2015; Crocker et al., 2010).###People with high compassionate goals want to be supportive of others (Crocker & Canevello, 2008; Niiya & Crocker, 2019) and hold nonzero-sum beliefs that people’s needs and well-being are mutually connected (Crocker et al., 2017).###As such, people with compassionate goals tend to be responsive toward others’ needs (Crocker et al., 2017), receive more support from others (Crocker & Canevello, 2008), and report fewer degrees of distress (Crocker et al., 2010).###In line with previous research that focused on various types of relationships (Crocker & Canevello, 2008; Crocker et al., 2010; Niiya et al., 2021), we found that interpersonal goals were related to aggressive inclinations regardless of the relationship with a person who displayed either rejection…###According to the egosystem-ecosystem theory of social motivation (Crocker & Canevello, 2008, 2015), people are assumed to have two types of social motivation that consequently affect their behavior, thoughts, emotions, and well-being in relationships.###Although we investigated our mediation pathways based on the theoretical models of interpersonal goals (Crocker & Canevello, 2008, 2015) and self-compassion (Neff, 2011), further work should directly test the causal or temporal influences of interpersonal goals on aggression using an experimental…###The egosystem-ecosystem theory suggests that people with egosystem motivation consider relationships as a means to meet their selfish needs and seek to maximize their benefits and minimize their costs in relationships (Crocker & Canevello, 2008, 2015).###…University, 2-1-15 Nishiai, Ibaraki-shi, Osaka 567-8502, Japan
2 Department of Psychology, Tezukayama University, Nara-shi, Nara, Japan
1 3
motivation consider relationships as a mutually connected system with others, and care for others as much as for the self (Crocker & Canevello, 2008, 2015).###Unfortunately, despite their best efforts to attain a positive self-view, research shows that people with high self-image goals often feel alone (Crocker & Canevello, 2008), perceive that other people are not responsive to them (Canevello & Crocker, 2010), and experience distress (Crocker et al.,…###The egosystem motivation is reflected and measured by self-image goals to appeal to their competence and avoid showing their weaknesses to others (Crocker & Canevello, 2008, 2015).###The egosystem–ecosystem theory of social motivation suggests that people with self-image goals want to show their competence to others without revealing their weaknesses (Crocker & Canevello, 2008; Niiya & Crocker, 2019).###The ecosystem motivation is reflected and measured by compassionate goals to genuinely support and care for others (Crocker & Canevello, 2008; Niiya & Crocker, 2019).###The egosystem–ecosystem theory has suggested that nonzero-sum beliefs account for why compassionate goals relate to harmonious relationships (Canevello & Crocker, 2015; Crocker & Canevello, 2008, 2015; Niiya et al., 2013).",impact-revealing,highlighting the relationship between interpersonal goals and aggressive inclinations
1172,,ab1b6559930625840222d9d8f5ad48256d3cabbb,Pyramid: A large-scale array-oriented active storage system,,,"###Moreover, SciDB does not address chunking layout management which may be a problem in exa-scale.###Similarly to SciDB, ArrayStore partitions large arrays into chunks and stores them in a distributed fashion.###SciDB departs from the relational database model to offer an array-oriented database model.###In future work, we plan to explore the possibility of using Pyramid as a storage backend for SciDB [3] and HDF5 [1].###SciDB [3, 8] is a recent effort to design a science-oriented database system from scratch.###Overview of SciDB: large scale array storage, processing and analysis.###Requirements for Science Data Bases and SciDB.",impact-revealing,acknowledge limitations and future directions for SciDB
3605,5d9edc1647c8f76646032985,10a4db59e81d26b2e0e896d3186ef81b4458b93f,Named Entity Recognition with Bidirectional LSTM-CNNs,5736954f6e3b12023e47acab,"A Joint Model for Entity Analysis: Coreference, Typing, and Linking.","Finally, the performance characteristics of our model appears to be quite different than the previous CRF models (Finkel and Manning, 2009; Durrett and Klein, 2014), likely because we apply a completely different machine learning method.###Luo et al. (2015) achieved state of the art results on CoNLL2003 by training a joint model over the NER and entity linking tasks, the pair of tasks whose interdependencies contributed the most to the work of Durrett and Klein (2014).###18OntoNotes results taken from (Durrett and Klein, 2014) 19Evaluation on OntoNotes 5.0 done by Pradhan et al. (2013) 20Not directly comparable as they evaluated on an earlier ver-
Then, letting [y]T1 be the true tag sequence, the sentence-level log-likelihood is obtained by normalizing the above…###Durrett and Klein (2014) combined coreference resolution, entity linking, and NER into a single CRF model and added cross-task interaction factors.###Almost all state of the art NER systems make use of lexicons as a form of external knowledge (Ratinov and Roth, 2009; Passos et al., 2014; Durrett and Klein, 2014; Luo et al., 2015).###Following Durrett and Klein (2014), we applied our model to the portion of the dataset with gold-standard named entity annotations; the New Testaments portion was excluded for lacking gold-standard annotations.###OntoNotes results taken from (Durrett and Klein, 2014) (12)It was unclear whether or not they evaluated their system on the CoNLL-2012 split of the OntoNotes dataset.",other,highlighting differences in model performance and methodology
2743,5e2d653a3a55acc8374367fd,36ff7927f9049d37b314c2a114769b517b3f5f7a,Joint Recognition of Names and Publications in Academic Homepages,55323c5545cec66b6f9dbd43,"Towards building a scholarly big data platform: Challenges, lessons and opportunities","There have been extensive research interests in the extraction and mining of such information from academic homepages [3, 8, 16, 20, 28, 31].",other,highlighting research interest in information extraction from academic homepages
3315,5c8bd2e54895d9cbc6af9826,d14afc470cd90521147130e153c0d3e1324cd104,Learning Language Representations for Typology Prediction,53e9b2c6b7602d9703d6e793,Reconstructing Native Language Typology from Foreign Language Usage.,"…and tools for contrastive linguistic analysis, work in inferring typology from bilingual data ( ¨Ostling, 2015) and English as Second Language texts (Berzak et al., 2014), as well as work in NLP (Shi et al., 2016; Kuncoro et al., 2017; Belinkov et al., 2017) showing that syntactic knowledge can…###strating strong links between translation studies and tools for contrastive linguistic analysis, work in inferring typology from bilingual data (Östling, 2015) and English as Second Language texts (Berzak et al., 2014), as well as work in NLP (Shi",other,acknowledge existing research in contrastive linguistic analysis and its connections to translation studies
3018,5a73cb4d17c44a0b3035672d,7cd5d7f8295b219b029a4231ae5cffb261e00ebe,Early Active Learning with Pairwise Constraint for Person Re-identification,573697846e3b12023e66aa64,Sparse Re-Id: Block Sparsity For Person Re-Identification,"The primary target of person re-identification (re-id) is to identify a person from camera shots across pairs of non-overlapping camera views, and research on this topic has attracted considerable attention in recent years [8,9,10,15,29].",other,highlighting the growing interest in person re-identification research
157,5cf48a3cda56291d5829f0d4,81664382e5db10bc6598db0b8814a8e765d30576,Sliced Score Matching: A Scalable Approach to Density and Score Estimation,53e9b30fb7602d9703dd438c,Estimation of Non-Normalized Statistical Models by Score Matching,"Then, applying multivariate integration by parts ( cf ., Lemma 4 in Hyvärinen (2005)), we obtain where s m,j ( x ; θ ) denotes the j -th component of s m ( x ; θ ) .###To avoid this, score matching (Hyvärinen, 2005) minimizes the Fisher divergence between p d and p m ( · , θ ) , which is deﬁned as Since s m ( x ; θ ) does not involve Z θ , the Fisher divergence does not depend on the intractable partition function.###We also adopt the assumption in Hyvärinen (2005) that all densities are strictly positive (Assumption 5).###By applying integration by parts, Hyvärinen (2005) shows that L ( θ ) can be written as L ( θ ) = J ( θ ) + C ( cf ., Theorem 1 in Hyvärinen (2005)), where C is a constant that does not depend on θ , tr( · ) denotes the trace of a matrix, and is the Hessian of the log-density function.###Score matching (Hyvärinen, 2005) is particularly suitable for learning unnormalized statistical models, such as energy based ones.###In Hyvärinen (2005), the authors only showed that J ( θ ) = 0 ⇔ θ = θ ∗ , which leads to “local consistency” of score matching.###The basic idea of this proof is similar to that of Theorem 1 in Hyvärinen (2005).###Other than our requirements on p v , the assumptions are exactly the same as in Theorem 1 of Hyvärinen (2005).###To avoid this, score matching (Hyvärinen, 2005) minimizes the Fisher divergence between pd and pm(·,θ), which is defined as",impact-revealing,providing context for score matching in statistical models
3693,5d9b0cb93a55acb0384964ef,bcfdbb6b8911272139170ef4b24e31d9145093e7,Deep Anomaly Detection on Attributed Networks.,573695ff6e3b12023e513303,Scalable Anomaly Ranking of Attributed Neighborhoods,"• AMEN [24] uses both attribute and network structure information to detect anomalous neighborhoods.###AMEN [24] considers the ego-network information for each node and discovers anomalous neighborhoods on attributed networks.###Among them, one family of methods study the problem at the mesoscope with ego-network [24] or community analysis [10] and then identify anomalies by measuring the abnormality of ego-networks or comparing the current node with other nodes within the same community.",other,describing the approach of AMEN for detecting anomalies in networks
109,5f7fdd328de39f08283980ba,2fce1ef37391cd685fc5459e1cbfcb8490b85242,Graph information bottleneck,5736971f6e3b12023e612bee,Deep learning and the information bottleneck principle,"In particular, the Information Bottleneck (IB) [18, 19] provides a critical principle for representation learning: an optimal representation should contain the minimal sufficient information for the downstream prediction task.",impact-revealing,highlighting the significance of the Information Bottleneck principle in representation learning
1656,,5add3661aab04b02bffdbcc491d65b0a12cb84de,Reaching out across lines: Determinants of positive intergroup interaction in conflict-affected communities in Myanmar,,,"###Tajifel and Turner explain that external factors, such as scarce resources, are not necessary for in-group/out-group competition and conflict (Tajfel & Turner, 1979).###…intergroup conflict of interests, it is difficult for an individual to participate in interactions for fear of being seen as betraying their group (Tajfel &
Turner, 1979) or depending on social norms, majority group members may have fears of ostracization when interacting with minority or…###Deprived or disadvantaged groups may have a positive inclination toward the dominant group, believing that their position within society is warranted or inevitable within society (Tajfel & Turner, 1979).###The former, is motivated by rewards and is based on competition over resources, which Tajifel explains, enhances intra-group cohesiveness and cooperation (Tajfel & Turner, 1979).###Social Identity Theory is an extension of this, explaining that the mere categorization of populations into groups will lead them to prefer their members, and ultimately an “us vs. them” mentality (Tajfel & Turner, 1979).",impact-revealing,highlighting the significance of Tajfel and Turner's work on social identity theory and its implications for group dynamics
3771,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,5a260c8117c44a4ba8a30adf,Representation Learning on Graphs: Methods and Applications,"For a complete introductions to the vast topic we refer interested readers to detailed surveys [6, 8, 25].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3742,5a4aef9e17c44a2190f7a8e4,faa98e73eeee551c40923c896817ab640925ce20,Deep Image Prior,599e95f49c05cae4992b4eee,Globally and locally consistent image completion,"We compare to a learning-based method of [15] in fig.###GAN-based [10] methods SRGAN [18] and EnhanceNet [27] (not shown in the comparison) intelligently hallucinate fine details of the image, which is impossible with our method that uses absolutely no information about the world of HR images.###(a) Corrupted image (b) Global-Local GAN [15] (c) Ours, LR = 0.###Despite using no learning, the results may be comparable to [15] which does.",other,highlighting the limitations of the proposed method compared to learning-based methods
689,53e9afd3b7602d9703a26d76,a17a0793186e1efaed8604783606a17484bb161f,Coverage directed test generation for functional verification using Bayesian networks,558ac597e4b031bae1f9949d,Cost evaluation of coverage directed test generation for the IBM mainframe,"In [11], the coverage analysis results trigger a set of generation rules that modify the testing directives.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
27,58d82fc8d649053542fd5862,2a94c84383ee3de5e6211d43d16e7de387f68878,Feature Pyramid Networks For Object Detection,573696056e3b12023e5186fe,Ssd: Single Shot Multibox Detector,"The Single Shot Detector (SSD) [22] is one of the ﬁrst attempts at using a ConvNet’s pyramidal feature hierarchy as if it were a featurized image pyramid (Fig.###Moreover, our method does not exploit many popular improvements, such as iterative regression [9], hard negative mining [35], context modeling [16], stronger data augmentation [22], etc .###SSD [22] and MS-CNN [3] predict objects at multiple layers of the feature hierarchy without combining features or scores.",impact-revealing,acknowledge limitations and context of the Single Shot Detector method
2318,5ea2b8c391e01167f5a89e2d,38643c2926b10f6f74f122a7037e2cd20d77c0f1,Supervised Contrastive Learning,5dcd263a3a55ac58039516c5,Momentum Contrast for Unsupervised Visual Representation Learning,"We also experimented with memory based alternatives [15].###[15] introduces the approximation of only back-propagating through part of the loss, and also the approximation of using stale representations in the form of a memory bank.###This property is important for representation learning via self-supervised contrastive learning, with many papers showing increased performance with increasing number of negatives [18, 15, 48, 3].###In recent years, a resurgence of work in contrastive learning has led to major advances in self-supervised representation learning [54, 18, 38, 48, 22, 3, 15].",other,highlighting advancements in self-supervised representation learning through contrastive learning
1602,,0dd3af69223ac33f6701ef4741ee8496ca463949,URET: Universal Robustness Evaluation Toolkit (for Evasion),,,"###To fill the need for a general adversarial evaluation toolkit, Counterfit builds upon three existing libraries, the Adversarial Robustness Toolbox (ART) [35], TextAttack [34] and AugLy [36], and exposes a command line interface to run adversarial attacks and evaluations from these frameworks.###Although several libraries exist for deploying machine learning attack, most of these libraries are designed for image and text inputs [21,24, 34,35,37].###We recognize that much prior work exists with respect to generating adversarial text inputs, namely Counterfit [5] and TextAttack [34].",impact-revealing,highlighting the development of a general adversarial evaluation toolkit and its relation to existing libraries
1546,,d414ef9ab615ec11ed6aaa780c2287b1d3508a2a,DA Based Systematic Approach Using Speculative Addition for High Speed DSP Applications,,,"###The speculative techniques [3-4] are widely used as countermeasure measures, which can be used to reduce sub-logarithmic delays by reducing critical path of the active input operands.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3814,5a9cb66717c44a376ffb87ea,fe9b8aac9fa3bfd9724db5a881a578e471e612d7,efficient neural architecture search via parameter sharing,5a260c8617c44a4ba8a322d4,Dynamic Evaluation of Neural Sequence Models.,"Since the goal of our work is to discover cell architectures, we only employ the standard training and test process on Penn Treebank, and do not utilize post-training techniques such as neural cache (Grave et al., 2017) and dynamic evaluation (Krause et al., 2017).",other,highlighting the specific focus of the work on discovering cell architectures
1120,,596e887c01b07785a34d5dcd8aa15c91fa6cda76,Diffusion-Based Environment-Aware Trajectory Prediction,,,"###To address the limitations of the DDPM, the DDIM model was proposed [66].###Diffusion models [23,32,64,66], a type of generative model, have seen a surge in popularity in recent years.###These models have made a significant impact in the field of image synthesis [23,56,59,66]; and more recently, been innovatively applied in other domains, including molecule generation [25], temporal data modeling [2,57], traffic scenario generation [54], and more [79].###The sampling procedure can be conducted using various transition rules, e.g ., DDPM [23], DDIM [66], or EDM [32].###The formulation of diffusion models can be done differently depending on how the forward and reverse processes are modeled: Either through a Markovian chain [23], a non-Markovian process [66], a stochastic differential equation (SDE) [67], or as a (deterministic) probability flow ODE [67].###The underlying principle of diffusion models is to progressively perturb the observed data with a forward (diffusion) process, then recover the original data through a reverse process [23,32,64,66].###Instead of predicting ϵ t as formulated in [23,66], the model learns to predict the clean signal, i.e ., ˆ x 0 = M θ ( x t , t, c ) using the simple objective [23], This adaptation is inspired by the arguments made in [68], where the addition of geometric losses is used to enforce physical…",impact-revealing,highlighting the significant impact and applications of diffusion models in various domains
199,5d0b00ed8607575390fedd4e,493d5f344eea1468260946b29a80dc81b2be409c,HopSkipJumpAttack: A Query-Efficient Decision-Based Attack,5c8cc9ab4895d9cbc6216df6,Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models.,"A version normalized by image dimension was employed by Brendel et al. [14] for evaluating Boundary Attack.###A. Efﬁciency evaluation a) Baselines: We compare HopSkipJumpAttack with three state-of-the-art decision-based attacks: Boundary Attack [14], Limited Attack [9] and Opt Attack [16].###Most related to our work is the Boundary Attack method introduced by Brendel et al. [14].###In recent work, Brendel et al. [14] proposed Boundary Attack, which generates adversarial examples via rejection sampling.",impact-revealing,acknowledge related methods and comparisons
3495,58d82fcbd649053542fd5d36,81db3f78f346eecf2f378070712feade6d45d6b1,MOLIERE: Automatic Biomedical Hypothesis Generation System,55a655e565ce054aad64bf28,Stem cell-based therapies for ischemic stroke.,"For example, imagine that a medical doctor believed that stem cells could be used to repair the damaged neural pathways of stroke victims (as some did in 2014 [22]).",other,providing an example related to medical beliefs about stem cells
2835,558b3e9384ae84d265c24c24,fed9ac8dc6ea64141e5526894a146e476b8fea52,SCD: A scalable coherence directory with flexible sharer set encoding,53e9b6d6b7602d97042617b9,Piranha: A Scalable Architecture Based On Single-Chip Multiprocessing,", tracking 1024 16-way caches would require 16384 ways), so they are limited to small-scale systems [2, 29].###Traditional schemes scale poorly with core count: Duplicate-tag directories [2, 29] maintain a copy of all tags in the tracked caches.###Invalidation delays have a small performance impact in our simulations (Section 6), but should they become an issue, they can be reduced by processing invalidations in a hierarchical fashion, using multicast networks, or cruise-missile invalidates [2].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1083,,d442da81ee10a3872aac513be648afee1fe2fab5,Recent trends in human computer interface to analysis the cognitive skill of students based on user interface,,,###GOMS is a hierarchical model [3] for task analysis and is widely used for analyzing human Computer Interaction process.,impact-revealing,describing a widely used model for task analysis in HCI
493,5e09a801df1a9c0c41680233,1438d8c68b4495947fd5de001b87e6ef5a263a3e,Learning to Jointly Generate and Separate Reflections,58d83028d649053542fe5030,Depth Of Field Guided Reflection Removal,"Here, α and β are the mixing coefficients [5, 27, 26].###The proposed method is compared with seven state-ofthe-art single image reflection removal methods, including Wan18 [24], Zhang18 [29], CycleGAN [30], FY17 [5], NR17 [1], WS16 [26], and LB14 [17].###Instead of the one-toone mapping in previous methods, our generator learn the mapping as G : (B,R) → M, where the non-linear mappings can produces more realistic reflection appearances (first to third columns in Figure 4 ) than previous linear functions [5, 26, 17, 1] with fixed coefficients.###In this scenario, image priors such as different blur levels between the background and reflection [26, 17], ghosting effects [22], and the non-local similarity in the images [25], have been explored.###77 WS16 [26] 0.###Moreover, we introduce the gradient constraints [5, 26] to make the model learning more effective, in which the edge map estimation is elegantly dealt with as an auxiliary task via a multi-task learning structure.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
103,5f92ba1691e011edb3573ba0,268d347e8a55b5eb82fb5e7d2f800e33c75ab18a,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,599c7987601a182cd2648373,Attention Is All You Need.,"Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become the model of choice in natural language processing (NLP).###In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.###Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks.###The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-attention (MSA, see Appendix A) and MLP blocks (Eq.",impact-revealing,highlighting the significance and widespread adoption of Transformers in NLP
1634,,987e9dc131521b526f43e020de64d7bb5f084102,Genetic Characterization and Evaluation of Antimicrobial Resistance Patterns of Salmonella Typhi Isolates,,,"###, 2007) and the sequencing of 16S rRNA is a well-established method for identification of bacterial species, including Salmonella (Woo et al., 2008).###Genome sequences have been widely used for the development of new diagnostic tests for the known as well as emerging pathogens (Millar et al., 2007) and the sequencing of 16S rRNA is a well-established method for identification of bacterial species, including Salmonella (Woo et al., 2008).",impact-revealing,acknowledge established methods in bacterial identification
500,57d063e8ac443673542950ad,0509facb30efa81c4730f98bfc28cdbd6178822e,"Image sentiment analysis using latent correlations among visual, textual, and sentiment views",558c06c9e4b02b9f07a4a567,Canonical Correlation Analysis: An Overview with Application to Learning Methods,"This subsection describes how to ﬁnd latent correlations among multiple views using a framework of the generalization of canonical correlation analysis [11].###The canonical correlation problem can be transformed into a distance problem such that the distances in the resulting space between each pair of views for the same image are minimized [11].###Then, using a framework of multi-view canonical correlation analysis (CCA) [11], we calculate a latent embedding space in which correlations among the three views are maximized.###Several nonlinear extensions such as kernel CCA [11] and Deep CCA [22] have been proposed to reveal nonlinear relationship between the variables.###In the conventional kernel CCA [11], kernel trick is used in Eq.",impact-revealing,describing a method for finding latent correlations among multiple views
1440,,81d749bdedca92f78ed8007f60343a0305e59935,"A Randomized Controlled Trial Comparing Learners' Decision-making, Anxiety, and Task Load During a Simulated Airway Crisis Using Two Difficult Airway Aids",,,"###Nonetheless, the literature also demonstrates that clinicians often do not use cognitive aids even when available.(28,30,31) Two of these three studies tested the use of two different cognitive aids on performance in simulated environments.###When used, the cognitive aids improved management decisions; however, in both of these studies, nearly 20% of the participants did not refer to the cognitive aid at all even after knowing that it existed.(28,30) The still high percentage of practitioners who do not refer to cognitive aids during stressful situations emphasizes the need for cognitive aids that are easy to learn and use, even if theymay not be used as they are intended.",impact-revealing,highlighting the need for more user-friendly cognitive aids in clinical settings
2241,5f803c8f91e01119a5df749b,39ca8f8ff28cc640e3b41a6bd7814ab85c586504,deformable detr: deformable transformers for end-to-end object detection,5aed14d617c44a443815932c,Path Aggregation Network for Instance Segmentation,"Most works (Liu et al., 2018a; Parmar et al., 2018; Child et al., 2019; Huang et al., 2019; Ho et al., 2019; Hu et al., 2019; Parmar et al., 2019; Qiu et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020) follow this paradigm.###PANet (Liu et al., 2018b) further adds an bottom-up path on the top of FPN. Kong et al. (2018) combines features from all scales by a global attention operation.",other,acknowledge existing methodologies in the field
1747,,b7787bf9b9fb8efcfab1ab0b1409cc4e050bddb8,High frequency oscillatory ventilation attenuates the activation of alveolar macrophages and neutrophils in lung injury,,,"###This procedure provides a surfactant depletion model which is widely recognized as a suitable model for experimental ARDS [34].###After initial stabilization, the rabbits' lungs were lavaged four times to remove lung surfactant, using a previously described method [34] with minor modifications (first lavage).",impact-revealing,describing a widely recognized model for experimental ARDS
2707,5bbacb9e17c44aecc4eaff64,5b1516c87818084dc5d195cc274e1ee8923210d2,Neural Cross-Lingual Named Entity Recognition with Minimal Resources,5d9edc1647c8f76646032985,Named Entity Recognition with Bidirectional LSTM-CNNs,"Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018).",other,highlighting advancements in named entity recognition through neural architectures
953,,1f154faa27582cf84c9cafea233784530e57222c,Psychological need satisfaction in physical activity : Implications for well-being and physical activity behaviour,,,"###Realizing a possible self may therefore create a positive emotional state and a desire to maintain this state (Cross & Markus, 1994).###As a consequence, selfschemas may serve as a foundation for the development of future oriented selves, or possible selves (Cross & Markus, 1994).###Self-schemas are characterized by domain specific attributes or abilities and experiences in that domain (Cross & Markus, 1994).###Beliefs about competence should influence self-schemas because an individual who experiences incompetence in a particular domain is unlikely to continue engagement in that behaviour, thereby reducing the likelihood that they will become schematic in that domain (Cross & Markus, 1994).###Self-schemas and possible
selves are thought to require a sense of self-efficacy (Markus & Nurius,1986) or competence (Cross & Markus, 1994).###Articulating a possible self may bring about a positive affective state that is energizing
and promotes action (Cross & Markus, 1994).",impact-revealing,highlighting the role of self-schemas in emotional states and future selves
1208,,bba67c012751714489a582c628bf0a57ac3e62a5,Computer-aided assessment of breast density: comparison of supervised deep learning and feature-based statistical learning,,,"###In this study, we chose our network structure based on previous studies (Pinto et al 2009, Cox and Pinto 2011).###Our network structure was inspired by HT-L3 network (Pinto et al 2009).",impact-revealing,reporting inspiration from previous studies for network structure
325,5db9295f47c8f766461f5135,2c1006c856fefdbd6cd710e840e57153f2d6cd04,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,58437722ac44360f1082f0b6,A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples.,"Natural images live on a low-dimensional manifold, with the training and testing images as samples from it [26, 51, 44, 56].###When the decision boundary lies close to the manifold for its out of manifold part, adversarial perturbations lead to a tilting effect on the data manifold [56]; at places where the classification boundary is far from the manifold for its out of manifold part, the adversarial perturbations will move the points towards the decision boundary, effectively shrinking the data manifold.###However, the classification boundary that extends beyond the manifold is less constrained, contributing to the existence of adversarial examples [56, 59].",impact-revealing,highlighting the relationship between adversarial perturbations and decision boundaries in image classification
2771,5a4aef9e17c44a2190f7a8e4,faa98e73eeee551c40923c896817ab640925ce20,Deep Image Prior,573697846e3b12023e66a8a1,Learning To Generate Chairs With Convolutional Neural Networks,"6 were the following: z ∈ R3×W×H ∼ U(0, 1 10 ) nu = nd = [8, 16, 32, 64, 128] ku = kd = [3, 3, 3, 3, 3] ns = [0, 0, 0, 4, 4] ks = [NA, NA, NA, 1, 1] σp = 1 30 num iter = 2400###Popular approaches for image generation such as generative adversarial networks [20], variational autoencoders [32] and direct pixel-wise error minimization [16,5] also use ConvNets.###z ∈ R32×W×H ∼ U(0, 1 10 ) nu = nd = [16, 32, 64, 128, 128, 128] kd = [3, 3, 3, 3, 3, 3] ku = [5, 5, 5, 5, 5, 5] ns = [0, 0, 0, 0, 0, 0] ks = [NA, NA, NA, NA, NA, NA] σp = 0 num iter = 5000",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2584,5f7fdd328de39f0828397e7f,9fa283d4f9c2ed991383c0434ef6043bee0dc8e2,Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification,573695fe6e3b12023e511ba3,Stacked Attention Networks for Image Question Answering,"For example, in image captaining and visual question answering, models are trained to concentrate on the related regions of the image when generating the word sequence [63, 53, 51, 27, 65].",other,highlighting the application of models in image captioning and visual question answering
1013,,faee7ce0dd362294915ab08d3c925fdb4c14cf3e,The reward value of sucrose in leptin-deficient obese mice,,,"###A choice behavior denotes the relative value of each option, as this optogenetic assay builds on neuroeconomic analyses of behavior, which can be generalized from humans to any species capable of making behavioral decisions [20–23].",impact-revealing,providing context on the optogenetic assay and its relation to neuroeconomic analyses
2812,5ebbc75d91e0119bc4e43623,407f1d16ba4eb3cb4851429cae46c97d723a35a5,invertible image rescaling,53e9a812b7602d9703158137,Facial feature detection using Haar classifiers,"To achieve this, we apply the Haar Transformation as the first layer in each downscaling module, which can explicitly decompose the input images into an approximate low-pass representation, and three directions of high-frequency coefficients [53][35][4].",other,providing context for the method used in image processing
3482,5a260c8117c44a4ba8a30a57,908272f8e6340971600148d4e73f50e1e8843aaf,"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec",5736960c6e3b12023e51f650,The Spacey Random Walk: A Stochastic Process for Higher-Order Data,"˝ In our analysis of node2vec, we assume the first two vertices of the 2nd-order random walks are sampled from the stationary distribution X . existence of such X is guaranteed by Perron-Frobenius theorem [3].###We have noticed some recent works [2, 3, 14] that try to understand or approximate 2nd-order random walk by assuming a rank-one factorization X i , j = x i x j for stationary distribution X .",other,acknowledging recent works on 2nd-order random walks
2572,5fdc8e9d91e01104c91811a8,849b88ddc8f8cabc6d4246479b275a1ee65d0647,A Generalization of Transformer Networks to Graphs,5cede0edda562983788cba9b,Fake News Detection on Social Media using Geometric Deep Learning.,"…network architectures on graph datasets and have achieved signiﬁcant success on a wide range of applications, such as in knowledge graphs (Schlichtkrull et al. 2018; Chami et al. 2020), in social sciences (Monti et al. 2019), in physics (Cranmer et al. 2019; Sanchez-Gonzalez et al. 2020), etc.",other,reporting applications of network architectures on various datasets
2515,5736974d6e3b12023e6388bf,1d7e580740a3f96f1529e3d771915af4e996a259,Verbal and Nonverbal Clues for Real-life Deception Detection,55503ef945ce0a409eb2b6a5,Everyday deception or a few prolific liars? The prevalence of lies in text messaging.,"As deceptive behavior occurs on a daily basis in different areas of life (Meyer, 2010; Smith et al., 2014), the need arises for automated methodologies to detect deception in an efficient, yet reliable manner.",other,highlighting the necessity for automated deception detection methods
680,599c797a601a182cd2641eda,d65ce2b8300541414bfe51d03906fca72e93523c,on calibration of modern neural network,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"We train state-of-the-art convolutional networks: ResNets (He et al., 2016), ResNets with stochastic depth (SD) (Huang et al., 2016), Wide ResNets (Zagoruyko & Komodakis, 2016), and DenseNets (Huang et al., 2017).###Recent research suggests that these normalization techniques have enabled the development of very deep architectures, such as ResNets (He et al., 2016) and DenseNets (Huang et al.###This is visualized in Figure 1, which compares a 5-layer LeNet (left) (LeCun et al., 1998) with a 110-layer ResNet (right) (He et al., 2016) on the CIFAR-100 dataset.###, 1998) with a 110-layer ResNet (right) (He et al., 2016) on the CIFAR-100 dataset.###Recent advances in deep learning have dramatically improved neural network accuracy (Simonyan & Zisserman, 2015; Srivastava et al., 2015; He et al., 2016; Huang et al., 2016; 2017).###On the other hand, the ResNet’s accuracy is better, but does not match its confidence.###Figure 4 contains reliability diagrams for 110-layer ResNets on CIFAR-100 before and after calibration.###It is now common to see networks with hundreds, if not thousands of layers (He et al., 2016; Huang et al., 2016) and hundreds of convolutional filters per layer (Zagoruyko & Komodakis, 2016).###We train state-of-the-art convolutional networks: ResNets (He et al., 2016), ResNets with stochastic depth (SD) (Huang et al.###The top performing ImageNet models of 2015 all use an order of magnitude less weight decay than models of previous years (He et al., 2016; Simonyan & Zisserman, 2015).###Recent research suggests that these normalization techniques have enabled the development of very deep architectures, such as ResNets (He et al., 2016) and DenseNets (Huang et al., 2017).",impact-revealing,acknowledging advancements in deep learning architectures and their impact on neural network accuracy
1491,,3c2c969b4bd054fc32f26890eb38fabb99153734,Conditional score-based diffusion models for solving inverse problems in mechanics,,,"###In contrast, we use the score network to directly approximate the score function, similar to [33].###Compared to diffusion models, other generative models suffer from various limitations, such as mode collapse in GANs, poor synthesis quality in VAEs, and the need for specialized architecture with large memory footprints in NFs [33].###Moreover, a single neural network can approximate the posterior distribution for different realizations of the covariate Y , which only requires adding a channel to the input of the neural network [33, 34, 35].###Whereas Saharia et al. [33] considers the problem of image super-resolution, we are motivated by inverse problems arising in solid mechanics.",impact-revealing,highlighting the differences and limitations of various generative models
137,5f02f25491e011ee5e0258e0,7d4dcee4628745f79f5e352859172817fece3e83,Adaptive Graph Encoder for Attributed Graph Embedding,5de0e035df1a9c0c415c6700,Symmetric Graph Convolutional Autoencoder For Unsupervised Graph Representation Learning,"To build a symmetric graph autoencoder, [24] proposes Laplacian sharpening as the counterpart of Laplacian smoothing in the encoder.###GALA [24] proposes a symmetric graph convolutional autoencoder recovering the feature matrix.###Thirdly, we also argue that training objectives of these algorithms (either reconstructing the adjacency matrix [23, 31] or feature matrix [24, 32]) are not compatible with real-world applications.",impact-revealing,highlighting the contributions and limitations of existing graph autoencoder methods
3702,5eede0b091e0116a23aafb82,1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af,Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,573696f46e3b12023e5f176c,Learning to See by Moving,"Many self-supervised methods manipulate the input data to extract a supervised signal in the form of a pretext task [1, 13, 26, 28, 30, 36, 38, 41, 42, 47, 48, 57].",other,describing self-supervised methods in data manipulation
187,5736977f6e3b12023e666356,a188bc7c5b74cf4fa1d719e3955f9a9b81e0a146,Study on the Relationship between Profile Images and User Behaviors on Twitter,53e9ab25b7602d97034b12e2,Cultural differences on visual self-presentation through social networking site profile images,"On the other hand, several user profiling studies based on profile images have been conducted for Facebook [5, 7, 13].###They analyzed the relationship between internal properties such as self-construction [5], narcissism [7], self-presentation [13] and the corresponding user profiles on Facebook.",impact-revealing,acknowledge existing user profiling studies on Facebook
3204,5a73cb4d17c44a0b3035672d,7cd5d7f8295b219b029a4231ae5cffb261e00ebe,Early Active Learning with Pairwise Constraint for Person Re-identification,56d8494fdabfae2eeec7aed9,Bayesian Active Learning with Evidence-Based Instance Selection,"They include uncertainty sampling methods [11,6,1,22] query by committee methods [21,3].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
547,5d84a3433a55acc20782ce9e,554d300f00fc14c2e4f48a740019496137d060c1,self-training for end-to-end speech recognition,5550411645ce0a409eb38730,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.,"Our sequence-to-sequence model is an encoder-decoder architecture with attention [12, 13].",impact-revealing,describing the architecture of the sequence-to-sequence model
3337,5ce937225ced2477cb328bd9,e4350bcfcadc5b4c0bfbc61ffd8bce05f0b8fe15,ABM-SpConv: A Novel Approach to FPGA-Based Acceleration of ConvolutionaI NeuraI Network Inference,58d82fc8d649053542fd5964,Designing Energy-Efficient Convolutional Neural Networks using   Energy-Aware Pruning,"One of the key challenges in designing FPGA-based CNN accelerator is to take full advantage of the on-chip computing resource to speedupmultiply-and-accumulate (MAC) operations in the convolution (CONV) and fully-connected (FC) layers, which account for over 99% [9] of the total operations for most CNNs.",other,highlighting a key challenge in designing FPGA-based CNN accelerators
894,5aed147c17c44a4438153ea5,665c0dde22c2f8598869d690d59c9b6d84b07c01,domino temporal data prefetcher,53e99f3bb7602d970280aaa9,Practical Off-Chip Meta-Data For Temporal Memory Streaming,"As both HT and IT require multi-megabyte storage for STMS to have a reasonable coverage, both tables are placed off chip in the main memory [10].###It benefits from a stream-end detection heuristic [10], [40] to reduce useless prefetches.###Figure 6 shows the timing of events with STMS [10].###As the size of these two tables is very large (several megabytes), just like prior work [10], both tables are stored in the main memory.###It has been shown that this implementation offers the level of performance similar to that of the non-practical always-update implementation [10].###The delay of the search is tolerable because it is considerably smaller than the off-chip latency [10].###, STMS [10]), frequently prefetch incorrectly.###STMS [10] records miss sequences in a global per-core HT and locates streams through an IT.###One of the promising prefetching techniques is temporal prefetching [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17].###Figure 1 shows the opportunity of temporal prefetching and what STMS [10] and ISB [13], two state-of-the-art temporal data prefetchers, offer for several big-data server applications.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
175,5d9ed2d847c8f76646f797b7,c3715947bbbf648dcf29a1aa4b35cfb68044f919,GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification,5c2348ceda562935fc1d578d,Recurrent Relational Networks.,"Inspired by recent work on semi-supervised graph learning and relational reasoning (Kipf and Welling, 2017; Velickovic et al., 2018; Palm et al., 2018), we propose an evidence reasoning network (ERNet) to propagate information among the evidence nodes.",impact-revealing,drawing inspiration from recent advancements in semi-supervised graph learning for proposing a new network
3593,5f0d85c69fced0a24be4f04c,a43ba805d13785378fecdb408a571ee50d0afb8e,auto-predication of critical branches,53e9b206b7602d9703c9f944,Skipper: a microarchitecture for exploiting control-flow independence,Skipper [41] proposed out-of-order fetch-and-execute of instructions post-control flow convergence to exploit control independence but required large area (about 6KB) for supporting its learning and application.,other,reporting prior findings on out-of-order instruction execution
1277,,e5f8dc016293a75ef1066a3d47603b7e517ba744,Parameter identifiability analysis of power system transient models based on profile likelihood,,,###The current work is inspired by a profile likelihood based identifiability analysis method presented in [22].###Numerous discussions have been made on the identifiability of biological model parameters [18]-[22].,impact-revealing,drawing inspiration from prior work on identifiability analysis
45,5b1643ba8fbcbf6e5a9bc5b5,8c1b00128e74f1cd92aede3959690615695d5101,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,58d82fc8d649053542fd5be4,Bidirectional Attention Flow for Machine Comprehension.,"Table 4: Speed comparison between our model and BiDAF (Seo et al., 2016) on SQuAD dataset.###We adopt the strategy of Seo et al. (2016) to predict the probability of each position in the context being the start or end of an answer span.###Most high performing models additionally use some form of query-to-context attention, such as BiDaF (Seo et al., 2016) and DCN (Xiong et al.###A great number of end-to-end neural network models have been proposed to tackle these challenges, including BiDAF (Seo et al., 2016), r-net (Wang et al.###A great number of end-to-end neural network models have been proposed to tackle these challenges, including BiDAF (Seo et al., 2016), r-net (Wang et al., 2017), DCN (Xiong et al., 2016), ReasoNet (Shen et al., 2017b), Document Reader (Chen et al., 2017), Interactive AoA Reader (Cui et al., 2017)…###As a simple comparison, our model can achieve the same accuracy (77.0 F1 score) as BiDAF model (Seo et al., 2016) within 3 hours training that otherwise should have taken 15 hours.###For instance, if we allow our model to train for 18 hours, it achieves an F1 score of 82.7 on the dev set, which is much better than (Seo et al., 2016), and is on par with best published results.###The similarity function used here is the trilinear function (Seo et al., 2016):###Most high performing models additionally use some form of query-to-context attention, such as BiDaF (Seo et al., 2016) and DCN (Xiong et al., 2016).###7 on the dev set, which is much better than (Seo et al., 2016), and is on par with best published results.###In addition, we also use the same hardware (a NVIDIA p100 GPU) and compare the training time of getting the same performance between our model and the BiDAF model13(Seo et al., 2016), a classic RNN-based model on SQuAD.###In addition, we also use the same hardware (a NVIDIA p100 GPU) and compare the training time of getting the same performance between our model and the BiDAF model 14 (Seo et al., 2016), a classic RNN-based model on SQuAD.###Then we learn the interactions between context and question by standard attentions (Xiong et al., 2016; Seo et al., 2016; Bahdanau et al., 2015).###Then the context-to-query attention is computed as The similarity function used here is the trilinear function (Seo et al., 2016): where (cid:12) is the element-wise multiplication and W 0 is a trainable variable.###A successful combination of these two ingredients is the Bidirectional Attention Flow (BiDAF) model by Seo et al. (2016), which achieve strong results on the SQuAD dataset (Rajpurkar et al., 2016).###Following Seo et al. (2016), we also adopt a two-layer highway network (Srivastava et al., 2015) on top of this representation.###Similar to Seo et al. (2016), the input of this layer at each position is [ c, a, c (cid:12) a, c (cid:12) b ] , where a and b are respectively a row of attention matrix A and B .###0 F1 score) as BiDAF model (Seo et al., 2016) within 3 hours training that otherwise should have taken 15 hours.###According to the observations from our experiments and previous works, such as (Seo et al., 2016; Xiong et al., 2016; Wang et al., 2017; Chen et al., 2017), the validation score is well correlated with the test score.###…/ 78.8 Dynamic Coattention Networks (Xiong et al., 2016) 66.2 / 75.9 66.2 / 75.9 FastQA (Weissenborn et al., 2017) 68.4 / 77.1 68.4 / 77.1 BiDAF (Seo et al., 2016) 68.0 / 77.3 68.0 / 77.3 SEDT (Liu et al., 2017a) 68.1 / 77.5 68.5 / 78.0 RaSoR (Lee et al., 2016) 70.8 / 78.7 69.6 / 77.7 FastQAExt…",impact-revealing,comparing performance and training efficiency of models on SQuAD dataset
2078,,09090647d531231015d0fb9bf912588f20f6db10,Adapting Stochastic Block Models to Power-Law Degree Distributions,,,"###Here, the use of degenerated distribution q(δi) is inspired by the Viterbi-type EM algorithm used for training Hidden Markov Models (HMM) [50], and is based on whether it should be concentrated somewhere in the positive half real line in the posterior sense.",impact-revealing,providing context for a method inspired by existing algorithms
1072,,a576eed0e48aa49d288f54a46b9ca1bf3647a21e,A Dual-Attention Hierarchical Recurrent Neural Network for Dialogue Act Classification,,,"###While perplexity has been widely used for model selection for LDA (Lin, 2011; He et al., 2012), we employ a topic coherence measure proposed by (Röder et al., 2015) to determine the optimal topic number for each dataset, which combines the indirect cosine measure with the normalised pointwise mutual information (Bouma, 2009, NPMI) and the Boolean sliding window.###…(Lin, 2011; He et al., 2012), we employ a topic coherence measure proposed by (Röder et al., 2015) to determine the optimal topic number for each dataset, which combines the indirect cosine measure with the normalised pointwise mutual information (Bouma, 2009, NPMI) and the Boolean sliding window.###, 2015] to determine the optimal topic number for each dataset, which combines the indirect cosine measure with the normalised pointwise mutual information (NPMI) [Bouma, 2009] and the boolean sliding window.",impact-revealing,describing the method for determining optimal topic number
2410,5fae6dced4150a363cec41f7,71c97e7d4a529a21f32d98aad72fbbf32c9ee32e,Anomaly-Based Intrusion Detection From Network Flow Features Using Variational Autoencoder,53e9a957b7602d97032ab80e,Introduction to Semi-Supervised Learning,SSL covers several different settings including [56]: a) Semi-supervised classiﬁcation: Its alternative name is classiﬁcation with labelled and unlabeled data (or partially labelled data).,other,providing context on semi-supervised learning settings
104,5eccb534e06a4c1b26a838ac,2709167f1c3a03fa5b970a665ea48ed243aab582,Designing Network Design Spaces,5e63725991e011ae97a69c43,On Network Design Spaces For Visual Recognition,"Our basic training settings follow [21] as discussed in §3.###The core insight from [21] is that we can sample models from a design space, giving rise to a model distribution, and turn to tools from classical statistics to analyze the design space.###[21].###As in [21], our primary tool for analyzing design space quality is the error empirical distribution function (EDF).###The overall process is analogous to manual de-sign, elevated to the population level and guided via distribution estimates of network design spaces [21].###[21], who propose to quantify the quality of a design space by sampling a set of models from that design space and characterizing the resulting model error distribution .###Following [21], we characterize the quality of a design space by sampling models and inspecting their error distribution .###Recently, the authors of [21] proposed a methodology for comparing and analyzing populations of networks sampled from a design space.",impact-revealing,acknowledge methodology for analyzing design space quality
3852,58437725ac44360f108302aa,03a5b2aac53443e6078f0f63b35d4f95d6d54c5d,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,53e999e7b7602d970222a18a,Eigenface-domain super-resolution for face recognition,"This task, referred to as super-resolution (SR), finds direct applications in many areas such as HDTV [15], medical imaging [28, 33], satellite imaging [38], face recognition [17] and surveillance [53].",other,highlighting the applications of super-resolution in various fields
2136,,1023c1320e391c02159a7314f3ef7e0029b88ce8,Ribosome Composition Maximizes Cellular Growth Rates in E. coli.,,,"###Finally, note that the number of nucleotides in a mature ribosome, 4566 nts [3], is less than that in its rRNA precursor (Fig.###Cellular growth is limited by ribosome self-replication since cells can only double as fast as their ribosomes [3,4].",impact-revealing,acknowledging prior findings on ribosome nucleotide counts and their implications
3651,573696cd6e3b12023e5ce382,e2820bffe5b42cb7d88b7f65c12171c62ab4aae2,Gradient-based Hyperparameter Optimization through Reversible Learning,56d88972dabfae2eeea99e7b,Nested Variational Compression in Deep Gaussian Processes,"Variational inference also allows gradient-based tuning of hyperparameters in Bayesian neural-network models such as deep Gaussian processes (Hensman & Lawrence, 2014).",other,reporting prior findings on variational inference
3285,5def6ca63a55ac6095fe0607,2a6f2656c716082e34f9b86d9e589390842e0853,DistTC: High Performance Distributed Triangle Counting,53e9a878b7602d97031c7790,Doulion: Counting Triangles In Massive Graphs With A Coin,"Approximation techniques for triangle counting [24], [31] have also been studied.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3076,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,53e9a757b7602d970308e97b,"Story boundary detection in large broadcast news video archives: techniques, experience and trends.",[31] have emphasized on the need to utilize multimodal features (text with audio-visual) for segmenting news video into story units.,other,acknowledge the need for multimodal features in news video segmentation
3782,5e524da993d709897cb69fde,09e48adf1a3f012ec56fbb4f92acae975ee43d87,a deep learning approach to antibiotic discovery,5843770dac44360f1082be74,Pathway Tools version 19.0 update: software for pathway/genome informatics and systems biology.,"Transcript cluster enrichment was performed using EcoCyc Pathway Tools (Karp, 2001; Karp et al., 2016; Keseler et al., 2013).",other,reporting the use of a specific tool for data enrichment
1330,,4da7af7aba9a17776475c6d956ebb1f73e627eb3,Multi-Target Approaches in Metabolic Syndrome,,,"###Insulin resistance is a key component for MetS and metabolic diseases such as type 2 diabetes and NAFLD.###Nevertheless, the strategy to introduce PPARγ activity has been excessively used in various approaches to multi-target drugs for MetS and were excellently reviewed by Ammazzalorso et al. (2019) Although not currently approved for humans, FXR agonists are being developed to treat metabolic diseases like NAFLD and fibrotic diseases (Ali et al., 2015; Oseini and Sanyal, 2017; Sumida and Yoneda, 2018).###Obeticholic acid is an
Frontiers in Pharmacology | www.frontiersin.org March 2021 | Volume 11 | Article 5549613
FXR agonist that has entered clinical trials for non-alcoholic steatohepatitis (NASH) and NAFLD; however, this FXR agonist has the unwanted effect of increasing total cholesterol levels and increasing the HDL-c to non-HDL-c ratio (Mudaliar et al., 2013; Han, 2018).###(2019) Although not currently approved for humans, FXR agonists are being developed to treat metabolic diseases like NAFLD and fibrotic diseases (Ali et al., 2015; Oseini and Sanyal, 2017; Sumida and Yoneda, 2018).###Indeed, due to the complex pathophysiology, the current therapeutic approaches to treat MetS, type 2 diabetes, and NAFLD need multiple treatments regulating lipid and glucose homeostasis as well as blood pressure control (Grundy et al., 2005; Grundy, 2006; Oseini and Sanyal, 2017; Sumida and Yoneda, 2018).###These studies point out that in NAFLD/NASH the CYP epoxygenase pathway is an important regulator (Liu et al., 2012; Schuck et al., 2014; Mangels et al., 2016).###Apart from the metabolic abnormalities per se, inflammation associated with these metabolic diseases plays a crucial role in increasing cardiovascular events, causing NAFLD progression to hepatocellular cancer, and causing diabetic complications such as nephropathy, neuropathy, and retinopathy (Dandona et al., 2005; Esser et al., 2014).###Likewise, NAFLD is the most common chronic liver disease and affects up to one-third of the adult population (Basaranoglu and NeuschwanderTetri, 2006; Sumida and Yoneda, 2018).###Novel bifunctional molecules described in this review build on this anti-inflammatory action and will have the capacity to impact multiple factors including blood pressure, lipid and triglyceride levels, and insulin signaling in metabolic diseases like MetS, type 2 diabetes and NAFLD.###Major metabolic diseases include metabolic syndrome (MetS), type 2 diabetes, and non-alcoholic fatty liver disease (NAFLD).###Furthermore, representatives of both, bile acid analogues and non-steroidal FXR agonists like nidufexor (Chianelli et al., 2020) or tropifexor (Tully et al., 2017) are under clinical investigation for treatment of NAFLD and NASH (Mudaliar et al., 2013; Neuschwander-Tetri et al., 2015).###AA arachidonic acid
ACE angiotensin converting enzyme
AcH acetylcholine
AMP adenosine monophosphate
Ang I/ II angiotensin I/ II
APP aminopeptidase P
APN aminopeptidase N
AT1 angiotensin II receptor subtype 1
BA bile acid
cAMP cyclic adenosine monophosphate
CB1 cannabinoid type 1
CDCA Chenodeoxycholic acid
CHF congestive heart failure
COX cyclooxygenase
CYP cytochrome P450
DASH dipeptidyl peptidase-IV activity and/or structure homologues
DHETs dihydroxyepoxyeicosatrienoic acids
DML designed multitarget ligand
DPP4 dipeptidyl peptidase-4
EDR endothelium dependent relaxation
EETs epoxyeicosatrienoic acids
ENaCs epithelial sodium channels
EPC epithelial progenitor cells
FGS focal glomerulosclerosis
FLINT Farnesoid X nuclear receptor ligand obeticholic acid for noncirrhotic, non-alcoholic steatohepatitis
FXR farnesoid X receptor
GK Glucokinase
GLP-1 glucagon-like peptide-1
GP(C)R G-protein coupled receptor
HbA1c hemoglobin A1c
hERG human ether-a-go-go-related gene
HETE hydroxyeicosatrienoic acid
HIF-1α Hypoxia-inducible factor 1-alpha
HSCs human hepatic stellate cells
IL-6 interleukin-6
HDL high-density lipoprotein
HMG-CoA 3 hydroxy-3-methyl-glutaryl-coenzyme A
LDL low-density lipoprotein
LH lateral hypothalamus
MCH melanin-concentrating hormone
MCH-1R melanin concentrating hormone receptor 1
MCP-1 monocyte chemoattractant protein-1
MetS Metabolic syndrome
NAFLD non-alcoholic fatty liver disease
NASH non-alcoholic steatohepatitis
NEP neutral endopeptidase
OCA obeticholic acid
OEA oleoylethanolamides
PPARs proliferator-activated receptors
PPCE post-proline cleaving enzyme
RA(A)S renin-angiotensin-aldosterone system
RXR retionoid-X-receptor
SAR structure–activity relationship
sDDP4 soluble dipeptidyl peptidase-4
sEH soluble epoxide hydrolase
sEH-I soluble epoxide hydrolase-inhibitor
SGLT-2 sodium-glucose cotransporter-2
SHR spontaneously hypertensive rats
SHROB rat spontaneously hypertensive obese rat
T2DM type 2 diabetes mellitus
TGF-β transforming growth factor beta
TNF-α tumor necrosis factor-α
TZD thiazolidinedione
UUO unilateral ureteral obstruction
VEGF vascular endothelial growth factor
ZDF ratzucker diabetic fatty rat
ZSF1 ratzucker fatty/spontaneously hypertensive heart failure F1 hybrid rat
Frontiers in Pharmacology | www.frontiersin.org March 2021 | Volume 11 | Article 55496118###The disease complex of NAFLD/NASH is considered to be a hepatic manifestation of the MetS (Petäjä and YkiJärvinen, 2016).###In addition, there are currently no approved drugs for the treatment of NAFLD (Oseini
and Sanyal, 2017; Sumida and Yoneda, 2018).###In addition, there are currently no approved drugs for the treatment of NAFLD (Oseini and Sanyal, 2017; Sumida and Yoneda, 2018).###FXR activation reduces hepatic fat accumulation and has been demonstrated to have anti-fibrotic and anti-inflammatory actions (Oseini and Sanyal, 2017; Sumida and Yoneda, 2018).###A large portion of patients with NAFLD display typical features of MetS including abdominal obesity, dyslipidemia, hypertension, insulin resistance, or type 2 diabetes (Chalasani et al., 2012; El-Kader and El-; El-Kader and El-Den Ashmawy, 2015).",impact-revealing,highlighting the significance of FXR activation in treating metabolic diseases
120,5dd7b1be3a55ac97f763dd30,948839277bface5780896e8e8791906818aa41ac,Adversarial Examples Improve Image Recognition,5aed14d617c44a4438158fb6,Adversarial Logit Pairing.,"Meanwhile, recent works [18, 16, 45] also suggest that training with adversarial examples on large datasets, e.###However, previous works [18, 16] show adversarial training always degrades performance.###5 in [18] shows the result of training with random normal perturbations) or adversarial noise [16, 18, 42], fail to improve accuracy on clean images.###These results contradict previous conclusions [18, 42, 16] that the performance degradation is always observed if adversarial examples are used for training.###We hypothesize this distribution mismatch between clean examples and adversarial examples is a key factor that causes the performance degradation in previous works [16, 18, 45].###Compared to [18, 16], we make two changes in our reimplementation: (1) using stronger networks; and (2) training with weaker attackers.###Since the first discovery of the vulnerability of ConvNets to adversarial attacks [40], many efforts [2, 7, 15, 16, 18, 23, 29, 36, 42, 45, 50] have been made to improve network robustness.",impact-revealing,highlighting the ongoing debate and challenges in adversarial training for neural networks
3697,5a73cb3517c44a0b303556bf,eac48f406c46527f5ca821de7fe8d62d6db56a27,"Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer",573696126e3b12023e5246d6,End-to-End Attention-based Large Vocabulary Speech Recognition,"In this work we explore a particular sequence-to-sequence architecure, RNN-T, and show how text and pronunciation data may be included to improve end-to-end ASR performance.###Recently, there has been considerable interest in training end-to-end models for ASR [7, 8, 9], which directly output word transcripts given the input audio.1 Thus, these models are much simpler than conventional ASR systems as a single neural network can be used to directly recognize utterances, without requiring separately-trained acoustic, pronunciation and language model components.###The current state-of-the-art automatic speech recognition (ASR) systems break down the ASR problem into three main sub-problems: acoustic, pronunciation and language modeling.###, [7, 9]) is that the entire input sequence is encoded before the output sequence may be decoded and thus these models cannot be used for real-time streaming speech recognition.###Index Terms— ASR, end-to-end, sequence-to-sequence
models, recurrent neural networks transducer, wordpiece.###Recently, there has been considerable interest in training end-to-end models for ASR [7, 8, 9], which directly output word transcripts given the input audio.###[18] F. Eyben, M. Wöllmer, B. Schuller, and A. Graves,
“From speech to letters-using a novel neural network architecture for grapheme based asr,” in Workshop on Automatic Speech Recognition and Understanding (ASRU).###A particular class of architecures known as sequence-to-sequence models [10] are particularly suited for end-to-end ASR as they include an encoder network which corresponds to the acoustic model of a conventional system and a decoder network which corresponds to the language model.###We compare the RNN-T end-to-end recognizer with a conventional ASR system consisting of separate acoustic, pronunciation and language models.###Despite recent work on end-to-end ASR, conventional systems still remain the state-of-the-art in terms of word error rate (WER) performance.###CTC has been widely used in previous works to train end-toend ASR models [8, 18, 19].",other,highlighting the evolution and challenges in automatic speech recognition systems
644,558ab239e4b037c08758a249,c1757f34c23241960a0089c5117fa3b676902951,the effect of code reordering on branch prediction,53e9bdefb7602d9704aa322f,A comparative analysis of schemes for correlated branch prediction,"This is called prediction table interference, and is the main cause for decreased prediction accuracy [29].###Negative int rference happens more often than positive interference, and is the main cause of decreased prediction accuracy [29, 20].",impact-revealing,describing a phenomenon affecting prediction accuracy
4007,5f92ba1691e011edb3573ba0,268d347e8a55b5eb82fb5e7d2f800e33c75ab18a,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016).###For the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization layers (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized convolutions (Qiao et al., 2019).",other,reporting the use of specific convolutional architectures in computer vision
970,,4727b4cdd3dd6eee16656e48feedb1c7b2055b81,Mechanism Design and Intentions ∗,,,"###An extensive experimental literature – examples include Andreoni et al. (2002), Falk et al. (2003) and Falk et al. (2008) – has concluded that behavior is most likely influenced by both types of considerations.3 The theoretical models proposed by Levine (1998), Charness and Rabin (2002), Falk and…###This question is motivated by the empirically well-documented individual heterogeneity in social preferences (Fehr and Schmidt 1999, Engelmann and Strobel 2004, Falk et al. 2008, Dohmen et al. 2009).",impact-revealing,highlighting the influence of experimental literature on understanding behavior
3696,5def6ca63a55ac6095fe0607,2a6f2656c716082e34f9b86d9e589390842e0853,DistTC: High Performance Distributed Triangle Counting,53e9a45cb7602d9702d783b7,"What is Twitter, a social network or a news media?","rmat26 is a synthetic graph generated by an RMAT generator [41]; twitter40 [42], friendster [43] uk-2007, gsh-2015, and clueweb12 [44], [45], [46], [47] are",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
509,5ec7a32791e0118397f3ee40,2523aea79e9776dc34020f3d150dd49211aa144d,Leveraging Text Data Using Hybrid Transformer-LSTM Based End-to-End ASR in Transfer Learning,5de632593a55ac4f55c25743,Independent language modeling architecture for end-to-end ASR,"To this end, we extend our prior work [1], and propose a hybrid Transformer-LSTM based architecture.###To tackle this problem, [1] has proposed long short term memory (LSTM)-based encoderdecoder architecture which allows improving the LM capacity of the decoder using the extra text data.###In this work, we propose a hybrid Transformer-LSTM architecture which combines the advantages of [1] and [6].###A LSTM-based encoder-decoder architecture [1], denoted as A1 in the rest of this paper, consists of a Bidirectional LSTM encoder and a LSTM-based decoder which are shown in Fig.###Section 2 describes baseline architectures mentioned in [1] and [6].###Figure 1: LSTM-based encoder-decoder architecture (A1) [1], where the decoder acts as an independent language model.###Results show that the proposed architecture outperforms the previous LSTM-based architecture [1] by 24.###Similar to [1], we empirically found that the second step is necessary to improve overall performance.###In other words, the LSTM acts as an independent language model that can be easily updated with text-only data [1].###This avoids a so-called catastrophic forgetting problem as mentioned in [1].",impact-revealing,proposing a new hybrid architecture based on prior work
653,5c0495fa17c44a2c747059aa,172f096eecc0290442b35908fbef01d62e668e0a,RFUZZ: Coverage-Directed Fuzz Testing of RTL on FPGAs,53e9a32eb7602d9702c392c9,MicroGP—An Evolutionary Assembly Program Generator,erage has been used in related academic [13] and industrial [5] work.###Another difference is that MicroGP is clearly targeted towards slow DUT execution in a software simulation.###The prior work most similar to rfuzz is MicroGP [13] which focuses on maximizing statement coverage in the HDL description of various processor implementations.,impact-revealing,acknowledge related work in software simulation
2006,,b8dd608fc86359af215d7a9b6722454436d2a34b,Recent Advances in the Use of Chemical Markers for Tracing Wastewater Contamination in Aquatic Environment: A Review,,,"###Carbamazepine, benzotriazole and primidone were determined to be suitable markers based on their low sorption affinity to soil, while the use of sulfamethoxazole as a marker was limited by the formation of non-extractable residues at lower pH [152].###[152] also highlighted that organic carbon could impact sorption process.",impact-revealing,highlighting the findings on suitable markers and their limitations in soil sorption
2615,53e99c2fb7602d97024dc163,fa4cb30bc910cf0d128c9d863ad9f0705b03850c,A Predictive Model for Dynamic Microarchitectural Adaptivity Control,53e9b6dcb7602d970426f0ba,Efficiency trends and limits from comprehensive microarchitectural adaptivity,"PRIOR WORK ON MICROARCHITECTURAL ADAPTIVITY Recently, Lee and Brooks [1] showed that it is possible to significantly increase processor energy efficiency by adapting it as a program is running.###Recently, Lee and Brooks [1] showed that it is possible to significantly increase processor energy efficiency by adapting it as a program is running.###[5], Lee and Brooks [7] and Joseph et al. [6] proposed predictive
modelling (i.e., machine learning) for architectural design space exploration.###Although previous work has quantified the theoretical benefits of high adaptivity [1], predicting and delivering this adaptation is still an open and challenging problem.###It represents the design space of a high-performance out-of-order superscalar processor and is similar to spaces that other researchers have considered [1].",other,highlighting the significance of prior work on processor energy efficiency and adaptation
2722,5a9cb60d17c44a376ffb3c4c,fc3384d631f5e2b2a9d66623d4d3e1d28b96dee7,Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search,599c7968601a182cd263a485,End-to-End Neural Ad-hoc Ranking with Kernel Pooling,"K-NRM uni_x0080_ed the progress of IR customized embeddings and interaction based model [29].###Interaction based models thrive with encoding word-word translations using word embeddings, and utilizing new pooling methods to be_x008a_er summarize the word translations into ranking signals [11, 13, 29].###Such counting-based pooling methods have shown be_x008a_er performance than score-based ones like mean-pooling or max-pooling [13, 29].###_x008c_is part extends K-NRM [29] to n-grams.###A recent success of neural methods in information retrieval (neural IR) is the development of interaction based models [13, 21, 29].###Learned end-to-end from user feedbacks [23, 29], the word embeddings can encode so_x0089_ matches tailored for relevance ranking, which has signi_x0080_cant advantages over traditional feature-based methods [29, 30].###When trained with user feedback in a search log, K-NRM outperforms both neural IR methods and feature-based learning-to-rank by a large margin [29].###Conv-KNRM adds the ability of so_x0089_ matching n-grams to the recent state-of-the-art K-NRM model [29] with convolutional neural networks (CNNs).###_x008c_e current state-of-the-art kernel pooling and learningto-rank techniques are then used to combine the n-gram so_x0089_matches to the _x0080_nal ranking score [29].",other,reporting advancements in interaction-based models for information retrieval
1270,,d11a83665286cccec8f56d632c0079c86ff240f3,FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space,,,"###As a representative self-supervised learning (SSL) paradigm, contrastive learning (CL) has achieved unprecedented success in vision tasks [3, 14, 4, 12, 55] and are increasingly leveraged in learning from time series [10, 7, 63, 65, 36, 58].###…discrimination (SimCLR [3], MoCoV3 [5], and MAE [13]), four modality-matching contrastive frameworks for multimodal input (CMC [48], Cosmo [35], Cocoa [6], GMC [37]), three SOTA contrastive frameworks for time series (TS2Vec [63], TNC [49], TS-TCC [8]), and one predictive baseline (MTSS [42]).###First, simply considering temporally close samples as positive pairs and pushing their semantical similarities to 1 as [63, 49] can be problematic because they ignore the continuously evolving factors ( e.g. , distance) that make differences between temporally neighboring samples.###TS2Vec [63], TFC [65], TNC [49], and TS-TCC [7] were based on the time-series properties but did not consider the multi-modal collaboration properties.###There has been increasing interest in developing contrastive learning frameworks for time-series data [10, 63, 44, 7, 49, 22, 65, 36].",impact-revealing,highlighting the growing interest and application of contrastive learning in various domains
1035,,30c07e3426056bb582f47eef361ab2ef97b00e43,Participant Use of Digital Diaries in Qualitative Research: A Strong Structuration Analysis,,,"###…experiences and perceptions missed in the limited time frame that tends to be captured in interviews or focus group discussions, a position reinforced by existing literature (see Bernays et al., 2014; Broekhuis et al., 2020; Chen, 2011; Elliott, 1997; Herron et al., 2019; Jacelon & Imperio, 2005).",impact-revealing,highlighting the limitations of traditional data collection methods in capturing experiences
2401,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,558c7077e4b00c3c48e24942,Detecting hiding malicious website using network traffic mining approach,2004 [64] Statistic patterns DoS and DDoS 2005 [29] Host behavior based Worm outbreaks 2005 [30] IP aggregation Detection and monitoring 2006 [110] Flow aggregation IDS 2007 [109] Trust and reputation model IDS 2007 [28] Flow signature and honeypot logs Worm detection 2008 [18] Heuristics Worm detection 2008 [158] Statistic Anomaly detection 2009 [70] Heuristics NAT detection 2009 [137] Decision tree Dictionary attack 2009 [43] K-means Behavior-based NAC 2009 [150] Information theory Risk detection 2009 [155] Statistic Top N detection 2009 [136] Statistic Spam machines 2010 [135] NBA Malware 2010 [54] Spatial-temporal aggregating Malicious website detection 2011 [114] Statistic of host behavior Attack Detection 2011 [61] Dynamic entropy DoS 2011 [44] Statistic DDoS and port scan 2011 [41] Host behavior and PageRank Botnets detection 2011 [125] Time series IDS 2011 [42] PageRank Botnets detection,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2345,5a73cb6317c44a0b30358203,a072c2a400f62f720b68dc54a662fb1ae115bf06,tacotron: towards end-to-end speech synthesis,58d83008d649053542fe0622,Recent Advances in Google Real-time HMM-driven Unit Selection Synthesizer,"We compare our model with a parametric (based on LSTM (Zen et al., 2016)) and a concatenative system (Gonzalvo et al., 2016), both of which are in production.###We compare our model with a parametric (based on LSTM [19]) and a concatenative system [25], both of which are in production.",other,comparing model performance with existing systems
144,5e09a7e4df1a9c0c4167dacc,3f82c0b2ca9d3ce40ad867bb5b0a3e93c7517b82,Attention-Based Dual-Source Spatiotemporal Neural Network for Lightning Forecast,5736986b6e3b12023e72fa3b,Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting,"The ConvLSTM in this paper does not include peephole connections, as mentioned in [1].###[1] proposed convolutional LSTM (ConvLSTM) for precipitation nowcasting.###However, although extrapolationbased methods for weather nowcasting [1]–[3] can be migrated to lightning forecast tasks, they encounter rapid decline in forecasting accuracy beyond two to three hours because of the complex evolution of the lightning region.",impact-revealing,highlighting the limitations of extrapolation-based methods for lightning forecasting
87,5d3ed25a275ded87f97dea9b,0ddd7c9a1955c76eb831a230ef20a68b662886bb,Mining Algorithm Roadmap in Scientific Publications,599c7987601a182cd2648373,Attention Is All You Need.,"[37] proposes a method using self-attention [36] and bi-affine scoring algorithm to predict biological relations between all mention pairs in the abstract simultaneously.###We adapt Transformer [36] to encode word sequences in a paragraph, where we calculate the self-attention of the words, and use a convolutional layer in self-attention blocks similar to [37] to alleviate the burden on the model to attend to local features.###The Transformer contains stacked layers of Transformer block, which contains its own set of parameters.###We base on recent Transformer architecture [7, 36] to build this module, due to its better performance in encoding long-distance context compared to Long Short Term Memory Networks (LSTMs) [15] and Convolutional neural networks (CNNs).",impact-revealing,describing the adaptation of Transformer architecture for biological relation prediction
1108,,fa188115cbc26acd08e1bd9559ad0280df80c6ac,Matting by Generation,,,"###Our approach builds upon the diffusion model [Ho et al. 2020; Song et al. 2021], a generative model that has garnered significant attention owing to its exceptional generative capabilities [Rombach et al. 2022].",impact-revealing,highlighting the significance of the diffusion model in generative modeling
1646,,9b9f547a292a8783dde38a7fcb330b8926620c4c,Green CSR Communication to Internal Audiences: The Persuasive Power of Normative Appeals to Employees’ Green Behaviors,,,"###The impacts of social norms on green behaviors at work can further be grounded in social identity theory (Tajfel and Turner 1979).###Building upon social identity theory (Tajfel and Turner 1979), Study 1 shows that communicating colleagues’ pro-environmental actions effectively persuades individuals to exercise green behaviors while working.###In light of the collective environment in which employees share their membership in a company, Study 1 builds upon social identity theory (Tajfel and Turner 1979) and the group engagement model (Tyler and Blader 2000, 2003) to examine how social norms as a green CSR message appeal influence…",impact-revealing,highlighting the theoretical foundation and significance of social identity theory in understanding green behaviors at work
905,5cede104da562983788e4508,6303bac53abd725c3b458190a6abe389a4a1e72d,deep high-resolution representation learning for human pose estimation,53e9b672b7602d97041dc126,Articulated pose estimation with flexible mixtures-of-parts,"Most traditional solutions to single-person pose estimation adopt the probabilistic graphical model or the pictorial structure model [76, 49], which is recently improved by exploiting deep learning for better modeling the unary and pair-wise energies [9, 63, 44] or imitating the iterative inference process [13].",impact-revealing,highlighting advancements in single-person pose estimation methods
3787,5db929e947c8f766461fd005,11ead744b2edbdabe174258a8dfe4023be8e501d,Hierarchical Attention Prototypical Networks for Few-Shot Text Classification,5a4aef9e17c44a2190f7a591,Few-Shot Learning with Graph Neural Networks,"Firstly, we compare our model with several traditional models such as Finetune and kNN, Then we compare our model with ﬁve state-of-the-art few-shot learning models based on neural networks, they are MetaN (Munkhdalai and Yu, 2017), GNN (Garcia and Bruna, 2018), SNAIL (Mishra et al., 2018), Proto (Snell et al., 2017) and PHATT (Gao et al., 2019) respectively.###…such as Finetune and kNN, Then we compare our model with ﬁve state-of-the-art few-shot learning models based on neural networks, they are MetaN (Munkhdalai and Yu, 2017), GNN (Garcia and Bruna, 2018), SNAIL (Mishra et al., 2018), Proto (Snell et al., 2017) and PHATT (Gao et al., 2019) respectively.###For FewRel dataset, we cite the results reported by Snell et al. (2017) which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by Gao et al. (2019) which includes Proto and PHATT.",other,comparing model performance with traditional and state-of-the-art methods
2335,5e7345fd91e011a051ebf85f,9c6dccf7e17221adc3b02bfc202a0e0e061fe28a,deliberation model based two-pass end-to-end speech recognition,5a73cb6317c44a0b30358231,Generation Of Large-Scale Simulated Utterances In Virtual Rooms To Train Deep-Neural Networks For Far-Field Speech Recognition In Google Home,"We augment the clean training utterances by artificially corrupting them by using a room simulator, varying degrees of noise, and reverberation such that the signal-to-noise ratio (SNR) is between 0dB and 30dB [23].",other,describing the method for data augmentation
754,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,"While GNNs are flexible enough to allow for unsupervised losses, most work follows the semi-supervised setting for node classification from [29].###Graph convolutional networks (GCNs) [29] are simple yet effective [51] message-passing networks that fit our criteria.###Graph Neural Networks (GNNs) [49, 15, 40, 21, 29] allow end-to-end differentable losses over data with arbitrary structure.",impact-revealing,acknowledge the effectiveness of GNNs in node classification
3952,5c2c7a9217c44a4e7cf317b4,61946177ea5d44ad18ff09de7929556d7b34cd8b,simd-x: programming and processing of graph algorithms on gpus.,557c8ec66feeaa8086da191e,BFS and Coloring-Based Parallel Algorithms for Strongly Connected Components and Related Problems,"Other algorithms, such as, weakly connected component and strongly connected component algorithms [67] also fall into this category.",other,acknowledge related algorithms
801,5736977f6e3b12023e66632b,0834e74304b547c9354b6d7da6fa78ef47a48fa8,line: large-scale information network embedding,53e9b253b7602d9703cf4028,DeepWalk: online learning of social representations,"The most recent work related with ours is DeepWalk [16], which deploys a truncated random walk for social network embedding.###The Flickr network is denser than the Youtube network (the same network as used in DeepWalk [16]).###For other networks, the dimension is set as 128 by default, as used in [16].",impact-revealing,acknowledge related work in social network embedding
1817,,0b9eed8a692c7448c5f04a1e39c913dda4916fe7,The Relationship Between Citizen Participation and Organizational Processes and Outcomes and the Benefits of Citizen Participation in Neighborhood Organizations,,,"###…study builds on and expands existing research by conceptualizing the benefits of
volunteer involvement in terms of personal and collective benefits including self- and collective efficacy and sense of community (see, e.g., Brodsky et al., 1999; Sampson et al., 1997; Zimmerman & Rappaport, 1988).###In another study, greater participation among students and community residents in a variety of community organizations was related to increased expectations and actual experiences of personal and political efficacy ( Zimmerman & Rappaport, 1988 ).###In another study, greater participation among students and community residents in a variety of community organizations was related to increased expectations and actual experiences of personal and political efficacy (Zimmerman & Rappaport, 1988).",impact-revealing,highlighting the benefits of volunteer involvement based on existing research
2979,5e5e18e493d709897ce3a0f2,7a064df1aeada7e69e5173f7d4c8606f4470365b,albert: a lite bert for self-supervised learning of language representations,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Following Devlin et al. (2019), we set the feed-forward/filter size to be 4H and the number of attention heads to be H/64.###We adapt these hyperparameters from Liu et al. (2019), Devlin et al. (2019), and Yang et al. (2019).###The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in (Devlin et al., 2019).###BERT (Devlin et al., 2019) uses a loss based on predicting whether the second segment in a pair has been swapped with a segment from another document.###, 2018), to full-network pre-training followed by task-specific fine-tuning (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019).###Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard & Ruder, 2018) has led to a series of breakthroughs in language representation learning.###The results we report in this section make use of the training data used by Devlin et al. (2019), as well as the additional data used by Liu et al. (2019) and Yang et al. (2019).###To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019).###Model Hidden Size Parameters RACE (Accuracy) BERT-large (Devlin et al., 2019) 1024 334M 72.###We also observe that simply growing the hidden size of a model such as BERT-large (Devlin et al., 2019) can lead to worse performance.###…the shift from pre-training word embeddings, whether standard (Mikolov
et al., 2013; Pennington et al., 2014) or contextualized (McCann et al., 2017; Peters et al., 2018), to full-network pre-training followed by task-specific fine-tuning (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019).###The latter appears to be a particularly strong improvement, a jump of +17.4% absolute points over BERT (Devlin et al., 2019; Clark et al., 2019), +7.6% over XLNet (Yang et al., 2019), +6.2% over RoBERTa (Liu et al., 2019), and 5.3% over DCMI+ (Zhang et al., 2019), an ensemble of multiple models…###Learning representations of natural language has been shown to be useful for a wide range of NLP tasks and has been widely adopted (Mikolov et al., 2013; Le & Mikolov, 2014; Dai & Le, 2015; Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; 2019).###In this section, we present the design decisions for ALBERT and provide quantified comparisons against corresponding configurations of the original BERT architecture (Devlin et al., 2019).###Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019).###In addition to the masked language modeling (MLM) loss (Devlin et al., 2019), BERT uses an additional loss called next-sentence prediction (NSP).###We follow the finetuning procedures from prior work (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) and report the held-out test set performance obtained from GLUE submissions.###To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in using the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretraining baseline models.###For example, Devlin et al. (2019) show that across three selected natural language understanding tasks, using larger hidden size, more hidden layers, and more attention heads always leads to better performance.",other,describing the design decisions and performance comparisons of ALBERT and BERT
678,5f2bd70491e011b36ba9ce89,902fc14117f67575067d8b2a1989ea900eaceb1c,TOAD-GAN: Coherent Style Level Generation from a Single Example,5ce3aebeced107d4c65ebaaf,Singan: Learning A Generative Model From A Single Natural Image,"Our work is inspired by SinGAN (Shaham, Dekel, and Michaeli 2019), a recent Generative Adversarial Network (GAN) (Goodfellow et al. 2014) architecture that learns a generative model given only one example image.###SinGAN (Shaham, Dekel, and Michaeli 2019) is a novel GAN architecture that enables learning a generative model from a single image.###In summary, our contributions are: • With TOAD-GAN, we present a novel generative model
that allows for level generation following the one-shot training approach of SinGAN.###We
expand on the novel SinGAN architecture to generate tokenbased levels instead of natural images.###Since SinGAN was developed for natural RGB images, it is unable to generate convincing video game levels that are based on 2D token maps.###On natural images, SinGAN uses zero-centered gaussian spatial noise that is constant over all color channels for a given pixel, i.e. it only changes the brightness of that pixel.###For a more in-depth explanation please refer to the original SinGAN paper by Shaham, Dekel, and Michaeli (2019).###The variance σ2 controls how much information from the lower scales is passed through to the upper layers of SinGAN.",impact-revealing,drawing inspiration from a novel GAN architecture for a new generative model
3670,5c04966a17c44a2c74708508,5e93a30656ef091f55ce42374d498f2b881e0c47,Bytes Are All You Need: End-to-end Multilingual Speech Recognition and Synthesis with Bytes,5ac1829d17c44a1fda917e9e,Efficient Neural Audio Synthesis,"Similar to Tacotron 2, we separately train a WaveRNN [32] to invert mel spectrograms to a time-domain waveform.",other,describing a method for audio processing
2588,5bdc315017c44a1f58a05a1d,f9fb0ad85f903e3ecf5d4bafdf7c74f647e75f58,LEMNA: Explaining Deep Learning based Security Applications,57aa28de0a3ac518da9896e8,Interpretable Decision Sets: A Joint Framework for Description and Prediction,", decision tree [6] and decision set [31]) to incrementally approximate the target detection boundaries.###Explanation Method Support RNN/MLP Local Non-linear Support Blackbox Representative Works Whitebox method (forward) G# # G# Occlusion [17, 32, 74, 76], AI2 [19], Whitebox method (backword) G# # # Saliency Map [3, 54, 57], Grad-carm [50], DeepLIFT [53] Blackbox method G# # LIME [45], SHAP [34], Interpretable Decision Set [31] Our method LEMNA LEMNA",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3824,5bdc315817c44a1f58a05e88,9ea992f009492888c482d5f4006281eaa8b758e7,"X2Face: A network for controlling face generation by using images, audio, and pose codes",53e9982cb7602d9702050587,Poisson image editing,"5a, X2Face+p.p.) by transferring hidden regions using Poisson editing [28].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3977,5e3940c73a55ace46ed436d2,845b4941d8c016aa5f8967da2f86d38ef6c18fa3,a survey on knowledge graphs representation acquisition and applications,59ae3c152bbe271c4c71e8f3,Generating Natural Answers By Incorporating Copying And Retrieving Mechanisms In Sequence-To-Sequence Learning,"To generate natural answers in a user-friendly way, COREAQ [157] introduces copying and retrieving
mechanisms to generate smooth and natural responses in a seq2seq manner, where an answer is predicted from the corpus vocabulary, copied from the given question or retrieved from the knowledge graph.###To generate natural answers in a user-friendly way, COREAQ [157] introduces copying and retrieving mechanisms to generate smooth and natural responses in a seq2seq manner, where an answer is predicted from the corpus vocabulary, copied from the given question or retrieved from the knowledge graph.",other,describing the method for generating natural answers
2923,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",53e9b1bcb7602d9703c48dfe,Perfect Reconstruction Two-Channel Wavelet Filter-Banks for Graph   Structured Data,"These types of filterbanks have been designed for bipartite graphs [68], [102], thus requiring the graph to be decomposed into a series of bipartite subgraphs [68], [103].###Coming from another angle, motivated by processing data collected by sensor networks where sensors are irregularly placed, different authors develop regression algorithms [62], wavelet decompositions [61], [63]–[66], filter banks on graphs [67], [68], denoising [69], and compression schemes using the graph Laplacian [70].",other,acknowledge existing research on filterbanks and graph processing
3744,5c2c7a9217c44a4e7cf3161b,e6926981ef9c1d06d6c075cdae7b298d3dbf3a7d,Learning Latent Representations for Style Control and Transfer in End-to-end Speech Synthesis,57a4e921ac44365e35c98d4b,Tutorial on Variational Autoencoders.,The decoder may produce the expected reconstruction if the output of decoder is averaged over many samples of x and z [14].,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1809,,dfa6d7af6f93f67c8bd1fe1c8baa997d33518305,"The Rational Child: Theories and Evidence in Prediction, Exploration, and Explanation",,,"###…of causal mechanisms is fundamental to an understanding of causal relationships (Ahn, et al., 1995; Bullock, Gelman, & Baillargeon, 1982; Koslowski, 1996; Shultz, 1982) and it would be interesting to know how offering children explicit information about causal mechanisms might affect…###(p. 15, Theories and Evidence, Koslowski, 1996).###(Ahn, et al., 1995; Bullock, Gelman, & Baillargeon, 1982; Koslowski, 1996; Shultz, 1982) and it would be interesting to know how offering children explicit information about causal mechanisms might affect their learning.###…importance of integrating domain-specific knowledge with domain-general strategies (Barrett, Abdi, Murphy, & Gallagher, 1993; Klahr & Dunbar, 1988; Koslowski, 1996; Koslowski, Okagaki, Lorenz, & Umbach, 1989; Pazzani, 1991; Penner & Klahr, 1996; Schauble, 1990), those studies have focused…###…the relationship between a hypothesis and the evidence that might support or disconfirm it, and to have difficulty generating effective experiments (Klahr, Fay, & Dunbar, 1993; Koslowski, 1996; Kuhn, 1989; Kuhn et al., 1995; Kuhn, Amsel, & O’Loughlin, 1988; Masnick & Klahr, 2003; Schauble, 1990).###Because previous research suggests that children have a poor metacognitive understanding of confounding and experimental design (Chen & Klahr, 1999; Inhelder & Piaget, 1958; Kuhn, 1989; Kuhn, et al., 1988; Koslowski, 1996; Masnick & Klahr, 2003), we expected that children might not be aware of their own motivation for exploration.###learners to overlook relevant evidence in favor of irrelevant variables, to fail to understand the relationship between a hypothesis and the evidence that might support or disconfirm it, and to have difficulty generating effective experiments (Klahr, Fay, & Dunbar, 1993; Koslowski, 1996; Kuhn, 1989; Kuhn et al., 1995; Kuhn, Amsel, & O’Loughlin, 1988; Masnick & Klahr, 2003; Schauble, 1990).###Although some research on the development of scientific reasoning has emphasized the importance of integrating domain-specific knowledge with domain-general strategies (Barrett, Abdi, Murphy, & Gallagher, 1993; Klahr & Dunbar, 1988; Koslowski, 1996; Koslowski, Okagaki, Lorenz, & Umbach, 1989; Pazzani, 1991; Penner & Klahr, 1996; Schauble, 1990), those studies have focused primarily on adolescents and adults.###…difficulty learning from statistical evidence that conflicts with their prior beliefs (Klahr, Fay,
& Dunbar, 1993; Kuhn, 1989; Kuhn, Amsel, & O’Loughlin, 1988; Kuhn & Phelps, 1982; Koslowski, 1996; Kushnir & Gopnik, 2006; Masnick & Klahr, 2003; Schauble, 1990; Schulz, Bonawitz, & Griffiths, 2007).",impact-revealing,highlighting the importance of understanding causal mechanisms in learning
1671,,e316582f7adfc2de15a8d3b0e247d598174f2805,I am (not) my avatar: A review of the user-avatar relationships in Massively Multiplayer Online Worlds,,,"###Other studies (e.g. Gabbiadini, Mari, Volpato, & Monaci, 2014; van Looy, Courtois, de Vocht, & de Marez, 2012) were inspired by the Social Identity (Tajfel & Turner, 1979) and Social Categorisation (Oakes & Turner, 1980) theories, focusing on the social identities that can emerge using MMOWs, such as those related to guild or to community memberships.###Other studies (e.g. Gabbiadini, Mari, Volpato, & Monaci, 2014; van Looy, Courtois, de Vocht, & de Marez, 2012) were inspired by the Social Identity (Tajfel & Turner, 1979) and Social Categorisation (Oakes & Turner, 1980) theories, focusing on the social identities that can emerge using MMOWs, such…",impact-revealing,acknowledge existing research inspired by social identity theories
320,5f1ff7ea91e011d50a621ab3,39b45b0d60807296db97d9eb37b1fb99d244e2ed,MACU-Net for Semantic Segmentation of,5ecbc5829fced0a24b4e836e,Unet 3+: A Full-Scale Connected Unet For Medical Image Segmentation,"Thus, full-scale skip connections are designed in U-Net 3+ [13] to alleviate this limitation, while leading to huge computational complexity.###To test the effectiveness of MACU-Net, we compare the performance of the proposed method with U-Net [11], FGC [19], U-Net++ [12], U-NetPPL [20], WRAU-Net [21], CE-Net [22] and U-Net 3+ [13].###To evaluate the effectiveness of MACU-Net, the U-Net [11], FGC [19], U-Net++ [12], U-NetPPL [20], WRAU-Net [21], CE-Net [22] and U-Net 3+ [13] methods were used as benchmark comparators.",impact-revealing,comparing the performance of different methods
496,5d971ab13a55ac1c613f54e6,6e47b9e4ea00300450984088c8366d28782b17f1,A Multi-Attentive Pyramidal Model for Visual Sentiment Analysis,599c7987601a182cd2648373,Attention Is All You Need.,"Inspired by the recent Transformer model [23], we implant a self attention module adapted to image features.###• Self-Attention [23] is a variant derived from the Transformer model.###Lastly, inspired by the recent Transformer [23] model in dealing with a number of difficult NLP tasks such as machine translation successfully without attaching to any traditional neural networks, we implant a self-attention mechanism in the MAP model to capture the associations between the local features for the final sentiment representation.###Transformer is a simple network architecture that based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.###Moreover, inspired by the recent Transformer [23] model in dealing with a number of difficult NLP tasks such as machine translation successfully, we implant a self-attention mechanism in the model to mine the associations between the above extracted local visual regions before obtaining the final sentiment representation.",impact-revealing,highlighting the influence of the Transformer model on the proposed approach
2836,5e09a83ddf1a9c0c41685fc3,90820988957e74db0fbb1df79175610e08016ba4,Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling,5a9cb60d17c44a376ffb3c4c,Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search.,"Additionally, the use of CNN layers allows us to explicitly control the window size for phrase modeling, which has been shown to be critical for relevance matching (Dai et al., 2018; Rao et al., 2019).",other,highlighting the importance of CNN layers for relevance matching
38,5f75aa6a9fced0a24b64599d,69fb130409d48478fab0c4545dc2ff102a2630ef,The forward slice core microarchitecture,573697786e3b12023e660116,The load slice core microarchitecture,"1 Load Slice Core (LSC) [5] was the first work to propose an sOoO core; Freeway [10] builds upon the LSC proposal and exposes more MHP than LSC by adding one more in-order queue for uncovering additional independent loads.###In this section, we briefly cover the background on the two prior sOoO cores — LSC [5] and Freeway [10] — and we elaborate on their shortcomings.###We extensively discussed the Load Slice Core [5] and Freeway [10] throughout the paper.",impact-revealing,acknowledge prior work on sOoO cores and their shortcomings
1864,,c9c757680d6ad91cb7a1568cd2b4206846d9e7ed,Five Challenges When Managing Mass Casualty or Disaster Situations: A Review Study,,,"###In this study, the analysis process was inspired by the integrative review methodology—incorporating both quantitative and qualitative data [30].###This analysis process was inspired by the integrative review analysis process [30,31].",impact-revealing,describing the analysis methodology used in the study
2186,,72e8c6b5e30d9b7ee2b593baed3e5796ebdbec36,Assisted migration of enterprise applications to the Cloud - A hybrid Cloud approach,,,"###Nowadays research institutions tend to build upon the NIST definition striving for standardization (Vaquero, Rodero-Merino, Caceres, & Lindner, 2008) (Vouk, 2008).",impact-revealing,highlighting the trend towards standardization in research institutions
3865,573696cd6e3b12023e5ce382,e2820bffe5b42cb7d88b7f65c12171c62ab4aae2,Gradient-based Hyperparameter Optimization through Reversible Learning,53e9a806b7602d9703149263,Gaussian Processes For Machine Learning,"For example, this ability has been used to construct complex kernels for Gaussian process models (Rasmussen & Williams, 2006, Chapter 5).",other,providing context on the application of Gaussian process models
1437,,447b43e974cab6bdac2cb43057cb7dbd58ab770f,SLTR: Simultaneous Localization of Target and Reflector in NLOS Condition Using Beacons,,,"###The beacon-based approach is one of the other methods that have been widely used for the target localization in NLOS [1, 2, 10, 12-18].###For NLOS condition, one of the solutions is to combine the mentioned methods, i.e. the DOA, TOA, TDOA and RSSI; in [7], the authors proposed a hybrid TOA and AOA cooperative localization for the NLOS environments in a WSN.###For example, the authors in [15, 16, 19, 20] proposed the use of a single beacon that moves in a determined path, to ensure the localization of the target.###Conclusion
In this paper we addressed the target localization issue, after reflection from a reflector, in NLOS condition and solved it using beacons.###Localization of a target in NLOS condition still one of the open problems yet.###Other studies adopt the use of non-moving beacons, in which their number grows with respect to the searching areas[1, 12, 13, 15].###Key words: Beacon, Target Localization, Mirror space, NLOS condition, Size constancy concept.",impact-revealing,highlighting the ongoing challenges in target localization under NLOS conditions
2231,5f576c1591e011f4c3d5dd7e,c36571ac50808c75fa8a5d37f1041af22e89e6ee,GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training,573696ce6e3b12023e5ce95a,Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift,"Hence, normalization decouples the optimization of direction and length of the parameters (Kohler et al., 2019), implicitly tunes the learning rate (Ioffe & Szegedy, 2015;
Hoffer et al., 2018; Arora et al., 2018b; Li & Arora, 2019), and smooths the optimization landscape (Santurkar et al., 2018).###[16] Sergey Ioffe and Christian Szegedy.###For example, Batch normalization (BatchNorm) is a standard component in computer vision (Ioffe & Szegedy, 2015); Layer normalization (LayerNorm) is popular in natural language processing (Ba et al., 2016; Xiong et al., 2020); Instance normalization (InstanceNorm) has been found effective for style…###We visualize the statistics from BatchNorm layers under different settings of batch sizes [8, 16, 32, 64], as in Figure 9.###We apply the normalization after the linear transformation as in previous works (Ioffe & Szegedy, 2015; Xiong et al., 2020; Xu et al., 2019).###In computer vision, batch normalization [16] is a standard component.###During testing, the estimated dataset-level statistics (running mean µD and standard deviation σD) are used instead of the batch-level statistics (Ioffe & Szegedy, 2015).###For example, in computer vision, BatchNorm [16] is the de facto method that normalizes the feature values in the same channel across different samples in the batch.###Normalization methods shift and scale the hidden representations and are shown to help the optimization for deep neural networks (Ioffe & Szegedy, 2015; Ulyanov et al., 2016; Ba et al., 2016; Salimans & Kingma, 2016; Xiong et al., 2020; Salimans et al., 2016; Miyato et al., 2018; Wu & He, 2018;…###Using this property, [20] suggests that normalization decouples the optimization of direction and length of the parameters; [1, 13, 16, 21] show that the normalization implicitly tunes the learning rate.###In batch normalization (BatchNorm), the mean and standard deviation in a sampled batch are random variables which try to provide accurate estimations for the mean and standard deviation over the whole dataset [16, 23, 32].###1 Related Work Normalization is important in optimizing deep neural networks, and different normalization techniques have been proposed to improve the training process in different applications [4, 16, 24, 26, 27, 33, 36].###the normalization after the linear transformation as in previous works [16, 36, 37].###Each step in this branch can boost the performance of GNNs: subtracting graph mean has preconditioning effect; introducing a learnable shift avoids the expressiveness degradation; further scaling to unit norm enjoys “scale-invariant” property [1, 13, 16].###During testing, the estimated dataset-level statistics are used instead of the batch-level statistics [16].",other,highlighting the importance of normalization techniques in optimizing deep neural networks
1136,,939e96c890a0b43ef88a72182389e52420c91934,Accelerating Diffusion Models for Inverse Problems through Shortcut Sampling,,,"###Given the deterministic nature of the DDIM generation process, we can establish the DDIM Inversion process by reversing Eq.###Since a deterministic inversion process like DDIM Inversion produces unrealistic re-sults, while a stochastic inversion process like DDPM Inversion yields unfaithful results.###Why DDIM Inversion Cannot Work Well.###When facing low-quality input images, the distribution of predicted noise ϵ θ ( x t , t ) during the DDIM Inversion process exhibits a greater deviation from the standard normal distribution.###1 in a similar form of DDIM Inversion in Eq.###To satisfy Criteria (i) , a naive approach is to apply DDIM Inversion, capable of providing a deterministic mapping from the input image y to a noisy transitional state[Hertz et al. , 2022; Tumanyan et al. , 2023; Zhang et al. , 2024].###Meanwhile, [Song et al. , 2021a ] generalize DDPM via a non-Markov diffusion process that shares the same training objective, whose generation process is outlined as follows: When σ t = 0 , DDIM samples images through a deterministic generation process, which allows for high-quality sampling with fewer steps:###More specifically, given a low-quality input image y , the predicted noise { ϵ θ ( x t , t ) } during DDIM Inversion process deviates from the standard normal distribution, resulting in the deviation of E from the predefined noise distribution.###Recently, diffusion models [ Sohl-Dickstein et al. , 2015; Ho et al. , 2020; Song et al. , 2021c] have demonstrated phenomenal performance in generation tasks [Rombach et al. , 2022; Dhariwal and Nichol, 2021; Xiao et al. , 2022].###(7) in the following manner: LQ Image (a) The transitional state obtained through DDIM Inversion preserves most information of the input images since we can reconstruct it by iteratively executing Eq.###Meanwhile, DDIM [29] proposed a non-Markov process that shares the same forward process as DDPM, while possessing a deterministic generation process, outlined as follows: xt−1 = √ αt−1fθ(xt, t) + √ 1− αt−1εθ(xt, t) (5) where fθ(xt, t) is the prediction of clean image x0 at time-step t:###…complex distributions, recent methods [Kawar et al. , 2021; Kawar et al. , 2022; Chung et al. , 2022b; Chung et al. , 2023; Wang et al. , 2023; Lugmayr et al. , 2022; Song et al. , 2021b] have sought to utilize pre-trained diffusion models for solving inverse problems in an unsupervised manner.###Conversely, an alternative, DDIM Inversion[Song et al. , 2021a], widely adopted for image editing[Hertz et al. , 2022; Tumanyan et al. , 2023], tends to produce unrealistic outcomes.###Diffusion models [Sohl-Dickstein et al. , 2015; Ho et al. , 2020; Song et al. , 2021c] are a family of generative models designed to model complex probability distributions of high-dimensional data.###Meanwhile, [Song et al. , 2021a ] generalize DDPM via a non-Markov diffusion process that shares the same training objective, whose generation process is outlined as follows: When σ t = 0 , DDIM samples images through a deterministic generation process, which allows for high-quality sampling with…",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3221,5a260c0217c44a4ba8a1d1dd,28e4ae18a652e7d67df3e3fa6f4703ae9ef930e9,BestConfig: tapping the performance potential of systems via automatic configuration tuning,5736973b6e3b12023e62b1ae,ALOJA-ML: A Framework for Automating Characterization and Knowledge   Discovery in Hadoop Deployments,"Though sporadic proposals are found on automatically suggesting conﬁguration settings for Web servers [6, 41, 43], databases [11] and Hadoop [4, 19] respectively, these solutions are not generally applicable to the variety of systems in the cloud.###Aloja[4] adopts the common machine learning methods, exploiting a large database of samples.",other,highlighting the limitations of existing configuration suggestion proposals for cloud systems
537,573696cd6e3b12023e5ce382,e2820bffe5b42cb7d88b7f65c12171c62ab4aae2,Gradient-based Hyperparameter Optimization through Reversible Learning,53e9a841b7602d970318684f,Gradient-based optimization of hyperparameters.,"Larsen et al. (1998), Eigenmann & Nossek (1999), Chen & Hagan (1999), Bengio (2000), Abdel-Gawad & Ratner (2007), and Foo et al. (2008) showed that gradients of regularization parameters are available in closed form when training has converged exactly to a local minimum.###e caching of all parameter vectors w 1;w 2;:::;w T , making it impractical for large problems with many training iterations. Larsen et al. (1998 ),Eigenmann &amp; Nossek 1999 Chen &amp; Hagan (1999 ),Bengio 2000 Abdel-Gawad &amp; Ratner (2007), andFoo et al. 2008) showed that gradients of regularization parameters are available in closed form when training has converged exactly to a local minimum. In contras###Applying RMD to hyperparameter optimization was proposed by Bengio (2000) and Baydin & Pearlmutter (2014), and applied to small problems by Domke (2012).",impact-revealing,reporting prior findings on gradients of regularization parameters
3100,5e09aa66df1a9c0c416bebf6,4e7dd1e79f0f13650b2612325e6ba8d206dc04fb,DeepGCNs: Can GCNs Go As Deep As CNNs?,5c89c5f14895d9cbc6e0840a,Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling,"Graphs are also popular modes of representation in natural language processing [2, 23], where they are used to represent complex relations between large text units.",other,acknowledging the use of graphs in natural language processing
1296,,625cfa4170a12aee17e76c18b7165ae13e522c4f,SemEval-2020 Task 3: Graded Word Similarity in Context,,,"###We created a new dataset, CoSimLex (Armendariz et al., 2020), which builds on the familiar pairwise, graded similarity task of SimLex-999, but extends it to pairs of words as they occur in context; speciﬁcally, each pair of words appears together in two different shared contexts (see Figure 1).###As starting point for our annotation methodology, we adapted the instructions used for SimLex-999.###They proposed a system in which they calculated K-Means inspired centroids from the words in the context and used them to modify the original SimLex-999 non contextualised similarity scores.###CoSimLex (Armendariz et al., 2020) is based on pairs of words from SimLex-999 (Hill et al., 2015); the reliability and common use of SimLex makes it a good starting point and allows comparison of judgements and model outputs to the context-independent case.###For Croatian and Finnish we use existing translations of SimLex-999 (Mrkši´c et al., 2017; Venekoski and Vankka, 2017; Kittask, 2019).###Our dataset is based on pairs of words from SimLex-999 (Hill et al., 2015).###In the case of Slovene, we have produced our own new translation, 1 following Mrkši´c et al. (2017)’s methodology for Croatian.###For non-contextualised models, resources like WordSim-353 (Finkelstein et al., 2002) and SimLex-999 (Hill et al., 2015) were instrumental to evaluate their ability to reﬂect human similarity judgements.###However we were still able to replicate SimLex-999’s proportions of nouns, verbs and adjectives (about two thirds nouns, two ninths verbs and one ninth adjectives).###Reliability of annotation was ensured by an adapted version of SimLex-999’s post-processing method, which includes rating calibration and the ﬁltering of annotators with very low correlation to the rest, see the original paper for details (Hill et al., 2015).###We expected that the relative complexity of the annotation process and the increased confounding effects could affect inter-rater agreement; however, as we can see in Table 1, the different CoSimLex datasets show correlation scores very close to SimLex-999’s IRA ( ρ = 0.77 vs ρ = 0.78 in English).###Each line of CoSimLex is made of a pair of words selected from SimLex-999; two different contexts extracted from Wikipedia in which these two words appear; two scores of similarity, each one related to one of the contexts, calculated as the mean of annotator ratings for that context; two scores of standard deviation; the p-value given by applying the Mann-Whitney U test to the two score distributions; and the four inﬂected forms of the words exactly as they appear in the contexts (including case; note that in the morphologically rich languages, many inﬂections are possible).",impact-revealing,describing the creation and methodology of a new dataset for word similarity
2184,,b5aa3d0dc9ef60817b1d9bfc44f4158e422f76c3,Demand for Cloud Services as an Infrastructure in Higher Education,,,"###For Vouk (2008), cloud computing “embraces cyber-infrastructure, and builds on virtualization, distributed computing, grid computing, utility computing, networking, and web and software services.”",impact-revealing,providing a definition of cloud computing
2973,5f7fdd328de39f0828397fae,645054d31fa26b29bbfb0cf73b75f8906c359415,spectral temporal graph neural network for multivariate time-series forecasting,5fa6d3a1d4150a363c53b90f,FFORMA: Feature-based forecast model averaging,"Time-series forecasting is an emerging topic in machine learning, which can be divided into two major categories: univariate techniques [20, 22, 18, 27, 32, 19, 18] and multivariate techniques [24, 21, 17, 31, 3, 29, 25, 16, 15].",other,providing context on time-series forecasting techniques
3001,5bbacbad17c44aecc4eb007e,fdb87d305c59e9ab82a96198e34e46581b838c4e,one-shot relational learning for knowledge graphs,57a4e91aac44365e35c97887,Complex Embeddings for Simple Link Prediction,"Few-Shot Learning Recent deep learning based few-shot learning approaches fall into two main categories: (1) metric based approaches (Koch, 2015; Vinyals et al., 2016; Snell et al., 2017; Yu et al., 2018), which try to learn generalizable metrics and the corresponding matching functions from a set…###Following the standard one-shot learning settings (Vinyals et al., 2016; Ravi and Larochelle, 2017), we assume access to a set of training tasks.###To enlarge our model’s capacity, we leverage a LSTM-based (Hochre-iter and Schmidhuber, 1997) recurrent “process-ing” block (Vinyals et al., 2015, 2016) to perform multi-step matching.###One example is the Matching Networks (Vinyals et al., 2016), which make predictions by comparing the input example with a small labeled support set; (2) meta-learner based approaches (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Finn et al., 2017; Li et al., 2017), which aim to learn the optimization of model parameters (by either out-putting the parameter updates or directly predicting the model parameters) given the gradients on few-shot examples.###One example is the Matching Networks (Vinyals et al., 2016), which make predictions by comparing the input example with a small labeled support set; (2) meta-learner based approaches (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Finn et al., 2017; Li et al., 2017), which aim to learn the…",other,describing categories of few-shot learning approaches
3919,5e09a801df1a9c0c41680233,1438d8c68b4495947fd5de001b87e6ef5a263a3e,Learning to Jointly Generate and Separate Reflections,5a4aef9e17c44a2190f7a33a,The Perception-Distortion Tradeoff,Recent research [3] pointed out that PSNR and SSIM may not exactly tell the perceptual visual quality.,other,highlighting limitations of PSNR and SSIM in assessing visual quality
3734,5e5e18ca93d709897ce315f0,68f86237dadcf2f570f0cd5b5e56161693619a74,Residual Energy-Based Models for Text Generation,5bdc31b817c44a1f58a0bfc9,Discriminator Rejection Sampling.,Azadi et al. (2018) also share our same goal but their generator is not locally normalized and they propose to improve the sampling from the generator by using the discriminator for rejection sampling.,other,acknowledge similarities and differences in approaches to claim generation
159,5fc4cfdf91e011abfa2faf94,633e2fbfc0b21e959a244100937c5853afca4853,score-based generative modeling through stochastic differential equations,5ef0816891e0112aee042b88,Denoising Diffusion Probabilistic Models,"Sohl-Dickstein et al. (2015); Ho et al. (2020) consider a sequence of positive noise scales 0 ă β1, β2, ¨ ¨ ¨ , βN ă 1.###The model tested here is a DDPM model trained with the same settings in Ho et al. (2020).###, 2017; Du & Mordatch, 2019), have proven effective at generation of images (Song & Ermon, 2019; 2020; Ho et al., 2020), audio (Chen et al.###…(Kingma & Dhariwal, 2018) 3.35 - MintNet (Song et al., 2019b) 3.32 - Residual Flow (Chen et al., 2019) 3.28 46.37 FFJORD (Grathwohl et al., 2018) 3.40 - Flow++ (Ho et al., 2019) 3.29 - DDPM (L) (Ho et al., 2020) ď 3.70* 13.51 DDPM (Lsimple) (Ho et al., 2020) ď 3.75* 3.17 DDPM 3.28 3.37 DDPM cont.###Denoising diffusion probabilistic modeling (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020) trains a sequence of probabilistic models to reverse each step of the noise corruption, using knowledge of the functional form of the reverse distributions to make training tractable.###Although DDPM (Ho et al., 2020) was recently reported to achieve higher sample quality than SMLD (Song & Ermon, 2019; 2020), we show that with better architectures and new sampling algorithms allowed by our framework, the latter can catch up—it achieves new state-of-the-art Inception score (9.###(8) in Ho et al. (2020)), C is a constant that does not depend on θ, z „ N p0, Iq, and xipx0, zq “ x0 ` σiz.###We follow Ho et al. (2020) for optimization, including the learning rate, gradient clipping, and learning rate warm-up schedules.###Our architecture is mostly based on Ho et al. (2020).###This unifies the sampling method in Ho et al. (2020) as a numerical solver to the reverse-time VP SDE in our continuous framework.###Main results: (i) For the same DDPM model in Ho et al. (2020), we obtain better bits/dim than ELBO, since our likelihoods are exact; (ii) Using the same architecture, we trained another DDPM model with the continuous objective in Eq.###Here in order to match the convention used in Karras et al. (2018); Song & Ermon (2019) and Ho et al. (2020), we report the lowest FID value over the course of training, rather than the average FID value over checkpoints after 0.5M iterations (used for comparing different models of VE SDEs) or…###As in Ho et al. (2020), we let τi “ c
σ2i´1pσ2i´σ2i´1q σ2i .###In our experiments, we let β̄min “ 0.1 and β̄max “ 20 to match the settings in Ho et al. (2020).###Although DDPM (Ho et al., 2020) was recently reported to achieve higher sample quality than SMLD (Song & Ermon, 2019; 2020), we show that with better architectures and new sampling algorithms allowed by our framework, the latter can catch up—it achieves new state-of-the-art Inception score (9.89)…###(3) described here is Lsimple in Ho et al.
(2020), written in a form to expose more similarity to Eq.###The specific architecture of the noise-conditional score-based model in Ho et al. (2020) uses sinusoidal positional embeddings for conditioning on integer time steps.###3, we use a DDPM model trained on 256ˆ 256 CelebA-HQ with the same settings in Ho et al. (2020).###Training We use the same architecture in Ho et al. (2020) for our score-based models.###…techniques (Bordes et al., 2017; Goyal et al., 2017; Du & Mordatch, 2019), have proven effective at generation of images (Song & Ermon, 2019; 2020; Ho et al., 2020), audio (Chen et al., 2020; Kong et al., 2020), graphs (Niu et al., 2020), and shapes (Cai
˚Work partially done during an internship…###To condition the NCSN++ model on continuous time variables, we change positional embeddings, the layers in Ho et al. (2020) for conditioning on discrete time steps, to random Fourier feature embeddings (Tancik et al., 2020).###As expected, the ancestral sampling of DDPM (Ho et al., 2020) (Eq.###We name this model “DDPM++”, following the naming convention of Ho et al. (2020).###Following Ho et al. (2020), we can compute
qpxi´1 | xi,x0q “ N ˆ xi´1; σ2i´1 σ2i xi ` ´ 1´ σ2i´1 σ2i ¯ x0, σ2i´1pσ2i ´ σ2i´1q σ2i I ˙ .",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2351,599c7945601a182cd2629f72,000178cd12c8a6e5da8215b6365fae03c20fd18d,End-to-End Representation Learning for Correlation Filter Based Tracking,573697826e3b12023e668e6c,Long-Term Correlation Tracking,"We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s [3], Staple [2] and LCT [22].###bounding box regression [25], ensembling of multiple cues [22, 2], optical flow [28]).###[6, 18, 22, 2]), where its efficiency enables a tracker to adapt its internal model of the object on the fly at every frame.###Notice that the forward pass of the architecture in Figure 1 corresponds exactly to the operation of a standard CF tracker [13, 6, 22, 3] with CNN features, as proposed in previous work [21, 7].###We compare our methods against state-of-the-art trackers that can operate in realtime: SiamFC-3s [3], Staple [2] and LCT [23].",other,comparing methods against state-of-the-art trackers
3836,5c04966a17c44a2c74708401,51203e9d5620abdcdf6c9be93b1e221e79cda67d,Transfer Learning of Language-independent End-to-end ASR with Language Model Fusion,5b8c9f4a17c44af36f8b6f14,A Comparison of Techniques for Language Model Integration in Encoder-Decoder Speech Recognition.,"Another usage of the external LM is to initialize the lower layer in the decoder network with the pre-trained LM [19, 22].###The research question in this paper is: Is linguistic context also helpful for adaptation to new languages? The most common approach to integrate the external language model (LM) is referred to as shallow fusion, where LM scores are interpolated with scores from the S2S model [5,18,19].###Apart from the external LM, the MTL approach with LM objective are investigated in [19, 23].###Therefore, shallow fusion shows performance gains in many ASR tasks [5, 18, 19].###Although the MTL approach does not require any additional parameters, it gets minor gains compared to LM fusion methods [19].###In the original formulation in [19, 21], scores from the external LM are not used.###In, [19], these fusion methods are compared in middlesize English conversational speech (∼300h) and large-scale Google voice search data.###We use the hidden state as a feature from RNNLM instead of logits because we use the universal character vocabulary for multilingual experiments, which results in the large softmax layer and increases the computational time [19].",other,discussing the integration of external language models in adaptation to new languages
1286,,54d1fe1a164e0020e3a3cbb70df2b3f9d2dfaff0,Posteromedial cortex glutamate and GABA predict intrinsic functional connectivity of the default mode network,,,"###, 1996) and thus is commonly used as an internal reference in brain spectroscopy, absolute creatine concentrations were determined from coincident voxels in a subset (n=10) of our participants using a standard short echo time PRESS acquisition and LCModel's internal water reference method (Provencher, 1993) showing values and intra-subject variation consistent with those published in the literature (mean+/−SD, [Cr]=5.###…coincident voxels in a subset (n=10) of our participants using a standard short echo time PRESS acquisition and LCModel's internal water reference method (Provencher, 1993) showing values and intra-subject variation consistent with those published in the literature (mean+/−SD, [Cr]=5.85+/−0.40 mM).",impact-revealing,reporting methodology and consistency with existing literature
2656,5fe4094e9e795e14f30e634a,94497472eecb7530a2b75c564548c540ebd61e9b,Learning to Pre-train Graph Neural Networks,5736986c6e3b12023e7308a0,Convolutional Networks on Graphs for Learning Molecular Fingerprints,"R EADOUT is typically implemented as a simple pooling operation like sum, max or mean-pooling (Atwood and Towsley 2016; Duvenaud et al. 2015) or more complex approaches (Bruna et al. 2014; Ying et al. 2018b).###READOUT is typically implemented as a simple pooling operation like sum, max or mean-pooling (Atwood and Towsley 2016; Duvenaud et al. 2015) or more complex approaches (Bruna et al.",other,providing context on READOUT implementation methods
816,558bbd9ce4b00c3c48de3915,f144ee3e4988ef0c309bd595d2bca8ab5042dfe7,A Static Binary Instrumentation Threading Model for Fast Memory Trace Collection,558bad79e4b00c3c48ddddf7,PEBIL: Efficient static binary instrumentation for Linux,"With these factors as motivation, PMaC’s Efficient Binary Instrumentation for Linux/x86 (PEBIL) [3] has recently been upgraded to support multithreaded x86 64 code.###There are many other x86/Linux binary instrumentation projects: Pin [4], Dyninst [5], and Valgrind [7] being among the most popular.###PEBIL’s threading model is explained (Section II) and compared to two other popular x86 64/Linux binary instrumentation toolkits, Pin [4] and Dyninst [5], with particular attention given to each of the tools’ support for providing thread-specific program analysis (Section III).",impact-revealing,acknowledge existing binary instrumentation projects and compare threading models
1997,,824568ecad52c343581a4441e628031eadb49598,Sources of copper into the European aquatic environment,,,"###This approach may overestimate the transfer from soil to water, as it does not consider for example retention of copper in soils caused by aging reactions (Ma et al., 2006; Smolders et al., 2012).",impact-revealing,Critique of the approach's limitations in estimating transfer from soil to water
2148,,36aa579e213ae22666839aba134e24be0a6a1eed,Applying Demand Response Programs for Electric Vehicle Fleets,,,"###We focus on one of the main functions of IS in the framework, described as “manage supply and demand to avoid high costs resources” (Watson et al. 2010).###Second, we followed the call by Watson et al. (2010) and Dedrick (2010) by contributing to the research stream of energy informatics.###There has been a growing number of IS-related research publications on DR since Watson et al. (2010) emphasized the importance of IS research in increasing the efficiency of energy demand and supply systems.###…Americas Conference on Information Systems, Puerto Rico, 2015 4
Our research model is positioned within the energy informatics framework by Watson et al. (2010) which partitions an intelligent energy system into four basic elements:
• A flow network represents the transport components…",impact-revealing,acknowledging the importance of IS research in energy informatics and its growing relevance
3274,5d245bb6da56295a28fcd54f,25fd9e491c748995e94719f52d896a41299b5b75,geometric scattering for graph data analysis,53e99acab7602d970234a328,Weisfeiler-Lehman Graph Kernels,"Out of these, five are graph kernel methods, namely: Weisfeiler-Lehman graph kernels (WL, Shervashidze et al., 2011), propagation kernel (PK, Neumann et al., 2012), Graphlet kernels (Shervashidze et al., 2009), Random walks (RW, Gärtner et al., 2003), deep graph kernels (DGK, Yanardag &…",other,reporting various graph kernel methods
2583,5d3044363a55ac8b59feaf5b,31f8b4ca13c0b333324044bd96aac20c3d0b67bd,JumpSwitches: Restoring the Performance of Indirect Branches In the Era of Spectre,53e99dfeb7602d97026bec54,Lightweight Feedback-Directed Cross-Module Optimization,"Compilers can determine and promote likely targets at compile-time [9, 23], through the use of profile-guided optimization or feedback-directed optimization (PGO/FDO) [10, 11, 26, 32, 41, 42].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2061,,a4906f803cae64959ef6657d386dfad2204b875a,Atrial fibrillation: Now one of the most common causes for hospitalization,,,"###However, even the age-adjusted prevalence of AF has increased recently, suggesting that age alone does not account for all of the increased frequency with which physicians are encountering patients with this arrhythmia [3].###Thus, the prevalence of AF is increasing markedly in industrialized nations secondary to growth in the population of elderly individuals in these societies [1–3].",impact-revealing,highlighting the increasing prevalence of atrial fibrillation in aging populations
4058,5da1a6d447c8f766460688bc,9570147aee646e15affffc30a6b18be0568cfa82,SWQUE: A Mode Switching Issue Queue with Priority-Correcting Circular Queue,53e9a042b7602d970292a18f,A family of 45nm IA processors.,"In fact, Intel switched from 6T to 8T cells for its 45nm line of Core processors [18].",other,reporting a change in manufacturing process for processors
2437,5fef1dfc91e0113b265a0220,85e7d63f75c0916bd350a229e040c5fbb1472e7a,making pre-trained language models better few-shot learners,5d3ed2653a55ac61d998598b,RoBERTa: A Robustly Optimized BERT Pretraining Approach,"…work, we study a more practical scenario in which we only assume access to a moderatelysized language model such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019), and a small number of examples (i.e., a few-shot setting), which we can use to fine-tune the weights of the language model.###, 2019) or RoBERTa (Liu et al., 2019), and a small number of examples (i.",other,describing the practical scenario of using moderately-sized language models
1949,,9ddddd3cedf78fc2e10b02f187fefe4fe397ab8c,Core-Stateless Forwarding With QoS Revisited: Decoupling Delay and Bandwidth Requirements,,,"###Both delay-aware scheduling and bandwidth allocation have rich literature including utility functions [12], [13], host-based admission control [14], stateful [4], [5], [15], [16] and stateless [1], [2], [17] resource sharing, and priority-based packet scheduling [18]–[21] that our work builds on.",impact-revealing,acknowledge existing literature on scheduling and resource allocation
461,5f7af09591e011983cc81efc,87b008a6289fa22c72e1726a8929e815dfbbc65f,Hard Negative Mixing for Contrastive Learning,5dcd263a3a55ac58039516c5,Momentum Contrast for Unsupervised Visual Representation Learning,"A popular and highly successful loss function for contrastive learning [8, 21, 38] is the following:###Let the query q and key k embeddings form the positive pair, which is contrasted with every feature n in the bank of negatives (Q) also called the queue in [21].###Contrastive learning was recently shown to be a highly effective way of learning visual representations in a self-supervised manner [8, 21].###a) We delve deeper into a top-performing contrastive self-supervised learning method [21] and observe the need for harder negatives; b) We propose hard negative mixing, i.###In a number of recent successful approaches [8, 21, 31, 39] the query and key are the embeddings of two augmentations of the same image.###[21] keeps a queue with features of the last few batches as memory.###It is however shown [8, 21] that increasing the memory/batch size leads to diminishing returns in terms of performance: more negative samples does not necessarily mean hard negative samples.###These include MoCo [10, 21], SimCLR [8, 9], PIRL [31], CMC [38] or SvAV [7].###We delve deeper into learning with a momentum encoder [21] and show evidence that harder negatives are required to facilitate better and faster learning.###Most of the top-performing contrastive methods leverage data augmentations [8, 10, 18, 21, 31, 38].",impact-revealing,highlighting the effectiveness and challenges of contrastive learning methods
3142,5e5e18ad93d709897ce2654c,19605cad33f79d3070b7c4b24aa49653ab7c90da,Inductive Matrix Completion Based on Graph Neural Networks,5992a1c65ba2006b76482de3,Weisfeiler-Lehman Neural Machine for Link Prediction,"Zhang & Chen (2017) propose Weisfeiler-Lehman Neural Machine (WLNM), which learns graph structure features using a fully-connected neural network on the subgraphs’ adjacency matrices.###recommender systems [9] rely on such heuristics. However, in this work, we do not use any predefined fixed heuristics, but learn heuristics from the existing bipartite graph. Present work Inspired by [35, 36], we aim to learn graph structure features related to ratings automatically from local enclosing subgraphs around user-item links. An h-hop enclosing subgraph for a user-item pair (u,v)is defined to b",other,reporting prior findings on graph structure features in recommender systems
775,5dea04309e795e693620e97c,b0c35bf9ddffefb0dab4f76c20b30e00a22b1e0a,unsupervised author disambiguation using heterogeneous graph convolutional network embedding,5d3ed25a275ded87f97deb35,Heterogeneous Graph Neural Network,[36] propose a heterogeneous graph neural network model which considers both types and heterogeneous attributes of nodes.,impact-revealing,reporting prior findings on heterogeneous graph neural network models
670,5c45b4d03a55ac25e7f0f55c,e5badcfa663c30a983da24dd682288141d00fcc3,GraphSAR: A Sparsity-Aware Processing-in-Memory Architecture for Large-Scale Graph Processing on ReRAMs,5a260c8117c44a4ba8a30e15,GraphR: Accelerating Graph Processing Using ReRAM.,"The parameter of ReRAM cell model is from the same source [20] in GraphR [1] (read/write energy consumption: 1.###We share ADC among bitlines, based on Previous works [1, 25].###We take the design of GraphR [1] as the baseline and compare it with GraphSAR in Figure 1.###To ensure the scalability, GraphR [1] and HyVE [19] divide a graph into subgraphs using interval-block partitioning.###Previous work like GraphR [1] converted the edge list into the adjacency matrix by sequentially writing each edge into the ReRAM crossbar.###19x lower energy-delay product, EDP) against previous graph processing architecture on ReRAMs (GraphR [1]).###, RPBFS [18], GraphR [1], and HyVE [19]), achieving great speedup and energy efficiency improvement.###Thus, GraphSAR is different fromGraphR [1] andHyVE [19], by both computing and storing edge data in ReRAM crossbar.###The order of blocks and edges in lists is the same as that in GraphR [1], where data are stored in the columnoriented order, leading to less usage of registers and write cost.",impact-revealing,reporting on the design and performance of GraphSAR compared to previous architectures
3628,5ec49a639fced0a24b4de7ed,1eed0659d561354d5c471a723cf3381430561d04,Graph Neural News Recommendation with Unsupervised Preference Disentanglement,53e9aa23b7602d970338f333,Collaborative topic modeling for recommending scientific articles.,"Traditional collaborative filtering (CF) based methods (Wang and Blei, 2011) often utilize historical interactions between users and###Traditional collaborative ﬁltering (CF) based methods (Wang and Blei, 2011) often utilize historical interactions between users and news to deﬁne the objective function for model training, aiming to predict a personalized ranking over a set of candidates for each user.",other,describing traditional collaborative filtering methods
3987,5b8c9f4a17c44af36f8b6ef6,97cc458d7dcb0c60c81f4de07ccca5c5838c9071,A Dynamic Service Migration Mechanism in Edge Cognitive Computing,5aed14a717c44a44381562db,Secure Enforcement in Cognitive Internet of Vehicles.,"In terms of security and privacy in edge computing, there is an increasing realisation that edge devices, which are closer to the user, can play an important part in supporting latency and privacy-sensitive applications [8, 9].",other,highlighting the significance of edge devices in enhancing security and privacy in edge computing
3813,5fe31b9491e01125d4b5b744,2ad565fb0ce9cda15a9e5ce37b5678ec09b134b9,Graph-Evolving Meta-Learning for Low-Resource Medical Dialogue Generation,5550411c45ce0a409eb3897f,Neural Machine Translation by Jointly Learning to Align and Translate.,"Following (Bahdanau, Cho, and Bengio 2014), the attention coefﬁcient e ij is computed as where a ∈ R H × 1 is a trainable vector, W 1 ∈ R H × 2 F is a weight matrix and || indicates the concatenation.###The aggregation vector h at is computed through the weighted sum over the node representations h ei , and α ei is the attention weight calculated as (Bahdanau, Cho, and Bengio 2014).",other,describing the computation of attention coefficients in a model
10,59ae3c262bbe271c4c71f4a2,610cff0a09c76c43739be1a6e5b0ed7a1a24ee60,metapath2vec: Scalable Representation Learning for Heterogeneous Networks,53e9b45eb7602d9703f5b32d,Mining Heterogeneous Information Networks: Principles and Methodologies,"Previous work has shown that many data mining tasks in heterogeneous information networks can bene_x0080_t from the modeling of meta-paths [6, 25, 27].###In speci_x0080_c, we leverage the de_x0080_nition of heterogeneous networks in [25, 27] and present the learning problem with its inputs and outputs.###denoted in the form of V1 R1 −−→ V2 R2 −−→ · · ·Vt Rt −−→ Vt+1 · · · Rl−1 −−−→ Vl , wherein R = R1 ◦ R2 ◦ · · · ◦ Rl−1 de_x0080_nes the composite relations between node types V1 and Vl [25].###Yet a large number of social and information networks are heterogeneous in nature, involving diversity of node types and/or relationships between nodes [25].###In metapath2vec, we _x0080_rst propose meta-path [25] based random walks in heterogeneous networks to generate heterogeneous neighborhoods with network semantics for various types of nodes.###In contrast to conventional meta-path-based methods [25], the advantage of latent-space representation learning lies in its ability to model similarities between nodes without connected meta-paths.###In addition, meta-paths are commonly used in a symmetric way, that is, its _x0080_rst node typeV1 is the same with the last oneVl [25, 26, 28], facilitating its recursive guidance for random walkers, i.",impact-revealing,highlighting the benefits of modeling meta-paths in data mining tasks
3724,58d82fced649053542fd692f,29e944711a354c396fad71936f536e83025b6ce0,categorical reparameterization with gumbel-softmax,5550443b45ce0a409eb4c39d,Stochastic Backpropagation and Approximate Inference in Deep Generative Models.,"ST Gumbel-Softmax allows samples to be sparse even when the temperature τ is high.###In particular, the variance of SF scales linearly with the number of dimensions of the sample vector (Rezende et al., 2014a), making it especially challenging to use for categorical distributions.###This reparameterization trick is commonly applied to training variational autooencoders with continuous latent variables using backpropagation (Kingma & Welling, 2013; Rezende et al., 2014b).",other,providing context on the challenges of using ST Gumbel-Softmax for categorical distributions
802,558c2b08e4b00c3c48e0a105,368e031ce85bee93ad5bda8c0970cda76c9cf140,the heterogeneous block architecture,53e9b02fb7602d9703a8b569,Enhancing instruction scheduling with a block-structured ISA,", [8, 9, 25, 38, 39, 42, 47, 53, 57]) exploited the notion of large atomic code blocks to improve performance, efficiency and design simplicity.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3314,5f7fdd328de39f08283980ba,2fce1ef37391cd685fc5459e1cbfcb8490b85242,Graph information bottleneck,5bdc31b417c44a1f58a0ba6c,How Powerful are Graph Neural Networks?,"Graph Neural Networks (GNNs) [1, 3, 5–7] have demonstrated impressive performance, by learning to fuse information from both the node features and the graph structure [8].",other,highlighting the performance and capabilities of Graph Neural Networks
3124,5da1a6d447c8f7664606888c,404d82a8da658f16c46714442155490d79a413f1,temporal prefetching without the off-chip metadata,53e9a232b7602d9702b39d85,Amoeba-Cache: Adaptive Blocks for Eliminating Waste in the Memory Hierarchy,"The extra logic is similar to that used in the AmoebaCache [31] and may incur additional latency or pipeline stages, but only for metadata accesses.",other,providing context on the logic used in AmoebaCache
445,5a73cbcc17c44a0b3035f3c9,5e2bb96c47ccaa16a4e7192e8fadb3b3e1c3acdc,deep learning: a critical appraisal,57a4e921ac44365e35c98f2b,Building Machines that Learn and Think Like People,"Another potential valuable place to look is human cognition (Davis & Marcus, 2015; Lake et al., 2016; Marcus, 2001; Pinker & Prince, 1988).",impact-revealing,suggesting a potential area of exploration in human cognition
2612,5a9cb66717c44a376ffb87ea,fe9b8aac9fa3bfd9724db5a881a578e471e612d7,efficient neural architecture search via parameter sharing,53e9a5e2b7602d9702f0bc11,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,"The idea has apparent complications, as different child models might utilize their weights differently, but was encouraged by previous work on transfer learning and multitask learning, which established that parameters learned for a particular model on a particular task can be used for other models on other tasks, with little to no modifications (Razavian et al., 2014; Zoph et al., 2016; Luong et al., 2016).###…encouraged by previous work on transfer learning and multitask learning, which established that parameters learned for a particular model on a particular task can be used for other models on other tasks, with little to no modiﬁca-tions (Razavian et al., 2014; Zoph et al., 2016; Luong et al., 2016).",other,highlighting the relevance of previous work on transfer learning and multitask learning
3145,5f0d85c69fced0a24be4f028,5d9073cfec34aea00247ec625fa94f6279ff580d,tailored page sizes,53e99846b7602d97020753ea,Mondrian Memory Protection,"Prior work in fine-grained memory protection [24], [57], [58] identify similarity and contiguity across many conventional base pages, similar to how TPS identifies contiguity in order to tailor a page of appropriate size.",other,acknowledge existing methods in fine-grained memory protection
2102,,5e956bfbdce2ae701a4e680ed42f768c9dad6c6f,Protocoles de sécurité efficaces pour les réseaux de capteurs IP sans-fil et l'Internet des Objets. (Lightweight security protocols for IP-based Wireless Sensor Networks and the Internet of Things),,,"###[65] works but the keys are mapped on twodimensional positions.###Several solutions are inspired by this scheme [65, 46, 114, 110].###al [65] propose a key pre-distribution scheme that relies on the deployment knowledge and avoids unnecessary key assignments.###Deterministic key distribution OKD Blom’s scheme based [65, 33] l m m m m m l l l m m",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1707,,4a2a4720c23dfede1a24388dee19258562bd32c3,Personal and Social Facets of Job Identity: A Person-Centered Approach,,,"###…understanding of identity in organizations by uncovering specific contributions of personal (Erikson, 1968; Marcia, 1966; Meeus, 2011) and social (Tajfel and Turner, 1979; Turner et al. 1987) facets of job identity to job outcomes, and then analyzing interconnections between these different…###In this study, we sought to reach a more comprehensive understanding of identity in organizations by uncovering specific contributions of personal (Erikson, 1968; Marcia, 1966; Meeus, 2011) and social (Tajfel and Turner, 1979; Turner et al. 1987) facets of job identity to job outcomes, and then analyzing interconnections between these different facets.###The current study builds upon an established theoretical background (Erikson, 1968; Marcia, 1966; Meeus, 2011; Tajfel and Turner, 1979; Turner et al. 1987) suggesting that identity statuses and identification profiles can account for differences in job outcomes.###One of most influential conceptualizations in the social field is the Social Identity Approach, which comprises two related, although distinct, theories: Social Identity Theory (SIT; Tajfel and Turner 1979) and Self-Categorization Theory (SCT; Turner et al.###One of most influential conceptualizations in the social field is the Social Identity Approach, which comprises two related, although distinct, theories: Social Identity Theory (SIT; Tajfel and Turner 1979) and Self-Categorization Theory (SCT; Turner et al. 1987).###The social identity approach (Tajfel and Turner 1979; Turner et al. 1987), in contrast, has focused on people’s identification with the groups and social categories to which they belong.",impact-revealing,building upon established theoretical background in job identity
3559,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,558b4f2684ae84d265c2ab1a,Scale-out processors.,"Hence, some services might beneﬁt from trading LLC capacity for additional cores [68].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3093,5db9295f47c8f766461f5135,2c1006c856fefdbd6cd710e840e57153f2d6cd04,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,5a73cbcc17c44a0b3035f54b,Audio Adversarial Examples: Targeted Attacks on Speech-to-Text,"Adversarial examples have been shown to be ubiquitous beyond classification, ranging from object detection [64, 18] to speech recognition [11, 9].",other,highlighting the widespread occurrence of adversarial examples across various domains
1738,,15086124840fef0525be62c4e584cf896978fedd,"Developing K-8 Computer Science Teachers' Content Knowledge, Self-efficacy, and Attitudes through Evidence-based Professional Development",,,"###This result is in line with other research showing that PD can prepare teachers to teach computational thinking and computer science concepts[14, 18, 44].###In 2019 the content course highlighted the importance of providing the teachers with CT concepts instruction[44].",impact-revealing,acknowledging alignment with prior research on professional development for teaching computational thinking
2794,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,5c04967517c44a2c7470926f,Pitfalls of Graph Neural Network Evaluation,"Coauthor CS and Coauthor PHY [50] are co-authorship networks based on the Microsoft Academic Graph for the computer science and physics fields respectively; nodes are authors, which are connected by edge if they co-authored a paper together; node features are a collection of paper keywords for author’s papers; class labels indicate most common fields of study.###On Amazon PC and Amazon Photo MinCutPool failed to converge, even with tuning the orthogonality regularization parameter.###Amazon PC and Amazon Photo [50] are subsets of the Amazon co-purchase graph for the
computers and photo sections of the website, where nodes represent goods with edges between ones frequently purchased together; node features are bag-of-word reviews, and class labels are product category.###Table 4: Results on four datasets from [50] in terms of graph conductance C, modularity Q, NMI with ground-truth labels, and pairwise F1 measure.###(GCNs) [29] are simple yet effective [50] message-passing networks that fit our criteria.###Amazon PC and Amazon Photo [50] are subsets of the Amazon co-purchase graph for the computers and photo sections of the website, where nodes represent goods with edges between ones frequently purchased together; node features are bag-of-word reviews, and class labels are product category.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3597,5eabf3cd91e011664efc496f,916595fc4e701ac6125725408912a1fac3d7a60b,CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows,53e99dc5b7602d97026846ec,Increasing Processor Performance Through Early Register Release,"…various register renaming schemes which can be broadly categorized into two groups: 1) delaying the allocation of physical registers from rename to write back [70], [71], and 2) releasing short-lived registers as soon as their values are fetched to all consumers [34], [72], [73], [74], [75], [76].###In addition, the proposed renaming scheme is an attractive approach for an energy-constrained environment, because it requires neither a prediction mechanism nor checkpointed PRF [74], [75], [76], [77].",other,providing context on register renaming schemes and their benefits
286,5db929b747c8f766461fa94f,2a6d160b529272964ce1a6707adf52f3d6ba4861,Diffusion Improves Graph Learning,53e99fb4b7602d970288b7c9,Diffusion Kernels on Graphs and Other Discrete Input Spaces,"Two popular examples of graph diffusion are personalized PageRank (PPR) [57] and the heat kernel [36].###Graph diffusion and random walks have been extensively studied in classical graph learning [13, 14, 36, 37], especially for clustering [34], semi-supervised classification [12, 22], and recommendation systems [44].",impact-revealing,acknowledge existing methods in graph diffusion and their applications
532,5cede10fda562983788ef75c,2a69ddbafb23c63e5e22401664bea229daaeb7d6,Res2Net: A New Multi-Scale Backbone Architecture,58d82fcbd649053542fd67d7,Aggregated Residual Transformations for Deep Neural Networks,"The Res2Net strategy exposes a new dimension , namely scale (the number of feature groups in the Res2Net block), as an essential factor in addition to existing dimensions of depth [37], width 2 , and cardinality [43].###3, we can easily integrate the cardinality dimension [43] and SE block [19] with the proposed Res2Net module.###3: The Res2Net module can be integrated with the dimension cardinality [43] (replace conv with group conv) and SE [19] blocks.###2(a) is a basic building block in many modern backbone CNNs architectures, e.g. , ResNet [17], ResNeXt [43], and DLA [47].###The dimension cardinality indicates the number of groups within a ﬁlter [43].###Numerous neural network modules have been proposed in recent years, including cardinality dimension introduced by Xie et al. [43], as well as squeeze and excitation (SE) block presented by Hu et al. [19].###…a more efﬁcient network architecture is the key to further improving the performance of CNNs. Inthe past few years, several backbone networks, e.g. , [6], [9], [17], [19], [20], [22], [37], [39], [43], [47], have made signiﬁcant advances in numerous vision tasks with state-of-the-art performance.###…the overall network structure and the multi-scale representation ability of the Res2Net module is orthogonal to the layer-wise feature aggregation models of CNNs, we can easily integrate the proposed Res2Net module into the state-of-the-art models, such as ResNet [17], ResNeXt [43], and DLA [47].###Recent years have witnessed numerous backbone networks [9], [17], [20], [22], [37], [39], [43], [47], achieving state-of-the-art performance in various vision tasks with stronger multi-scale representations.###Extensive experimental results show that the Res2Net module can further improve the performance of state-of-the-art CNNs, e.g. , ResNet [17], ResNeXt [43], and DLA [47].",impact-revealing,highlighting the significance of the Res2Net strategy in improving CNN performance
3990,5e09a7e4df1a9c0c4167dacc,3f82c0b2ca9d3ce40ad867bb5b0a3e93c7517b82,Attention-Based Dual-Source Spatiotemporal Neural Network for Lightning Forecast,5c75754df56def97989dadf0,Performance Evaluation of an Explicit Lightning Forecasting System,"Our evaluation metrics include four types of commonly used skill-scores in meteorological forecasts [18], [39], [40]: probability of detection (POD), false alarm ratio (FAR), threat score (TS) and equitable threat score (ETS).",other,reporting evaluation metrics used in meteorological forecasts
1399,,963b02d24bb9f2b4d484f7d8ec8c880b4d33b742,Implants for HIV prevention in young women: Provider perceptions and lessons learned from contraceptive implant provision,,,###These were then validated by reviewing a sub-set of transcripts [22].,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
4,5843777eac44360f108419f2,d864a3bc5188284d5be8750b1244c0923b846e32,Web User Profiling using Data Redundancy,53e99846b7602d9702074a58,Markov logic networks,"For a general introduction of first-order logic, please refer to [12].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1191,,f2f0ae914caedb34a3af1da086be0b70ab490ce0,A Single Shot Framework with Multi-Scale Feature Fusion for Geospatial Object Detection,,,"###Compared with region-based CNN models, our work is motivated by the SSD and YOLO approaches [30,33,34,48].",impact-revealing,highlighting motivation from existing models
2946,5c2c7a9217c44a4e7cf317b4,61946177ea5d44ad18ff09de7929556d7b34cd8b,simd-x: programming and processing of graph algorithms on gpus.,53e99aacb7602d970232585c,Task management for irregular-parallel workloads on the GPU,"Second, current systems either ignore workload imbalance as in [25, 76], or resolve it reactively as in [63, 59], both of which result in undesired system performance.",other,highlighting limitations in current systems regarding workload imbalance
3642,5fd0a7f691e01147f1d1e367,d4e9bf1d5fab52698a3aa48c65b098ea327deea6,CTRLsum: Towards Generic Controllable Text Summarization,5a260c8617c44a4ba8a31cdb,Controllable Abstractive Summarization.,"…Dragic added 20 as the Miami Heat handed LeBron James another loss on his former home floor with a 106-92 victory over the Cleveland Cavaliers on Monday …… [ ignoring 60 tokens ] James scored 16 of his 26 points in the fourth quarter for Cleveland, which had its four-game winning streak snapped.###Oracle entity setup: We first follow the evaluation in (Fan et al., 2018) to simulate a possible user preference by providing the model with oracle entities extracted from the ground-truth target.###Note that keyword dropout is applied at training time only.###…between relapsing - remitting and secondary - progressive forms of multiple sclerosis ; ( 2 ) we test the performance of the proposed pipeline on a data set collected from patients currently enrolled in an ongoing funded project ; ( 3 ) we discuss the potential role of patient reported outcome…###Our training procedure is model-agnostic so that CTRL SUM can employ any sequence-to-sequence architecture as the backbone without###Any entity setup: We assess whether the model can deal with diverse preferences where every entity in the document is a possible input.###In this setting, generated summaries depend solely on the document x without requiring human involvement.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
740,5e6cacbe91e011436e692287,317cf17506eab84cab8e36f81f6b1bcd7e427871,a graph neural network approach for scalable wireless power control,58d82fced649053542fd7243,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,"For the first requirement, the idea is to use a symmetric function on transformed elements in the point set to approximate a general function defined on the set [21]:###(Robustness) [21] Suppose u : X → R such that u = MAXxi∈S and f = γ ◦ u.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
213,5c8f2a8b4895d9cbc62ecf7c,10ab21b120e305b6d3cbf81c5a906d36521152f1,Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors,5a260c2817c44a4ba8a2332e,ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models.,"…of the gradient with all the standard basis vectors e 1 , . . . , e d : We can then easily implement the PGD attack (c.f. (1)) using this estimator: Indeed, [CZS + 17] were the ﬁrst to use ﬁnite diﬀerences methods in this basic form to power PGD–based adversarial attack in the black-box setting.###Recent work [CZS + 17, BHLS17, IEA + 18] provides a number of attacks for this threat model.###Building on the work of [CZS + 17], Ilyas et. al [IEA + 18] designed a black-box attack strategy that also uses ﬁnite diﬀerences but via natural evolution strategies (NES) to estimate the gradients.###Chen et. al [CZS + 17] show how to use a basic primitive of zeroth order optimization, the ﬁnite diﬀerence method, to estimate the gradient from classiﬁcation queries and then use it (in addition to a number of optimizations) to mount a gradient based attack.###Chen et. al [CZS + 17] were the ﬁrst to design black-box attack based on ﬁnite-diﬀerences and gradient based optimization.",impact-revealing,reporting prior findings on black-box attacks using finite differences
2784,5db1765a3a55ac101c887e97,6c4b76232bb72897685d19b3d264c6ee3005bc2b,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,599c7978601a182cd264164c,An Overview of Multi-Task Learning in Deep Neural Networks.,"An alternative approach, called “multitask learning” (Ruder, 2017; Caruana, 1997), is to train the model on multiple tasks at a time.###An alternative approach, called “multi-task learning” [Ruder,
2017; Caruana, 1997], is to train the model on multiple tasks at a time.",other,describing an alternative approach in model training
1413,,5e86a1e80cd7a84a5ff316f59345f00c402bddb5,Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress,,,"###Baselines include kickstarting [74], JSRL [83], rehearsal [66], offline pretraining [46] and DQfD [36].###However, ﬁne-tuning degrades performance with 1 -step returns, which is more pronounced with higher values of CQL loss coefﬁcient (Figure A.13).###[46] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.###To do so, we use CQL [46], a widely used offline RL algorithm, which jointly minimizes the TD and behavior cloning on logged transitions in DT (Equation A.###The choice of CQL is motivated by its simplicity as well as recent ﬁndings that ofﬂine RL methods that do not estimate the behavior policy are more suited for online ﬁne-tuning [63].###13: RL pretraining using CQL [46], followed by fine-tuning with Left.###To do so, we use CQL [46], a widely used ofﬂine RL algorithm, which jointly minimizes the TD and behavior cloning on logged transitions in D T (Equation A.3).",impact-revealing,reporting on baseline methods and their performance
2398,57a4e91dac44365e35c9886f,52b2bca872dac6cd708d36c009daa6f90db371be,A Soft Processor Overlay with Tightly-coupled FPGA Accelerator,61ca36005244ab9dcbe54996,Enhancing the RISC-V Instruction Set Architecture,"While the design of Algorithm 1 may be specific to the particular implementation of Sobel edge detection, it highlights several challenges commonly faced by many real-world hardware-software designers.",other,Highlighting common challenges in hardware-software design
2018,,641bc068ab27deb1a2210c23911012c19bad2780,Neutrophil-Lymphocyte Ratio and Platelet to Lymphocyte Ratio in Prediction of Severe Acute Pancreatitis: A Prospective Single Center Study,,,"###However, the APACHE II scoring system is exhaustive and cannot be widely adopted for AP patients outside the intensive care setting ((19)).",impact-revealing,highlighting limitations of the APACHE II scoring system for broader application
1076,,d2d321a9f2f3e8f636fc3457f76d2cf4a640bbc6,SIMULTANEOUSLY ACQUIRING THE SYNTAX AND SEMANTICS OF SPATIAL REFERRING EXPRESSIONS,,,"###PMI is commonly used for natural language tasks such as collocation extraction, (Bouma, 2009), sentiment or affect analysis (Recchia and Jones, 2009; Read, 2004), and other types of implicit feature identification (Su et al.###PMI is commonly used for natural language tasks such as collocation extraction, (Bouma, 2009), sentiment or affect analysis (Recchia and Jones, 2009; Read, 2004), and other types of implicit feature identification (Su et al., 2006).",impact-revealing,reporting prior applications of PMI in natural language tasks
2671,5ebe685391e0117693a52241,99314a532a3358cb86064fc8917ed2c283227539,NAT: Noise-Aware Training for Robust Neural Sequence Labeling,5d0b00858607575390fcd001,FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP,"Implementation We implemented our models using the FLAIR framework (Akbik et al., 2019) 8 .",other,reporting the implementation framework used
2069,,eed1245bf888a27244de4c410b1dd54ef5537a43,Efficacy of Omadacycline-Containing Regimen in a Mouse Model of Pulmonary Mycobacteroides abscessus Disease,,,"###abscessus strain ATCC 19977 (53), commonly used as a reference isolate, was obtained from the ATCC (Manassas, VA) and authenticated by genome sequencing (46).",impact-revealing,reporting the source and authentication of a reference isolate
245,5736974d6e3b12023e6388bf,1d7e580740a3f96f1529e3d771915af4e996a259,Verbal and Nonverbal Clues for Real-life Deception Detection,55a4fbe6612c6b12ab0075cd,Cues to deception.,"This agrees with the findings in (Depaulo et al., 2003) that liars who are more motivated to get away with their lies (i.e., trials) are likely to increase their eye-contact behavior.###This set of features is motivated by previous research that has suggested that deceivers’ speech has lower complexity (Depaulo et al., 2003).###…community, we use the term “gesture” to broadly refer to body movements, including facial expressions and hand gestures.
specifically focus on the annotation of facial displays and hand movements, as they have been previously found to correlate with deceptive behavior (Depaulo et al., 2003).###specifically focus on the annotation of facial displays and hand movements, as they have been previously found to correlate with deceptive behavior (Depaulo et al., 2003).###This agrees with the findings in (Depaulo et al., 2003) that liars who are more motivated to get away with their lies (i.",impact-revealing,highlighting the significance of findings related to deceptive behavior
2888,5f0d85c69fced0a24be4f04c,a43ba805d13785378fecdb408a571ee50d0afb8e,auto-predication of critical branches,558c6c66e4b02b9f07a703d0,Control-Flow Decoupling,Control Flow Decoupling (CFD) [8] is a branch pre-computation based solution which modifies the targeted branches by separating the control-dependent and controlindependent branch body using the compiler.###predictors [6]–[8].,other,providing context for Control Flow Decoupling method
3484,5aed147c17c44a4438153ea5,665c0dde22c2f8598869d690d59c9b6d84b07c01,domino temporal data prefetcher,5736982b6e3b12023e6fd15e,Efficiently Prefetching Complex Address Patterns,"While stride [3], [4], [5] or spatial [30], [31], [32], [33], [34] prefetchers are usually incapable of prefetching dependent misses [22] due to the lack of stride/spatial access patterns among dependent misses, temporal prefetchers can capture such misses, and hence, boost performance through substantially increasing###We include VLDP [34] because it has similarities with the lookup mechanism of our proposal.###We compare Domino prefetcher against ISB [13], VLDP [34], STMS [10], and Digram [21].",other,highlighting the advantages of temporal prefetchers over traditional methods
73,5ce3ad3fced107d4c65b6bd9,eeecea3097cf5629eb72a06e5caaf24d774adce7,Unsupervised label noise modeling and loss correction,5a260c8417c44a4ba8a31511,mixup: Beyond Empirical Risk Minimization.,"Loss correction approaches (Reed et al., 2015; Jiang et al., 2018b; Patrini et al., 2017; Zhang et al., 2018) modify either the loss directly, or the probabilities used to compute it, to compensate for the incorrect guidance provided by the noisy samples.###, 2015) and mixup data augmentation (Zhang et al., 2018) to deal with the closed-set label noise scenario.###Pushing the state-of-the-art one step forward by combining our approach with mixup data augmentation (Zhang et al., 2018).###Finally, mixup data augmentation (Zhang et al., 2018) has recently demonstrated outstanding robustness against label noise without explicitly modeling it.###The proposed approach clearly outperforms (Zhang et al., 2018) for different levels of label noise, obtaining consistent results with the CIFAR experiments.###Table 6 shows the results of the proposed approaches MDYR-H and MD-DYR-SH compared to mixup (Zhang et al., 2018) on TinyImageNet to demonstrate that our approach is useful far from CIFAR data.###, 2018)), and use the configuration reported in (Zhang et al., 2018) for mixup.###1, (b)(e) mixup (Zhang et al., 2018) and (c)(f) our proposed M-DYR-H.###Recently (Zhang et al., 2018) proposed a data augmentation technique named mixup that exhibits strong robustness to label noise.",impact-revealing,reporting on loss correction approaches and their effectiveness
3501,5feb068b91e011f5d3420813,e339c5d31ffc7029c1f72d567ac07b4606701c72,ALP-KD: Attention-Based Layer Projection for Knowledge Distillation,5b1643998fbcbf6e5a9bc3b6,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.,"We followed the same tradition in this paper and selected a set of eight GLUE tasks (Wang et al. 2018) including CoLA, MNLI, MRPC, QNLI, QQP, RTE, SST-2, and STS-B datasets to benchmark our models.",other,acknowledge the selection of benchmark tasks for model evaluation
148,5e09aa66df1a9c0c416bebf6,4e7dd1e79f0f13650b2612325e6ba8d206dc04fb,DeepGCNs: Can GCNs Go As Deep As CNNs?,5a9cb66717c44a376ffb8c0f,Deeper insights into graph convolutional networks for semi-supervised learning,"Recent work [19, 43, 52] suggests that GCNs do not scale well to deep architectures, since stacking multiple layers of graph convolutions leads to high complexity in back-propagation.###[19] studied the depth limitations of GCNs and showed that deep GCNs can cause over-smoothing, which results in features at vertices within each connected component converging to the same value.###In contrast, it is not yet clear how to properly train deep GCN architectures, where several works have studied their limitations [19, 43, 52].###This means that back-propagating through these networks causes oversmoothing, eventually leading to features of graph vertices converging to the same value [19].",impact-revealing,highlighting limitations in the scalability of GCNs
826,5550414745ce0a409eb39ec8,6010ebf22c6cc07e93c5335fa1d128be8c6b190b,understanding big data analytics workloads on modern processors,56d853e0dabfae2eee192b58,Clearing the Clouds: A Study of Emerging Workloads on Modern Hardware,"We adopt larger input data sets varying from 147 to 187 GB that are stored in both the memory and disk systems instead of completely storing data (only 4.5GB for Naive Bayes in [17]) in the memory system.###For the service workloads, our observations corroborate the previous work [17]: the L2 cache is ineffective.###And for each workload, we collect the performance data of the whole run time after the warm-up instead of a short period (180 seconds in [17]).###We ﬁrst characterize big data analytics workloads with a more pragmatic experiment approach in comparison with that of CloudSuite described in [17].###…applications share many inherent characteristics, which place them in a different class from desktop (SPEC CPU2006), HPC (HPCC), traditional service (SPECweb2005 and TPC-W), chip multiprocessors (PARSEC), and scale-out service (four among six benchmarks in ClousSuite paper [17]) workloads.###The state-of-the-art work of characterizing scale-out (data center) workloads on a micro-architecture level is Cloud-Suite [17].###• The signiﬁcant differences between the big data analytics workloads and the service workloads (four among six benchmarks in ClousSuite [17], SPECweb and TPC-W) in terms of processor pipeline stall breakdown: the big data analytics workloads suffer more stalls in the out-of-order part of the…###So reducing the capacity of last level cache properly may beneﬁt the performance, since a smaller last level cache can shorten last level cache hit latency and reduce L2 cache miss penalty, which corroborates previous work [17, 31].###…work proposed to evaluate data mining algorithms or evaluate clusters using data analytics workloads in different aspects, such as [33, 19, 12, 11, 17] and etc. Narayanan et al. [33] characterize traditional data analytics workloads on single node other then workloads running at data center scale.###• Corroborating previous work [17], both the big data analytics workloads and service workloads suffer from notable pipeline front end stalls.",impact-revealing,highlighting differences in big data analytics workloads and corroborating previous findings
3381,5c87a964da56296d04a90b97,a20bc2ecd8eb9bd735e6b3ba3d1a88f1c872d8eb,Shinjuku: Preemptive Scheduling for μsecond-scale Tail Latency,5390bded20f70186a0f481a5,Network Stack Specialization For Performance,"Optimized network stacks: There is significant work in optimizing network stacks, including polling based processing (DPDK [3]), multi-core scalability (mTCP [31]), modularity and specialization (Sandstorm [40]), and OS bypass (Andromeda [22]).",other,acknowledge existing work in optimizing network stacks
2754,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,559163420cf2e89307ca980e,Profiling a warehouse-scale computer,"Relative to alternative benchmarks, our microservices exhibit (1) a greater IPC diversity than Google’s services [1] and (2) a lower IPC than most widely-studied SPEC CPU2006 benchmarks.###The increasing user base and feature portfolio of web applications is driving precipitous growth in the diversity and complexity of the back-end services comprising them [1].###We reproduce selected data from published reports on SPEC CPU2017 [20], Cloud-Suite [21], and Google’s services [1,23] measured on Haswell, Westmere, and Haswell, respectively.###The microarchitectural trends we discover differ markedly from those of SPEC CPU2006/2017 [19,20], academic cloud workloads [21,22], and even some of Google’s major services [1,23].###Other works [1,95] propose architectural optimizations for diverse applications.###We contrast our measurements with some CloudSuite [21], SPEC CPU2006 [19], SPEC CPU2017 [20], and Google services [1,23] where possible.###We contrast our results with IPCs for commonly studied benchmark suites [20, 21] and published results for comparable Google services [1, 23].###Priorworks [1,21,23,66] typically ﬁnd current LLC sizes to be sufﬁcient to encompass server applications’ entire code footprint.###Kanev et al. [1] proﬁle different Google services and propose architectural optimizations.",other,highlighting differences in microservices performance compared to benchmarks
2669,5e5e191993d709897ce5087d,674b6321ae1d12c83f28ade1850a27256c20f0d4,Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset,5a73cb3517c44a0b303556e2,Scalable multi-domain dialogue state tracking,"As virtual assistants incorporate diverse domains, recent work has focused on zero-shot modeling (Bapna et al. 2017; Xia et al. 2018; Shah et al. 2019), domain adaptation and transfer learning techniques (Yang, Salakhutdinov, and Co-hen 2017; Rastogi, Hakkani-T¨ur, and Heck 2017; Zhu and Yu 2018).",other,acknowledge recent advancements in virtual assistant modeling
3859,5dd6604a3a55ac78684acf68,af54c890ffce96bae303310182be2ca301f2f97e,On Using SpecAugment for End-to-End Speech Translation,573697016e3b12023e5fb275,Audio Augmentation For Speech Recognition,"Inspired by the success of augmentation methods in ASR [16, 17], as a remedy to avoid overfitting while using lowresource translated speech data, we study the use of spectrogram augmentation (SpecAugment) for direct ST model.",other,highlighting the application of augmentation methods to improve model performance
712,53e9a45cb7602d9702d79e76,09bb082db1b675e0e80a3bd397ee59db298b9a22,Performance evaluation of exclusive cache hierarchies,557cd93a6feeaa8086da2830,On the inclusion properties for multi-level cache hierarchies,"technology, the larger the cache, the slower the cache will become [2], and larger caches increase the cost of manufacturing.###An inclusive cache system implies that the contents of the L1 cache be a subset of the L2 cache [2].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2329,5d79a6ff3a55ac5b650357fb,6fb4facc2d16c76047f7cd96af5a691ab16c08c5,Combining Prefetch Control and Cache Partitioning to Improve Multicore Performance,5a260c4617c44a4ba8a26da1,Application Clustering Policies to Address System Fairness with Intel_x0092_s Cache Allocation Technology,"[17] shows there is a strong positive correlation between its count and an application’s slowdown.###[17] groups cores into several clusters and each cluster occupies a separate partition.###They are compared with the best known algorithm, Dunn in [17], in Sec.###As [17] pointed out, as the number of concurrently running applications grows, the stall cycles caused by interference will dominate in total stall cycles experienced by the cores.###[17] proposes a clustering-based cache partitioning mechanism to improve the fairness of multi-core processors.###[17] used CAT to improve fairness of multi-programmed workloads.###In this case, CMM will use the ”Dunn” algorithm in prior work [17] for cache partitioning.",other,reporting prior findings on cache partitioning mechanisms
3958,5c7a561ff56def9798e6a297,abe4e92d6e4868115aef57a4eba97ec4d48ac124,language-adversarial transfer learning for low-resource speech recognition,573696086e3b12023e51ae96,Very Deep Multilingual Convolutional Neural Networks for LVCSR,"The transfer learning methods can be roughly classified into two categories: transferring bottleneck features [17], [23]–[26] and transferring model parameters [10], [11], [27].###The other kind of SHL-Models is proposed to use hidden layers to learn more language dependent information [23], [32].###A lot of variants of SHL-Models are proposed to train multilingual models [10], [23], [27], [32].",other,providing an overview of transfer learning methods
307,5f8ebbb99fced0a24b4e1966,d299c78121f39c3a4cd09e0994e47ec8cd0c20d6,Diversifying Search Results using Self-Attention Network,599c7987601a182cd2648373,Attention Is All You Need.,"network), such as Transformer [16] in the Neural Machine Translation (NMT) task, have achieved great successes on many NLP tasks.###So the self-attention components are modified with a sequence mask used in the original Transformer decoder structure.###Comparing with SetRank, DESA has got the following differences: (1) SetRank is not designed for search result diversification task, it’s Transformer encoder structure will be unable to take the subtopics into consideration.###Similar as the original Transformer encoder block, each of those self-attention encoder layers contains a dropout layer and a fully connected feed-forward network (denoted as FeedForward) with ReLU function as activation function.###Different from RNN, self-attention network will not model the sequence information explicitly, so the standard Transformer structure also includes an optional component of positional encoding to incorporate the sequence information.###We use the multi-layer encoder block of the Transformer to implement DESA’s self-attention component, based on the scaled dot-product attention function denoted as Attn follows: where 𝒒 , 𝑲 and 𝑽 denote the query, key and value matrices of the attention function.",impact-revealing,providing context on the Transformer architecture and its modifications
1444,,bdc89380a86721d143e57c7b85e97527a9b01978,Academic Advising at a Satellite Campus of a Large Multi-Campus University: A Qualitative Case Study Using Systems Theory Constructs,,,"###From a systems perspective, how does academic advising function on the satellite campus? In order to build on the research of Musser (2006), Hutchins’ (1996) ten basic principles of systems theory are used here to explain the way that the academic advising system functions on the satellite campus at MAU based on the discussion of the findings in Chapter 4.###Scholars of systems theory maintain that all problems in the sciences (physical and social) are fundamentally systemic in nature (Hutchins, 1996; Meadows, 2008; Wheatley, 2006).###From the perspective of systems theory, a system (such as advising) is defined by its behavior, not by its “rhetoric and stated goals” (Meadows, 2008).###93) Every system consists of multiple functional subsystems (Hutchins, 1996; Meadows, 2008).",impact-revealing,building on previous research to explain the academic advising system
2097,,e7c61b653a12ece87d6006b13ae5d39046154cf4,EXPLAINING THE EXPLANATORY GAP,,,"###This research was inspired by an apparently familiar phenomenon: “We frequently discover that a theory that seems crystal clear and complete in our head suddenly develops gaping holes and inconsistencies when we try to set it down on paper” (Rozenblit and Keil, 2002, p. 522).###For example, Keil’s illusion of explanatory depth (or “IED”) looks to be a kind of cognitive bias that applies only to explanatory reasoning, and not to reasoning about other domains such as facts or narratives (Keil and Wilson (2000); Rozenblit and Keil (2002); Keil (2005)).",impact-revealing,drawing inspiration from cognitive biases in explanatory reasoning
3311,5a4aef9e17c44a2190f7a4bd,1deb7f96fc92d5c9e04d2cbb277473fee878e144,Cascaded Pyramid Network for Multi-person Pose Estimation,5a73cb7417c44a0b3035a1fd,A Coarse-Fine Network For Keypoint Localization,"Top-down approaches [28, 18, 15, 9] interpret the process of detecting keypoints as a twostage pipeline, that is, firstly locate and crop all persons from image, and then solve the single person pose estimation problem in the cropped person patches.",other,describing the process of keypoint detection in top-down approaches
3277,57d063f6ac44367354296741,908f7931de8768786d9ef7d64f5a8156860709dd,Dynamic Pricing and Traffic Engineering for Timely Inter-Datacenter Transfers,552612de0cf23e25ac4649fc,Guaranteeing deadlines for inter-datacenter transfers,"SAM is related to recent work on multi-timestep traffic engineering [22, 34] but extends it in two ways: (1) it considers potentially non-linear link costs and (2) optimizes for social welfare.###Centralized traffic engineering (TE) techniques have been proposed to improve network utilization [18, 20] without affecting low latency traffic and with explicit support for deadlines [22, 34].",other,highlighting the extensions and improvements of SAM over existing traffic engineering methods
534,5f058d15dfae54570ec57ea1,fb93ca1e004cbdcb93c8ffc57357189fa4eb6770,Resnest: Split-attention networks,58d82fcbd649053542fd67d7,Aggregated Residual Transformations for Deep Neural Networks,"ResNeXt [61] adopts group convolution [32] in the ResNet bottle block, which converts the multi-path structure into a unified operation.###As in ResNeXt blocks [61], the feature can be divided into several groups, and the number of featuremap groups is given by a cardinality hyperparameter K.",impact-revealing,providing context for ResNeXt architecture
557,5a73cb6317c44a0b30358203,a072c2a400f62f720b68dc54a662fb1ae115bf06,tacotron: towards end-to-end speech synthesis,5550410f45ce0a409eb384f8,Sequence to Sequence Learning with Neural Networks.,"In this paper, we propose Tacotron, an end-to-end generative TTS model based on the sequence-to-sequence (seq2seq) (Sutskever et al., 2014) with attention paradigm (Bahdanau et al., 2014).###In this paper, we propose Tacotron, an end-toend generative TTS model based on the sequence-to-sequence (seq2seq) [6] with attention paradigm [7].",impact-revealing,introducing a new generative TTS model
875,53e9a4b1b7602d9702dd0241,2210492bd41923258997b3becd1989a93cf8b0a1,Replacing Testing with Formal Verification in Intel CoreTM i7 Processor Execution Engine Validation,6218afac5aee126c0f69a686,Symbolic Trajectory Evaluation,"Technically our verification work is carried out in the Forte verification framework, originally built on top of the Voss system [8].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2381,57d063e0ac44367354294777,9c62532d78d6eb31f7b0489b6bcb4c2baf517860,CASH: Supporting IaaS Customers with a Sub-core Configurable Architecture,573696756e3b12023e5818fc,A Reconfigurable Fabric For Accelerating Large-Scale Datacenter Services,"Microservers [4, 37, 59], data center optimized accelerators [19, 31, 43], and fine-grain reconfigurable processors [64] are all examples.",other,reporting examples of computing architectures
2210,5aed14d117c44a4438158af2,30f86d38f0660af5ea2e16d996434c72eee8c5ee,espnet: end-to-end speech processing toolkit,53e9b557b7602d970409073b,Spontaneous speech corpus of japanese,"The other experiments compare the performance of ESPnet with state-of-the-art ASR systems for the CSJ and HKUST tasks.###Although most of ASR recipes supported in ESPnet are standard English tasks, current ESPnet recipes deal with other languages including Japanese (CSJ), Mandarin Chinese (HKUST CTS), and other European languages through VoxForge.###Note that several benchmarks including HKUST and CSJ score comparable/superior performance to the state-of-the-art hybrid DNN/HMM systems based on lattice-free maximum mutual information training [27].###With these state-of-theart end-to-end ASR techniques, ESPnet also provides a number of recipes for major ASR benchmarks including Wall Street Journal (WSJ) [18], Librispeech [19], TED-LIUM [20], Corpus of Spontaneous Japanese (CSJ) [21], AMI [22], HKUST Mandarin CTS [23], VoxForge [24], CHiME-4/5 [25, 26], etc.###With these state-of-the-art end-to-end ASR techniques, ESPnet also provides a number of recipes for major ASR benchmarks including Wall Street Journal (WSJ) [18], Librispeech [19], TED-LIUM [20], Corpus of Spontaneous Japanese (CSJ) [21], AMI [22], HKUST Mandarin CTS [23], VoxForge [24], CHiME-4/5 [25, 26], etc.###This section discusses the experimental results of our three main tasks, WSJ, CSJ, and HKUST.",other,acknowledge performance comparison of ESPnet with ASR systems
3976,5eccb534e06a4c1b26a838ac,2709167f1c3a03fa5b970a665ea48ed243aab582,Designing Network Design Spaces,5a9cb65d17c44a376ffb820b,Regularized Evolution for Image Classifier Architecture Search,"We observe that REGNETS are surprisingly effective in this regime considering the substantial body of work on finding better mobile networks via both manual design [7, 21, 16] and NAS [30, 19, 14, 15].###AMOEBANET-C [19] 0.###, [30, 19, 14, 17, 15, 25].",other,highlighting the effectiveness of REGNETS in mobile network design
1297,,32cd263a694073d372bcf9079210ff450402739b,"The sympathoadrenal cell lineage: specification, diversification, and new perspectives.",,,"###…analysis is limited by the fact that multiple BMPs and different type I receptors may be involved in the process and that some of the relevant mouse mutants, BMP-4,7 or BMPRIA knockout mice (Luo et al., 1995; Mishina et al., 1995; Winnier et al., 1995) die at very early stages of their development.###In mammalian embryos the role of BMPs for SA development has not been investigated in vivo, since such an analysis is limited by the fact that multiple BMPs and different type I receptors may be involved in the process and that some of the relevant mouse mutants, BMP-4,7 or BMPRIA knockout mice (Luo et al., 1995; Mishina et al., 1995; Winnier et al., 1995) die at very early stages of their development.",impact-revealing,highlighting limitations in existing research on BMPs and their role in SA development
3840,5f7d8a8391e011346ad27d2b,3bb1e24eb3429f807397833105d1e137d9927767,SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup,5eede0b091e0116a23aafcd1,STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths,", 2020b), external weak supervision (Lison et al., 2020; Liang et al., 2020; Ren et al., 2020; Zhang et al., 2019; Yu et al., 2020) and active learning (Shen et al.###…methods (Clark et al., 2018; Chen et al., 2020b), ex-ternal weak supervision (Lison et al., 2020; Liang et al., 2020; Ren et al., 2020; Zhang et al., 2019; Yu et al., 2020) and active learning (Shen et al., 2017; Hazra et al., 2019; Liu et al., 2018; Fang et al., 2017; Gao et al., 2019).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
563,5550443b45ce0a409eb4c3b9,0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f,Towards End-To-End Speech Recognition with Recurrent Neural Networks,558c90d4e4b02b9f07a7d5e9,Speech recognition with deep recurrent neural networks,"2, a BRNN computes the forward hidden sequence −→ h , the backward hidden sequence ←− h and the output sequence y by iterating the backward layer from t = T to 1 , the forward layer from t = 1 to T and then updating the output layer: Combing BRNNs with LSTM gives bidirectional LSTM (Graves & Schmidhuber, 2005), which can access long-range context in both input directions.###If LSTM is used for the hidden layers the complete architecture is referred to as deep bidirectional LSTM (Graves et al., 2013).###1 illustrates a single LSTM memory cell.###The network had ﬁve levels of bidirectional LSTM hidden layers, with 500 cells in each layer, giving a total of ∼ 26.5M weights.###For the version of LSTM used in this paper (Gers et al., 2002) H is implemented by the following composite function: where σ is the logistic sigmoid function, and i , f , o and c are respectively the input gate , forget gate , output gate and cell activation vectors, all of which are the same size as the hidden vector h .###However we have found that the Long Short-Term Memory (LSTM) architecture (Hochreiter & Schmidhuber, 1997), which uses purpose-built memory cells to store information, is better at ﬁnding and exploiting long range context.###The combination of bidirectional LSTM and CTC has been applied to character-level speech recognition before (Eyben et al., 2009), however the relatively shallow architecture used in that work did not deliver compelling results (the best character error rate was almost 20%).###The spectrograms are processed by a deep bidirectional LSTM network (Graves et al., 2013) with a Connectionist Temporal Classiﬁcation (CTC) output layer (Graves et al., 2006; Graves, 2012, Chapter 7).",impact-revealing,describing the architecture and functionality of bidirectional LSTM networks
3604,5b1642d68fbcbf6e5a9b7e77,0be19fd9896e5d40222c690cc3ff553adc7c0e27,Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods,5550447f45ce0a409eb4e4df,Unequal Representation And Gender Stereotypes In Image Search Results For Occupations,", 2017), online news (Ross and Carter, 2011), web search (Kay et al., 2015) and advertisements (Sweeney, 2013).###Similar observations have been made in vision and language models (Zhao et al., 2017), online news (Ross and Carter, 2011), web search (Kay et al., 2015) and advertisements (Sweeney, 2013).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2903,5736986b6e3b12023e730129,424561d8585ff8ebce7d5d07de8dbf7aae5e7270,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,557c4e606feeaa8086d9926b,Scalable Object Detection Using Deep Neural Networks,"The MultiBox methods [26], [27] generate region proposals from a network whose last fully-connected layer simultaneously predicts multiple class-agnostic boxes, generalizing the “single-box” fashion of OverFeat.###Several papers have proposedways of using deep networks for predicting object bounding boxes [9], [25], [26], [27].",other,providing context on MultiBox methods for object detection
2421,5ac1829d17c44a1fda9180a3,632d25ef7914ce962d258920460a9405b8c4553a,Sequence-Based Multi-Lingual Low Resource Speech Recognition,58437725ac44360f1082fccd,Achieving Human Parity in Conversational Speech Recognition.,"State-of-the-art speech recognition systems with human-like performance [1, 2] are trained on hundreds of hours of well-annotated speech.",other,highlighting the requirements for state-of-the-art speech recognition systems
2517,5c234870da562935fc1d4da6,94bd59e507ba8496b36605be0f6740e5731e91d5,CounterMiner: Mining Big Performance Data from Hardware Counters,53e9b8d4b7602d97044b6e1f,Experiences and Lessons Learned with a Portable Interface to Hardware Performance Counters,"The measurement errors caused by MLPX have been observed for nearly two decades [29]–[31], [33], [34], [38], [54]–[56] and several approaches were proposed to reduce them.",other,acknowledging prior findings on measurement errors in MLPX
279,5d1eb9dbda562961f0b15d56,1e9c4b836c2693732961017a226780785b3612ba,Adversarial Representation Learning on Large-Scale Bipartite Graphs,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"y.Let femb be a general bipartitegraph embeddingmodelwithparametersθ,thentherepresentation of Hu and Hv is deﬁned as follow: Hu,Hv = femb(Xu,Bu,Xv,Bv;θ) (1) GNN-based methods (Kipf and Welling, 2017; Hamilton, Ying, and Leskovec, 2017) deﬁne neural propagation for representation learning on general graphs. When customized to bipartite graphs, the intra-domain connections are absent, so Eq. (1) is divided into two terms as H v→ u= ###depth increases. We contrast the performance of our algorithm against several unsupervised graph learning counter-parts: Node2Vec (Groverand Leskovec, 2016), VGAE (Kipf and Welling, 2016), GraphSAGE (Hamilton, Ying, and Leskovec, 2017), and ASGCN (Huanget al., 2018). For the evaluation, we apply a large-scale scale social network from Tencent Platform, and also construct three synthesized datasets based on the citation networks Co###at same node attributes should be assumed. Our work is also relevant to those GCNbased methods that deal with the scalability issue on largescale graphs, like the node-wise sampling method GraphSAGE (Hamilton, Ying, and Leskovec, 2017) and the ayerwise sampling method AS-GCN (Huang et al., 2018). Differ from these methods, our BGNN has better scale performance by proposinga cascaded training architecture. 3 Proposed model: BGNN Th###e the complexity of graph data, Graph Convolutional Networks (GCN)-based methods have a rapid development recently. Among them, GCN (Kipf and Welling, 2017), VGAE (Kipf and Welling, 2016), GraphSAGE (Hamilton, Ying, and Leskovec, 2017) show state-of-the-art performance in many graph representation learning applications. In general, these methods perform a convolution by aggregating the neighbor nodes’ information so that each node### and Xu. The embedding Hu (resp. Hv) merely captures one-hop topology structure of Bu (resp. Bv) as well as feature information from Xu and Xv. As presented in previous works (Kipf and Welling, 2017; Hamilton, Ying, and Leskovec, 2017), the one-hop aggregation is insufﬁcient to characterize diverse graph structures; hence a multi-hop mechanism (or equivalently a deep network) is in demand. Other than leveraging typical end-to-end ",impact-revealing,describing the performance of GNN-based methods in graph representation learning
1799,,3bc62daf94b5ef1f270e6f89c9b1c08a57a64c7d,Adjunctive Uses of the Radial Artery for Emergency Infrapopliteal Bypass in Patients Presenting With Acute Limb-Threatening Ischemia,,,"###are superior to saphenous vein grafts.(3,4) However, the reported use of the radial artery as a conduit for lower extremity revascularization is extremely rare, as its use as a primary graft conduit is limited by its short length.",impact-revealing,highlighting the rarity and limitations of radial artery use in revascularization
994,,58147333eeaf94516eddb6a1d37a6828475d2e71,"On some cracks in the ""engine"" of knowledge-creation : a conceptual critique of Nonaka and Takeuchi's (1995) model",,,"###Thus tacit knowledge can be acquired through a master-apprentice relationship, or learning by doing, and does not require the use of language (Nonaka & Takeuchi 1995: 62-3, 70, 85;  Nonaka et.al., 1994:  340).###They claimed (Nonaka & Takeuchi, 1995: 91) that the four knowledge conversion modes were validated by a survey of managers ( Nonaka et.al., 1994 ).",impact-revealing,highlighting the significance of tacit knowledge acquisition methods
3344,5c7a561ff56def9798e6a297,abe4e92d6e4868115aef57a4eba97ec4d48ac124,language-adversarial transfer learning for low-resource speech recognition,599c7988601a182cd2648ecb,Adversarial Multi-Criteria Learning For Chinese Word Segmentation,[36] utilize adversarial learning for Chinese word segmentation on various,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
873,53e9b6c4b7602d970424e729,579cf876bc66b2311f163c5a6d57079df505da54,Optimal bypass monitor for high performance last-level caches,53e9ba39b7602d970464970c,Bypass and insertion algorithms for exclusive last-level caches,A bypass and insertion algorithm for exclusive LLCs was presented recently [7].,impact-revealing,reporting recent findings on algorithm development
2233,5c6a37d03a69b1c9e12a9fc4,81b6d24e8f313fd88b0fe5ff6c21dd154fbe32d2,SimGNN: A Neural Network Approach to Fast Graph Similarity Computation,53e9b281b7602d9703d2a10c,A Novel Software Toolkit for Graph Edit Distance Computation.,"Since graphs from AIDS and LINUX are relatively small, an exponential-time exact GED computation algorithm named A* [36] is used to compute the GEDs between all the graph pairs.###2Although other variants of GED exist [36], we adopt this basic version.",other,describing the method used for graph edit distance computation
3475,5c9df4643cb210d271bea0dd,62c13867cc9d80639100dc7bd4151f728c27d9ab,efficient load value prediction using multiple predictors and filters,5b8c9f0f17c44af36f8b23e3,Vpsec: Countering Fault Attacks In General Purpose Microprocessors With Value Prediction,"proposed a hardware-only fault attack mitigation scheme that leverages the overlap between value predictors to reverse the trust model, trusting the predicted value over the faulted value [30], [31], [32].",other,reporting existing findings on hardware fault attack mitigation
2395,5f7fdd328de39f0828397e22,21e33bd0ad95ee1f79d8b778e693fd316cbb72d4,Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs,5e09a891df1a9c0c4168df72,Graph convolutional networks: a comprehensive review,"Besides the models mentioned above, there are various comprehensive reviews describing previously proposed architectures [42, 5, 41].",other,acknowledge existing reviews on architectures
3965,5ef0816891e0112aee042b88,5c126ae3421f05768d8edd97ecd44b1364e2c99a,denoising diffusion probabilistic models,5ac1829d17c44a1fda917e9e,Efficient Neural Audio Synthesis,"Generative adversarial networks (GANs), autoregressive models, flows, and variational autoencoders (VAEs) have synthesized striking image and audio samples [14, 27, 3, 58, 38, 25, 10, 32, 44, 57, 26, 33, 45], and there have been remarkable advances in energy-based modeling and score matching that have produced images comparable to those of GANs [11, 55].",other,highlighting advancements in generative models for image and audio synthesis
1280,,cbd40898c3d993a9e0111a56b595b4e91543d376,Chronic cerebral blood flow alterations in traumatic brain injury and sports-related concussions,,,"###It should be noted that Cr is commonly used as a concentration reference because it is relatively stable with negligible (or very small) changes with age, or in a variety of brain disorders (71,72).",impact-revealing,providing context for the use of Cr as a concentration reference
2679,5f75aa6a9fced0a24b64599d,69fb130409d48478fab0c4545dc2ff102a2630ef,The forward slice core microarchitecture,53e9b998b7602d97045880d6,"McPAT: an integrated power, area, and timing modeling framework for multicore and manycore architectures.","Dynamic power is calculated by combining the per-access power values with the activity factors obtained from the timing model, which are then added to the power consumption numbers provided by McPAT.###We estimate power consumption and chip area usingMcPAT [11] and CACTI v6.###We estimate power consumption and chip area using McPAT [11] and CACTI v6.###We use McPAT [11] to calculate InO and OoO core power consumption.",other,reporting methods for power consumption estimation
3634,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",55a4075065ce5cd7b3c1081b,Dynamic reconfiguration of human brain networks during learning.,"Interestingly, regions with strong signal in low and high graph frequency components overlap well with the regions known to contribute to better motor learning [172].",other,highlighting the relationship between graph frequency components and motor learning
3803,59ae3c262bbe271c4c71ea21,83e7654d545fbbaaf2328df365a781fb67b841b4,Enhanced LSTM for Natural Language Inference,58d82fcbd649053542fd62d2,A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference,"Sha et al. (2016) proposes a special LSTM variant which considers the attention vector of another sentence as an inner state of LSTM. Paria et al. (2016) use a neural architecture with a complete binary tree-LSTM encoders without syntactic information.###The parse trees used in this paper are produced by the Stanford PCFG Parser 3.5.3 (Klein and Man-ning, 2003) and they are delivered as part of the SNLI corpus.###The approach proposed by Munkhdalai and Yu (2016a) (2) 300D LSTM encoders (Bowman et al., 2016) 3.0M 83.9 80.6 (3) 1024D pretrained GRU encoders (Vendrov et al., 2015) 15M 98.8 81.4 (4) 300D tree-based CNN encoders (Mou et al., 2016) 3.5M 83.3 82.1 (5) 300D SPINN-PI encoders (Bowman et al., 2016) 3.7M 89.2 83.2 (6) 600D BiLSTM intra-attention encoders (Liu et al., 2016) 2.8M 84.5 84.2 (7) 300D NSE encoders (Munkhdalai and Yu, 2016a) 3.0M 86.2 84.6 (8) 100D LSTM with attention (Rocktäschel et al., 2015) 250K 85.3 83.5 (9) 300D mLSTM (Wang and Jiang, 2016) 1.9M 92.0 86.1 (10) 450D LSTMN with deep attention fusion (Cheng et al., 2016) 3.4M 88.5 86.3 (11) 200D decomposable attention model (Parikh et al., 2016) 380K 89.5 86.3 (12) Intra-sentence attention + (11) (Parikh et al., 2016) 580K 90.5 86.8 (13) 300D NTI-SLSTM-LSTM (Munkhdalai and Yu, 2016b) 3.2M 88.5 87.3 (14) 300D re-read LSTM (Sha et al., 2016) 2.0M 90.7 87.5 (15) 300D btree-LSTM encoders (Paria et al., 2016) 2.0M 88.6 87.6 (16) 600D ESIM 4.3M 92.6 88.0 (17) HIM (600D ESIM + 300D Syntactic tree-LSTM) 7.7M 93.5 88.6 Table 1: Accuracies of the models on SNLI.###The original SNLI corpus contains also “ the other ” category, which includes the sentence pairs lacking consensus among multiple human annotators.###More recently, Bowman et al. (2015) made available the SNLI dataset with 570,000 human annotated sentence pairs.###…NTI-SLSTM-LSTM (Munkhdalai and Yu, 2016b) 3.2M 88.5 87.3 (14) 300D re-read LSTM (Sha et al., 2016) 2.0M 90.7 87.5 (15) 300D btree-LSTM encoders (Paria et al., 2016) 2.0M 88.6 87.6 (16) 600D ESIM 4.3M 92.6 88.0 (17) HIM (600D ESIM + 300D Syntactic tree-LSTM) 7.7M 93.5 88.6 Table 1: Accuracies…###An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).###Our ﬁnal model achieves the accuracy of 88.6%, the best result observed on SNLI, while our enhanced sequential encoding model attains an accuracy of 88.0%, which also outperform the previous models.###5 (15) 300D btree-LSTM encoders (Paria et al., 2016) 2.###…have been developed since then (Bowman et al., 2016; Ven-drov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a; Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016).###We propose neural network models for natural language inference, which achieve the best results reported on the SNLI benchmark.###More speciﬁcally, we show that our sequential inference model achieves an accuracy of 88.0% on the SNLI benchmark.###A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a; Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016).###Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman et al., 2015, 2016; Munkhdalai and Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016).###Data The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) focuses on three basic relationships between a premise and a potential hypothesis: the premise entails the hypothesis ( entailment ), they contradict each other ( contradiction ), or they are not related ( neutral ).",other,reporting on the development and performance of neural network models for natural language inference
1521,,4fee5ad1bce32c976de8e0e4953546ca5af54119,Assessment of HIF-1alpha function and identification of a novel degradation mechanism,,,"###These studies have underscored the importance of these enzymes during the reoxygenation processes (Berra et al., 2003; D'Angelo et al., 2003; Marxsen et al., 2004; Stiehl et al., 2006).",impact-revealing,highlighting the significance of enzymes in reoxygenation processes
47,5b1643ba8fbcbf6e5a9bc5b5,8c1b00128e74f1cd92aede3959690615695d5101,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,599c7987601a182cd2648373,Attention Is All You Need.,"For the self-attention-layer, we adopt the multi-head attention mechanism deﬁned in (Vaswani et al., 2017a) which, for each position in the input, called the query, computes a weighted sum of all positions, or keys, in the input based on the similarity between the query and key as measured by the…###…(one of convolution, self-attention, or feed-forward-net) inside the encoder structure is wrapped inside a residual block. used extensively in Vaswani et al. (2017a), the combination of convolutions and self-attention is novel, and is signiﬁcantly better than self-attention alone and gives…###A positional encoding is added to the input at the beginning of each encoder layer consisting of sin and cos functions at varying wavelengths, as deﬁned in (Vaswani et al., 2017a).###Recently, attempts have been made to replace the recurrent networks by full convolution or full attention architectures (Kim, 2014; Gehring et al., 2017; Vaswani et al., 2017b; Shen et al., 2017a).",impact-revealing,describing the self-attention mechanism and its implementation
287,5db929b747c8f766461fa94f,2a6d160b529272964ce1a6707adf52f3d6ba4861,Diffusion Improves Graph Learning,53e9aa16b7602d9703385070,Heat kernel based community detection,"Graph diffusion and random walks have been extensively studied in classical graph learning [13, 14, 36, 37], especially for clustering [34], semi-supervised classification [12, 22], and recommendation systems [44].###Most importantly, there are fast approximations for both PPR [3, 77] and the heat kernel [34], with which GDC achieves a linear runtimeO(N).",impact-revealing,acknowledge existing research on graph diffusion and random walks
2495,59a03016b161e8ad1a7b6ed2,3d1f9a530e710fdd4e2313bda4c8a1f574e60ab6,neural factorization machines for sparse predictive analytics,599c7982601a182cd264583f,Item Silk Road: Recommending Items from Information Domains to Social   Users,"Later, the NCF framework was extended to model attribute interactions for attribute-aware CF [37].###For ranking task, we can optimize pairwise personalized ranking loss [29, 37] or contrastive max-margin loss [43].",other,acknowledge extensions and optimizations in collaborative filtering frameworks
3719,58d82fd2d649053542fd75bc,e0b207e96351671453aa8bf05b7225c8a340a0b2,towards end-to-end speech recognition with deep convolutional neural networks,53e99b94b7602d9702439f1d,Deep Maxout Neural Networks For Speech Recognition,"Another type of activation function which has been shown to improve the results for the task of speech recognition [16, 24, 25, 26] is the maxout function [27].",other,highlighting the effectiveness of the maxout function in speech recognition
2461,5ea0166591e01173f5bbf71e,a4238e80c279230679281b9d898f44371e3e737b,Assessing Mental Stress Based on Smartphone Sensing Data: An Empirical Study,573696db6e3b12023e5dacd0,Trajectories Of Depression: Unobtrusive Monitoring Of Depressive States By Means Of Smartphone Mobility Traces Analysis,Canzian et al. [18] use GPS data to predict people’s depression degree.,other,reporting prior findings on GPS data and depression prediction
3016,5b67b45517c44aac1c860884,9fa3e53b5937a0ec92499ed415e339ede6c92010,DeepInf: Social Influence Prediction with Deep Learning,53e9b6eeb7602d970427f098,Social Influence Analysis In Large-Scale Networks,"Examples of such work include pairwise influence [19, 39], topic-level influence [42], group formation [2, 38] and structural diversity [14, 29, 46].###Our study is closely related to a large body of literature on social influence analysis [42] and graph representation learning [22, 37].###With the global penetration of online and mobile social platforms, people have witnessed the impact of social influence in every field, such as presidential elections [7], advertising [3, 24], and innovation adoption [42].###Indeed, extensive work has been done on social influence prediction in the literature [26, 32, 42, 43].",other,highlighting the extensive research on social influence and its applications
80,5edf5dd891e011bc656deed4,ae0988c55095827384bf70e346e192a89216e75c,IterefinE: Iterative KG Refinement Embeddings using Symbolic Knowledge,5b67b46b17c44aac1c861fee,Type-Sensitive Knowledge Base Inference Without Explicit Type Supervision,"SCORING FUNCTION FOR TYPEE-X To incorporate the type inferences for entities generated by PSL-KGI in KG embeddings (the second stage), we modify the typed model [Jain et al., 2018] as follows: Instead of just using the implicit type embeddings, we concatenate them with embeddings of explicit types transferred from PSL-KGI.###, 2013] to KG embedding methods like type-ComplEx [Jain et al., 2018].###Taking a different approach [Jain et al., 2018] propose extending standard KG embeddings without explicit type supervision by representing entities as a two-part vector with one part encoding only the type information while the other one is a traditional vector embedding of the entity (and…###We have looked at the KG refinement task and methods for the same, from probabilistic rule based methods like PSL-KGI [Pujara et al., 2013] to KG embedding methods like type-ComplEx [Jain et al., 2018].###While these works have their own strengths and weaknesses, our focus in this paper is on the use of ontological rules (exemplified by PSL-KGI) and embeddings (we use ComplEx, ConvE and [Jain et al., 2018]).###…to Appendix A.3
To incorporate the type inferences for entities generated by PSL-KGI in KG embeddings (the second stage), we modify the typed model [Jain et al., 2018] as follows:
Instead of just using the implicit type embeddings, we concatenate them with embeddings of explicit types transferred…###Table 7: Weighted F1 scores on relation triples in the test set by [Jain et al., 2018] and TypeE-ComplEx.###In Table 7 we compare the performance of TypeE-ComplEx which has explicit type supervision with the unsupervised type-compatible embeddings-based method proposed by Jain et al. [Jain et al., 2018].###Taking a different approach [Jain et al., 2018] propose extending standard KG embeddings without explicit type supervision by representing entities as a two-part vector with one part encoding only the type information while the other one is a traditional vector embedding of the entity (and corresponding change to the relation embeddings as well).###(ii) Explicit type supervised models also outperform the implict type supervised models [Jain et al., 2018].###In addition, we also compare our explicitly supervised TypeE-X methods with the implicitly supervised embeddings proposed by [Jain et al., 2018].###The resulting framework called IterefinE is based on the observation that the mispredictions by the embeddings based methods are often due to the lack of type compatibility between the entities due to their typeagnostic nature [Xie et al., 2016, Jain et al., 2018].",impact-revealing,describing modifications to a scoring function for entity type inference
1082,,e06125a464b6f79e3f37667bc1521f5e47cff148,A Comprehensive Review of Design Goals and Emerging Solutions for Adaptive Instructional Systems,,,"###Social aspects of human-VH dialogue may also be reinforced by agent-based VHs using human information processor models like GOMS (goals, operators, FIGURE 7 A Simple Conversational Tree in GIFT. methods, and selectors; John & Kieras, 1996) to guide interaction between humans and agents.",impact-revealing,highlighting the role of agent-based virtual humans in dialogue
3374,5eabf3cd91e011664efc496f,916595fc4e701ac6125725408912a1fac3d7a60b,CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows,558ab523e4b031bae1f93377,On pipelining dynamic instruction scheduling logic,"Such in-ﬂight instructions are scheduled by the IQ which consists of the wakeup logic, select logic, and payload RAM [21], [22], [23], [24].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1874,,b5c437b8fc063126c8c4237445235237446f8edf,Meta-integration for synthesizing data in a systematic mixed studies review: insights from research on autism spectrum disorder,,,"###Despite the Whittemore and Knafl (2005) description of stages in data analysis, namely, reduction, display, comparison, conclusion and verification, this work provides more concrete descriptions.###Whittemore’s paper explains that the ‘data analysis stage’ of an integrative review requires, that ‘‘data from primary sources are ordered, coded, categorized, and summarized into a unified and integrated conclusion about the research problem’’ (Whittemore and Knafl 2005, p. 550).###On a final note, the Whittemore and Knafl (2005) description of stages in data analysis, includes a final step of verification.###In 2005 Whittemore and Knafl published a paper presenting the concept of integrative reviews that brought together the combination of diverse methodologies in one review (Whittemore and Knafl 2005).###…in the Whittemore et al. paper was inspired by Miles and Huberman ‘Qualitative Data Analysis’ (Miles and Huberman 1994) the stages of data analysis in an integrative review consists of data reduction, data display, data comparison, conclusion drawing, and verification (Whittemore and Knafl 2005).###Even though the integrative review allows inclusion of experimental and non-experimental research and may combine theoretical as well as empirical literature (Whittemore and Knafl 2005), no specific designs for integration of QUAN and QUAL methods were presented.###Whittemore and Knafl (2005) utilize five stages: (1) problem identification, (2) literature search, (3) data evaluation, (4) data analysis and (5) presentation.###Despite the pioneering work of Whittemore and Knafl (2005), Sandelowski et al. (2006), Heyvaert et al. (2013) and Pluye and Hong (2014) exploring procedures for systematic reviews when there are QUAL, QUAN and MM papers, a key gap in the literature has been the lack of practical methodological…###The work of Whittemore and Knafl (2005) contributed to the idea of combining varied perspectives on a phenomenon and was related to previous work from Cooper and Cooper (1998) and Kirkevold (1997).###Despite the additional literature on specific steps, Whittemore and Knafl (2005), Sandelowski et al. (2006), Heyvaert et al. (2013) and Pluye and Hong (2014) cover the full spectrum of procedures.###Still, the method of data analysis gave a hint about elements of relevant synthesis procedures as ‘‘synthesis of important elements or conclusions of each subgroup into an integrated summation of the topic or phenomenon’’ was emphasized (Whittemore and Knafl 2005).###In addition, Whittemore and Knafl (2005) did not refer to any specific designs for integration of QUAN and QUAL papers and used the nomenclature of methodologies instead of a nomenclature related to MM.###In short, Whittemore and Knafl (2005) highlights the advantage of comprehensive understanding of a phenomenon, Sandelowski et al. (2006) provide detailed information about differences in QUAN and QUAL synthesis procedures while Heyvaert et al. (2013), Pluye and Hong (2014) and Cochrane (Higgins and…",impact-revealing,highlighting the contributions and limitations of Whittemore and Knafl's work in data analysis
1030,,9216e9aa190a8c6646d33c7dd40b91681bb7a141,"PERSONALITY PROCESSES AND INDIVIDUAL DIFFERENCES Motivated Response Styles : The Role of Cultural Values , Regulatory Focus , and Self-Consciousness in Socially Desirable Responding",,,"###Self-deceptive enhancement is the tendency to describe oneself in an inflated yet honestly held manner and to see oneself in a positive, overconfident light, and is motivated by the desire to see oneself as competent and self-reliant (Paulhus & John, 1998).###Moreover, selfdeceptive exaggeration of positive skills and abilities among promotion-focused individuals makes desired achievements seem more attainable (Paulhus & John, 1998; Taylor & Brown, 1988).###In addition, Yik, Bond, and Paulhus (1998; also see Paulhus & John, 1998) found that personality variables associated with self-deceptive enhancement and impression management loaded on two distinct factors in a Chinese sample from Hong Kong.",impact-revealing,providing context and background on self-deceptive enhancement
656,53e99b1bb7602d97023b2390,813c5f11e2ef0305a9cc9d9e16841de93f085fb9,Linearizing irregular memory accesses for improved correlated prefetching,53e9bc6eb7602d97048eba78,Data Cache Prefetching Using a Global History Buffer,"PC localization has been used to improve the accuracy and coverage of correlation-based prefetchers [29, 25, 28, 39, 38], but until now, the combination of PC localization and address correlation has been too expensive to be practically considered.###We also show that the ISB is su­perior to two other recent prefetchers, SMS [39], which exploits spatial 
locality, and PC/DC [28, 13], which uses delta correlation instead of address correlation.###Nesbit and Smith introduce the GHB as a general structure for prefetching streams of temporally correlated memory requests [28].###Recent solutions use the Global History Bu.er (GHB) [28], which organizes correlation 
information by storing recent memory accesses in a time-ordered circular history bu.er; a spatially organized 
index table is used to .nd addresses within the history bu.er (see Figure 1).###Using Nesbit and Smith’s terminology [28], in which the name before the slash describes the reference scheme and the name after the slash describes the type of correlation that is used, a G/AC prefetcher trains on a Global reference stream and uses Address Correlation.###In addition to the reduced memory tra.c provided 
by our caching scheme, the ISB enjoys several other signi.cant bene.ts: Improved Prediction Capability: 
Unlike GHB-based so­lutions (see Section 3), the ISB can use PC localiza­tion, a technique that segregates 
the prefetcher input into multiple streams based on the PC of the loading instruction, which is known 
to improve coverage and accuracy [28, 39, 38, 25].###Unfortunately, this proposed caching 
scheme is ill-suited to temporally organized structures such as the GHB.###We also show that the ISB is superior to two other recent prefetchers, SMS [39], which exploits spatial locality, and PC/DC [28, 13], which uses delta correlation instead of address correlation.###Third, we simulate Nesbit and Smith’s PC/DC prefetcher, which which learns the deltas, or differences, between consecutive memory addresses [28].###Recent solutions use the Global History Buffer (GHB) [28], which organizes correlation information by storing recent memory accesses in a time-ordered circular history buffer; a spatially organized index table is used to find addresses within the history buffer (see Figure 1).###For example, assume in Figure 
1 that physical addresses B, X, and D reside on the same page; we see that these addresses are scattered 
throughout the history bu.er and are likely to appear multiple times in the history bu.er, so there is 
no e.cient way to evict these entries from the history bu.er  Figure 1: Address correlation using the 
GHB. when their TLB entry is evicted, nor is it easy to reuse the scattered evicted entries of the history 
bu.er.###Rather than use address correlation, other GHBbased prefetchers use delta correlation [28, 27], whose space requirements are dramatically smaller, but we show that for irregular accesses, delta correlation leads to low coverage and accuracy.###For address correlation, GHB-based prefetchers can amortize the cost of o.-chip meta-data access by fetching 
long temporal streams [43, 9].###• Improved Prediction Capability: Unlike GHB-based solutions (see Section 3), the ISB can use PC localization, a technique that segregates the prefetcher input into multiple streams based on the PC of the loading instruction, which is known to improve coverage and accuracy [28, 39, 38, 25].###Nesbit and Smith introduce the GHB as 
a general struc­ture for prefetching streams of temporally correlated mem­ory requests [28].###As a result, GHB-based designs forsake either PC localization [43] or address correlation [27, 28, 12], sacrificing significant coverage for design simplicity.###However, 
when used to record address correlation [42], the GHB is quite large, requiring about 4 MB of o. chip 
storage for scienti.c workloads and about 48 MB for commercial server workloads.###Rather than use address correlation, other GHB­based prefetchers use delta correlation 
[28, 27], whose space requirements are dramatically smaller, but we show that for irregular accesses, 
delta correlation leads to low coverage and accuracy.",impact-revealing,highlighting the limitations and challenges of existing prefetching methods
647,58d82fced649053542fd7453,2f85b7376769473d2bed56f855f115e23d727094,wasserstein gan,58d82fced649053542fd69ba,Towards Principled Methods for Training Generative Adversarial Networks.,"In this paper, we direct our attention on the various ways to measure how close the model distribution and the real distribution are, or equivalently, on the various ways to deﬁne a distance or divergence ρ ( P θ , P r ).###This highlights the fact that the KL, JS, and TV distances are not sensible cost functions when learning distributions supported by low dimensional manifolds.###It is then unlikely that the model manifold and the true distribution’s support have a non-negligible intersection (see [1]), and this means that the KL distance is not deﬁned (or simply inﬁnite).###Then, there is a solution f : X → R to the problem and we have when both terms are well-deﬁned.###Note the signiﬁcant degree of mode collapse in the GAN MLP.###This is a very high amount of noise, so much that when papers report the samples of their models, they don’t add the noise term on which they report likelihood numbers.###Furthermore, we showed that the corresponding optimization problem is sound, and provided extensive theoretical work highlighting the deep connections to other distances between distributions.",impact-revealing,highlighting the limitations of traditional distance measures in model distribution learning
3174,5db9295f47c8f766461f5135,2c1006c856fefdbd6cd710e840e57153f2d6cd04,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,53e9a766b7602d970309e558,Practical Bilevel Optimization: Algorithms and Applications,"More specifically, it can be regarded as an instance of the more general bilevel optimization problem [13, 3].",other,providing context for a specific optimization problem
747,5fe1ccc591e0119a161edd6a,4ae59c8ec126f57decc00cfc8474cc9e3c875b7f,LieTransformer: Equivariant self-attention for Lie Groups,573696116e3b12023e52463f,Group Equivariant Convolutional Networks.,"The group equivariant convolution (Cohen & Welling, 2016; Cohen et al., 2018; Finzi et al., 2020; Romero et al., 2020) is an example of such a group equivariant map that has been studied extensively.###Among works that deal with equivariant self-attention, we are the ﬁrst to propose a methodology for general groups and domains (unspeciﬁed to 2D images (Romero et al., 2020; Romero & Cordonnier, 2021) or 3D point clouds (Fuchs et al., 2020)).###…kernel or attention module are relaxed at the cost of an increased dimensionality of the input to the neural network (Cohen & Welling, 2016; Cohen et al., 2018; Es-teves et al., 2018; Finzi et al., 2020; Bekkers, 2020; Romero & Hoogendoorn, 2020; Romero et al., 2020; Hoogeboom et al., 2018).###Among works that deal with equivariant self-attention, we are the first to propose a methodology for general groups and domains (unspecified to 2D images (Romero et al., 2020; Romero & Cordonnier, 2021) or 3D point clouds (Fuchs et al.###However with lifting, the equivariant map is defined between the space of functions/features on G, and aforementioned constraints on the convolutional kernel or attention module are relaxed at the cost of an increased dimensionality of the input to the neural network (Cohen & Welling, 2016; Cohen et al., 2018; Esteves et al., 2018; Finzi et al., 2020; Bekkers, 2020; Romero & Hoogendoorn, 2020; Romero et al., 2020; Hoogeboom et al., 2018).",impact-revealing,highlighting the novelty of the proposed methodology for equivariant self-attention
3031,5db929ff47c8f766461fd7e4,a73051e08af289a50ef8ed53e69f91c189dd01e5,Induction Networks for Few-Shot Text Classification,58437725ac44360f1082fd77,Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification.,"Early studies (Salamon and Bello, 2017) applied data augmentation and regularization techniques to alleviate the overfitting problem caused by data sparseness, only to a limited extent.",other,acknowledge limitations of early studies in addressing overfitting
2635,599c7945601a182cd2629f72,000178cd12c8a6e5da8215b6365fae03c20fd18d,End-to-End Representation Learning for Correlation Filter Based Tracking,573696f46e3b12023e5f115e,Conditional Random Fields as Recurrent Neural Networks,"When the solution to the optimization problem is obtained iteratively, an alternative is to treat the iterations as a Recurrent Neural Network, and to explicitly unroll a fixed number of iterations [36].",other,providing context for optimization problem solutions
1597,,74093e2fd3176f6e64fa7bbb21275ba03e71cc49,The Central Role of RMTg GABA Neurons in Morphine-induced Locomotion,,,"###RMTg neurons express high levels of μ-OR and nociception reeptors (Jhou et al., 2009a, 2012; Jalabert et al., 2011).###VTA DA neurons are disinhibited, however, this alone is not sufficient for sustained DA efflux (Jalabert et al., 2011).###RMTg neurons are immunoreactive against μ-OR and are also inhibited by opioid administration (Alexander et al., 2009; Jhou et al., 2009a, 2012; Ikemoto, 2010; Jalabert et al., 2011; Lecca et al., 2011, 2012).###, 2009b) and are inhibited by opioid administration (Ikemoto, 2010; Jalabert et al., 2011; Lecca et al., 2012, 2012).###…the neuropeptide nociceptin (Jhou et al., 2012), high levels of μ-OR immunoreactivity
20 CHAPTER 1: General Introduction
(Jhou et al., 2009a, 2012; Jalabert et al., 2011), and high expression of Fos and related immediate early genes following the administration of psychostimulants and certain…###…vivo, and optogenetic electrophysiological approaches have all demonstrated that morphine excites VTA DA neurons by targeting receptors localized to RMTg cell bodies as well as their terminals within the VTA (Jalabert et al., 2011; Lecca et al., 2011; Matsui and Williams, 2011; Lecca et al., 2012).###Inhibition of RMTg neurons led to increased DA cell activity (Jalabert et al., 2011), while RMTg electrical stimulation strongly decreased DA activity (Jhou et al., 2009a; Hong et al., 2011; Lecca et al., 2011, 2012; Matsui and Williams, 2011).###by targeting receptors localized to RMTg cell bodies as well as their terminals within the VTA (Jalabert et al., 2011; Lecca et al., 2011; Matsui and Williams, 2011; Lecca et al., 2012).###Parallel to previously observed in VTA DA neurons (Hong et al., 2011; Jalabert et al., 2011; Lecca et al., 2011, 2012; Matsui and Williams, 2011), RMTg stimulation decreases while RMTg inhibition increases SNc DA cell activity (Bourdy et al.###Inhibition of RMTg neurons led to increased DA cell activity (Jalabert et al., 2011), while RMTg electrical stimulation strongly decreased DA activity (Jhou et al.###While inhibition of RMTg neurons leads to increased VTA DA neuronal activity (Jalabert et al., 2011), RMTg stimulation strongly decreases VTA DA neuronal activity (Jhou et al.###Previous studies have found that inhibition of RMTg GABA neurons (with morphine, WIN55 or muscimol) resulted in EPSPs in VTA DA neurons (Lecca et al., 2010, 2011; Jalabert et al., 2011).###(Jhou et al., 2009a, 2012; Jalabert et al., 2011), and high expression of Fos and related immediate early genes following the administration of psychostimulants and certain aversive###RMTg neurons are immunoreactive against µ-OR and are also inhibited by opioid administration (Alexander et al., 2009; Jhou et al., 2009a, 2012; Ikemoto, 2010; Jalabert et al., 2011; Lecca et al., 2011, 2012).",impact-revealing,reporting findings on RMTg neurons and their role in dopamine activity
3488,5dcd263a3a55ac58039516c5,add2f205338d70e10ce5e686df4a690e2851bdfc,Momentum contrast for unsupervised visual representation learning,573696f46e3b12023e5f160a,Unsupervised Visual Representation Learning by Context Prediction,"Some pretext tasks form pseudo-labels by, e.g ., transformations of a single (“exemplar”) image [17], patch orderings [13, 45], tracking [59] or segmenting objects [47] in videos, or clustering features [3, 4].###…to measure the difference between a model’s prediction and a ﬁxed target, such as reconstructing the input pixels ( e.g ., auto-encoders) by L1 or L2 losses, or classifying the input into pre-deﬁned categories ( e.g ., eight positions [13], color bins [64]) by cross-entropy or margin-based losses.###The RelPos (relative position) [13] result is the best single-task case in the Multi-task paper [14].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
370,5f7fdd328de39f0828397c88,edcb65ea0954067d9137599423790fbd331de7b3,how hard is to distinguish graphs with graph neural networks,5e5e18a093d709897ce21291,What graph neural networks cannot learn: depth vs width,"The aforementioned insights can be pessimistic in the non-anonymous case, where permutation equivariance is either learned from data [22, 23] or obtained by design [24].###Even further, it is unclear whether depth and width needs to grow with the number of nodes solely in the worst-case (as proven in [23]) or with certain probability over the input distribution.###anonymous MPNN are universal and can solve graph isomorphism [23, 5], as well as that they can learn to be permutation invariant [22].###Along those lines, recent work provided evidence that the power of MPNN grows as a function of depth and width for certain graph problems [23], showing that (both anonymous and non-anonymous) MPNN cannot solve many tasks when the product of their depth and width does not exceed a polynomial of the number of nodes.###Communication capacity is an effective generalization of the previously considered product between depth and width [23], being able to consolidate more involved properties, as well as to characterize MPNN with global state [8, 9, 28] and adaptive architecture [29–32].###With node features acting as identifiers, MPNN were shown to become universal in the limit [23], which implies that they can solve the graph isomorphism testing problem if their size is allowed to depend exponentially on the number of nodes [5].###In addition, the proposed lower bounds rely on a new technique which renders them applicable not only to worst-case instances [23], but in expectation over the input distribution.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
967,,83699b2b54edeba78a3f6e3b45c52e18c0290a64,"Information Sharing, Advice Provision or Delegation: What Leads to Higher Trust and Trustworthiness?",,,"###retailer; this method has been commonly used as a means of obtaining a second-mover's full range of behavior given the di erent possible decisions of the rst-mover (e.g., Fehr and Fischbacher 2004; Falk et al. 2008; Charness et al. 2012).###Fo
r pe
rs on
al u
se o
nl y,
a ll
ri gh
ts r
es er
ve d.
Falk et al. 2008, Charness et al. 2012).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2926,5dc5488edf1a9c0c41511e82,d33e7907ae6e8a0c8396822df09806661de85710,scaling the capacity of memory systems: evolution and key approaches,53e9bcc0b7602d970493ff80,The implications of working set analysis on supercomputing memory hierarchy design.,"…fill the gap between the slower main memory and the large data capacity demands of the applications [37], but due to the relative limited size of caches, memory accesses are still very frequent with applications often still requiring to access a byte of memory for every operation computed [54].",other,highlighting the challenges of memory access in applications
2225,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,5a260c3517c44a4ba8a251a1,Multi-channel BiLSTM-CRF Model for Emerging Named Entity Recognition in Social Media.,"Unsuper-vised cross-domain adaptation aims to transfer knowledge learned from high-resource domains (source domains) to boost performance on low-resource domains (target domains) of interests such as social media messages (Lin et al., 2017).",other,highlighting the goal of unsupervised cross-domain adaptation
145,5cf48a40da56291d582a2f8e,44842bba66366522de782f537d9bc61d8868bf08,Revisiting Graph Neural Networks: All We Have is Low-Pass Filters,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,"We then characterize the graph neural networks solution to this problem and provide insights to the mechanism underlying the most commonly used baseline model GCN [15], and its simplified variant SGC [26].###In semi-supervised vertex classification, we observe that the parameters of a graph convolutional layer (GCN) [15] only contribute to overfitting.###Recently, graph neural networks for vertex classification and graph isomorphism test have achieved excellent results on several benchmark datasets and continuously set new stateof-the-art performance [1, 15, 24, 27].###Started with the early success of ChebNet [5] and GCN [15] at vertex classification, many variants of GNN have been proposed to solve problems in social networks [11, 29], biology [24, 25], chemistry [7, 9], natural language processing [2, 30], computer vision [19], and weakly-supervised learning [8].###GCN [15] Graph Convolutional Neural Network ($) is the most commonly used baseline.###Theorem 7 implies that, under Assumption 1, both gfNN and GCN [15] have similar high performance.###In contrast to the recent design principle of graph neural networks [1, 15, 27], our results suggest that the graph convolution layer is simply low-pass filtering.###The product (I − L̃)X is understood as features averaging and propagation [11, 15, 26].",impact-revealing,providing insights into graph neural networks and their performance
1977,,be06eda151141dd2798f037299d636a5e738be50,Predictive Modeling of Seasonal Mosquito Population Patterns with Neural Networks,,,"###ANNs models are using empirical and semi-parametric methods which are inspired by human brain to simulate complex functions [15,16].",impact-revealing,describing the methodology of ANN models
1097,,d6ea3aac8a79562854ca34bddc28af670a3c2438,Directional NMF for joint source localization and separation,,,"###TF masking [12] is motivated by the disjointness of speech in the TF domain [13].###When multiple (K) signals are active simultaneously, we can still apply the one-source model given that the signals are approximately pair-wise disjoint in the TF plane [13].",impact-revealing,providing context for TF masking in speech processing
388,573695fd6e3b12023e510ff5,06c06885fd53b2cbd407704cf14f658842ed48e5,deeply-recursive convolutional network for image super-resolution,573697836e3b12023e66a525,Recurrent convolutional neural network for object recognition,"To overcome overﬁtting, Liang and Hu [17] uses a recurrent layer that takes feed-forward inputs into all un-folded layers.###This is in accordance with the limited success of previous methods using at most three recursions so far [17].",impact-revealing,reporting a method to overcome overfitting
1638,,3d654e6999606481b90271406b77d582ebe5d250,Towards the identification of structural determinants of toxicity of amorphous silica nanoparticles and carbon nanotubes: an in vitro study,,,"###an active one and is reinforced by immune mechanisms preventing future response, like making reactiveT cells permanently unresponsive (anergic) or inducing regulatory T cells (Treg) which maintain tolerance by secreting immunosuppressive cytochines [41-42] .",impact-revealing,providing context on immune mechanisms
1320,,e10f2ea0ade5232270e7420060c349017b4adc0b,Comparison of methods for dealing with missing values in the EPV‐R,,,"###Schafer and Graham (2002) suggest that this procedure can lead to biased estimates.###This recommendation is consistent with the conclusions provided by other authors for the treatment of missing values (Schafer & Graham, 2002).###Item-mean imputation is not recommended either, because it does not consider the results of the person, it reduces the variability of the analyzed variable, and it can affect the relations with other variables (Schafer & Graham, 2002).",impact-revealing,highlighting the potential biases in estimation procedures and supporting the recommendation against item-mean imputation
3289,5e5e18ca93d709897ce315f0,68f86237dadcf2f570f0cd5b5e56161693619a74,Residual Energy-Based Models for Text Generation,5bdc315017c44a1f58a05bba,Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency,"With the theoretical guarantee of NCE, we can show that the optimum of the above objective is reached at data distribution with inﬁnite amount of data and model with enough capacity, which is also proved in Ma & Collins (2018) 2 .###, 2015; 2017; Wang & Ou, 2017; 2018a; Parshakova et al., 2019). In particular, our residual modeling form and the training algorithm is the same as in Wang & Ou (2018b), where they used an LSTM as the generator and a CNN-LSTM as the energy function, and showed significant gains compared to LSTM baselines in speech recognition.###Instead, we train our residual energy function using Noise Contrastive Estimation (NCE) (Gutmann & Hyv¨arinen, 2010), and more speciﬁcally its conditional version (Ma & Collins, 2018).###In particular, we adopt the conditional noise contrastive estimation (NCE) objec-tive (Ma & Collins, 2018; Gutmann & Hyv¨arinen, 2010) to our residual model energy function and then sample from the joint model using importance sampling.###While Ma & Collins (2018) used conditional NCE to predict the next word in a sequence, we apply it to produce a whole sequence at once with the pretrained auto-regressive language model as the noise distribution.",other,describing the method and its theoretical guarantees
289,5e5e18ad93d709897ce2654c,19605cad33f79d3070b7c4b24aa49653ab7c90da,Inductive Matrix Completion Based on Graph Neural Networks,53e99b31b7602d97023d0f07,Provable Inductive Matrix Completion.,"To accurately predict missing entries, IMC methods have strong constraints on the content quality, which often leads to inferior performance when high-quality content is not available.###For ML-100K, we compare against matrix completion (MC) (Candès & Recht, 2009), inductive matrix completion (IMC) (Jain & Dhillon, 2013), geometric matrix completion (GMC) (Kalofolias et al., 2014), as well as GRALS, sRGCNN, GC-MC, F-EAE and PinSage.###To make matrix completion inductive, Inductive Matrix Completion (IMC) has been proposed, which leverages content (side information) of users and items (Jain & Dhillon, 2013; Xu et al., 2013).###In IMC, a rating is decomposed by rij = x>i Qyj , where xi and yj are content feature vectors of user i and item j, respectively, and Q is a learnable matrix modeling the feature interactions.",impact-revealing,highlighting constraints and performance issues in IMC methods
3022,573696026e3b12023e516718,f8e79ac0ea341056ef20f2616628b3e964764cfd,"You Only Look Once: Unified, Real-Time Object Detection",5550472145ce0a409eb64af9,Edge Boxes: Locating Object Proposals From Edges,"These classifiers or localizers are run either in sliding window fashion over the whole image or on some subset of regions in the image [34, 15, 38].",other,providing context on classifier operation in image processing
207,5c5ce4fd17c44a400fc38abb,d524f10d653ba09e36456475da0aed92d244f795,Image Super-Resolution As A Defense Against Adversarial Attacks,599c797a601a182cd2641df7,MagNet: a Two-Pronged Defense against Adversarial Examples.,"Closely related to our approach are the Defense-GAN [38] and MagNet [26], which first estimate the manifold of clean data to detect adversarial examples and then apply a mapping function to reduce adversarial noise.",impact-revealing,acknowledge related approaches in adversarial example detection
506,5ecbc7639fced0a24b502b60,326974bd97dea3278515bcb3551a59e304d6d755,Improving Low-Resource Speech Recognition Based on Improved NN-HMM Structures,53e9aef1b7602d970391c7bd,Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups,"This makes automatic speech recognition with the goal of human-computer interaction popular, and it has been a research hotspot in recent decades [1].",impact-revealing,highlighting the popularity and significance of automatic speech recognition in human-computer interaction
2314,5efdaf7b91e01191d3d28242,6ff1eb9cdf64a464bf43b54d852456e9ddf55b28,Debiased Contrastive Learning,573696ce6e3b12023e5ce561,Convex Formulation for Learning from Positive and Unlabeled Data.,"Common applications of PU learning are retrieval or outlier detection [ 10 – 12 ].###Our approach is related to unbiased PU learning , where the unlabeled data is used as negative examples, but down-weighted appropriately [ 10 , 11 , 22 ].",other,acknowledge common applications and related approaches in PU learning
3178,5e2d653a3a55acc8374367fd,36ff7927f9049d37b314c2a114769b517b3f5f7a,Joint Recognition of Names and Publications in Academic Homepages,5bdc315017c44a1f58a05bc3,Joint Learning for Targeted Sentiment Analysis.,"To accomplish this, a straightforward method is to train a model for the two tasks together by solving a joint optimization problem, i.e., minimising the total loss of the two tasks, or using simple concatenation procedures when training [6, 13, 32].###• Joint-Concat is resulted from replacing the AM and PM modules with a concatenation procedure similar to Ma et al. [13] and Hashimoto et al. [6].###A straightforward method to learn a model for the two tasks jointly is to train them together by minimising the total loss of the two tasks or simply concatenating the representation of publication and person name when training [6, 13].",other,describing a method for joint optimization in model training
3162,5f7ee07491e011a5faf0feb2,c54fb59319288d0d17ad09e6b7dfee6d042f83a8,Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer,5cede105da562983788e48c5,RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space.,"For base-line methods, besides the single-embedding TransE (Bordes et al., 2013) and RotatE (Sun et al., 2019b), we also include DistMult (Yang et al., 2015), TransD (Ji et al., 2015), and HolE (Nickel et al., 2016).###TransE models relations as translations between head entities and tail entities in a Euclidean space, while RotatE models relations as rotations in a complex space.###We here consider two representative triple scoring techniques: TransE (Bordes et al., 2013) and RotatE (Sun et al., 2019b).###The results on the Greek and Japanese KG are thus omitted for RotatE+PARIS.###We also include a baseline named RotatE+PARIS , which trains RotatE on 5 KGs and uses the representative non-embedding symbolic entity alignment tool PARIS (Suchanek et al., 2011) for entity matching.###Particularly, on the low-resource Greek KG, KEnS b (RotatE) improves Hits @1 by as much as 13.0% over its single-KG counterpart.###RotatE (Sun et al., 2019b) employs a complex embedding space and models the relation r as the rotation instead of translation of the complex vector h toward t , which leads to the SOTA performance on KG embedding.",other,reporting baseline methods and their characteristics
2459,5c8ddce94895d9cbc6a97820,8fb5a6fe93a3d19ffee50677c0ae563e3377d2d4,Chinese NER Using Lattice LSTM,5d9edc1647c8f76646032985,Named Entity Recognition with Bidirectional LSTM-CNNs,"In this respect, our automatic lexicon also played to some extent the role of a gazetteer (Ratinov and Roth, 2009; Chiu and Nichols, 2016), but not fully since there is no explicit knowledge in the lexicon which tokens are entities.###The current stateof-the-art for English NER has been achieved by using LSTM-CRF models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018) with character information being integrated into word representations.",other,acknowledge the role of automatic lexicon in NER and report on state-of-the-art methods
1968,,de5eb23ad19d04585e770bfaee9d585a118aaa6e,Application of Improved Median Filter on Image Processing,,,"###Our choice, the weighted mean filter, is motivated by the fact that it provides a good compromise between simplicity and robust noise detection, especially for high level noise ratios.###Different remedies of the median filter have been proposed, e.g. the adaptive median filter [8], the
multi-state median filter [9], or the median filter based on homogeneity information [10], [12].",impact-revealing,discussing the choice of a noise detection method and its advantages
2975,5f02f17c91e011ee5e0258c8,3da4626411d83c19c9919bb41dba94fff88da90e,Scaling Graph Neural Networks with Approximate PageRank,5b67b45517c44aac1c8607aa,Large-Scale Learnable Graph Convolutional Networks.,"The success of GNNs on academic datasets has generated significant interest in scaling these methods to larger graphs for use in real-world problems [13, 14, 16, 21, 25, 27, 41, 54].###[21] collect the representations from a node’s neighborhood into a matrix, sort independently along each column/feature, and use the k largest entries as input to a 1-dimensional CNN.###The majority of previous approaches are evaluated on a small set of publicly available benchmark datasets [2, 13, 14, 21, 25, 27, 41, 49].###While several approaches have been proposed to improve the efficiency of graph neural networks [13, 14, 16, 21, 25, 27, 41, 49, 54], the scalability of GNNs to massive (web-scale) graphs is still under-studied.###based on different importance scores for the nodes [16, 21, 25, 41, 54].",other,highlighting the growing interest in scaling GNNs for real-world applications
1778,,36b180da3703a95cd587b7051cdb4f931e8c66d0,Indonesian language learning based on ecological intelligence: A case of Bengawan Solo Nature School,,,"###The researcher examines a single entity or phenomenon (a case), is limited by time and activity (an event, process, institution, or social group), and collects detailed information using various data collection procedures over a continuous period of time (Hancock, Algozzine, & Lim, 2021).",impact-revealing,describing a research methodology
1766,,d2dfdbb7a5d64176968f51342a2eff0ff45db4b5,Bit-Pragmatic Deep Neural Network Computing,,,"###Given their breadth of application and high computational demand, DNNs are an attractive target for fixed-function accelerators [5, 6, 12].###16-bit fixed-point is commonly used for DNN hardware implementations [5, 6].###Ineffectual activations and weights have also been exploited to reduce power [6, 26] The occurrence of ineffectual activations appears to be an intrinsic property of CNNs as their neurons are designed to detect the presence of relevant features in their input [2].###Skipping the computation of zero valued activations is an optimization employed in recently proposed DNN accelerators, both to save power [6, 26] and processing time [2, 12, 24].",impact-revealing,highlighting the significance of DNNs in hardware implementations and optimizations
2727,5db9295f47c8f766461f5135,2c1006c856fefdbd6cd710e840e57153f2d6cd04,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,58d82fcbd649053542fd64c5,Adversarial Machine Learning at Scale.,"Recently, [2] showed that many existing defence methods suffer from a false sense of robustness against adversarial attacks due to gradient masking, and adversarial training [24, 32, 58, 36] is one of the effective defense method against adversarial attacks.###Firstly, some adverse effects such as label leaking is still an issue hindering adversarial training [32].###Label leaking [32] and gradient masking [43, 58, 2] are some well-known issues that hinder the adversarial training [32].###Currently available remedies either increase the number of iterations for generating the attacks [36] or use classes other than the ground-truth for attack generation [32, 65, 61].",other,highlighting challenges and limitations in adversarial training methods
973,,2b37aa5286afcca7c6090492906b82e3c571cdeb,Coordination Patterns Related to High Clinical Performance in a Simulated Anesthetic Crisis,,,"###For example, research in various (mostly nonclinical) settings has highlighted the importance of verbalizing situation assessments to maintain good coordination.(21,22) We therefore believe that this coordination mechanism should be a focus during team training addressing the role of the sender and the receiver of verbal communication.",impact-revealing,highlighting the importance of verbal communication in team training
880,5cf48a33da56291d5829579e,10973505dd4d872c13a37322a50453bf5157c552,Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search,5a260c8617c44a4ba8a320de,SMASH: One-Shot Model Architecture Search through HyperNetworks.,"More recent studies (Brock et al., 2018; Shirakawa et al., 2018; Pham et al., 2018; Liu et al., 2019; Xie et al., 2019; Cai et al., 2019), on the other hand, optimize the weights and the architecture simultaneously within a single training by treating all possible architectures as subgraphs of a supergraph.###SMASH (Brock et al., 2018) employs HyperNet that takes an architecture
c as its input and returns the weights for the network with architecture c.###SMASH (Brock et al., 2018) employs HyperNet that takes an architecture###After the architecture search phase, we retrain the network with the most likely architecture, ĉ = argmaxc pθ(c), from scratch, which is a commonly used technique (Brock et al., 2018; Liu et al., 2019; Pham et al., 2018) to improve final performance.###More recent studies (Brock et al., 2018; Shirakawa et al., 2018; Pham et al., 2018; Liu et al., 2019; Xie et al., 2019; Cai et al., 2019), on the other hand, optimize the weights and the architecture simultaneously within a single training by treating all possible architectures as subgraphs of a…###After the architecture search phase, we retrain the network with the most likely architecture, ĉ = argmaxc pθ(c), from scratch, which is a commonly used technique (Brock et al., 2018; Liu et al., 2019; Pham et al., 2018) to improve final performance.###08 SMASHv2 (Brock et al., 2018) 1.",impact-revealing,acknowledge recent advancements in architecture optimization
266,5f0277e911dc830562231dab,6deae79dec438eaaa524bca3b82c6b8d93553b20,A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data,5550417545ce0a409eb3b767,Distilling the Knowledge in a Neural Network.,"Hinton’s work first proposes the concept of knowledge distillation [10].###Note that we use a general definition of distillation in the study rather than the past knowledge distillation approaches such as considering the level of sample [21, 27] and model structure [10, 22].###But, by introducing softmax and temperature operations to relax the label, training the student network to keep the same output as the teacher network on a soft label will result in a significant improvement [10].",impact-revealing,highlighting the significance of Hinton's work on knowledge distillation
1373,,ce59e2ac23e9d38424daa94fea92a32bac11eca5,Next-Generation Sequencing of the BRCA 1 and BRCA 2 Genes for the Genetic Diagnostics of Hereditary Breast and / or Ovarian Cancer,,,"###However, in this initial phase, we still recommend that novel mutations are validated by Sanger sequencing before informing the patient, although the feasibility of targeted NGS as a standalone diagnostics test has already been suggested.(39) Clinical diagnostic tools must meet very stringent sensitivity and specificity parameters, while keeping their costand time effectiveness.",impact-revealing,highlighting the importance of validating novel mutations in clinical diagnostics
3714,5c04967517c44a2c7470926f,c2d40522eaa5523d67a0de5e4098e7031fdccb3d,Pitfalls of Graph Neural Network Evaluation,5ce2d032ced107d4c635260c,Predict then Propagate: Graph Neural Networks meet Personalized PageRank,"If we were interested in comparing one model versus the rest, we could perform pairwise t-tests, as done in Klicpera et al. [2019]. Since we are interested in comparing all the models to each other, we consider the relative accuracy of each model instead.",other,describing the approach for model comparison
2739,5d0b003a8607575390fb4f6a,43d74cd04fb22bbe61d650861766528e369e08cc,An Encoding Strategy Based Word-Character LSTM for Chinese NER,5a260c5217c44a4ba8a28acf,Deep learning with word embeddings improves biomedical named entity recognition.,"Red labels(without underline) denote predicted labels, and blue labels(with underline) denote gold labels. works (Huang et al., 2015; Lample et al., 2016; Habibi et al., 2017) have been introduced to NER task.",other,acknowledge prior work in named entity recognition
466,5bdc315017c44a1f58a05bba,ac61568c081c03730c58bd34c023a6952803da13,Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency,53e9aebcb7602d97038e4188,A fast and simple algorithm for training neural probabilistic language models.,"• We discuss application of our results to approaches of (Mnih and Teh, 2012; Mikolov et al., 2013; Levy and Goldberg, 2014; Jozefowicz et al., 2016) giving a unified account of these methods.###, 2013), see also (Levy and Goldberg, 2014)), and the Noise Contrastive Estimation methods of (Mnih and Teh, 2012; Jozefowicz et al., 2016) for estimation of language models.###We show the following (throughout we define K 1 to be the number of negative examples sampled per training example): • For any K 1, a binary classification variant of NCE, as used by (Mnih and Teh, 2012; Mikolov et al., 2013), gives consistent parameter estimates under the assumption that Z(x; ✓) is constant with respect to x (i.###This is the most straightforward extension of NCE to the conditional case; it is used by (Mnih and Teh, 2012).",impact-revealing,discussing the application of results to unify various methods
2177,,5f3cd5186f024591bb66aa9af97bd71537119f8e,Derivations of Information Technology Strategies for Enabling the Cloud Based Banking Service by a Hybrid MADM Framework,,,"###It implies a service oriented architecture, reduced information technology overhead for the end-user, great flexibility, reduced total cost of ownership, on-demand services and many other things (Vouk 2008).###Cloud computing builds on decades of research in virtualization, distributed computing, utility computing, and more recently networking, web and software services (Vouk 2008).###Cloud computing services integration and provisioning experts should be able to focus on creation of composite and orchestrated solutions needed for an enduser which sample and combine existing services and images, customize them, update existing services and images, and develop new composites (Vouk 2008).###The cloud computing implies a service oriented architecture, reduced information technology overhead for the end-user, greater flexibility, reduced total cost of ownership, on demand services and many other things (Vouk 2008).",impact-revealing,providing context and background on cloud computing
1484,,88095907212a198757aab2766501ca138f06ccdb,Enhancing Text Annotation through Rationale-Driven Collaborative Few-Shot Prompting,,,"###This automation not only enhances efficiency but also improves consistency across annotations, mitigating the variability often associated with human annotators [3, 4, 5].###Recent studies suggest that widely utilized LLMs, such as ChatGPT, can outperform human annotators across various annotation tasks, demonstrating a higher degree of consistency compared to manual annotations [3].###While some studies suggest that widely used LLMs, such as ChatGPT, can achieve or even exceed the quality of human annotations [3, 8], many researchers argue that relying solely on LLMs is insufficient for high-quality text annotation, necessitating additional verification processes [2, 9].",impact-revealing,highlighting the benefits and challenges of using LLMs for text annotation
3546,5f8d6be69fced0a24bbab01e,a87e4124f7305a97a8efaa574c1b270dccf4a563,DisenHAN: Disentangled Heterogeneous Graph Attention Network for Recommendation,5d3c233f3a55acd386d4dde4,Capsule Graph Neural Network.,"Inspired by the capsule neural network (CapsNet)[24], some works replace scalar-valued neurons with vector-valued ones to learn disentangled node representations with GNNs’ framework in homogeneous network[21, 33, 39].",other,acknowledge existing research inspired by capsule neural networks
2623,5e6cae3493d709897ccff2f9,804f23ac1a4a56b8dc5bb7201dab7b8cece76a70,classifying memory access patterns for prefetching,5550481f45ce0a409eb6bc91,Author retrospective for compiler-directed data prefetching in multiprocessors with memory hierarchies.,"Compiler-based techniques [1, 10, 18, 46] perform static code analysis to generate prefetch targets.###Prior works on compiler assisted prefetching [1, 8, 10, 18, 33, 35, 46, 52] struggle to address these questions.",other,acknowledge challenges in prior works on compiler-assisted prefetching
3594,5b1643998fbcbf6e5a9bc32d,2fe2cfd98e232f1396f01881853ed6b3d5e37d65,Taskonomy: Disentangling Task Transfer Learning,57a4e91dac44365e35c98c54,Cross-stitch Networks for Multi-task Learning,"Multi-task learning has experienced recent progress and the reported advantages are another support for existence of a useful structure among tasks [93, 100, 50, 76, 73, 50, 18, 97, 61, 11, 66].###…years of modern computer science, e.g. with Turing arguing for using learning elements [95, 98] rather than the ﬁnal outcome or Jean Piaget’s works on developmental stages using previously learned stages as sources [74, 39, 38], and have extended to recent works [76, 73, 50, 18, 97, 61, 11, 66].",other,highlighting the historical context and support for multi-task learning
2898,5b3d98cc17c44a510f80212a,cb91c2f8d3cac0b655a39be318b603334eb18987,learning to optimize tensor programs,573696d96e3b12023e5d8d36,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,"GBT and TreeGRU represent two distinct ML approaches to problem resolution.###Our second model is a TreeGRU[39], which recursively encodes a low-level AST into an embedding vector.###We apply batching to the TreeGRU model and use GPU acceleration to make training and prediction fast enough to be usable in our framework.###TreeGRU, the deep learning-based approach, is extensible and requires no feature engineering, but it lags in training and predictive speed.###Context Encoded TreeGRU.###Figure 3c shows a way to encode the program by learning an embedding vector for each identifier and summarizing the AST using TreeGRU.###Both the GBT and TreeGRU models outperformed the black-box methods and found operators that were 2⇥ faster than those found with random searches.###Our second model is a TreeGRU[38], which recursively encodes a low-level AST into an embedding vector.",other,comparing distinct machine learning approaches and their performance
12,53e9b253b7602d9703cf4028,fff114cbba4f3ba900f33da574283e3de7f26c83,DeepWalk: online learning of social representations,53e9b108b7602d9703b85b88,Distributed Representations of Words and Phrases and their Compositionality.,"A recent relaxation in language modeling [26, 27] turns the prediction problem on its head.",impact-revealing,describing a shift in language modeling approach
2649,5f75aa6a9fced0a24b64599d,69fb130409d48478fab0c4545dc2ff102a2630ef,The forward slice core microarchitecture,55323c6d45cec66b6f9dc0c5,A Front-End Execution Architecture for High Energy Efficiency.,Shioya et al. [18] propose the front-end execution architecture which executes instructions that have their operands ready in the front-end of the pipeline; other non-ready instructions are dispatched to the out-of-order back-end.,other,reporting prior findings on execution architecture
4036,5db929b747c8f766461fa94f,2a6d160b529272964ce1a6707adf52f3d6ba4861,Diffusion Improves Graph Learning,5c75722cf56def97987ea577,Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting,"However, they only became widely adopted in recent years, when they started to outperform classical models in many graph-related tasks [19, 33, 42, 82].",other,highlighting the recent adoption and superiority of new models in graph-related tasks
2008,,f71cf6cb2652b09a6ddf59179df304ad81953f5f,The value of sestamibi single-photon emission computed tomography/computed tomography in differentiating and staging renal cell carcinomas: A systematic review,,,"###Histopathological examination was performed on resected kidney in 2 studies,([2,8]) while either biopsy or resected kidney were used for histopathological examination in the other 2 studies.###The risk of bias as regards the reference standard (histopathological examination) was high in the studies by Gorin et al.([8]) and Tzortzakakis et al.###The total sample size was 80 patients as the data of the same cohort of patients was used for analysis in 2 studies.([2,8]) All studies were single-center and prospective in design.###Sheikhbahaei and colleagues([2]) conducted a modified secondary analysis of the cohort of Gorin et al.([8]) The secondary analysis was published in 2017 and included 48 patients with clinical stage T1 solid renal masses.###Gorin and colleagues([8]) published a prospective study in 2016 which was carried out in the United States and included 50 patients with a solid clinical T1 renal mass to assess the diagnostic performance of Tc-sestamibi SPECT/CT for renal oncocytomas.###There were 4 observational studies: 2 American,[2,8] 1 Swedish,[9] and 1German.###as the eligibility criteria were not defined, uncertain in the studies by Gorin et al.([8]) and Sheikhbahaei et al.###Finally, we included four studies([2,3,8,9]) for this systematic review (Fig.###Study SPECT/CT brand Dose (MBq) Timing of SPECT/CT postinjection (min)
Rowe et al. (2015) Siemens Symbia16-slice SPECT/CT 925 75 Gorin et al. (2016) Siemens Symbia16-slice SPECT/CT 925 75 Sheikhbahaei et al. (2017) Siemens Symbia16-slice SPECT/CT 925 75 Tzortzakakis et al. (2017) Siemens…###Basic characteristics of the included studies There were 4 observational studies: 2 American,([2,8]) 1 Swedish,([9]) and 1German.###USA Sheikhbahaei et al. (2017)
48 Median (range)= 59 (40–81)
72.9 Mean (IQR)= 2.95 (2.20–4.55)
Oncocytoma 6 (12.5###USA Gorin et al. (2016)
50 Median (range)= 61.8 (53.2–70.8)
74% Median (range)= 3.0 (2.2–4.8)
Oncocytoma 6 (12###After vigorous searching of the scientific databases, 4 studies were identified,([2,3,8,9]) which were related to the review question.###Although the findings of this study are encouraging, it is limited by the same limitations of Gorin et al.([8]) Tzortzakakis and colleagues([9]) conducted a nonrandomized prospective study in Sweden that was published in 2017 and included 24 patients with 31 T1 solid renal tumors (4 patients had multiple bilateral renal lesions).",impact-revealing,summarizing findings and limitations of multiple studies on renal oncocytomas
2498,53e9ba8ab7602d97046ac03f,67b3d45164531806e14697a3b4d268d5f294bb82,Object Storage on CRAQ: High-Throughput Chain Replication for Read-Mostly Workloads,53e9a6dfb7602d9703017622,Peer-assisted content distribution with prices.,"These include a DNS service supporting dynamic service migration, rendezvous servers for a peer-assisted CDN [5], and a largescale virtual world environment.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2820,53e9a232b7602d9702b3a1a9,327722247ffc70a0d51f5c2246bc9a53c0e7daa3,Accurate branch prediction for short threads,53e9a93eb7602d9703291073,Implicit Parallelism With Ordered Transactions,"Recently, it has been shown that an architecture that supports transactional memory can do some amount of thread-level speculation [6, 13, 48].",other,reporting recent findings on transactional memory architecture
3212,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e99a79b7602d97022ebb16,Nested Monte-Carlo Search,"The purpose of Cutoff AMAF is to warm-up the tree with AMAF data, then use the more accurate UCT data later in the search.###In Cutoff AMAF, the AMAF algorithm is used to update statistics for the ﬁrst k simulations, after which only the standard UCT algorithm is used [101].###It is similar to α -AMAF, except that the α value used at each node decreases with each visit.###This approach is the same as the standard AMAF algo-rithm except that the history used to update nodes is truncated after the ﬁrst m random moves in the simulation stage [101].###If m = 0 then only actions selected in the tree are used to update nodes, similarly if m is larger than the number of moves in the simulation, this is equivalent to the AMAF algorithm.",other,describing the Cutoff AMAF method and its purpose
304,5e5e191993d709897ce5087d,674b6321ae1d12c83f28ade1850a27256c20f0d4,Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset,5c8f8bf64895d9cbc65118e4,Multi-task learning for Joint Language Understanding and Dialogue State Tracking,"Addressing these concerns, approaches utilizing a dynamic vocabulary of slot values have been proposed (Rastogi, Gupta, and Hakkani-Tur 2018; Goel, Paul, and Hakkani-T¨ur 2019; Wu et al. 2019).",impact-revealing,acknowledge existing approaches to dynamic vocabulary
2152,,b491a321c617532c50ab62ac105b5e0122a3fc7a,DSP Systems vs ANN Ensembles for Motion Detection and Filtering,,,"###The 2D Fourier Transform (FT) (1) and Discrete Fourier Transform (DFT) (2) is widely used for image processing operations [2], [4].###Aliasing and filtering are considered to be problematic processes in DSP based image processing systems [4], [7], [9].",impact-revealing,acknowledge common image processing techniques
488,5c8c52bc4895d9cbc6ddad8d,76e4d56d712d64ec2f77fd5b2fcb504888c07eab,Island loss for learning discriminative features in facial expression recognition,58437777ac44360f10840171,A Discriminative Feature Learning Approach For Deep Face Recognition,"Most recently, a center loss was introduced into CNNs [48] to reduce the intra-class variations of the learned features for face recognition.###[48] introduced a center loss for face recognition, which targets directly on one of the learning objectives, i.###1 [48] as the summation of squared distances between samples and their corresponding centers in the feature space: LC = 1",impact-revealing,reporting a recent advancement in face recognition methods
467,599c7959601a182cd2633b3e,c0c0990b84a350d5efde8d3b2cb2636b6b57c21c,On Sampling Strategies for Neural Network-based Collaborative Filtering,573696ce6e3b12023e5ce889,Stochastic optimization with importance sampling for regularized loss minimization,"Several sampling techniques, such as stratified sampling [35] and importance sampling [36] are proposed to achieve the variance reduction.###As shown in [35, 36], the reduction of variance can lead to faster convergence.",impact-revealing,acknowledge existing sampling techniques and their benefits
2639,558bfbace4b00c3c48df9828,3c5b532f1b46013a3519f09a35fd1c8387ae59a7,Fast thread migration via cache working set prediction,53e9ba64b7602d970468183c,Execution-based prediction using speculative slices,"Speculative Precomputation [40, 11] targets memory instructions which degrade performance due to poor cache behavior, using alternate contexts on multithreaded or CMP [4] architectures.###Helper threads [7, 11, 40] also utilize parallel hardware for speedup, without actually offloading computation; each new helper thread executes within the same address space as the main thread, inheriting its memory state.",other,providing context on speculative precomputation and helper threads
306,5c5ce50d17c44a400fc38e42,350c5f528b557cde46177866121c40e250201a0f,Goal-based Course Recommendation,5c79a19c4895d9cbc65c34b3,Session-based Recommendations with Recurrent Neural Networks.,"While RNNs have been previously applied to make recommendations based on collaborative filtering principles [15, 16, 23], they have not been re-purposed to make more targeted personalized goal-based recommendations in any domain.",impact-revealing,highlighting a gap in the application of RNNs for personalized recommendations
1620,,535c982a88ce42b597d28854bc28c3168fe0f900,Reliable Fix Patterns Inferred from Static Checkers for Automated Program Repair,,,"###The usage of GZoltar and Ochiai reduces the comparison biases, since both are widely used by APR systems in the literature.###7.2/Ochiai.###1.1 + Ochiai + prioritization) against the normal fault localization (normal FL, i.e., GZoltar-0.###In the framework, we leverage the Ochiai [1] ranking metric to actually compute the suspiciousness 96:9 scores of statements that are likely to be the faulty code locations.###For example, ELIXIR [56], PraPR [9], and GenPat [15] rely on the Ochiai technique to identify potential buggy statements, but more details about off-the-shelf fault localization techniques are not provided.###In the framework, we leverage the Ochiai [1] ranking metric to actually compute the suspiciousness###7.2 + Ochiai ranking metric) as Avatar.###Fault Localization: For evaluation purposes, we apply different fault localization settings to the experiment of each research question, while the default setting of Avatar is to use the GZoltar framework with the Ochiai ranking metric.###7.2 and the Ochiai ranking metric) by Liu et al. [34] as Avatar.###CapGen [63] applies GZoltar and Ochiai to detect bug positions, but 96:30 K. Liu et al. the exact version of GZoltar is not clarified.###The research community has developed generate-and-validate repair pipelines [4, 16, 23, 25, 31, 62, 63, 68] where program test cases are leveraged not only for localizing the bug locations [1, 29, 52, 72] but also as the oracle for validating the generated patches [22, 37, 53, 67].###1.1 + Ochiai).###1.1 with the Ochiai metric [52]) but only considering the suspicious statements within the known faulty meth-ods.###To that end, we attempt to replicate two scenarios of fault localization used in APR assessments: The first scenario assumes that the faulty method name is known [24] and thus focuses on ranking the inner-statements based on Ochiai suspiciousness scores; the second scenario makes no assumption on fault location and thus uses the default setting of Avatar.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1585,,0cc3ded5184f413f7718281656d8341ebd3ac238,"An Insight into Bio-inspired and Evolutionary Algorithms for Global Optimization: Review, Analysis, and Lessons Learnt over a Decade of Competitions",,,"###These are inspired by different biological behaviors: movements of birds [179], bats [180], or small insects such as fireflies [181, 182], grasshoppers [183], or even mussels [184]; mechanisms to locate food exhibited by colony animals such as ants in Artificial Ant Colony (ACO) [185, 186], or bees in Artificial Bee Colony (ABC) algorithms [187]; hunting mechanisms used by different animals, from small ones such as dragonflies [188], to wild wolfs [189] or marine animals such as dolphins [190] or whales [191]; even the reproduction of corals [192], the behavior of very small animals such as krill [193] or the immune system in Artificial Immune System (AIS) optimization [194], to name a few.",impact-revealing,highlighting the inspiration drawn from various biological behaviors for algorithm development
361,5fd0a7f691e01147f1d1e367,d4e9bf1d5fab52698a3aa48c65b098ea327deea6,CTRLsum: Towards Generic Controllable Text Summarization,5ed0e04291e011915d9e43ee,Language Models are Few-Shot Learners,"In addition, inspired by the multi-task ability of language models through prompting (Radford et al., 2019; Brown et al., 2020), we further study the possible combination of keywords and prompts in CTRL SUM for more generic control purposes.",impact-revealing,highlighting the exploration of multi-task capabilities in language models
2262,5ed12ca69e795e8ab1c11568,b3b60a15502abca31ae7b20ce16bf50049183c5a,knowledge enhanced personalized search,5550456245ce0a409eb55cee,Glove: Global Vectors for Word Representation.,"We train the word embedding using GloVe [27], taking the query texts and document titles in the search log as training corpus.",other,describing the method for training word embeddings
4030,5d5e6b9a3a55acfce79a16dd,6303bac53abd725c3b458190a6abe389a4a1e72d,Deep High-Resolution Representation Learning for Human Pose Estimation,58437722ac44360f1082ef17,Human pose estimation via Convolutional Part Heatmap Regression,"The high-to-low process aims to generate low-resolution and high-level representations, and the low-to-high process aims to produce highresolution representations [4, 11, 22, 70, 39, 60].",other,describing the process of generating representations
273,5d3ed25a275ded87f97deb36,37f7eaf57b88ef03efd4196f2f57ae0b4c657a79,Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommendation,5cd7fa07ced107d4c65bf34f,Knowledge Graph Convolutional Networks for Recommender Systems,"To account for the relational heterogeneity in KGs, similar to [28], we use a trainable and personalized relation scoring function that transforms the KG into a user-specific weighted graph, which characterizes both the semantic information of the KG and the user’s personalized interests.###To this end, similar to [28], we use a user-specific relation scoring function su (r ) that provides the importance of relation r for user u: su (r ) = д(u, r), where u and r are feature vectors of user u and relation type r , respectively, and д is a differentiable function such as inner product.###Our former work [28] made a preliminary trial on exploring GNN architecture in heterogeneous KGs for recommendation, but we show later that simply applying GNN to KGs without proper regularization is prone to overfitting and leads to performance degradation.###[28]), which justifies our claim that LS regularization can assist learning the edge weights in a KG and achieve better generalization in recommender systems.###Existing KG-aware recommender systems can be classified into path-based methods [8, 33, 36], embedding-based methods [9, 26, 27, 34], and hybrid methods [18, 24, 28].",impact-revealing,highlighting the need for personalized relation scoring in knowledge graphs
3668,5c7572b7f56def97988385ce,c342c71cb23199f112d0bc644fcce56a7306bf94,active learning for convolutional neural networks: a core-set approach,599c796f601a182cd263cad0,Deep Bayesian Active Learning with Image Data.,"We only report the best performing one for each dataset since they perform similar to each other. iii) Deep Bayesian Active Learning (DBAL)(Gal et al., 2017): We perform Monte Carlo dropout to obtain improved uncertainty measures and report only the best performing acquisition function among…###Although Bayesian active learning has been shown to be effective for small datasets (Gal et al., 2017), our empirical analysis suggests that they do not scale to large-scale datasets because of batch sampling.###We only report the best performing one for each dataset since they perform similar to each other. iii) Deep Bayesian Active Learning (DBAL)(Gal et al., 2017): We perform Monte Carlo dropout to obtain improved uncertainty measures and report only the best performing acquisition function among max-entropy, BALD and Variation Ratios for each dataset. iv) Best Oracle Uncertainty: We also report a best performing oracle algorithm which uses the label information for entire dataset.###ii)Best Empirical Uncertainty: Following the empirical setup in (Gal et al., 2017), we perform active learning using max-entropy, BALD and Variation Ratios treating soft-max outputs as probabilities.###DBAL[GIG 17] BMDR [WY 15] K-Median Our Method
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 60
65
70
75
80
85
90
Random Empirical-Unc.###iii) Deep Bayesian Active Learning (DBAL)(Gal et al., 2017): We perform Monte Carlo dropout to obtain improved uncertainty measures and report only the best performing acquisition function among max-entropy, BALD and Variation Ratios for each dataset.###…i)Random: Choosing the points to be labelled uniformly at random from the unlabelled pool. ii)Best Empirical Uncertainty: Following the empirical setup in (Gal et al., 2017), we perform active learning using max-entropy, BALD and Variation Ratios treating soft-max outputs as probabilities.###DBAL[GIG 17] BMDR [WY 15] K-Median Our Method
0.0 0.2 0.4 0.6 0.8 1.065
70
75
80
85
90
95
Random Empirical-Unc.###DBAL[GIG 17] BMDR [WY 15] K-Median Our Method
Figure 4: Results on Active Learning for Fully-Supervised Model (error bars are std-dev)
We conducted experiments on active learning for fully-supervised models as well as active learning for weakly-supervised models.",other,reporting experimental results on active learning methods
1248,,acd7f4adffd7f6c632b8819fee2c5a96edffdfa5,FACILITATING DL REASONERS THROUGH ONTOLOGY PARTITIONING by SADIQ CHARANIYA,,,"###Several OWL Description Logic (DL) reasoners (Racer, FaCT++, Pellet, etc.) have been developed for reasoning with ontologies.###Tsarkov, D., & Horrocks, I. FaCT++ description logic reasoner: System description.###Several OWL reasoners (Racer, FaCT++, Pellet, etc.) have been developed to serve the purpose.###Of these OWL-DL reasoners, Pellet, FaCT++, and Racer are based on tableau algorithms [3] and are most widely used for reasoning with OWL-DL ontologies.###Some examples of reasoners are Pellet [3], FaCT++ [12], RacerPro [16], KAON [20], and Hoolet [13].",impact-revealing,reporting on the development and application of OWL Description Logic reasoners
3650,5cede10dda562983788ed645,690edf44e8739fd80bdfb76f40c9a4a222f3bba8,BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer,59a02642b161e8ad1a7b651a,Sequential User-based Recurrent Neural Network Recommendations,"7https://github.com/hexiangnan/neural_collaborative_filtering 8https://github.com/hidasib/GRU4Rec 9https://github.com/graytowne/caser_pytorch 10https://github.com/kang205/SASRec
We implement BERT4Rec11 with TensorFlow.###For example, Li et al. [28] incorporate an attention mechanism into GRU to capture both the user’s sequential behavior and main purpose in session-based recommendation.###The basic idea of these methods is to encode user’s previous records into a vector (i.e., representation of user’s preference which is used to make predictions) with various recurrent architectures and loss functions, including session-based GRU with ranking loss (GRU4Rec) [15], Dynamic REcurrent bAsket Model (DREAM) [59], user-based GRU [7], attention-based GRU (NARM) [28], and improved GRU4Rec with new loss function (i.e., BPR-max and TOP1-max) and an improved sampling strategy [14].###For NCF7, GRU4Rec8, GRU4Rec+8, Caser9, and SASRec10, we use code provided by the corresponding authors.###Furthermore, SASRec performs distinctly better than GRU4Rec and GRU4Rec+, suggesting that self-attention mechanism is a more powerful tool for sequential recommendation.###, Recurrent Neural Network (RNN), for sequential recommendation and obtain promising results [7, 14, 15, 56, 59].###• GRU4Rec+ [14]: It is an improved version of GRU4Rec with a new class of loss functions and sampling strategy.###Among all the baseline methods, sequential methods (e.g., FPMC and GRU4Rec+) outperforms non-sequential methods (e.g., BPR-MF and NCF) on all datasets consistently.###Recently, RNN and its variants, Gated Recurrent Unit (GRU) [4] and Long Short-Term Memory (LSTM) [17], are becoming more and more popular for modeling user behavior sequences [7, 14, 15, 28, 36, 56, 59].###, representation of user’s preference which is used to make predictions) with various recurrent architectures and loss functions, including session-based GRU with ranking loss (GRU4Rec) [15], Dynamic REcurrent bAsket Model (DREAM) [59], user-based GRU [7], attention-based GRU (NARM) [28], and improved GRU4Rec with new loss function (i.###This causes Caser to perform worse than GRU4Rec+ and SASRec, especially on sparse datasets.###• FPMC [40]: It captures users’ general taste as well as their sequential behaviors by combing MF with first-order MCs. • GRU4Rec [15]: It uses GRU with ranking based loss to model user sequences for session based recommendation.",other,acknowledge various methods and their performance in sequential recommendation
577,5d04e8dbda56295d08db13cf,56e3ce0ff4cbd05e404214d19ae264fe6c457a16,cif: continuous integrate-and-fire for end-to-end speech recognition,5a73cbcc17c44a0b3035f6a6,Monotonic Chunkwise Attention.,"Besides, CIF provides a concise calculation process by conducting the locating and integrating at the same time, rather than [11, 12] which need two separate steps of first using a hard monotonic attention to decide when to stop and then performing soft attention to calculate, also rather than [13] which needs a CTC trained model to conduct pre-partition before the attention decoding.",impact-revealing,highlighting the efficiency of CIF compared to other methods
2580,5d9edc1647c8f76646032985,10a4db59e81d26b2e0e896d3186ef81b4458b93f,Named Entity Recognition with Bidirectional LSTM-CNNs,5550456245ce0a409eb55cee,Glove: Global Vectors for Word Representation.,"n Wikipedia and the Reuters RCV-1 corpus. We also experimented with two other sets of published embeddings, namely Stanford’s GloVe embeddings3 trained on 6 billion words from Wikipedia and web text (Pennington et al., 2014) and Google’s word2vec embeddings4 trained on 100 billion words from Google News (Mikolov et al., 2013). 2http://ml.nec-labs.com/senna/ 3http://nlp.stanford.edu/projects/glove/ 4https://code.google.co### OntoNotes. Due to time constraints we did not perform new hyper-parameter searches with any of the word embeddings. As word embedding quality depends on hyper-parameter choice during their training (Pennington et al., 2014), and also, in our NER neural network, hyper-parameter choice is likely sensitive to the type of word embeddings used, optimizing them all will likely produce better results and provide a fairer compa###As word embedding quality depends on hyper-parameter choice during their training (Pennington et al., 2014), and also, in our NER neural network, hyper-parameter choice is likely sensitive to the type of word embeddings used, optimizing them all will likely produce better results and provide a…###For OntoNotes, GloVe embeddings perform close to Collobert embeddings while Google embeddings are again one point behind.###gure 1) decode output into a score for each tag category. In addition, as we hypothesized that word embeddings trained on in-domain text may perform better, we also used the publicly available GloVe (Pennington et al., 2014) program and an in-house re-implementation5 of the word2vec (Mikolov et al., 2013) program to train word embeddings on Wikipedia and Reuters RCV1 datasets as well.6 Following Collobert et al. (2011b),###We also experimented with two other sets of published embeddings, namely Stanford’s GloVe embeddings3 trained on 6 billion words from Wikipedia and Web text (Pennington et al., 2014) and Google’s word2vec embeddings4 trained on 100 billion words from Google News (Mikolov et al., 2013).###In addition, as we hypothesized that word embeddings trained on in-domain text may perform better, we also used the publicly available GloVe (Pennington et al., 2014) program and an in-house re-implementation5 of the word2vec (Mikolov et al., 2013) program to train word embeddings on Wikipedia and…###To test these hypotheses, we performed experiments with new word embeddings trained using GloVe and word2vec, with vocabulary list and corpus similar to Collobert et. al.###In addition, as we hypothesized that word embeddings trained on in-domain text may perform better, we also used the publicly available GloVe (Pennington et al., 2014) program and an in-house re-implementation5 of the word2vec (Mikolov et al., 2013) program to train word embeddings on Wikipedia and Reuters RCV1 datasets as well.6
Following Collobert et al. (2011b), all words are lower-cased before passing through the lookup table
2http://ml.nec-labs.com/senna/ 3http://nlp.stanford.edu/projects/glove/ 4https://code.google.com/p/word2vec/ 5We used our in-house reimplementation to train word vectors because it uses distributed processing to train much quicker than the publicly-released implementation of word2vec and its performance on the word analogy task was higher than reported by Mikolov et al. (2013).###For CoNLL-2003, publicly available GloVe and Google embeddings are about one point behind Collobert’s embeddings.###As shown in Table 7, our GloVe embeddings improved significantly29 over publicly available embeddings on CoNLL-2003, and our word2vec skip-gram embeddings improved significantly30 over Google’s embeddings on OntoNotes.",other,reporting findings on word embeddings and their performance
3309,5d08be648607575390f908ca,b30b850e726ee41f80a28c2a81da9c640a8541fc,QoSMT: supporting precise performance control for simultaneous multithreading architecture,53e9b2e0b7602d9703d99006,Per-Thread Cycle Accounting In Smt Processors,"PTA uses MLP correction to achieve higher accuracy [12].###Eyerman et al. [12] proposed the per-thread cycle accounting (PTA) mechanism that is able to estimate a work-load’s solo performance in a co-running mode on SMT processors.###Referring to prior work [12], we design shadow solo-cycle accounting (SSCA) approach to estimate workloads’ execution time in solo mode by T solo = T share − T interf , where T share is the execution time in SMT mode and the interference time, T interf , is the sum of the contention stall cycles…",other,reporting prior findings and methods in workload estimation
3057,5f7af09591e011983cc81efc,87b008a6289fa22c72e1726a8929e815dfbbc65f,Hard Negative Mixing for Contrastive Learning,5550415945ce0a409eb3a820,ImageNet Large Scale Visual Recognition Challenge,"We learn representations on two datasets, the common ImageNet-1K [35], and its smaller ImageNet-100 subset, also used in [36, 38].",other,reporting datasets used for learning representations
2014,,02e361ab2ed8207f3f6eaf1038c4edd83b640339,Birefringence-derived artifact in optical coherence tomography imaging of the lamina cribrosa in eyes with glaucoma,,,###Note that this does not correspond to the optical attenuation coefficient of tissues because it does not account for systematic effects on signal attenuation 23 .,impact-revealing,providing clarification on optical attenuation coefficient
1663,,055883c3204e69b798452b150308468b66b0f6fd,Examining individual intention to share knowledge with people from other tribes,,,"###Identification was adopted from the Social Identify Theory (SIT), which was propounded by Tajfel and Turner (1979).###The theory predicts that intergroup behaviours are based on the individuals’ perceived status in an intergroup environment (Tajfel & Turner, 1979).###Identification was adopted from the Social Identify Theory (SIT), which was propounded by Tajfel and Turner (1979). The theory predicts that intergroup behaviours are based on the individuals’ perceived status in an intergroup environment (Tajfel & Turner, 1979).###Tajfel and Turner (1979) believed that an individual would do anything to identify with the group or the organization.###Identification was adopted from the Social Identify Theory (SIT), which was propounded by Tajfel and Turner (1979). The theory predicts that intergroup behaviours are based on the individuals’ perceived status in an intergroup environment (Tajfel & Turner, 1979). Essentially, the theory argued that individuals are motivated by the status they get from affiliating themselves with a group. Tajfel and Turner (1979) believed that an individual would do anything to identify with the group or the organization.",impact-revealing,providing context for Social Identity Theory and its implications
4010,5d64ff713a55acf547f20de0,26d3f8db3a4275225d355a9ce8e132b8c19c225b,Once-for-All: Train One Network and Specialize it for Efficient Deployment on Diverse Hardware Platforms,5a73cbc317c44a0b3035ec55,Progressive Neural Architecture Search,"Given the target hardware and constraint, a predictor-guided architecture search (Liu et al., 2018) is conducted to get a specialized sub-network, and the cost is negligible.",other,describing a method for architecture search
3923,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,61ca46285244ab9dcb5973ab,X-XEN : Huge Page Support in Xen,μSKU varies SHP counts from 0 to 600 in 100-step increments by modifying kernel parameters [85].,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3329,5f9a9af391e0114d7e7813ed,0d67d3ddca1c4e370eaf1e99ec674f612c39c66c,Graph Contrastive Learning with Adaptive Augmentation,5f045f8491e0114d4aaa4bfe,Wiki-CS: A Wikipedia-Based Benchmark for Graph Neural Networks,"Wiki-CS has dense numerical features. While the other four datasets only contain sparse one-hot features. For the Wiki-CS dataset, we evaluate the models on the public splits shipped with the dataset [25]. Regarding the other four co-coauthor and co-purchase datasets, since they have no public splits available, we instead use the random splits where 10%, 10%, and the rest 80% of nodes are randomly sel###sics5, to study the performance of transductive node classification. The datasets are collected from real-world networks of different kinds; their detailed statistics is summarized in Table2. •Wiki-CS[25]isareferencenetworkconstructedfromWikipedia. The nodes correspond to articles about computer science and edges are hyperlinks between the articles. Nodes are labeled with ten (10) classes each represe",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1525,,be9fe09197ea67b13070d140cf7c0237e836d986,African American professionals in higher education: experiencing and coping with racial microaggressions,,,"###While microassaults are conscious and deliberate in nature (i.e. using racial slurs), other forms of racialmicroaggressions (microinsults andmicroinvalidations) are unconscious and subtle exchanges between perpetrators who are unaware of their hidden prejudices and biases, and how they impact People of Color (Constantine, 2007).###I was talking that I taught European History, and I taught Colonial Early American History – those are my areas of interest – and had another colleague – another Fellow – that told me how did I feel that I could teach those correctly seeing that I was not European and not White, and why wasn’t I just teaching African AmericanHistory or, as he said, “colored history”.###While some participants experienced various microinvalidations, the participants primarily experienced microinsults (i.e. ascription of intelligence, criminality assumptions, and pathologizing cultural values/communication styles) (Sue et al, 2007b).###Further, microinsults are verbal, nonverbal, or environmental actions that ‘convey insensitivity, are rude, or directly demean a person’s racial heritage or identity’ (Sue et al. 2007b, 274), while microinvalidations are used to dismiss the psychological thoughts, feelings, or experiences of People of Color (Sue et al. 2007a, 2007b).###Racial microaggressions in higher education
Racism is a pertinent issue for People of Color and the experiencing of racial microaggressions is quite commonplace at the university level (Louis et al. 2016).###According to Sue et al. (2007a), racial microaggressions are one of the ‘new faces of racism’, such as colorblind racism, and are defined as ‘brief and commonplace daily verbal, behavioral and environmental indignities, whether intentional or unintentional, that communicate hostile, derogatory, or…###In higher education, CRT challenges the practice of assuming the experiences of White students, faculty, and staff as the norm on college and university campuses while legitimizing People of Color’s experiences through counter-storytelling (LadsonBillings and Tate 1995; Dixson, Rousseau, and Donner 2016; McCoy and Rodricks 2015; Taylor, 2009).###Instead, many studies minimize the impact of racism on institutional norms and People of Color (Harper 2012).###…are verbal, nonverbal, or environmental actions that ‘convey insensitivity, are rude, or directly demean a person’s racial heritage or identity’ (Sue et al. 2007b, 274), while microinvalidations are used to dismiss the psychological thoughts, feelings, or experiences of People of Color (Sue…###Experiencing racial microaggressions at PWIs
Participants at PWIs often experienced being treated as if they were second-class citizens (Sue et al, 2007b), a theme that describes being treated as a lesser person or group.###Another common theme, pathologizing cultural values/communication styles (Sue et al, 2007b), emerged that communicated the notion that the values of People of Color are abnormal.###One common theme was the ascription of intelligence (Sue et al, 2007b) in that the participants often felt as if their intellect was being questioned or challenged.###…or environmental actions that ‘convey insensitivity, are rude, or directly demean a person’s racial heritage or identity’ (Sue et al. 2007b, 274), while microinvalidations are used to dismiss the psychological thoughts, feelings, or experiences of People of Color (Sue et al. 2007a, 2007b).###Another theme, the criminality/assumption of criminal status (Sue et al, 2007b), manifested in the participants’ collective experiences working in higher education, especially in predominantly White workplace contexts.###Sadly, racial microaggressions that are experienced by People of Color in higher education are often perpetuated by White faculty, administrators, staff, and students who are unaware of the racist origins or implications of their actions (Constantine et al. 2008; Louis et al. 2016; Sue et al. 2011).###Despite their quiet nature, the impacts of racial microaggressions have large implications for the People of Color that receive them.###While research suggests such adaptive coping strategies tend to alleviate psychological stress and frustration associated with the experiencing of racial microaggressions in the workplace (e.g. Forsyth and Carter 2012; Holder, Jackson, and Ponterotto 2015; Utsey et al. 2008), it fails to consider the unique role systemic racism plays in creating hostile work environments that prevent People of Color from experiencing safe spaces where their voices are heard and their experiences are embraced, as opposed to being denied or condemned.",impact-revealing,highlighting the prevalence and impact of racial microaggressions in higher education
2122,,f819f74fab079abc3f1610944a5ff6e7f85ede1b,Air traffic flow management under uncertainty using chance-constrained optimization,,,"###To overcome the computational limitation of the Lagrangian models, the Eulerian model of ATFM was proposed ( Menon et al., 2004 ), which is inspired by the Daganzo Cell Transmission Model ( Daganzo, 1994, 1995 ).",impact-revealing,reporting prior findings on computational models
2141,,031ffab4164a5a832b603a49c7577e7fabac5316,Constrained Environment Optimization for Prioritized Multi-Agent Navigation,,,"###The former is a stringent measure combining the success rate and the path length, which has been widely used as a primary quantifier in comparing the navigation performance [50], [51], [52].",impact-revealing,providing context for a measurement used in navigation performance
2451,573696026e3b12023e516718,f8e79ac0ea341056ef20f2616628b3e964764cfd,"You Only Look Once: Unified, Real-Time Object Detection",53e9a524b7602d9702e43f65,Region-based Segmentation and Object Detection,"These classifiers or localizers are run either in sliding window fashion over the whole image or on some subset of regions in the image [34, 15, 38].",other,providing context on the application of classifiers or localizers in image processing
76,5c04967517c44a2c74708f29,15c9684321f03744051bb73b4c1141507dc8ddb2,Embedding Uncertain Knowledge Graphs,56d86611dabfae2eeea1253d,A Short Introduction to Probabilistic Soft Logic,We thus introduce probabilistic soft logic (PSL) (Kimmig et al. 2012) to infer conﬁdence scores for these un-seen relation facts to further enhance the embedding performance.###Probabilistic Soft Logic Probabilistic soft logic (PSL) (Kimmig et al. 2012) is a framework for probabilistic reasoning.###We thus introduce probabilistic soft logic (PSL) (Kimmig et al. 2012) to infer confidence scores for these unseen relation facts to further enhance the embedding performance.,impact-revealing,introducing a framework for probabilistic reasoning to improve embedding performance
3853,5cede0f2da562983788d0d44,aecddd82840323e5bd43f9c73a32fed88ee93c8c,An Effective Approach To Unsupervised Machine Translation,53e99867b7602d97020a27c2,"A Simple, Fast, and Effective Reparameterization of IBM Model 2.","Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint reﬁnement procedure.",other,reporting method implementation details
1007,,91422927040d2cca9e9419b22e49959cb6fc0c2e,The Effect of Social Isolation on Depressive Symptoms Varies by Neighborhood Characteristics: A Study of an Urban Sample of Women with Pre-School Aged Children,,,"###Social support is widely recognized as a protective factor against depression as well as an important coping mechanism for those with depression (Harris et al. 1999; Kawachi and Berkman 2001; Vandervoort 1999).###At the same time, social networks do not always have positive effects (Kawachi and Berkman 2001; Stansfeld et al. 1998).",impact-revealing,highlighting the dual role of social support in relation to depression
316,5dbebb7447c8f766462c2328,b0d941cfa0a3c43703e5222221addb1ad0f9e68d,Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks,5b67b45517c44aac1c860885,STAMP: Short-Term Attention/Memory Priority Model for Session-based Recommendation.,"To alleviate the inﬂuence oftime order, STAMP [19] only uti-lizesthe self-attention mechanism withoutRNN.###• STAMP [19] uses attention layers to replace all RNN encoders in previous work to even make the model more powerful by fully relying on the self-attention of the last item in a sequence.###To further alleviate the bias introduced by time series, STAMP [19] entirely replaces the recurrent encoder with an attention layer.###Following previous work [19, 38], sessions in Yoochoose 1/64 are separated into two groups, i.###As a matter of fact, the attention of a node 𝑖 can extend to every node, which is a special case the same as how STAMP makes the attention of the last node of the sequence.###For the subsequent methods,NARMand STAMP, which bothincorporatea self-attention over the lastinputitem ofa session, they bothoutperformGRU4REC in a large margin.###Looking into the comparison between NARM, combining RNN and attention mechanism, and STAMP, the complete attention setting, there is a conspicuous gap of performancethatSTAMPoutperformsNARM.###In the ﬁnal stage, it again uses a self-attention the same as STAMP to output a session embedding.###• The recent approaches [16, 19, 38], which divide the user’s preference into the long-term (global) and the short-term (local) preference, are too simple to capture the complicated item transition pattern.###For the fairness and the convenience of comparison,we follow [16, 19, 38] to filter out sessions of length 1 and items which occur less than 5 times in each dataset respectively.###To alleviate the influence of time order, STAMP [19] only utilizes the self-attentionmechanismwithout RNN.###• SR-GNN [38] applies a gated graph convolutional layer [18] to obtain item embeddings, followed by a self-attention of the last item as STAMP does to compute the sequence level embeddings.###Following [16, 19, 38], for the Yoochoose dataset, the most recent",impact-revealing,acknowledging the performance of attention-based models in session-based recommendations
198,5b3d98cc17c44a510f8018e7,f7bb1636ced9036b3d0edafc7d82ad43164d41a3,Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models,53e9a93eb7602d97032928b5,Intriguing properties of neural networks.,"Despite their outstanding performance on several machine learning tasks, deep neural networks have been shown to be susceptible to adversarial attacks (Szegedy et al., 2014; Goodfellow et al., 2015).###A popular approach to defend against adversarial noise is to augment the training dataset with adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016).###In the context of classification, these perturbations cause the legitimate sample to be misclassified at inference time (Szegedy et al., 2014; Goodfellow et al., 2015; Papernot et al., 2016b; Liu et al., 2017).###It was found that such examples designed to fool the sub-stitute often end up being misclassiﬁed by the targeted classiﬁer (Szegedy et al., 2014; Papernot et al., 2017).###1 ADVERSARIAL TRAINING A popular approach to defend against adversarial noise is to augment the training dataset with adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016).###…the clas-siﬁer more robust against attacks, e.g., adversarial training which augments the training data of the classiﬁer with adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015), (2) modifying the training procedure of the classiﬁer to reduce the magnitude of gradients, e.g.,…###, adversarial training which augments the training data of the classifier with adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015), (2) modifying the training procedure of the classifier to reduce the magnitude of gradients, e.###It was found that such examples designed to fool the substitute often end up being misclassified by the targeted classifier (Szegedy et al., 2014; Papernot et al., 2017).###In the context of classiﬁcation, these perturbations cause the legitimate sample to be misclassiﬁed at inference time (Szegedy et al., 2014; Goodfellow et al., 2015; Papernot et al., 2016b; Liu et al., 2017).",impact-revealing,highlighting the vulnerability of deep neural networks to adversarial attacks and the common defense strategy of adversarial training
15,5dce788a3a55ac9580a162f8,56cafbac34f2bb3f6a9828cd228ff281b810d6bb,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,573698636e3b12023e7296e0,Representation learning of knowledge graphs with entity descriptions,"Besides, Logan et al. (2019); Hayashi et al. (2020) utilize relations between entities inside one sentence to help train better generation models, and Xiong et al. (2019) adopt entity replacement knowledge learning for improving entity-related tasks.",impact-revealing,acknowledge existing methods for improving generation models
3678,5c0f87a5da562944ac95a190,4d157bec76125b1121a999ab2b79730540ab7aad,"Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases",53e9b436b7602d9703f2d83e,"From in silico target prediction to multi-target drug design: current databases, methods and applications.","structure-based and ligand-based methods) [39, 40]; however, recent advances in PCM have put this field forward to be considered as a third group [37].",other,acknowledge advancements in PCM methods
2527,5eabf3cd91e011664efc496f,916595fc4e701ac6125725408912a1fac3d7a60b,CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows,53e9bdc6b7602d9704a7598b,"A large, fast instruction window for tolerating cache misses","In [5], [56], instructions dependent on long-latency operations are temporarily kept in a small buffer until their dependences are resolved.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2144,,443c320aa40a68144fc723a6fb3dbb1d71b25833,Digital Sustainability for Energy-Efficient Behaviours: A User Representation and Touchpoint Model,,,"###This builds on the established research domain of green IS which primarily focuses on physical IS artefacts for environmental sustainability outcomes (El Idrissi & Corbett, 2016; Gholami et al., 2016; Watson et al., 2010).###…design, development, and use of digital artefacts (e.g., data analytics, IoT, artificial intelligence) and digital resources (e.g., blockchain, cloud computing) to achieve environmentally sustainable objectives (Corbett et al., 2023; Kotlarsky et al., 2023; Pan et al., 2022; Watson et al., 2010).",impact-revealing,highlighting the evolution of green IS research towards digital artefacts for sustainability
1792,,0eace28a9e278477869d9717373578122350b769,Determinants of weight loss maintenance: a systematic review,,,"###Lastly, Elfhag and Rössner report that individuals successful in weight loss maintenance used support from a social context (6).###Elfhag and Rössner previously suggest that the environment influences opportunities and barriers for engaging in healthy eating and regular physical activity, and thereby weight loss maintenance (6).###A systematic review conducted by Elfhag and Rössner in 2005 identified and summarized factors in successful weight loss maintenance and characterized successful individuals as those who had substantial weight loss during initial treatment, those who reach a self-determined weight loss goal, those who lead an active lifestyle and those who engage in leisure time activities (6).###We are interested in the newest insights in the determinants of weight loss maintenance and build upon a previous narrative review by Elfhag and Rössner entitled ‘Who succeeds in maintaining weight loss?’ published in 2005 (6).###Elfhag and Rössner report achieving greater initial weight loss as a characteristic of an individual successful in weight loss maintenance (6).###Moreover, Elfhag and Rössner (6) and Wing and Hill (91) have mentioned the use of social support by individuals with successful weight loss maintenance in their respective publications.###This review identified several energy intake-reducing behaviours and energy expenditure-increasing behaviours previously identified in the Elfhag and Rössner review (6).",impact-revealing,highlighting the significance of social support in weight loss maintenance
3150,5ca72ae8181a2f3597ce6e45,b1051e81e527d841f0936c604aa6966c719e876d,TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators,573697826e3b12023e669567,Show and Tell: A Neural Image Caption Generator,"Recent NNs, such as ResNet [17] and various LSTMs [41, 45], feature complex DAG structures that go beyond the single linear chain of layers in early NNs.###This is the case for many recent NNs that use large numbers of layers but each individual layer is rather small [17, 18, 45].",other,highlighting the complexity and structure of recent neural networks
3493,5cf48a45da56291d582a8448,f937d6482ad162f55022c7dce5857855ece27c1e,Knowledge Graph Convolutional Networks for Recommender Systems with Label Smoothness Regularization,5b67b45517c44aac1c860876,Graph Convolutional Neural Networks for Web-Scale Recommender Systems.,"Recently, researchers also deployed GCNs in recommender systems: PinSage [31] applies GCNs to the pin-board bipartite graph in Pinterest.###Recently, several works aimed to use GCNs in recommender systems [13, 18, 29, 31] (see Section 2.",other,acknowledge recent applications of GCNs in recommender systems
3717,5c7a561ff56def9798e6a297,abe4e92d6e4868115aef57a4eba97ec4d48ac124,language-adversarial transfer learning for low-resource speech recognition,5a73cb6317c44a0b303583d6,Improving Deliverable Speech-To-Text Systems With Multilingual Knowledge Transfer,However it is still challenging to rapidly build an ASR system for a novel language with significantly less labeled training data [5]–[7].,other,highlighting the challenge of building ASR systems for novel languages with limited data
2788,5aed14d617c44a4438159123,921196c32213a229245a9705ee4768bc941e7a26,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,5b076eb4da5629516ce742be,Subword language modeling with neural networks,"We also used the text8 dataset for character-level language modeling (Mikolov et al., 2012). text8 is about 20 times larger than PTB, with about 100M characters from Wikipedia (90M for training, 5M for validation, and 5M for testing).",other,reporting dataset details for character-level language modeling
2030,,3053e1338fb783469580d756883974757e53c671,"Molecular docking, synthesis, and antimycobacterial activities of pyrrolyl hydrazones and their copper complexes",,,"###that TB may once again become a deadly disease.(6,7) Moreover, the increasing incidence of TB in immunocompromised patients along with longer durations of therapy emphasizes the need for new drugs to extend the range of effective TB treatment options.",impact-revealing,highlighting the need for new drugs in TB treatment due to increasing incidence
3312,5f896fa591e01149071e45df,1d16d4cdc3fcce26e2c2097d13896ec09683eee3,Self-training for Few-shot Transfer Across Extreme Task Differences,5a73cb7417c44a0b3035a19e,Low-Shot Visual Recognition By Shrinking And Hallucinating Features,"Surprisingly, larger, richer embeddings do not always transfer better , in contrast to in-domain results reported by Hariharan & Girshick (2017).###Other methods assume that modes of intra-class variation are shared, suggesting the possibility of learned, class-agnostic augmentation policies (Hariharan & Girshick, 2017; Wang et al., 2018; Chen et al., 2019b).",other,highlighting the contrast in transferability of larger embeddings
3177,5fe30a2291e01125d4b5b5e3,07fd366a8ebdefe54cdb57d87c81dcd22de25a91,A DISTRIBUTIONAL APPROACH TO CONTROLLED TEXT GENERATION,5b8c9f4a17c44af36f8b7333,An Elementary Introduction To Information Geometry,"We follow Csiszár & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).",other,acknowledging foundational work in Information Geometry
739,5f7ef3f59fced0a24bebb79d,2c60828b5393e98522f02533eba84c212d71f2b0,I Know If the Journey Changes: Flexible Source and Path Validation,555046eb45ce0a409eb62bcb,Lightweight source authentication and path validation,"Recently, there are several proposals addressing both source and path validation that ﬁll the void, such as ICING [3] and OPT [2].###We conduct evaluations of our PSVM on a real testbed built upon the prototype using the normal IP routing performance of the Click router as the baseline, and compare our PSVM with the-state-of-the-art OPT [2].###In particular, since network users cannot conﬁrm the source authenticity of data, and network operators also cannot guarantee that the user packets are not detoured in transmission, numerous network attack surfaces are opened up today [2]–[4].",impact-revealing,highlighting the need for source and path validation in network security
3415,5b8c9f5317c44af36f8b775c,a6876ea89e677a7cc42dd43f27165ff6fd414de5,UNet++: A Nested U-Net Architecture,573698016e3b12023e6da477,U-Net: Convolutional Networks for Biomedical Image Segmentation,"The state-of-the-art models for image segmentation are variants of the encoder-decoder architecture like U-Net [9] and fully convolutional network (FCN) [8].###Long et al. [8] ﬁrst introduced fully convolutional networks (FCN), while U-Net was introduced by Ronneberger et al. [9].",other,acknowledging state-of-the-art models for image segmentation
2438,5eccb534e06a4c1b26a838ac,2709167f1c3a03fa5b970a665ea48ed243aab582,Designing Network Design Spaces,57a4e91dac44365e35c981bb,FractalNet: Ultra-Deep Neural Networks without Residuals.,"We emphasize that R EG N ET models use our basic 100 epoch schedule with no regularization except weight decay, while most mobile networks use longer schedules with various enhancements, such as deep supervision [16], Cutout [4], DropPath [14], AutoAugment [2], and so on.",other,highlighting differences in training schedules and regularization techniques for model performance
2938,5f7ee07491e011a5faf0feb2,c54fb59319288d0d17ad09e6b7dfee6d042f83a8,Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer,5b67b46417c44aac1c86133b,Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment.,"Some others also leverage side information to enhance the alignment performance, including entity descriptions (Chen et al., 2018b; Zhang et al., 2019), attributes (Trsedya et al., 2019; Sun et al., 2017; Yang et al., 2019), neighborhood information (Wang et al., 2018; Yang et al., 2015; Li et al.,…",other,acknowledge methods leveraging side information for alignment performance
1744,,cd7c31b48650b08f39d560117f22bd4c392339ee,Marine Ecology Progress Series 621:1,,,"###LITs, typically selected by investigators (Harrison et al. 1984, Hughes et al. 1999), allow re - searchers to capture the cumulative recruitment in corals that utilize different reproductive strategies, including those that spawn, usually annually (‘spawners’), and corals that release offspring…###SITs were not widely used by the research community until synchronized mass spawning of corals was described in the early 1980s (Harrison et al. 1984).",impact-revealing,acknowledge the historical context and development of research methods in coral reproduction
2087,,cf6102250f3aa1147b5008809041b7fea2522ecf,"A revision of the genus Geitlerinema and a description of the genus Anagnostidinema gen. nov. (Oscillatoriophycidae, Cyanobacteria)",,,"###The tree was validated by maximum likelihood method in RAxML 7.0.4 (StamatakiS 2006) under a GTR model with 1,000 bootstrap repetitions and neighbor joining under maximum composite likelihood model with uniform rates among sites in MEGA 6.06 (tamura et al. 2011) with 1,000 bootstrap repetitions.",impact-revealing,reporting validation methods used in research
204,5d1eb9beda562961f0af981f,934d7bffdba0b560a80a518b99a791a16b3e198c,A Fourier Perspective on Model Robustness in Computer Vision,5b67b4b117c44aac1c866ad9,Motivating the Rules of the Game for Adversarial Example Research.,"These qualify as simple, few query black box attacks that satisfy the content preserving threat model (Gilmer et al., 2018).",impact-revealing,providing context for black box attacks
452,5c7a561ff56def9798e6a297,abe4e92d6e4868115aef57a4eba97ec4d48ac124,language-adversarial transfer learning for low-resource speech recognition,5550464f45ce0a409eb5d53d,Multilingual MRASTA features for low-resource keyword search and speech recognition systems,"is trained jointly on several languages [19], [20].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
312,5f0277e911dc830562231dea,60fc1eefcc4743fcd96597c2c9be11da688e4ef7,Reinforcement Learning to Rank with Pairwise Policy Gradient,5b67b46f17c44aac1c86329e,From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks.,"Similar MDP con gurations are used to model the sequential document selection process in search result diversi cation [11, 35] and multi-page search [41].",impact-revealing,acknowledge related configurations in document selection
3453,5cf48a48da56291d582ab75a,c5f5f179d80a3bf9b4f29750283a87eaca42e91b,neural graph collaborative filtering,573698636e3b12023e729de5,VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback,"tasets in Table 1. Gowalla: This is the check-in dataset [21] obtained from Gowalla, where users share their locations by checking-in. To ensure the quality of the dataset, we use the 10-core setting [10], i.e., retaining users and items with at least ten interactions. Yelp2018∗: This dataset is adopted from the 2018 edition of the Yelp challenge. Wherein, the local businesses like restaurants and bar",other,reporting dataset details and quality assurance measures
1062,,71ceff348eb4eca3055780d8b8787fdf3ae6d020,Cross-Modal Prototype Learning for Zero-Shot Handwriting Recognition,,,"###The convolutional neural network is widely used for offline HCCR [2] due to its powerful ability in dealing with image-like data, while the recurrent neural network is shown to be very effective for online HCCR [10] owing to its efficiency and generality in feature extraction for sequential data.###Improvements on HCCR are gradually reported like faster and more compact models [11], higher accuracies [5], and so on.###For a 3755-class HCCR problem, we show that by training with only 500 classes, CMPL can achieve near 50% accuracy on all 3755 classes, although more than 85% (3255/3755) of the characters are unseen in training stage.###To achieve zero-shot HCCR and motivated by the cooperated learning between printed and handwritten characters in human brain, we use two modalities in a joint learning process: the printed character image and the online handwriting trajectory.###1) Handwritten Chinese Character Recognition: Traditional approaches for handwritten Chinese character recognition (HCCR) usually contain multiple stages [9].###In this paper, we use the printed character images as an efficient and effective side information for zero-shot HCCR.
3) Multi-modal Learning: Learning from multiple related modalities is an important direction in machine learning.###The accuracies on both online and offline HCCR have been constantly improved in recent years [2].###Keywords-printed character; handwritten character; crossmodal; prototype learning; zero-shot
I. INTRODUCTION
Handwritten Chinese character recognition (HCCR) has been studied for more than fifty years [1] and is widely used to evaluate different pattern recognition techniques.",impact-revealing,acknowledge advancements and methods in handwritten Chinese character recognition
1848,,e67a3ef7220d44bd1390f31774a51fa63365196a,Software Security - Theories and Systems,,,"###MSR originated as a simple logic-oriented language aimed at investigating the decidability of protocol analysis under a variety of assumptions [12, 15].###CafeOBJ[12, 13] can be used to specify abstract machines as well as abstract data types.###Many formal approaches have been proposed for the analysis of security protocols including the BAN logic of authentication by Burrows, Abadi, and Needham[20], the CSP with a model checking FDR approach[10, 23, 29], equation rewriting tools [24], and the inductive approach for the analysis of security protocols by Paulson [12] which provides automated support using his Isabelle proof assistant.###examples, in Paulson approach [12], attacker’s knowledge is defined as a set of terms S and the attacker’s analysis of his knowledge S are defined by the fundamental operations parts(S), analz(S), and synth(S).###The verification methods we used were inspired by Schneider’s notion of rank function [16] and also influenced by the inductive theorem proving method Paulson et al. applied to the verification of Kerberos 4 [7–10].###The OTS is described in CafeOBJ[12, 13], an algebraic specification language.###In this section, we explain a basic encryption scheme based on the current literature for the formal analysis of security protocols [5, 6, 10, 12, 20, 23, 24, 29, 30].###The basic definition was given by Dolev and Yao [8] and has been used in the formal analysis of security protocols [5, 6, 12, 20, 23, 24, 29, 30].",impact-revealing,providing context on formal approaches to security protocol analysis
3292,599c797a601a182cd2641df7,63a010c69f00e65c946a68b546bbd42cbed03564,MagNet: A Two-Pronged Defense against Adversarial Examples,573697886e3b12023e66e328,Malware Classification With Recurrent Networks,"They are used in autonomous control for robots and vehicles [1, 3, 4], financial systems [32], medical treatments [31], information security [12, 29], and human-computer interaction [11, 13].",other,reporting applications of a technology in various fields
1169,,c06a5b1ba77e262f3a133dea9eee95662e038bd4,Analytical evaluation of a tradeoff between energy efficiency and responsiveness of neighbor discovery in self-organizing ad hoc networks,,,"###The analytical framework proposed in [4], however, does not consider the scenario in which several channels are utilized for the discovery process, which, instead, is very common in real systems, e.###More specifically, in [4] a family of neighbor discovery algorithms, called birthday protocols, is introduced.###Another shortcoming in [7] is that only the single channel case is considered, as in [4].",impact-revealing,highlighting limitations in existing analytical frameworks for neighbor discovery
1245,,1a5eba91f2a3ffc13a6eafa1e69e6dd9a39d5645,Measuring Inconsistencies Propagation from Change Operation Based on Ontology Partitioning,,,###Our decomposition method is based on the approach in [16] where concept hierarchies are used to extract.###The partitioning method is inspired by approach proposed in [16].,impact-revealing,reporting prior findings on decomposition methods
3354,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,558b0bf284ae84d265c12da1,DIPStorage: Distributed storage of IP flow records,DIPStorage [91] is a distributed flow storage platform for IP flow records based on DHT.,other,reporting on a specific storage platform
3660,5ee8986891e011e66831c452,a9872078cc6dabd2428750543862b45f4a482dfc,Graph Meta Learning via Local Subgraphs,5b67b45517c44aac1c860874,Learning Structural Node Embeddings via Diffusion Wavelets.,"(1) Cyclelc el :
we use a cycle basis graph and attach a distribution of shapes: House, Star, Diamond, Fan [9].###we use a cycle basis graph and attach a distribution of shapes: House, Star, Diamond, Fan [9].###Graph structures present an alternative source of strong signal for prediction [9, 45], especially when node labels are scarce.",other,providing context on graph structures for prediction
371,58d82fcbd649053542fd66af,8fbb115c578e8bfbcc1615bd7af990396abf6776,Identity Matters in Deep Learning,5736960a6e3b12023e51d5fb,Identity Mappings In Deep Residual Networks,"Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool.###In this work, we consider identity parameterizations from a theoretical perspective, while translating some of our theoretical insight back into experiments.",impact-revealing,highlighting the innovative approach inspired by theoretical insights
1034,,89fd42041d8deb508f37f08af98f5e061ade6f3b,Understanding Aging in Bipolar Disorder by Integrating Archival Clinical Research Datasets.,,,"###Additionally, with the support of ISBD, an international team of scientists has begun expansion of AGE-BD to include global datasets and new analyses specific to OABD.###This report describes an initial effort to create an integrated OABD database using the U.S. National Institute of Mental Health Data Archive (NDA).###Our global integrated dataset study (33), which builds upon the analysis presented here, involves over 15 international sites conducting research in OABD, with an expected OABD sample size of over 1,000 cases.###(30–33) In order to clarify the trajectory of functioning in OABD, cognitive assessments will need to be included and considered in the context of BD symptoms and illness progression.###Other literature reports suggest that OABD patients have an average of 3 to 4 comorbid medical conditions, including metabolic syndrome (up to 50%), hypertension (4569%), diabetes mellitus (18–31%), cardiovascular disease (9–49%), respiratory illness (4–15 %), arthritis (16–21%), endocrine abnormalities (17–22%), as well as atopic diseases such as allergic rhinitis and asthma (6–20###As a proof of concept, we tested several hypotheses about the shared variables relevant to OABD.###Due to the clinical heterogeneity of OABD, large samples will be needed to find subgroups or clusters of trajectories and risk factors.###While 25% of people with bipolar disorder (BD) are over age 60, there is a dearth of research on older age bipolar disorder (OABD).###The International Society for Bipolar Disorders (ISBD) convened a global panel that summarized the existing evidence base on OABD (2), and noted many important gaps such as a clear understanding of mood symptom evolution and medical comorbidity characterization.###This report describes a recent U.S. National Institute of Mental Health (NIMH) funded project to create an integrated OABD database using the resources of the NIMH Data Archive (NDA), the Aging & Geriatric Experiments in Bipolar Disorder Database (AGE-BD).###A uthor M anuscript A uthor M anuscript
A uthor M anuscript A uthor M anuscript
OABD are also evident throughout the lifespan including in youth and younger adults with BD (26, 27).###Keywords
bipolar disorder; geriatric; harmonization; medication adherence; functioning
Introduction:
While 25% of BD patients are over 60 years old (1), research on older age bipolar disorder (OABD) is limited.###• This report describes an initial effort to create an integrated OABD database using the U.S. National Institute of Mental Health Data Archive (NDA), the
Aging & Geriatric Experiments in Bipolar Disorder Database (AGE-BD).",impact-revealing,highlighting the need for research on older age bipolar disorder and the creation of an integrated database
617,53e9980eb7602d9702022371,d73cbcb051c3ba30bd037816fbc91852194dd8ca,continual flow pipelines,53e9a6bcb7602d9702ff2f83,Checkpoint Processing and Recovery: Towards Scalable Large Instruction Window Processors,In this paper we use Checkpoint Processing and Recovery (CPR) as the baseline architecture [2] since it has been shown to outperform conventional ROB-based architectures.###CPR is a ROB-free proposal for building scalable large instruction window processors [2].,impact-revealing,highlighting the advantages of the CPR architecture over conventional methods
3062,5efb0d5691e011063336d39c,0cee58946a13a5c2845647b4af8b9d2bf52a8b6b,BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision,5b67b46417c44aac1c861297,Improving Low Resource Named Entity Recognition using Cross-lingual Knowledge Transfer.,"For examples, Cotterell and Duh (2017); Feng et al. (2018) consider NER for a low resource target language.###For examples, [5, 7] consider NER for a low resource target language.",other,acknowledge existing research on NER for low resource languages
2182,,838d1cb723c048dd989cf84dfca486e8caf77798,A Multi-Perspective Framework for Modelling and Analysing the Determinants of Cloud Computing Adoption among SMEs in Australia,,,"###The term ‘cloud’ refers to a large pool of resources such as hardware and software that are accessible via the Internet (Vaquero et al. 2008; A Vouk 2008).###…both professional and academic perspectives which can be listed below:
Chapter 2: Literature Review
46
“Cloud computing embraces cyber-infrastructure and builds on virtualization, distributed computing, grid computing, utility computing, networking, and web and software services” (A Vouk 2008).",impact-revealing,providing a definition and context for cloud computing
3815,5550414745ce0a409eb39ec8,6010ebf22c6cc07e93c5335fa1d128be8c6b190b,understanding big data analytics workloads on modern processors,558b4f2684ae84d265c2ab1a,Scale-out processors.,"So reducing the capacity of last level cache properly may beneﬁt the performance, since a smaller last level cache can shorten last level cache hit latency and reduce L2 cache miss penalty, which corroborates previous work [17, 31].",other,highlighting the significance of cache performance improvements based on prior findings
1698,,87d7ed36f17e86fb6fe29e12438cbf37dd4e82dd,The social identity voting model: Ideology and community structures,,,"###To comport with the other aspects of SIT, groups must be coherent and facilitate intergroup comparison, which we accomplish by assigning utility to the legislator based on his/her actions with respect to the group’s positions.###Thus, the SIT framework suggests a model – the social identity voting model – whose components can be estimated from roll call data.###SIV, based on SIT, thus emphasizes the construction of social identity and its role in ideological identification.###The foundation of SIT (Tajfel and Turner, 1979, 1986) rests on a distinction between personal and social identities, where social identity is largely constructed through group membership.###To this end, the theoretical underpinnings of SIT motivate the articulation of group structure derived from legislator’s votes.###This is motivated by social identity theory (SIT), which posits that an individual’s social identity is determined by the collection of groups to which the individual belongs (for a review, see Hogg, 2006).###SIT is a theoretical framework for understanding group dynamics.###Central to SIT is the thesis that in social situations, social identity is the dominant force in decision-making and action.",impact-revealing,providing context and theoretical framework for social identity theory
3673,5d8dded23a55acd1b54967df,1ecbaf7a2cd3059e07261e72a1195a7c70b3d664,GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"More recent approaches include (Bruna et al., 2013; Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2016; Gilmer et al., 2017; Hamilton et al., 2017; Veli ˇ ckovi ´ c et al., 2018; 2019; Qu et al., 2019; Gao & Ji, 2019; Ma et al., 2019), among others.###More recent approaches include (Bruna et al., 2013; Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2016; Gilmer et al., 2017; Hamilton et al., 2017; Veličković et al., 2018, 2019; Qu et al., 2019; Gao & Ji, 2019; Ma et al., 2019), among others.",other,acknowledge recent approaches in the field
3214,5db92aec47c8f76646216865,146128184cd585b5a1298d3e75e15fb8fbbbd7a5,Improved low-resource Somali speech recognition by semi-supervised acoustic and language model training,573697296e3b12023e61afaf,A Human-Machine Collaborative System for Identifying Rumors on Twitter.,"In countries with a well established internet infrastructure, social media has become an accepted platform for sharing opinions and concerns [1–3].",other,acknowledging the role of social media in opinion sharing
483,5db929e947c8f766461fd005,11ead744b2edbdabe174258a8dfe4023be8e501d,Hierarchical Attention Prototypical Networks for Few-Shot Text Classification,599c794e601a182cd262e8f1,Prototypical Networks for Few-shot Learning.,"Prototypical networks (Snell et al., 2017) learns a metric space in which the model can perform well by computing distance between query and prototype representations of each class and classify the query to the nearest prototype’s class.###The overall architecture of the Hierarchical Attention Prototypical Networks is shown in Figure 1.###For FewRel dataset, we cite the results reported by Snell et al. (2017) which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by Gao et al. (2019) which includes Proto and PHATT.###At the training stage, we also compare the convergence speed between Proto, PHATT, and HAPN on the 10 way 5 shot and 10 way 15 shot FewRel task.###So we compare our model’s SSA with other models such as Proto and PHATT on the 10 way FewRel task, and the shot number ranges from 5 to 25.###A typical example of this approach is prototypical networks (Snell et al., 2017), which averages the vector of few support instances as the class prototype and computes distance between target query and each prototype, then classify the query to the nearest prototype’s class.###From 10 way 5 shot task to 10 way 15 shot settings, the Proto model takes almost twice time to achieve 70 % accuracy on validation set, in other words, the convergence speed will decrease sharply when we increase the number of support instances, but this problem can be effectively alleviated when we use hierarchical attention mechanism.###…distribution over the classes in L can be produced by a softmax function over distances between all prototypes vector and the target query q As Snell et al. (2017) mentioned, squared Euclidean distance is a reasonable choice, however, we will introduce a more effective method in section…###Firstly, we compare our model with several traditional models such as Finetune and kNN, Then we compare our model with ﬁve state-of-the-art few-shot learning models based on neural networks, they are MetaN (Munkhdalai and Yu, 2017), GNN (Garcia and Bruna, 2018), SNAIL (Mishra et al., 2018), Proto (Snell et al., 2017) and PHATT (Gao et al., 2019) respectively.###In this subsection, we will show the effects of hierarchical attention and support set augmentability of three Proto-based models and the convergence speed comparison.###The prototypical networks (Snell et al., 2017) has achieved excellent performance in few-shot image classiﬁcation and few-shot text classiﬁcation (Han et al., 2018; Gao et al., 2019) tasks respectively, so our model is based on prototypical networks and aims to get promotion.###…such as Finetune and kNN, Then we compare our model with ﬁve state-of-the-art few-shot learning models based on neural networks, they are MetaN (Munkhdalai and Yu, 2017), GNN (Garcia and Bruna, 2018), SNAIL (Mishra et al., 2018), Proto (Snell et al., 2017) and PHATT (Gao et al., 2019) respectively.###Prototypical Networks Prototypical networks compute a prototype vector as the representation of each class, and this vector is the mean vector of the embedded support instances belonging to its class.",impact-revealing,describing the prototypical networks and their application in few-shot learning
3927,555045d745ce0a409eb59fe4,460d131e081486491af532604351a0691b52b11f,User-level psychological stress detection from social media using deep neural network,53e9af94b7602d97039e0183,Learning multiple layers of representation,"Recent years, extensive researches on deep learning show superior ability of deep neural networks (DNN) in learning features from large scale unlabeled data [12-14].###An auto encoder is a basic unit in deep neural networks for learning distinctive attributes from data [12-14].",other,highlighting the effectiveness of deep neural networks in feature learning
1699,,7bc4aa8718b0896c4763d6fd8724c0de6286bb5f,The role of moral emotions and individual differences in consumer responses to corporate green and non-green actions,,,"###At the collective level of the self-concept, individuals are motivated by the welfare of the groups to which they belong (Brewer and Gardner 1996; Tajfel and Turner 1979).",impact-revealing,providing context on self-concept motivation
2813,5f53599a91e0110c40a7bc91,5e7047851d05b2ecef5de451dda5404acda726de,learning from protein structure with geometric vector perceptrons,5f6a1acb9fced0a24b69399e,Protein Sequence Design with a Learned Potential,"A number of CPD methods (Anand et al., 2020; Zhang et al., 2019; Shroff et al., 2019) and the MQA methods 3DCNN (Derevyanko et al., 2018) and Ornate (Pag ` es et al., 2019) exemplify the power of this approach.###A number of CPD methods (Anand et al., 2020; Zhang et al., 2019; Shroff et al., 2019) and the MQA methods 3DCNN (Derevyanko et al.",other,acknowledge existing methods and their effectiveness
3794,5cf48a33da56291d5829579e,10973505dd4d872c13a37322a50453bf5157c552,Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search,53e9b4b4b7602d9703fc7c89,Objective Improvement in Information-Geometric Optimization,"For the case of exponential family with expectation parameters, Akimoto & Ollivier (2013) have shown that (10) leads to a monotone increase of J(θ), summarized as follows.1
Proposition 2 (Theorem 12 of (Akimoto & Ollivier, 2013)).",other,reporting prior findings on exponential family parameters
809,5ef3247091e0110c353da5ff,06bf758b7e7fd675ceb2d008520db51631716d42,Embedding-based Retrieval in Facebook Search,5c35e429df5b8c0b3c3ac2db,An Introduction to Neural Information Retrieval.,"s issuing the query and the context where the searcher is. Because of this, embedding-based retrieval in Facebook search is not a text embedding problem, as is actively researched in the IR community [13]. Instead it is a more complex problem that requires understanding of text, user, and the context altogether. To deploy embedding-based retrieval in Facebook search, we developed approaches to address###omains including computer vision and recommendation system, it has been an active research topic in information retrieval community and search engine industry as the next generation search technology [13]. In general, a search engine comprises a recall layer targeting to retrieve a set of relevant documents in low latency and computational cost, usually called retrieval , and a precision layer targeti",impact-revealing,highlighting the complexity of embedding-based retrieval in search engines
1733,,77a8f18640aaae1e51b201569878fa7bb809f2d2,Evidence for automatic on-line adjustments of hand orientation during natural reaching movements to stationary targets.,,,"###16530022-3077/08 $8.00 Copyright © 2008 The American Physiological Societywww.jn.org
the slot and to align their hand with the orientation of the slot (Perenin and Vighetto 1988).###The paradigms used in this study were inspired by the tasks of Perenin and Vighetto (1988) and Goodale et al. (1991).",impact-revealing,acknowledge inspiration from prior studies
3931,5b67b4b917c44aac1c867dbc,d18b48f77eb5c517a6d2c1fa434d2952a1b0a825,hierarchical graph representation learning with differentiable pooling,5a260c8117c44a4ba8a30adf,Representation Learning on Graphs: Methods and Applications,"[17] provides a conceptual review of recent advancements in this area, and Bronstein et al.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
168,5ec49a639fced0a24b4de7d4,9eda533cf0badf8dbed5c8240bb828b622328183,Fine-grained Fact Verification with Kernel Graph Attention Network,5d9ed2d847c8f76646f797b7,Gear: Graph-Based Evidence Aggregating And Reasoning For Fact Verification,"The document retrieval step retrieves related Wikipedia pages and is kept the same with previous work (Hanselowski et al., 2018; Zhou et al., 2019; Soleimani et al., 2019).###In our experiments on FEVER (Thorne et al., 2018a), a large-scale fact verification benchmark, KGAT achieves a 70.38% FEVER score, significantly outperforming previous BERT and Graph Neural Network (GNN) based approaches (Zhou et al., 2019).###The per-node predictions are combined by the “readout” function in graph neural networks (Zhou et al., 2019), where KGAT uses node kernels to learn the importance of each evidence.###The aggregation is done by a graph attention mechanism, the same with previous work (Zhou et al., 2019).###BERT, the pre-trained deep bidirectional Transformer, has also been used for better text representation in FEVER and achieved better performance (Devlin et al., 2019; Li et al., 2019; Zhou et al., 2019; Soleimani et al., 2019).###Different from previous work (Zhou et al., 2019), we follow the standard graph label prediction setting in graph neural network (Veličković et al., 2017) and split the prediction into two components: 1) the label prediction in each node conditioned on the whole graph P (y|np, G); 2) the evidence…###GEAR (Zhou et al., 2019) formulates claim verification as a graph reasoning task and provides two kinds of attentions.###In addition, we replace kernel with dot product to implement our GAT version, which is similar to GEAR, to evaluate kernel’s effectiveness.###…as shown in Figure 1, a system could first
retrieve related evidence sentences from the background corpus, conduct joint reasoning over these sentences, and aggregate the signals to verify the claim integrity (Nie et al., 2019a; Zhou et al., 2019; Yoneda et al., 2018; Hanselowski et al., 2018).###Nevertheless, for more fair comparisons, our following experiments are all based on ESIM sentence retrieval, which is the one used by GEAR, our main baseline (Zhou et al., 2019).###BERT-pair, BERT-concat and GEAR are three baselines from the previous
2https://github.com/sheffieldnlp/ fever-scorer
work (Zhou et al., 2019).###The ESIM based sentence retrieval keeps the same as the previous work (Hanselowski et al., 2018; Zhou et al., 2019).###Table 5 shows the example claim used in GEAR (Zhou et al., 2019) and the evidence sentences retrieved by ESIM, among which the first two are required evidence pieces.###With ESIM sentence retrieval, same as the previous work (Zhou et al., 2019; Hanselowski et al., 2018), KGAT outperforms the graph attention models GEAR and our GAT on both development and testing sets.###Similar to previous research (Zhou et al., 2019), KGAT constructs the evidence graph G by using each claim-evidence pair as a node and connects all node pairs with edges, making it a fullyconnected evidence graph with l nodes: N = {n1, . . . , np, . . . , nl}.###This makes fact verification a rather challenging task, as it requires the fine-grained reasoning ability to distinguish the subtle differences between truth and false statements (Zhou et al., 2019).###GEAR utilizes a graph attention network to extract supplement information from other evidence and aggregate all evidence through an attention layer.",impact-revealing,reporting on the methodology and performance of the KGAT model in fact verification
868,5ebbc75d91e0119bc4e43623,407f1d16ba4eb3cb4851429cae46c97d723a35a5,invertible image rescaling,5b67b4b117c44aac1c866cea,Glow: Generative Flow with Invertible 1x1 Convolutions.,"Neither the conventional adversarial training techniques of generative adversarial nets (GANs) [21] nor the maximum likelihood estimation (MLE) method for existing invertible neural networks [15,16,29,4] could achieve our goal, since the model distribution doesn’t exist here, meanwhile these methods don’t guide the distribution in the latent space.###, maxθ Eq(x)[log f−1 θ #[p(y, z)]], which is widely adopted by prevalent flow-based generative models [15,16,29,4].###The invertible neural network (INN) [15,16,29,32,22,8,13] is a popular choice for generative models, in which the generative process x = fθ(z) given a latent variable z can be specified by an INN architecture fθ.",impact-revealing,highlighting limitations of conventional adversarial training techniques and their applicability
1357,,f2404290091b2663aaf0e29563d7a2355d5b4362,A miniaturized device for bioluminescence analysis of caspase-3/7 activity in a single apoptotic cell,,,"###Fluorescent markers are commonly used for the detection of caspase-3 in flow cytometry of free cells [18] and ELISA assays in cell lysates [19, 20].",impact-revealing,providing context for the use of fluorescent markers in specific assays
1942,,019b83d3399e20a310997aa77380636409fffb6c,The Effects of Morphological Awareness on Reading in Chinese and English Among Young Chinese Children: A Longitudinal Study,,,"###These results contrasted findings from some previous studies involving native Chinese-speaking children (Ku & Anderson, 2003; Li et al., 2002; Shu et al., 2006; Wang, 2000), in which morphological awareness was demonstrated to be significantly and uniquely associated with reading comprehension.###While vocabulary knowledge may be important for older children’s reading comprehension (Ku & Anderson, 2003; Shu et al., 2006), our results indicated that for children who are beginner readers of Chinese such as those involved in our study, oral vocabulary may be less predictive of reading…###Results from our study and others (e.g., Carlisle, 1995; Carlisle & Fleming,
98
2003; Chen et al., 2009; Shu et al., 2006) have shown that morphological awareness can predict English and Chinese vocabulary growth as well as reading comprehension over time.###A handful of studies have preliminarily demonstrated that morphological awareness contributes to reading comprehension among
18
Chinese children (Ku & Anderson, 2003; Li et al., 2002; McBride-Chang et al., 2007; Shu et al., 2006; Wang, 2000).###…children, studies have demonstrated the importance of compound awareness in character and word reading (Chen et al., 2009; McBride-Chang et al., 2003, 2005, 2008b; Shu et al., 2006), as well as the role of character reading in reading comprehension (e.g.,
93
Leong et al., 2007, 2008a, 2008b).",impact-revealing,highlighting the contrast between current findings and previous studies on morphological awareness and reading comprehension
2393,5e09aa66df1a9c0c416bebf6,4e7dd1e79f0f13650b2612325e6ba8d206dc04fb,DeepGCNs: Can GCNs Go As Deep As CNNs?,53e9a937b7602d9703289b23,Relational Learning Via Latent Social Dimensions,"GCNs are currently used to predict individual relations in social networks [36], model proteins for drug discovery [53, 40], enhance predictions of recommendation engines [24, 49], efficiently segment large point clouds [42], among other fields.###In social networks [36], graphs represent connections between individuals based on mutual interests/relations.",other,highlighting the diverse applications of GCNs in various fields
3766,5c7572b7f56def97988385ce,c342c71cb23199f112d0bc644fcce56a7306bf94,active learning for convolutional neural networks: a core-set approach,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,We initialized all convolutional filters according to He et al. (2016).,other,reporting method initialization
3543,5c2c7a9217c44a4e7cf3189c,9ae43e25b04f5c35173b0bf490612015bd86c08f,Face-Focused Cross-Stream Network for Deception Detection in Videos,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"In this paper, the RPN batchsize is set to 256, and the ResNet50 [13] is used as the backbone model for the face expression branch.###For the spatial stream, we downsample the feature maps of the ﬁnal residual block of ResNet50 [12] in the dimen-sion of depth and obtain a 1024-dimension feature vector.###As shown in Figure 2, ResNet50 is used to compute the temporal feature maps.###These samples are ﬁnally used to train ResNet50.###For the spatial stream, we downsample the feature maps of the final residual block of ResNet50 [13] in the dimension of depth and obtain a 1024-dimension feature vector.###For the temporal stream, given that ﬁve motion frames are matched to one face frame, we utilize the reshape pooling to obtain one 5 × 512-dimension feature vector after the third residual block of ResNet50.###In this paper, the RPN batchsize is set to 256, and the ResNet50 [12] is used as the backbone model for the face expression branch.",other,describing the model architecture and training process
813,5992a2ed5ba2006b76482df8,423ad8249c214e1346f337426f2da56deccf10a8,graph edge partitioning via neighborhood heuristic,573695516e3b12023e47c12e,A scalable distributed graph partitioner,"In this paper, we proposed a new graph edge partitioner Neighbor Expansion (NE) that outperforms other state-of-the-art ones including METIS [8] and Sheep [13] in terms of replication factors.###This does lead to the state-of-the-art replication factors on a great number of graphs [13], but it is not applicable to large graphs.###Among existing partitioners, METIS gives the lowest replication factor which is consistent with literature [3, 13].###Sheep [13] partitions the graph in a divide and conquer manner, which uses more graph structure than Oblivious and HDRF, but it works well only for tree-like graphs.###We compare our NE algorithm with six existing edge partitioners, including METIS [8], RAND [6], DBH [17], Oblivious [6], HDRF [15], and Sheep [13].###This important finding attracts great interests in edge partitioning recently [3, 6, 13, 15, 17].###Using only a single thread, NE and Sheep [13] have similar running time.###This result also echoes the fact reported by [13].",impact-revealing,Highlighting the performance of the proposed method compared to existing state-of-the-art algorithms
3833,5f8fffb591e01125c27ddec9,67f473caaa52a97e65bb1bcb9029a580c4f8d10f,FLAG: Adversarial Data Augmentation for Graph Neural Networks,5a4aef9e17c44a2190f7a591,Few-Shot Learning with Graph Neural Networks,"…Network (GCN) (Kipf and Welling, 2016) and its variants have been applied to a wide range of tasks, including visual recognition (Shen et al., 2018), meta-learning (Garcia and Bruna, 2017), social analysis (Qiu et al., 2018; Li and Goldwasser, 2019), and recommender systems (Ying et al., 2018).",other,reporting prior findings and applications
950,,03cf4d93d9ed19325c0bfc24fc53a4a155fdbcfd,Associations of Physical Fitness and Motor Competence With Reading Skills in 9- and 12-Year-Old Children: A Longitudinal Study,,,"###movement solution for a given task is experience-dependent, because it builds on self-produced instances of trial and error (Gottlieb, 1998; Hadders-Algra, 2000; Thelen & Smith, 1994).###Under this perspective, the gradual learning and performance of an efficient
movement solution for a given task is experience-dependent, because it builds on self-produced instances of trial and error (Gottlieb, 1998; Hadders-Algra, 2000; Thelen & Smith, 1994).",impact-revealing,highlighting the experience-dependent nature of movement solutions
3386,5cede0edda562983788cb3c2,1e43c7084bdcb6b3102afaf301cce10faead2702,BioBERT: a pre-trained biomedical language representation model for biomedical text mining.,5a9cb66717c44a376ffb8ac1,Deep contextualized word representations.,"For instance, ELMo (Peters et al. , 2018) uses a bidirectional language model while CoVe (McCann et al. , 2017) uses machine translation to embed context information into word representations.###Recent developmentofELMo(Peters et al. ,2018)andBERT(Devlin et al. ,2018) has proved the effectiveness of contextualized representations from a deeper structure (e.g., bidirectional language model) for transfer learning.",other,highlighting the effectiveness of contextualized representations in transfer learning
2100,,6bc729a25797ae1c4e08b832daa708f699c6669d,Cross-Task Knowledge Distillation in Multi-Task Recommendation,,,"###Most prior works (Ma et al. 2018a; Tang et al. 2020a; Ma et al. 2019) put efforts on designing different shared network
Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org).###…works either introduce constraints on task-specific parameters (Duong et al. 2015; Misra et al. 2016; Yang and Hospedales 2016) or separate shared and task-specific parameters (Ma et al. 2018a; Tang et al. 2020a; Ma et al. 2019)as a means to share knowledge about how to represent the input feature.###MTL has received increasing interests in recommender systems (Ma et al. 2018b; Lu, Dong, and Smyth 2018; Wang et al. 2018; Pan et al. 2019) due to its ability to share knowledge among different tasks.###Multi-Task Learning (MTL) (Caruana 1997) is widely adopted for predicting different types of user feedback using a unified model (Ma et al. 2018b; Lu, Dong, and Smyth 2018; Wang et al. 2018).###We choose the following MTL models with different shared network architectures for comparison: SharedBottom (Caruana 1997), Cross-Stitch (Misra et al. 2016), MMoE (Ma et al. 2018a), PLE (Tang et al. 2020a).",impact-revealing,acknowledge existing multi-task learning approaches and their applications
769,5c6a37d03a69b1c9e12a9fc4,81b6d24e8f313fd88b0fe5ff6c21dd154fbe32d2,SimGNN: A Neural Network Approach to Fast Graph Similarity Computation,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"ngmatrixfactorizationbasedmethods(NetMF[38]), skip-grambasedmethods(DeepWalk[37],Node2Vec[15],LINE[47]), autoencoderbasedmethods(SDNE[50]),neighboraggregationbased methods (GCN [9, 26, 27], GraphSAGE [16]), etc. Graph-levelembedding. Themostintuitivewaytogenerateone embedding per graph is to aggregate the node-level embeddings, either by a simple average or some weighted average [7, 10, 57], named the",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2452,5bdc31b417c44a1f58a0b240,6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da,Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks,5b67b4b917c44aac1c867dbc,Hierarchical Graph Representation Learning with Differentiable Pooling.,"GNNs have achieved state-of-the-art performance on several graph classiﬁcation benchmarks in recent years, see, e.g., (Ying et al. 2018b)—as well as applications such as protein-protein interaction prediction (Fout et al. 2017), recommender systems (Ying et al. 2018a), and the analysis of quantum…###, (Ying et al. 2018b)—as well as applications such as protein-protein interaction prediction (Fout et al.###Note that in very recent work, GNNs have shown superior results over kernels when using advanced pooling techniques (Ying et al. 2018b).###Recent extensions and improvements to the GNN framework include approaches to incorporate different local structures around subgraphs (Xu et al. 2018) and novel techniques for pooling node representations in order perform graph classiﬁcation (Zhang et al. 2018; Ying et al. 2018b).###Up to now, the evaluation and analysis of GNNs has been largely empirical, showing promising results compared to kernel approaches, see, e.g., (Ying et al. 2018b).###2018) and novel techniques for pooling node representations in order perform graph classification (Zhang et al. 2018; Ying et al. 2018b).###More reﬁned approaches use differential pooling operators based on sorting (Zhang et al. 2018) and soft assignments (Ying et al. 2018b).###…on several graph classiﬁcation benchmarks in recent years, see, e.g., (Ying et al. 2018b)—as well as applications such as protein-protein interaction prediction (Fout et al. 2017), recommender systems (Ying et al. 2018a), and the analysis of quantum interactions in molecules (Sch¨utt et al. 2017).",other,highlighting the achievements and applications of GNNs in various domains
479,5b67b46b17c44aac1c861edd,3bcb5a3e296d4fb427400e112d865abc52435b56,Disconnected Recurrent Neural Networks for Text Categorization,5843777eac44360f108417e2,Deep LSTM based Feature Mapping for Query Classification.,"Shi et al. (2016) replace convolution ﬁlters with deep LSTM, which is similar to what is proposed in this paper.###We ﬁnd that using GRU as recurrent units outperforms LSTM which is utilized by Shi et al. (2016).",impact-revealing,comparing methods and highlighting performance differences
1084,,0e5d223a5db1e6dd40a759aedb6820ca972cbcda,MyUI : Mainstreaming Accessibility through Synergistic User Modelling and Adaptability FP 7-ICT-2009-4-248606 Input to standardisation and report on VUMS cluster standardisation activities Public Document,,,"###There is a greater awareness of the value of inclusive design methodologies for both designer and end user, such as the user testing of product prototypes [10].###A simplified view of these cognitive architectures is known as the GOMS model [10] and still now is most widely used in human computer interaction though it does not consider people with disabilities or non-expert users in detail.",impact-revealing,highlighting the importance of inclusive design methodologies in human-computer interaction
251,5cf48a1eda56291d5827f814,d52961a91f03061c6732a69e292bd1e403e7f8b8,Universal Invariant and Equivariant Graph Neural Networks,5cede0efda562983788ce3b3,On the Universality of Invariant Networks,"In the invariant GNN case, the universality of architectures built using a single hidden layer of such equivariant operators followed by an invariant layer is proved in (Maron et al., 2019b) (see also (Kondor et al., 2018)).###Fully-invariant GNN. Designing Graph (and their higher-dimensional generalizations) NN which are equivariant or invariant to the whole permutation group (as opposed to e.g. only translations) requires the use of a small sub-space of linear operators, which is identiﬁed in (Maron et al., 2019a).###For any squashing function ρ , Comparison with (Maron et al., 2019b).###Maron et al. (2019a) elegantly characterize all such linear functions, and prove that they live in vector spaces of dimension, respectively, exactly b ( k ) and b ( k + (cid:96) ) , where b ( i ) is the i th Bell number.###They are also characterized by Maron et al. (2019a) and belong to a linear space of dimension b ( k s ) .###Indeed, through Noether’s theorem on polynomials, the proof of Maron et al. (2019b) shows that k s (cid:54) n d ( n d − 1) / 2 is suﬃcient for universality, which we cannot seem to deduce from our proof.###2 The case of invariant functions Maron et al. (2019b) recently proved that invariant GNNs of the form (1) are universal approximators of continuous invariant functions.###A variant of Theorem 1 was proved in (Maron et al., 2019b).###One improvement of our result with respect to the one of (Maron et al., 2019b) is that it can handle graphs of varying sizes.###By Maron et al. (2019a) a necessary and suﬃcient condition is P ⊗ k 1 + k 2 vec ( H 3 ) = vec ( H 3 ) , which we can easily check:###On the contrary, Maron et al. (2019b) work with a ﬁxed n , and it does not seem that their proof can extend easily to encompass several n at once.###In light of the characterization by Maron et al. (2019a) of linear invariant and equivariant operators described in the previous paragraph, a GNN of the form (1) is described by As mentioned earlier, this number of parameters does not depend on the number of nodes n , and a GNN described by a…",impact-revealing,discussing the universality and properties of invariant GNN architectures
963,,efd4a608cf6bf876ba56ff35001179ad19f19cbf,Effects of college students’ mindfulness on depression symptoms during the epidemic prevention and control period: The mediating effect of psychological resilience,,,"###This scale is widely used by scholars in China and abroad (52, 53).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3231,5ec49a639fced0a24b4de7d4,9eda533cf0badf8dbed5c8240bb828b622328183,Fine-grained Fact Verification with Kernel Graph Attention Network,5a9cb65d17c44a376ffb8492,DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference,"Many fact verification systems leverage Natural Language Inference (NLI) techniques (Chen et al., 2017b; Ghaeini et al., 2018; Parikh et al., 2016; Radford et al., 2018; Peters et al., 2018; Li et al., 2019) to verify the claim.",other,reporting prior findings on fact verification systems
3139,58437725ac44360f1082f7f7,79da740db9006b2aa3e7b571d038ec895e323121,Accelerating the Super-Resolution Convolutional Neural Network,573695fd6e3b12023e510ff5,Deeply-Recursive Convolutional Network for Image Super-Resolution,"Deeper structures have also been explored in [18] and [19].###networks [8, 18, 19] need to process the bicubic-upscaled LR images.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1001,,8b803339e2aec7e659b2348a54aaaf5baa6445a7,What Makes Digital Support Effective? How Therapeutic Skills Affect Clinical Well-Being,,,"###Peer support through online communities is able to fill gaps in accessing health services for many, helping those whose needs are unmet by traditional resources [32, 62, 78] or individuals who lack adequate peer networks in their daily lives to achieve needed support [44, 46, 89, 91].",impact-revealing,highlighting the role of peer support in addressing health service gaps
1113,,aa06fe5f7d474decb241891d2161ad6403a51688,MR to CT Synthesis Using 3d Latent Diffusion,,,"###Further, comparison did not consider denoising diffusion implicit models or 2D slice-slice evaluation [17].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1873,,66812a2b131761a9d7e942c0b62b085aa7ac28f9,Empowerment in people with COPD,,,"###In recognizing the patient’s empowerment as an important aspect of chronic disease management, this review sought to describe those interventions or approaches that sought to empower patients in the management of their COPD.###This review sought to describe those interventions or approaches that empower patients in the management of COPD.###This approach is particularly useful in undertaking a structured approach to a review of literature, while identifying the conceptual themes that contribute to the field of work.(22) eligibility criteria Papers were included if they were published in peer reviewed journals, written in the English language, and published between 1995 and 31 March 2015.###As a process and an outcome, it is best understood within a specific context, for example, within empowerment for chronic disease management in COPD.###A study by Robinson et al 35 trained community-based nurses to use the transtheoretical model of behavioral change to promote self-management through motivational interviewing with patients with COPD.###Specific questions asked in the review are as follows: • What interventions or approaches empower patients in the management of COPD? • What key factors support empowerment in this patient group? • How do current clinical relationships facilitate empowerment in this patient group? Method An integrative approach following the preferred reporting items for systematic reviews and meta-analyses (PRISMA) approach(21) was used to understand the empirical and theoretical literature on patient empowerment in COPD.(22) This approach is particularly useful in undertaking a structured approach to a review of literature, while identifying the conceptual themes that contribute to the field of work.###Thematic analysis was used to develop conceptual themes on patient empowerment in COPD.###Trust in health care providers was raised as a key theme in the qualitative study by Fotoukian et al 36 on the barriers and facilitators to empowerment in older Iranian patients with COPD.###Data analysis Thematic analysis was used to develop conceptual themes on patient empowerment in COPD.(22) These conceptual themes were validated by a panel of specialists in COPD, chronic disease management, self-management, and patient education (RTD, DAD, SCI, and MH).###An integrative approach following the preferred reporting items for systematic reviews and meta-analyses (PRISMA) approach 21 was used to understand the empirical and theoretical literature on patient empowerment in COPD.",impact-revealing,describing the review's focus on patient empowerment in chronic disease management
2197,5ecfae0d9e795eb20a615048,fde4e53ba166567f3b9b977a866020f10a996c02,LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition,53e99e5bb7602d970271fa2b,Phonetic learning as a pathway to language: new data and native language magnet theory expanded (NLM-e),"Since people coming from different nations and speaking different languages share similar vocal organs and thus similar pronunciations, the ability of alignment learning in one language can help the alignment in another language [19, 42].",other,highlighting the potential for cross-linguistic alignment learning
889,555048d145ce0a409eb71b05,df787a974fff59f557ed1ec620fc345568aec491,learning deep representations for graph clustering,53e9bcadb7602d970492cc34,Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion,This greedy layer-wise training process forms the model of the Stacked Sparse Autoencoder (Vincent et al. 2010).,impact-revealing,describing the training process of a specific model
526,5b67b47917c44aac1c8637c6,5aea95e1ae78a66474051a330ded374e199b658c,Representation Learning on Graphs with Jumping Knowledge Networks,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,"Deeper versions of the model that, in principle, have access to more information, perform worse (Kipf & Welling, 2017).###We use the hyper-parameters as described in Kipf & Welling (2017) for training the models.###Graph Convolu-tional Networks (GCN) (Kipf & Welling, 2017), initially motivated by spectral graph convolutions (Hammond et al., 2011; Defferrard et al., 2016), are a speciﬁc instantiation of this framework (Gilmer et al., 2017), of the form where deg ( v ) is the degree of node v in G .###We compare against three baselines: Graph Convolu-tional Networks (GCN) (Kipf & Welling, 2017), Graph-SAGE (Hamilton et al., 2017) and Graph Attention Networks (GAT) (Veliˇckovi´c et al., 2018).###Many of these approaches broadly follow a neighborhood aggregation (or “message passing” scheme), and those have been very promising (Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Veliˇckovi´c et al., 2018; Kearnes et al., 2016).###Such schemes have been shown to generalize the Weisfeiler-Lehman graph isomorphism test (Weisfeiler & Lehman, 1968) enabling to simultaneously learn the topology as well as the distribution of node features in the neighborhood (Shervashidze et al., 2011; Kipf & Welling, 2017; Hamilton et al., 2017).",impact-revealing,providing context on Graph Convolutional Networks and their performance
2907,5b3d98cc17c44a510f801acc,155b7782dbd713982a4133df3aee7adfd0b6b304,Unsupervised Feature Learning via Non-parametric Instance Discrimination,5a260c8617c44a4ba8a32701,Representation Learning by Learning to Count,"To learn a representation of images, the tasks could be: predicting the context [2], counting the objects [28], filling in missing parts of an image [31], recovering colors from grayscale images [47], or even solving a jigsaw puzzle [27].",other,providing examples of tasks for image representation learning
800,5f44e5bd91e011872f85ed90,9c160a71d3265eedaf7645c39be073c966f10433,A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild,5aed14d617c44a4438158f74,Lip Movements Generation at a Glance,"Although LMD [4] focuses on the lip region, we found that lip landmarks can be quite inaccurate on generated faces.",impact-revealing,highlighting inaccuracies in lip landmarks on generated faces
2212,5e2ac357df1a9c0c41e7fa48,73bc990757d54f2161eceacd330e0aeb189393bf,Building attention and edge message passing neural networks for bioactivity and physical–chemical property prediction,573696026e3b12023e5160cd,Molecular graph convolutions: moving beyond fingerprints,"They were introduced as a framework to generalise several proposed techniques [14, 24, 25, 28, 29, 37, 38], and have demonstrated state-of-the-art results on multiple related benchmarks.###Other molecular graph convolution methods were reported by Kearnes et al. [25] and Coley [26] as extensions to Duvenaud’s method.",other,highlighting the significance of a framework that generalizes techniques and achieves state-of-the-art results
4023,5550411c45ce0a409eb3897f,fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5,Neural Machine Translation by Jointly Learning to Align and Translate,53e9bd8cb7602d9704a3269b,Sequence Transduction with Recurrent Neural Networks,"A similar approach of aligning an output symbol with an input symbol was proposed recently by Graves (2013) in the context of handwriting synthesis.###Once a model is trained, we use a beam search to ﬁnd a translation that approximately maximizes the conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al. , 2013).",other,reporting prior findings in handwriting synthesis
3419,5a4aef9e17c44a2190f7a6fb,af03709f0893a7ff1c2656b73249d60030bab996,NISP: Pruning Networks Using Neuron Importance Score Propagation,59ae3be32bbe271c4c71ba45,Visual Relationship Detection with Internal and External Linguistic   Knowledge Distillation,"Despite their impressive predictive power on a wide range of tasks [33, 40, 41, 13, 15, 48, 46, 24, 44, 42, 12, 45], the redundancy in the parameterization of deep learning models has been studied and demonstrated [6].",other,highlighting the predictive power and redundancy issues in deep learning models
128,5e5e189993d709897ce1ddbc,055fd6a9f7293269f1b22c1470e63bd02d8d9500,Reformer: The Efficient Transformer,59ae3be32bbe271c4c71b791,The Reversible Residual Network: Backpropagation Without Storing Activations.,"…2020 Published as a conference paper at ICLR 2020 We introduce the Reformer model which solves these problems using the following techniques: • Reversible layers, ﬁrst introduced in Gomez et al. (2017), enable storing only a single copy of activations in the whole model, so the N factor disappears.###Reversible residual networks were introduced by Gomez et al. (2017) where it was shown that they can replace ResNets for image classiﬁcation.",impact-revealing,highlighting the innovative techniques of the Reformer model
697,53e9b42fb7602d9703f2696f,8c34cdd2bab66623d2831004fbd1fa1cdf8a0366,Improving memory scheduling via processor-side load criticality information,53e9a3abb7602d9702cbe7eb,Focusing processor policies via critical-path prediction,"Fields et al. proposed a method for statically determining the critical path of an application using directed graphs, and proposed a token-based hardware mechanism to approximate this in hardware [5].###The generally accepted method of determining the critical path of program execution was proposed by Fields et al. [5].",impact-revealing,reporting existing findings on critical path determination
1705,,c7538aec1e64eef8d8a439db4287fcbbf37ef22e,Social change in South Africa: a historical approach to relative deprivation.,,,"###Our theoretical reasoning builds on social identity theory (Tajfel, 1978; Tajfel & Turner, 1979, 1986).###Building on social identity theory (Tajfel & Turner, 1979, 1986), we argue that ingroup identification plays a central role in predicting which trajectory of relative deprivation group members will perceive.",impact-revealing,highlighting the theoretical foundation of the research
1518,,ff7eb4ba61789a45d5e75271943050c6f322c4bc,Chemoradiotherapy for limited-disease small-cell lung cancer in elderly patients aged 75 years or older.,,,"###There are four possible ways to modify the intensity of therapy: (1) administer chemotherapy alone; (2) change the relative timing of chemotherapy and radiotherapy; ( 3 ) decrease the drug doses and number of cycles of chemotherapy, and (4) decrease the dose and intensity of thoracic radiotherapy.###3 4 4 3 4 RBC Used Neutropenic fever ( 3 ), esophagitis (3) CR 65.6/Alive###6 4 4 2 1 None None Pneumoniti (5), neutropenic fever ( 3 ) CR 6.4/Dead###Because SCLC is highly sensitive to chemotherapy and radiotherapy, the standard treatment for limited-disease SCLC (LD-SCLC) has been a combination of platinum and etoposide with concurrently administered thoracic radiotherapy, as long as the patients are in good general condition (2,  3 ). Such elderly patients, however, may show decreased clearance of the anticancer agents commonly used for the treatment of SCLC, including cisplatin and ...###3 4 4 3 4 RBC Used Neutropenic fever (3), esophagitis ( 3 ) CR 65.6/Alive###2 3 4 1 2 None Used Pneumoniti ( 3 ), esophagitis (2), anorexia (2) CR 21.3/Dead###5 3 4 2 3 None Used Neutropenic fever ( 3 ), esophagitis (2), anorexia (2) CR 13.1/Dead###Thus, the conventional schedule at a total dose of 45 – 50 Gy in 25 fractions might be preferable in the elderly ( 3 ).###Thoracic radiotherapy with accelerated hyperfractionation at a total dose of 45 Gy in 30 fractions, the standard schedule for LD-SCLC, was associated with grade 3 – 4 esophagitis in as high as 32% of the patients and grade 4 leukopenia in 44% of the patients (2, 3 ,5).",impact-revealing,discussing treatment options and considerations for elderly patients with SCLC
1317,,a4a635a9ae8a98fff5593e850de48137adabf6e0,Atomic contact vectors in protein‐protein recognition,,,"###This is in agreement with previous studies, 2 which suggested that the surface area of the interface for recognition complexes is limited by the intrinsic physical requirement for each of the protomers to fold independently and exist in solution without aggregating.###2,3 While on average this is a true statement, there is great overlap and we show that hydrophobicity does no hold a sufﬁcient discriminatory power.",impact-revealing,acknowledging prior findings and limitations in recognition complexes
3490,5c0495fa17c44a2c747059aa,172f096eecc0290442b35908fbef01d62e668e0a,RFUZZ: Coverage-Directed Fuzz Testing of RTL on FPGAs,55465e030cf2939c2fee9562,Abstraction-Guided Simulation Using Markov Analysis for Functional Verification,[16] use a manually designed abstract model of the DUT to automatically generate inputs that maximize coverage.,other,reporting prior findings on abstract model generation
2710,5f58a1b491e011e46ee73247,435bc42450259a22cfba92b40217b8d26f4a7ed5,Adversarial Attack on Large Scale Graph,57d063b9ac4436735428e7bf,Deep Face Recognition.,"R ECENTLY , with the enormous advancement of deep learning, many domains like speech recognition [1] and visual object recognition [2], have achieved a dramatic improvement out of the state-of-the-art methods.",other,highlighting the impact of deep learning advancements in various domains
3350,5ddcf7f53a55ac1c5e8cce13,1f2577071ca2aa1086f4b1c12cd911061aeea960,meta-learning of neural architectures for few-shot learning,599c794e601a182cd262e8f1,Prototypical Networks for Few-shot Learning.,", learning to compare new samples to previously seen ones [47, 51] or meta-learning a subset of weights that is shared across tasks but fixed during task learning [57, 19].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2321,5fae6dced4150a363cec41f7,71c97e7d4a529a21f32d98aad72fbbf32c9ee32e,Anomaly-Based Intrusion Detection From Network Flow Features Using Variational Autoencoder,53e9a53ab7602d9702e5f540,Machine Learning Approach For Ip-Flow Record Anomaly Detection,[25] presented an anomaly detection method by processing large volumes of Netflow records based on SVM.,other,reporting prior findings on anomaly detection methods
2730,5dbab2523a55acea3c05b02b,395de0bd3837fdf4b4b5e5f04835bcc69c279481,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",5d3ed2653a55ac61d998598b,RoBERTa: A Robustly Optimized BERT Pretraining Approach,"We use the same pre-training data as Liu et al. (2019), consisting of 160Gb of news, books, stories, and web text.",other,reporting data sources used for pre-training
643,558ab239e4b037c08758a249,c1757f34c23241960a0089c5117fa3b676902951,the effect of code reordering on branch prediction,53e9aff4b7602d9703a47f83,Correlation and aliasing in dynamic branch predictors,"Negative int rference happens more often than positive interference, and is the main cause of decreased prediction accuracy [29, 20].",impact-revealing,highlighting the prevalence and impact of negative interference on prediction accuracy
1366,,d4cf3ba714c6b7fafdb44fcec90fc265c2ff67d4,Practical Context-Aware Permission Control for Hybrid Mobile Applications,,,"###The application-centric permission model is not sufficient for transitive policy enforcement allowing privilege escalation attacks as shown by the recent attacks [13, 15, 19].",impact-revealing,highlighting the insufficiency of the application-centric permission model in preventing privilege escalation attacks
529,5cede0e8da562983788c741f,b56e5fb4f367a8d54614f1047bd4f9a2d58b9973,Multi-Interest Network with Dynamic Routing for Recommendation at Tmall,599c7958601a182cd26331f6,Deep Interest Network for Click-Through Rate Prediction.,"Deep Interest Network (DIN)[31] makes the user representation vary over different items with attention mechanisms to capture the diversity of user interests.###Moreover, DIN focuses on the ranking stage as it handles thousands of items, however, MIND decouples the process of inferring user representations and measuring user-item compatibility, making it applicable to billion-scale items in the matching stage.###When the value of K in Algorithm 1 equals to 1, MIND degenerates to YouTube DNN, thus MIND can be viewed as generalization of YouTube DNN. DIN.###In terms of capturing diverse interests of users, MIND and DIN share the similar goal.###Besides the industrial applications proposed by [7, 31], various types of deep models have gained significant attention.###Nevertheless, the adoption of attention mechanisms also makes it computationally prohibitive for large-scale applications with billion-scale items as it requires re-calculation of user representation for each item, making DIN only applicable for the ranking stage.###To deal with diverse interests, DIN applies an attention mechanism at the item level, while MIND employs dynamic routing to generate interest capsules and considers diversity at the interest level.",impact-revealing,comparing and contrasting user representation methods in recommendation systems
1592,,868342ed03ef82c15d7e40e0df07604b75042483,Achievement of European Society of Cardiology/European Atherosclerosis Society lipid targets in very high-risk patients: Influence of depression and sex,,,"###They are also less likely to have their CV risk factors treated as intensively as males and post PCI to meet lipid guidelines targets, as well as more likely to have depression [8, 9].###Considering together the greater prevalence of depression in women and likelihood of reduced adherence to medication in those with depression, our findings highlight an important area in the field of CVD prevention that merits further research to understand and manage better these identified gaps in quality of clinical care [9, 27].",impact-revealing,highlighting gaps in clinical care for women with CVD
3348,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,53e9a79eb7602d97030dd4f3,A Netflow Based Internet-Worm Detecting System In Large Network,"Detection of worms can be categorized as trap-oriented, packet-oriented and connection-oriented [18].###[18] conducted a survey on understanding, detection and tracking, and defending against botnets.###[18] proposed FloWorM system that includes tracker, analyzer and reporter based on NetFlow data.###2004 [64] Statistic patterns DoS and DDoS 2005 [29] Host behavior based Worm outbreaks 2005 [30] IP aggregation Detection and monitoring 2006 [110] Flow aggregation IDS 2007 [109] Trust and reputation model IDS 2007 [28] Flow signature and honeypot logs Worm detection 2008 [18] Heuristics Worm detection 2008 [158] Statistic Anomaly detection 2009 [70] Heuristics NAT detection 2009 [137] Decision tree Dictionary attack 2009 [43] K-means Behavior-based NAC 2009 [150] Information theory Risk detection 2009 [155] Statistic Top N detection 2009 [136] Statistic Spam machines 2010 [135] NBA Malware 2010 [54] Spatial-temporal aggregating Malicious website detection 2011 [114] Statistic of host behavior Attack Detection 2011 [61] Dynamic entropy DoS 2011 [44] Statistic DDoS and port scan 2011 [41] Host behavior and PageRank Botnets detection 2011 [125] Time series IDS 2011 [42] PageRank Botnets detection",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1141,,501ba262e08d467d435a1bf75623726ecab7e556,CrossLoc3D: Aerial-Ground Cross-Source 3D Place Recognition,,,"###Inspired by diffusion model [11, 23, 4], we hope to close the representation gap between different sources, by gradually learning a distribution shift towards a canonical space at each refinement step in the training process.###Our approach is inspired by the diffusion model [11, 23] and cold diffusion [4] and we propose a novel iterative process to refine multi-grained features from coarse to fine.###The diffusion models [11, 23] use a deep neural network to learn a small distribution shift between two consecutive steps.",impact-revealing,highlighting the innovative approach inspired by diffusion models for refining features
2448,5c9df4643cb210d271bea0dd,62c13867cc9d80639100dc7bd4151f728c27d9ab,efficient load value prediction using multiple predictors and filters,55465e430cf2939c2feea5da,BeBoP: A cost effective predictor infrastructure for superscalar value prediction,"The seminal work on CVP found that the same holds true for all instruction values [7], [8], and subsequent###We find, like other work before [7], [8], that LVP needs a high confidence to avoid reducing performance through mispredictions, and therefore use a confidence threshold of 7, which, with the FPC vector shown in Table IV, corresponds to an effective confidence of 64 consecutive observations of a value.###In this work we assume a flush-based recovery microarchitecture, similar to the work of Perais and Seznec [7], [8].###Predicts Load values Load addresses Context agnostic Last Value Prediction (LVP) [1] Stride Address Prediction (SAP) [6] Context aware Context Value Prediction (CVP) [7], [8] Context Address Prediction (CAP) [3]###To learn more about the two load value prediction approaches and the recent advances towards practical implementations of value prediction, we encourage the readers to visit prior art papers [3], [7], [8].",other,acknowledging prior findings in load value prediction
2456,5c20b1fcda5629702063afe6,bb221fa131ac1aa89857b7dca2117bc0e70e32cc,Performance Improvement by Prioritizing the Issue of the Instructions in Unconfident Branch Slices,53e9ba64b7602d970468183c,Execution-based prediction using speculative slices,"Speculative precomputation extracts a program slice that includes the instructions that are necessary to compute the outcome of difﬁcult branches, and forking the slice from the original thread as a helper thread in a different context [31], [32].",other,providing context for speculative precomputation
2445,5eccb534e06a4c1b26a83a1b,afdac86a934df38748d6ded69a5ff48b06a40053,Defending and Harnessing the Bit-Flip Based Adversarial Weight Attack,573696ce6e3b12023e5ce95a,Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift,"Effect of Batch-Normalization and Dropout Nowadays, the presence of Batch-Normalization (BN) layer in the deep neural network is customary to accelerate the training of DNN [11], by normalizing the hidden features that forwarded along the inference path.",other,highlighting the role of Batch-Normalization in deep neural networks
265,5f8d6be69fced0a24bbaaf7b,6427b12aa3ddb4c89b7879c43267cd4a9f0ad1c7,DE-RRD: A Knowledge Distillation Framework for Recommender System,5e3a93a93a55ac06c6119df5,Collaborative Distillation for Top-N Recommendation,"Recently, the size of the recommender model is continuously increasing, and the computational time and memory cost required for the inference are also increasing accordingly [13, 25, 28, 30].###However, there are still limitations in existing methods [13, 25].###However, a growing scale of users (and items) and sophisticated model architecture to capture complex patterns make the size of the model continuously increasing [13, 25, 28, 30].###By using the additional supervisions from the teacher model, the state-of-the-art methods [13, 25] have achieved comparable or even better performance to###Unlike the existing methods [13, 25] that distill the knowledge of an item at a time, RRD formulates this as a ranking matching problem between the recommendation list of the teacher and that of the student.###Recently, inspired by the huge success of KD in the computer vision field, a few work [13, 25] have adopted KD to RS.###The student model trained with KD has comparable performance to that of the teacher, and also has a lower latency due to its small size [13, 25].###Figure 1: The existingmethods [13, 25] distill the knowledge###ance for distinguishing the items that each user would like and the items that each user would not be interested in among numerous unobserved items only labeled as ‘0’ [13].###Motivated by the significant success of knowledge distillation (KD) in the computer vision field, a few work [13, 25] have employed KD for RS to reduce the size of models while maintaining the performance.",impact-revealing,highlighting the increasing complexity and limitations of recommender systems
922,599c7cdf601a182cd27e33f3,53b047e503f4c24602f376a774d653f7ed56c024,practical black-box attacks against machine learning,53e9a93eb7602d97032928b5,Intriguing properties of neural networks.,"Efforts in the security [5, 2, 9, 18] and machine learning [14, 4] communities exposed the
∗Work done while the author was at Google.###This is a considerable departure from previous work, which evaluated perturbations required to craft adversarial examples using either: (a) detailed knowledge of the DNN architecture and parameters [2, 4, 9, 14], or (b) an independently collected training set to fit an auxiliary model [2, 4, 14].###…adversarial saliency value S(~x, t)[i] of component i for an adversarial target class t is defined as:
S(~x, t)[i] =
{ 0 if ∂Ft
∂~xi (~x) < 0 or ∑ j 6=t ∂Fj ∂~xi
(~x) > 0 ∂Ft ∂~xi (~x) ∣∣∣∑j 6=t ∂Fj∂~xi (~x)∣∣∣ otherwise (6)
1Our attack can be implemented with other adversarial example algorithms.",impact-revealing,highlighting a departure from previous work in adversarial example evaluation
2786,5ef0816891e0112aee042b88,5c126ae3421f05768d8edd97ecd44b1364e2c99a,denoising diffusion probabilistic models,599c7950601a182cd262f003,Learning to Generate Samples from Noise through Infusion Training.,"Other methods for learning transition operators of Markov chains include infusion training [2], variational walkback [13], generative stochastic networks [1], and others [47, 51, 33, 39].",other,acknowledge existing methods for learning transition operators
1085,,c8e3696e563d99c725c7079978597afd4d1e5f22,A Model and Simulation Environment for Symbiotic Automation and Assistive Devices,,,"###Again, the UCAADS model and USE build on advances in workflow technology [18] and GOMS models [19].###The model incorporates two types of elements: workflows [18] and GOMS models [19].",impact-revealing,acknowledge foundational models and technologies
2660,58437722ac44360f1082f13a,d2e4587744a89bad95fea69e08842cad6c8ff0dd,Temporal ensembling for semi-supervised learning,58437725ac44360f1082ff96,Making Neural Networks Robust to Label Noise: a Loss Correction Approach.,"mi-supervised learning task where part of the training process is to identify the labels that are not to be trusted. For recent work in this area, see, e.g., Sukhbaatar et al. [21] and Patrini et al. [12]. In this context of noisy labels, Reed et al. [14] presented a simple bootstrapping method that trains a classiﬁer with the target composed of a convex combination of the previous epoch output and th###For recent work in this area, see, e.g., Sukhbaatar et al. (2014) and Patrini et al. (2016).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3759,5d04eeba8607575390f83f3a,b67fd0612e3f72faee0fed9b1e930b69ed7ee98d,sparse reram engine: joint exploration of activation and weight sparsity in compressed neural networks,5c20a3d8df5b8c0b3cfaac71,NeuroSim: A Circuit-Level Macro Model for Benchmarking Neuro-Inspired Architectures in Online Learning.,"As ReRAM cells are nonideal [5, 31], the per-cell current deviation accumulates on the bitline and leads to overlap with neighboring states (i.",other,highlighting the non-ideal characteristics of ReRAM cells
1404,,44ba91854390e6795681e022adbeabfc51359bbc,A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem,,,"###All of these methods build on model-free single-agent approaches and constrain the policy to stay in the dataset’s distribution by using either SARSA-like schemes (such as ICQ [21] and IQL [25]) or policy regularization (such as CQL [55] and TD3+BC [56]).###All of these methods build on model-free single-agent approaches and constrain the policy to stay in the dataset’s distribution by using either SARSA-like schemes (such as ICQ [64] and IQL [30]) or policy regularization (such as CQL [31] and TD3+BC [19]).###Finally, we consider the independent learners extensions to [31] and [19], respectively ICQL and ITD3+BC, as well as the state-of-the-art model-free offline MARL method, OMAR [43].###COMBO [66] proposed a similar but more conservative version of MOPO by using CQL instead of SAC and learning on both generated and dataset’s states.###Finally, OMAR [43] proposes to alleviate miscoordination failure in offline MARL by adding a zeroth order optimization method on top of multi-agent CQL, achieving state-of-the-art performance on a variety of tasks.###[55] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.###Finally, we consider the independent learners extensions to [55] and [56], respectively ITD3+BC and ICQL, as well as the state-of-the-art model-free offline MARL method, OMAR [23].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
4022,5e6cae3493d709897ccff2f9,804f23ac1a4a56b8dc5bb7201dab7b8cece76a70,classifying memory access patterns for prefetching,53e9b49ab7602d9703fa1cae,A Compiler-Directed Data Prefetching Scheme For Chip Multiprocessors,"Compiler-based techniques [1, 10, 18, 46] perform static code analysis to generate prefetch targets.###Prior works on compiler assisted prefetching [1, 8, 10, 18, 33, 35, 46, 52] struggle to address these questions.",other,acknowledge challenges in prior works on compiler-assisted prefetching
1009,,1af0a64fb58b8c77c5a1b3c1439912472377099a,Clinical advances in obsessive-compulsive disorder: a position statement by the International College of Obsessive-Compulsive Spectrum Disorders,,,"###…of this article is prohibited.
and CBT have been thought to have broadly similar efficacy in acute treatment, current guidelines recommend taking account of patients’ clinical features, needs and preference as well as service availability when choosing a first-line treatment (Baldwin et al., 2014).###A number of evidence-based clinical guidelines for managing OCD have been published (Bandelow et al., 2012; Baldwin et al., 2014; Sookman et al., 2015).###As there is no available evidence suggesting a duration of treatment beyond which treatment can be discontinued safely, more recent guidelines emphasized the importance of maintaining medication for at least 12 months to reduce relapse risk (Baldwin et al., 2014).",impact-revealing,highlighting the importance of clinical guidelines in OCD treatment
451,5c04966a17c44a2c74708401,51203e9d5620abdcdf6c9be93b1e221e79cda67d,Transfer Learning of Language-independent End-to-end ASR with Language Model Fusion,5aed14d117c44a4438158af2,ESPnet: End-to-End Speech Processing Toolkit,All networks are implemented by ESPnet toolkit [37] with pytorch backend [38].,impact-revealing,reporting the implementation details of the networks
5,5fd8964891e0119b22c1f219,80bcfee1766e56a01e6feeaa3cb47d3291acdcdf,Pre-Training Graph Neural Networks for Cold-Start Users and Items Representation,57a4e91dac44365e35c9844d,Matching Networks for One Shot Learning.,"One kind of the methods is meta-learning [9, 23, 26, 34], which consists of metric-based recommendation [29] and model-based recommendation [7, 20, 22, 24].###Unlike the goal of recommendation, the pre-training task directly reconstructs the cold-start user/item embeddings by mimicking the meta-learning setting via episode based training, as proposed in [34].",impact-revealing,describing a method in meta-learning
1619,,11e1e64434d2aebab4f74f08e8b5d95d13f5feda,The symmetric tridigonal eigenproblem on a shared memory multiprocessor: Part I,,,"###The deflation technique for approximately equal eigenvalues has been investigated in [2].###Dongarra and Sorensen [2] implemented a further deflation technique to make the algorithm more efficient and more stable.###The implementation in [2] always computes the eigenvalues to high accuracy, but some specific examples [1–3] illustrate that it may not compute fully orthogonal eigenvectors.###Sorensen and Tang [5] presented an alternative implementation scheme which was inspired by the earlier work of Kahan [4].###A scheme for doing this is outlined in [2].###The theory given by Dongarra and Sorensen in [2] suggests that it would be
appropriate to relate the two tolerances by
qer ffi ed;
where q is the scalar 2 times the off-diagonal element of the original matrix.###In one rank-one modification step of order n this contributes tn(2) operations [2].###The theory given by Dongarra and Sorensen in [2] suggests that it would be appropriate to relate the two tolerances by",impact-revealing,discussing the implementation and efficiency of deflation techniques in eigenvalue computation
2704,5ebe685391e0117693a52241,99314a532a3358cb86064fc8917ed2c283227539,NAT: Noise-Aware Training for Robust Neural Sequence Labeling,59ae3bf12bbe271c4c71bb94,Adapting Sequence Models for Sentence Correction.,", 2013), sequence to sequence models (Afli et al., 2016; Schmaltz et al., 2017) and hybrid systems (Schulz and Kuhn, 2017).###…include correction candidates ranking (Fivez et al., 2017; Flor et al., 2019), noisy channel modeling (Brill and Moore, 2000; Duan and Hsu, 2011), voting (Wemhoener et al., 2013), sequence to sequence models (Aﬂi et al., 2016; Schmaltz et al., 2017) and hybrid systems (Schulz and Kuhn, 2017).",other,acknowledge various approaches in correction candidates ranking and modeling
1686,,9bef5d0410511d903654b84f606f3a2c749e1c11,Social psychological perspective on binge drinking in young people,,,"###As discussed in Chapter 4 (section 4.3), the Social Identity Theory (SIT) (Hogg & Abrams, 1988) is a theory of group processes and intergroup relations that distinguishes group phenomena from interpersonal phenomena.###4.3 Social Identity and TPB
This section will examine the Social Identity Theory (SIT) developed by Tajfel and Turner in the 1970s and research supporting the use of the theory alongside the TPB.###…have been excluded such as a lack of consideration of automaticity or impulsivity therefore the following section will sum up and discuss information about the TPB and additions to the model including habit, impulsivity, descriptive norms and Social Identity Theory (Tajfel & Turner, 1979).###Social Identity Theory is a general theory of group processes and intergroup relations distinguishing group phenomena from interpersonal phenomena and offers an important addition to the TPB because the TPB does not consider wider social normative influence on intentions and behaviour.###7.2 Introduction to Study 3: Exploring other cognitive and social factors that
may influence binge drinking behaviour
The basic foundations of the previous studies in this thesis have been the Theory of Planned Behaviour (Beck & Ajzen, 1991) and the Social Identity Theory (Tajfel & Turner, 1979).###87
Habit as an additional construct in the TPB ................................................................ 87
Impulsivity as an additional construct in the TPB ...................................................... 88
Other constructs as additions to the TPB .................................................................... 88
Summary of an expanded TPB .................................................................................... 89
4.3 Social Identity and TPB .................................................................................................. 89
What is the Social Identity Theory?###108
Social Identity Theory ............................................................................................... 110
Descriptive norms .....................................................................................................###Explanatory variables have been excluded such as a lack of consideration of automaticity or impulsivity therefore the following section will sum up and discuss information about the TPB and additions to the model including habit, impulsivity, descriptive norms and Social Identity Theory (Tajfel & Turner, 1979).###Social Identity Theory (Tajfel & Turner, 1979) is a general theory of group processes and intergroup relations which distinguishes group phenomena from interpersonal phenomena.###92
4.4 Chapter Summary ........................................................................................................... 93
5 Chapter 5: Applying an Expanded TPB to Binge Drinking ....................................... 97
5.1 Chapter Overview ........................................................................................................... 97
5.2 Introduction to Study 1: Binge Drinking and Young People: An Expanded Theory of
Planned Behaviour Including: Habit, Impulsivity and Social Identity Theory ............... 97
Habit and past behaviour as additions to the TPB ....................................................... 98
What is habit?###What is the Social Identity Theory?###…an expanded TPB
Because of the variance left unexplained by the TPB there is a search to fill the gap by researching various constructs ranging from impulsivity and habit to normative variables and social identity (Tajfel & Turner, 1979) as possible factors important to the decision making process.###Social Identity Theory
As discussed earlier in section 4.3, Social Identity Theory (SIT) is a theory of group processes and intergroup relations distinguishing group phenomena from interpersonal phenomena.###5.2 Introduction to Study 1: Binge Drinking and Young People: An Expanded
Theory of Planned Behaviour Including: Habit, Impulsivity and Social Identity Theory
The theory of planned behaviour is a deliberative processing model in that it implies that individuals make behavioural decisions based on a careful consideration of available information (Armitage & Conner, 2001; Beck & Ajzen, 1991; Conner & Norman, 1995).",impact-revealing,discussing the Social Identity Theory and its relevance to the Theory of Planned Behavior
582,5dee1b4b3a55ac3d409adcbb,a84689ef8eaf38344eb3de24850ec0720c815605,synchronous transformers for end-to-end speech recognition,5a73cbcc17c44a0b3035f6a6,Monotonic Chunkwise Attention.,Monotonic chunkwise attention [8] is proposed to adaptively split the encoded state sequence into small chunks based on the predicted selection probabilities.,impact-revealing,describing a method for attention in sequence processing
2260,5cede0eeda562983788cd285,e4d99f390901df5caac0b587ff685f9cde100342,end-to-end speech translation with knowledge distillation,5b1643998fbcbf6e5a9bc464,Syllable-Based Sequence-to-Sequence Speech Recognition with the   Transformer in Mandarin Chinese,"Recently, this model also begins to be used in ASR task, showing a decent performance [22, 23].",other,reporting recent applications of the model in ASR tasks
2798,5f9a9af391e0114d7e7813ed,0d67d3ddca1c4e370eaf1e99ec674f612c39c66c,Graph Contrastive Learning with Adaptive Augmentation,5dcd263a3a55ac58039516c5,Momentum Contrast for Unsupervised Visual Representation Learning,", visual representation learning [1, 17, 41] and natural language processing [4, 28].###Existing work [17, 41, 49] employs a memory bank for storing negative samples.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2130,,d59574e89db5935760f84b89cbaf8e7b6c68c697,Lax–Hopf Based Incorporation of Internal Boundary Conditions Into Hamilton-Jacobi Equation. Part II: Computational Methods,,,"###We compare the Lax-Hopf algorithm and the Godunov scheme [27], [30], [50] (and its specific instantiation as the Daganzo cell transmission model [21], [ 22 ]), which is widely used by the transportation research community.",impact-revealing,comparing two algorithms used in transportation research
1367,,a8432f8160dc899e66976b9887efa1d4a544cd56,Towards Taming Privilege-Escalation Attacks on Android,,,###Android's security framework (enforcing sandboxing and permission checks) is not sufficient for transitive policy enforcement allowing privilege escalation attacks as shown by the recent attacks [16] [12] [20] [35].###We successfully evaluated our framework against application-level privilege escalation attacks presented in [16] [12] [20] [35].,impact-revealing,highlighting the insufficiency of Android's security framework against privilege escalation attacks
3154,5b1643998fbcbf6e5a9bc25e,080e1bb6bbebeb78f822b3998b7ed898ab6457aa,End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction,55465e700cf2939c2feeb0fa,Phase Processing For Single-Channel Speech Enhancement,"To improve the consistency, one stream of research is focused on iterative methods such as the classic Griffin-Lim algorithm [14], multiple input spectrogram inverse (MISI) [15], ISSIR [16], and consistent Wiener filtering [17], which can recover the clean phase to some extent starting from the mixture phase and a good estimated magnitude by iteratively performing STFT and iSTFT [13].###It is well-known that this incurs a phase inconsistency problem [11, 12, 13], especially for speech processing, where there is typically at least half overlap between consecutive frames.",other,acknowledging existing iterative methods for improving consistency
3812,5d1eb9b7da562961f0af38c9,c0f5e89cf9f1b4f3d9c76a2ae3b13315e691554b,Personalized Student Stress Prediction with Deep Multitask Network,55503f0745ce0a409eb2bc48,Multi-task and multi-view learning of user state,"A similar approach was also taken in (Kandemir et al., 2014) for the prediction of affect (mood) by learning user speciﬁc kernels.###A similar approach was also taken in (Kandemir et al., 2014) for the prediction of affect (mood) by learning user specific kernels.",other,reporting prior findings on affect prediction methods
1530,,3f20bb29a404c81e2ee92d36657782265177b3b4,African American Males Navigate Racial Microaggressions,,,"###…literature extends the definition to include “brief and commonplace daily verbal, behavioral, and environmental indignities, whether intentional or unintentional, that communicate hostile, derogatory, or negative racial slights and insults to the target person or group” (Sue et al., 2007, p. 273).###…teachers having the power to punish them based on gendered racism (Mutua, 2013) not only confirmed systemic patterns of exclusion against Black males (Raffaele Mendez & Knoff, 2003) but also served as microassaults (Sue et al., 2007) to remind participants to “stay in our place or be put in it!”###” Perspectives like these contributed to participants’ belief that their ability to integrate fully in class was limited by White teachers who appeared disinterested by not personally engaging Black males, which is a form of microinvalidation (Sue et al., 2007).###(Howard, 2008, pp. 943–964)
My application of CRT served to parse out race and gendered occurrences of racial microaggressions (P. Davis, 1989; Pierce et al., 1978; Sue et al., 2007) as experienced by Black males and in determining how they constructed and applied avoidance.###Carlton, Phillip, and Geoffrey recognized that White teachers having the power to punish them based on gendered racism (Mutua, 2013) not only confirmed systemic patterns of exclusion against Black males (Raffaele Mendez & Knoff, 2003) but also served as microassaults (Sue et al., 2007) to remind participants to “stay in our place or be put in it!” (Nicky).###My application of CRT served to parse out race and gendered occurrences of racial microaggressions (P. Davis, 1989; Pierce et al., 1978; Sue et al., 2007) as experienced by Black males and in determining how they constructed and applied avoidance.###Perspectives like these contributed to participants’ belief that their ability to integrate fully in class was limited by White teachers who appeared disinterested by not personally engaging Black males, which is a form of microinvalidation (Sue et al., 2007).###This above interaction, a microassault (Sue et al., 2007), was characteristic of exchanges with White teachers who served as examples to participants that the Black male presence, and perspectives were devalued and seen as a deficit.",impact-revealing,highlighting the impact of racial microaggressions on Black males in educational settings
1068,,d8f5b5a575f012c26f1219ffe1cef444a2fe8314,News Topic Sentence Generation Based on Two-stage Summarization,,,"###…topic extraction, based on the topic coherence and topic diversity assessments commonly used by topic modeling tasks, we chose Normalized Pointwise Mutual Information(NPMI) [33] as an evaluation method for topic coherence and Topic Diversity(TD) [12] as an evaluation method for topic diversity.",impact-revealing,providing context for evaluation methods in topic modeling
3911,5b67b45517c44aac1c860884,9fa3e53b5937a0ec92499ed415e339ede6c92010,DeepInf: Social Influence Prediction with Deep Learning,56d8aa6fdabfae2eeeae71cc,Social dynamics of Digg,"Digg [23] Digg is a news aggregator which allows people to vote web content, a.k.a, story, up or down.",other,providing context about a news aggregator
1557,,c4bbbb7129803f8c738130966611efe672a2cdc8,Arrhythmia Monitoring for Risk Stratification in Hypertrophic Cardiomyopathy,,,"###In both HCM and general populations, the prevalence of NSVT increases with age.(57,60) Given the high prevalence of NSVT, in combination with the low rate of SCD in patients with HCM, NSVT has been criticized for having poor positive predictive value.###these variables based on 24- and 48-hour AECG monitoring suggested that they have little prognostic value.(56,57) The updated guidelines acknowledge that more NSVT episodes are detected on prolonged monitoring, but they maintain the prior 2011 guideline recommendations of AECG monitoring every 1 to 2 years.",impact-revealing,highlighting the prevalence and monitoring recommendations for NSVT
1762,,26b821e06607d997cf0bc5818a2b4c58f4ab7955,A High Efficient Architecture for Convolution Neural Network Accelerator,,,"###This will reduce the complexity of memory access, and we can remove the input/output buffer which is required by other solution for data reformatting [1][2].###Noted that our PE array is also fully used in CONV2-CONV5 and FC1, that is the reason why it has a better performance compare with other method [1].###Compare with [1], our CONV2 takes only 7.###Our solution is 30% faster than others [1] on Alexnet.###Most neural-network accelerator architectures [1][2] are designed for kernel reuse and have to use image batching method to improve the efficiency of their design.###4, the proposed method can outperform [1], and the hardware resource requirement is minimized.###Compared with other methods, the proposed structure has the following advantages: 1, data is loaded into PE in parallel; 2, the proposed structure can fully support FC layer efficiently; Other methods [1-2] use image batching to improve the efficiency of FC layer which can cause large latency and is not suitable for EDGE application; 3, we propose a smart memory access solution and it enables us to reformat the data sequence directly without input/output data buffer which are required by other proposals.",impact-revealing,highlighting the advantages and performance improvements of the proposed method over existing solutions
1975,,e5ace4c17f9621d8bb0ca04a4324762216e3c521,Pseudorandom Functions: Three Decades Later,,,"###In the realm of data structures, permutation-based hashing, which is inspired by the Feistel construction of PRPs, has been applied to improve the performance of dynamic dictionaries [12].",impact-revealing,highlighting the application of permutation-based hashing in data structures
3094,573698426e3b12023e70bf13,4e9dbca4218d32a9f92d58c340f3f8f3c5020a44,best-offset hardware prefetching,53e9bc6eb7602d97048eba78,Data Cache Prefetching Using a Global History Buffer,"For instance, some prefetchers record in a table some history about past memory accesses and use that history to predict future memory accesses [14, 16, 23, 22, 36, 2, 35, 11, 12, 17] (this list is not exhaustive).",other,providing context on prefetchers and their memory access prediction
165,5aed14d617c44a4438158f7c,ae1c89817a3a239e5344293138bdd80293983460,Attention U-Net:,5aed14d617c44a4438158c85,Learn To Pay Attention.,"The contributions of this work can be summarised as follows: • We take the attention approach proposed in [11] a step further by proposing grid-based gating that allows attention coefﬁcients to be more speciﬁc to local regions.###In contrast to [11] we propose a grid-attention technique.###Similar attention mechanisms have been proposed for natural image classiﬁcation [11] and captioning [1] to perform adaptive feature pooling, where model predictions are conditioned only on a subset of selected image regions.###Self-attention techniques [11, 33] have been proposed to remove the dependency on external gating information.###As suggested in [11], low-level feature-maps, i.e. the ﬁrst skip connections, are not used in the gating function since they do not represent the input data in a high dimensional space.###In [11, 32] self-attention is used to perform class-speciﬁc pooling, which results in more accurate and robust image classiﬁcation performance.###For instance, additive soft attention is used in sentence-to-sentence translation [2, 29] and more recently applied to image classiﬁcation [11, 32].###Attention Gates: AGs are commonly used in natural image analysis, knowledge graphs, and language processing (NLP) for image captioning [1], machine translation [2, 30], and classiﬁcation [11, 31, 32] tasks.###In image captioning [1] and classiﬁcation [11] tasks, the softmax activation function is used to normalise the attention coefﬁcients ( σ 2 ); however, sequential use of softmax yields sparser activations at the output.",impact-revealing,highlighting the advancements in attention mechanisms and their applications
2092,,e8da856688a3fc614a7e5b960c3eefece4d6ea45,Multishot codes for network coding: Bounds and a multilevel construction,,,"###Next, we base our description of the multilevel construction on the work of Calderbank [8], wher ein many references on this subject are listed.###M ULTILEVEL CONSTRUCTION In this section, we propose a method for constructing multishot codes which is inspired by the so-called multilevel construction for block-coded modulation schemes [7] , [8].",impact-revealing,drawing on prior work for method development
3545,5e524da993d709897cb69fde,09e48adf1a3f012ec56fbb4f92acae975ee43d87,a deep learning approach to antibiotic discovery,53e9ade9b7602d97037f11d3,On the selection of appropriate distances for gene expression data clustering,"The average linkage uses the algorithm called unweighted pair group method with arithmetic mean (UPGMA), which is the most popular and preferred algorithm for hierarchical data clustering (Jaskowiak et al., 2014; Loewenstein et al., 2008).",other,providing context for a clustering algorithm
3426,5b67b47917c44aac1c8637c6,5aea95e1ae78a66474051a330ded374e199b658c,Representation Learning on Graphs with Jumping Knowledge Networks,5c8a11324895d9cbc6121c34,Neural Message Passing for Quantum Chemistry.,"Graph Convolu-tional Networks (GCN) (Kipf & Welling, 2017), initially motivated by spectral graph convolutions (Hammond et al., 2011; Defferrard et al., 2016), are a speciﬁc instantiation of this framework (Gilmer et al., 2017), of the form where deg ( v ) is the degree of node v in G .###, 2016), are a specific instantiation of this framework (Gilmer et al., 2017), of the form###Many of these approaches broadly follow a neighborhood aggregation (or “message passing” scheme), and those have been very promising (Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Veliˇckovi´c et al., 2018; Kearnes et al., 2016).###Many of these approaches broadly follow a neighborhood aggregation (or “message passing” scheme), and those have been very promising (Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Veličković et al., 2018; Kearnes et al., 2016).",other,acknowledge existing approaches in graph convolutional networks
340,5da2f8aa3a55ac3402d8c092,fd2a0a326db4f034fe22340c20b7bacd9a14c3d6,second-order attention network for single image super-resolution.,59ae3be32bbe271c4c71ba2e,Enhanced Deep Residual Networks for Single Image Super-Resolution,"Some loss functions have been widely used, such as L 2 [2, 12, 29, 30], L 1 [14, 15, 20, 39], perceptual losses [11, 26].###Speciﬁcally, we 1 (a) HR (b) FSRCNN (c) LapSRN [14] (d) SRMD [36] (e) EDSR [36] (f) DBPN [20] (g) RDN [6] (h) Ours Figure 1.###As explored in [20, 39], we apply only one convolutional layer to extract the shallow feature F 0 from the LR input where H SF ( · ) stands for convolution operation.###For upscale part H ↑ ( · ) , we follow the works in [20, 39] and apply ESPCNN [28] to upscale the deep features, followed by one ﬁnal convolution layer with three ﬁlters to produce color images (RGB channels).###As in [20, 39, 38], we also adopt self-ensemble method to further improve our SAN denoted as SAN+.###Following [20, 6, 39, 38], we use 800 high-resolution images from DIV2K dataset [31] as training set.###To test the effectiveness of our SAN, we compare our SAN with 11 state-of-the-art CNN-based SR methods: SR-CNN [1], FSRCNN [3], VDSR [12], LapSRN [14], Mem-Net [30], EDSR [20], SRMD [36], NLRN [22], DBPN [6], RDN [39] and RCAN [38].###[20] proposed a very deep and wide network EDSR by stacking modiﬁed residual blocks.###It has been veriﬁed that stacking residual blocks is helpful to form a deep CNN in [20, 39].",impact-revealing,reporting on widely used loss functions in the context of image processing
1011,,8fcba287b1cd7744b7343bf7c90cae1ebff608ce,Personal self and collective self: when academic choices depend on the context of social comparison,,,"###Firstly, Guimond et al. (2007) pointed out that research revealed that the differences between men and women in terms of personality traits, values or emotions are also greater in egalitarian, individualistic cultures than in non-egalitarian, collectivist cultures.###As noted by Else-Quest et al. (2010), these surprising results are consistent with an hypothesis validated by Guimond et al. (2007) in a cross-cultural study examining the differences between men’s and women’s self-concept on agentic and relational dimensions.###Guimond et al. (2007) therefore hypothesized, and found, significant interaction effects between gender and culture on agentic and relational self-descriptions.###Guimond et al. (2007), in line with the five-stage theory of intergroup relations (Taylor and McKirnan 1984), suggested that the existence of considerable inequalities in a society tends to favor intragroup social comparisons and, at the level of gender, these comparisons reduce gender-related…",impact-revealing,highlighting significant findings on gender and culture interactions
3843,599c797d601a182cd2643e8a,cd8a9914d50b0ac63315872530274d158d6aff09,Modeling Relational Data with Graph Convolutional Networks,58437789ac44360f10843609,A Collection Of Benchmark Datasets For Systematic Evaluations Of Machine Learning On The Semantic Web,"Unless otherwise noted, we divide the labeled part of the dataset according to 10-fold stratified cross-validation or according to the official train/test benchmark splits from [30].###Test performance is reported on the train/test set splits provided by (Ristoski, de Vries, and Paulheim 2016).###For a more detailed description of the datasets the reader is referred to Ristoski, de Vries, and Paulheim (2016).###…(e.g. person or company), a successful model needs to reason about the relations with other entities that this entity is involved in. Datasets We evaluate our model on four datasets 3 in Resource Description Framework (RDF) format (Ristoski, de Vries, and Paulheim 2016): AIFB, MUTAG, BGS, and AM.###To eliminate these differences, we repeated the baselines in a uniform manner, using the canonical test/train split from (Ristoski, de Vries, and Paulheim 2016).###The raw datasets contain relations which were used to generate labels for the benchmark train/test splits from [30].###For entity classification, we evaluate our model on three knowledge graph datasets [30]: AIFB, MUTAG and BGS.###(*) denotes results on the benchmark test split from [30].",other,describing dataset evaluation methods and splits
696,558c6c66e4b02b9f07a703d0,603deea2983021dc759d0ba738198d6e5c2972c3,control-flow decoupling,53e9b5edb7602d97041417c7,Diverge-Merge Processor (DMP): Dynamic Predicated Execution of Complex Control-Flow Graphs Based on Frequently Executed Paths,"wish branches [17], dynamic predication based on frequently executed paths [16], and predicate prediction [29], to name a few.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2532,57d063f6ac44367354296741,908f7931de8768786d9ef7d64f5a8156860709dd,Dynamic Pricing and Traffic Engineering for Timely Inter-Datacenter Transfers,53e9b923b7602d970450bf9a,Online auctions with re-usable goods.,"Because of our online setting, standard auctions (such as a VCG auction) are not directly applicable; but there is a rich and relevant literature on incentive-aware online scheduling [4, 17] that can possibly be adapted to WAN allocation.",other,acknowledging relevant literature for potential adaptation
590,58d82fd2d649053542fd75bc,e0b207e96351671453aa8bf05b7225c8a340a0b2,towards end-to-end speech recognition with deep convolutional neural networks,558b5b5fe4b031bae1fcf01f,Applying Convolutional Neural Networks concepts to hybrid NN-HMM model for speech recognition,"Unlike the time windows applied in DNN systems [2, 3, 4], the temporal modeling is deployed within convolutional layers, where we perform a 2D convolution similar to vision tasks, and multiple convolutional layers are stacked to provide a relatively large context window for each output prediction of the highest layer.###In the context of Automatic Speech Recognition, CNNs are usually combined with HMMs/GMMs [5, 6], like regular Deep Neural Networks (DNNs), which results in a hybrid system [2, 3, 4].###Most of the CNN models [2, 3, 4] in the speech domain have large filters and use limited weight sharing which splits the features into limited frequency bands while performing convolution separately and the convolution is usually applied with no more than 3 layers.###Recently, Convolutional Neural Networks (CNNs) [1] have achieved great success in acoustic modeling [2, 3, 4].",impact-revealing,providing context on convolutional neural networks in speech recognition
2680,53e9a775b7602d97030b16b6,bcbef8e1945b432a474089796300f2d20d011d6a,Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers,558a3a56e4b037c08755e562,A hardware-based cache pollution filtering mechanism for aggressive prefetches,Zhuang and Lee [25] proposed to filter prefetcher-generated cache pollution by using schemes similar to two-level branch predictors.,other,reporting prior findings on cache pollution filtering
1456,,b72b2e9c41f0384447ead7e0e62a5b742083ff61,A bench-scale assessment for phosphorus release control of sediment by an oxygen-releasing compound (ORC),,,"###As a powdery peroxide, CaO2 has been widely used as an oxygen generator in bioremediation.([10,11]) However, the release of O2 from powdery CaO2 is relatively rapid; thus, the oxygen that is generated cannot be used effectively, and the subsequent formation of H2O2 results in disinfection.",impact-revealing,highlighting the application and limitations of CaO2 in bioremediation
1301,,2b5c7a2cb894f7f8e79e73b6feb25d5a3e4bb466,A Virtual Reality-Based Protocol to Determine the Preferred Control Strategy for Hand Neuroprostheses in People With Paralysis,,,"###These tasks were inspired by validated clinical tests of upper-limb function such as the ARAT [26] and the GRASSP [27] and designed to test various aspects of control strategies for a neuroprosthesis – also under stressed conditions – to help patients consciously determine the preferred solution.###…and eventual neurological recovery induced by its prolonged use could be assessed by evaluating changes in muscle strength using clinical measures such as the MRC scale and changes in grasp functionality and independence using measures such as the ARAT [26], the GRASSP [27], and the SCIM [31].",impact-revealing,highlighting the inspiration from clinical tests for neuroprosthesis evaluation
3301,5d04e8dbda56295d08db13cf,56e3ce0ff4cbd05e404214d19ae264fe6c457a16,cif: continuous integrate-and-fire for end-to-end speech recognition,5b67b4b117c44aac1c866bd0,Extending Recurrent Neural Aligner for Streaming End-to-End Speech   Recognition in Mandarin,"employs the powerful structure in [18] that utilizes a 2-dimensional strided convolutional network to conduct temporal down-sampling by 2, and a multiplicative unit (MU) [19] to further capture acoustic details.",other,describing a method for temporal down-sampling in acoustic analysis
2163,,edafd7f57995f395c4196312583770499d587d64,"Process fragmentation, distribution and execution using an event-based interaction scheme",,,"###Given a pattern’s execution traces, a distributed task or process element T and its LTL-event rule F , the following properties should evaluate to true: 1.###Note that we use LTL as a formal notation.###0 process engine Alfresco (2012) to interpret and execute the actual process model fragment and the LTL model checker included in the ProM framework van Dongen et al. (2005) to evaluate the LTL rules.###Note that a distinction is made between the use of the basic event rule (Section 4.2) and the use of an LTL event rule (Section 4.3).###In a sense, the declarative process model research is compatible with our approach in that the process engine mechanics to run the distributed process fragments are similar to the enactment engine interpreting the declarative LTL rules (where the LTL-Rules are the restrictions on the process fragments).###For this purpose, a visualization can be used which is able to provide for a comprehensible representation of the LTL rules (Brambilla et al., 2005).###The two properties have been checked and validated on the execution traces of the supported workﬂow patterns using the LTL checker included in the ProM framework (van Dongen et al., 2005) 10 .###First, the basic event rule from Algorithm 1 is rewrit-ten to incorporate LTL semantics.###The same authors also propose a BPMN to LTL mapping which is used in compliance checking of web applications (Brambilla et al., 2005).###If the rule evaluates to true (i.e. all events in a conjunction in the basic event rule are enabled, or the LTL-rule evaluates to true on the event trace), the fragment instance’s execution is triggered.###The correctness of the transformation is validated by checking the LTL event rules on all possible execution paths of the supported workﬂow patterns (van der Aalst et al., 2003).###For each of these patterns, execution traces are generated 9 and the LTL-event rules are created using the algorithm described in Section 4.###(a) The publish/subscribe wrapper issues subscriptions to all events in the process fragment’s description (i.e. all events found in the basic event rule or LTL event rule).###For the LTL event rules, the LTL notation of the LTL-Checker included in the Prom framework (van Dongen et al., 2005) is used.###The condition-places are then transformed into an LTL rule similar to equation 1 (see line 4).###The execution traces depict the original (desired) behavior of the workﬂow pattern and the LTL-event rule depicts the behavior of the distributed execution (of a given process fragment).###This mapping is used to visualize and simplify the construction of LTL statements.###To incorporate this extra behavior, events are appended with a time concept, which allows the expression of the event rule in Temporal Logic (LTL) (Pnueli, 1981).###Next, 20 (a) for a basic event rule the event notiﬁcation is matched to the correct event in the event rule, which will become enabled; (b) for a LTL event rule the received event is added to the event trace of the fragment instance.###Moreover, the event messages are small and evaluation of an (LTL) event rule can be done by checking a simple state machine (Gastin and Oddoux, 2001).###For this purpose the LTL rules can be rewritten in a rule language suitable for CEP.###A temporal aspect is needed to capture this information, with which the following (LTL) start rule can be constructed for task C: with ♦ the eventual temporal logic operator and the always temporal logic operator (Pnueli, 1981).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1780,,ac8f6c9892a7b50eca06204808e6ddd6106407d3,"Strategies for Oil and Gas Asset Retirement Sustainability in Alberta, Canada",,,"###However, Hancock and Algozzine (2016) emphasized the importance of using the research question and related literature as guidelines for data analysis.###…involved ensuring participants met established criteria, and I provided an informed consent form to participants, utilized an interview protocol, selected an ideal interview location, recorded the interview, and adhered to legal and ethical requirements (Hancock & Algozzine, 2016; Rosenthal, 2016).###Role of the Researcher
In qualitative research, the researcher is the primary instrument for data collection
(Hancock & Algozzine, 2016).###…that is rich in detail with the participants, the researcher needs to select an appropriate interview location free of disruptions and noise, manage the interview time, be an active listener, maintain eye contact, and ensure participants’ comfort (Hancock & Algozzine, 2016; Rosenthal, 2016).###Hancock and Algozzine (2016) noted that the successful interview data collection included ensuring participants meet established criteria, providing participants with an informed consent form, using an interview protocol, selecting an ideal interview location, recording the interview, and adhering…###The computer software NVivo facilitated the location of matching words or phrases after the researcher defines the set of codes (Hancock & Algozzine, 2016; Salmona & Kaczynski, 2016).###Effective data organization involves organizing and labeling the dataset using some organizational structure or scheme to facilitate easy data retrieval (Hancock & Algozzine, 2016).###Data Collection Instruments
In conducting qualitative research, the researcher is the primary instrument for
data collection (Hancock & Algozzine, 2016).###Hancock and Algozzine (2016) mentioned that semistructured interviews are a suitable data collection method in a case study.###Semistructured interviews involve preparing questions in advance that lead the direction of the conversation but allow the respondents to answer openly (Hancock & Algozzine, 2016).###I employed member checking for participants to review my interpretation of their interview responses, check for correctness, and provide further insights (Hancock & Algozzine, 2016).###An effective interviewer needs to be an active listener, ask relevant questions, have a firm grasp of the topic, and avoid bias (Hancock & Algozzine, 2016; Yin, 2018).",impact-revealing,emphasizing the importance of research guidelines in qualitative data analysis
3180,5bdc318017c44a1f58a08780,5ab5658a1666e26c66f0319a469228dbe19598a2,An e-learning recommendation approach based on the self-organization of learning resource,53e9b3c1b7602d9703ea9f33,E-Learning And Personalized Learning Path: A Proposal Based On The Adaptive Educational Hypermedia System,[42] studied the adaptive hypermedia system by using the definition of methodologies which are able to manage user 145,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2797,5ee8986891e011e66831c3b6,38f93092ece8eee9771e61c1edaf11b1293cae1b,Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,5eba73be91e01108d77cf8a1,Prototypical Contrastive Learning of Unsupervised Representations,"Among discriminative methods, contrastive methods [9, 10, 32, 33, 34, 11, 35, 36] currently achieve state-of-the-art performance in self-supervised learning [37, 8, 38, 12].",other,highlighting the effectiveness of contrastive methods in self-supervised learning
856,5d0616bd8607575390f86730,635b079447ee46acc813a08305988e77dfe3d1dd,Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi,5bdc315017c44a1f58a0565c,Towards Environment Independent Device Free Human Activity Recognition.,"For example, recent works [20, 50] borrow the ideas from machine learning, such as transfer learning and adversarial learning, and apply advanced learning methodologies to improve cross-domain recognition performance.###0 against several alternative state-of-the-arts methodologies, CARM[44], EI[20] and CrossSense[50], where the latter two are feasible for cross-domain recognition.###EI [20] incorporates an adversarial network to obtain domain-independent features from CSI.###As an example, we evaluate the performance of an adversarial learning based model, EI [20] over different domain factors (e.g., environment, location and orientation of the person).###Tempts to adapt recognition schemes in various domains fall into two categories: virtually generating features for target domains [39, 40, 50, 53] and developing domain-independent features [9, 20, 37].###Cross-domain learning methods such as transfer learning [50] and adversarial learning [20] latently generate features of data samples in the target domain, either by translating samples from the source domain, or learning domain-independent features.",impact-revealing,highlighting the application of advanced learning methodologies in cross-domain recognition
1209,,b3a50f46cb7f12443582d61c658de63b600ea996,Analysis of the Oxidative Stress Status in Nonspecific Vaginitis and Its Role in Vaginal Epithelial Cells Apoptosis,,,"###When cells were stimulated by apoptosis signals, the membrane permeability of mitochondria was changed, resulting in the release of cytochrome C into cytoplasm [27].###Previous reports emphasized the importance of mitochondria in ROS induced apoptosis of various cell types [27].",impact-revealing,acknowledging prior findings on mitochondria's role in apoptosis
2402,5f4f6ec291e0111f07b30a2b,3259c9ab1714a4cfdf6439cca6bdc5f78d78fda3,NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size,59ae3bf12bbe271c4c71bf7b,A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets,"In sum, ImageNet-16-120 contains 151.7K training images, 3K validation images, and 3K test images with 120 classes.###(I) Compared to candidates in St, ResNet shows competitive performance in three datasets, however, it still has room to improve, i.e., about 2% compared to the best architecture in CIFAR-100 and ImageNet-16-120, about 1% compared to the best one with the same amount of parameters in CIFAR-100 and ImageNet-16-120.###This is because the significantly increased searching data on CIFAR-100 and ImageNet-16-120 over CIFAR-10 alleviate the problem of incorrect gradient estimation in bi-level optimization.###ImageNet-16-120: We build ImageNet-16-120 from the down-sampled variant of ImageNet (ImageNet16×16).###(3) On ImageNet-16-120, BOHB significantly outperforms the other methods.###As indicated in [34], down-sampling images in ImageNet can largely reduce the computation costs for optimal hyperparameters of some classical models while maintaining similar searching results.###On ImageNet-16-120, we use a similar strategy but with random crop 16×16 patch and 2 pixels padding on each border.###The ranking of every architecture in our search space is shown in Figure 2, where the architectures ranked in CIFAR-10 (x-axis) are shown in relation to their respective ranks in CIFAR-100 and ImageNet-16-120 (y-axis), indicated by green and red markers respectively.###We train and evaluate each architecture on CIFAR-10, CIFAR-100 [33], and ImageNet-16-120 [34].###[34] down-sampled the original ImageNet to 16×16 pixels to form ImageNet16×16, from which we select all images with label ∈ [1, 120] to construct ImageNet-16-120.",other,reporting dataset details and performance comparisons
2324,5eff040a91e011ea6db8de11,21a4cd35f19cfe8df1065b066b16edd048d2535d,DAPPLE: a pipelined data parallel approach for training large models,5db1765a3a55ac101c887e97,Exploring the Limits of Transfer Learning with a Unified Text-to-Text  Transformer,"Many state-of-the-art DNN models (e.g., NLP [2], Internet scale E-commerce search/recommendation systems [3], [4]) have billions of parameters, demanding tens to hundreds of GBs of device memory for training.###, NLP[39], search/recommendation systems[13, 45]) have billions of parameters, demanding tens to hundreds of GBs of device memory for training.",other,highlighting the resource demands of state-of-the-art DNN models
618,53e9b355b7602d9703e2fc83,76dae13c293e7c3e1766df6b29e8b183e71df4c3,exploiting choice: instruction fetch and issue on an implementable simultaneous multithreading processor,53e9a298b7602d9702ba0469,APRIL: a processor architecture for multiprocessing,"Others, like MIT’s TX-2 and Alewife [1] architectures, utilized a less aggressive model, coarse-grain multithreading, which can be thought of as a hardware accelerator for context switches (able to do them in a handful of cycles, fast enough to profitably switch on long memory latencies).###Others, like MIT’s TX-2 and Alewife [1] architectures, utilized",impact-revealing,acknowledge existing hardware architectures
1513,,1ed2d3d1b2f89ee37f93d995169adc14161d5aa7,Molecular docking with Gaussian Boson Sampling,,,"###The goal of this paper is to demonstrate that GBS devices can improve the sampling of docking configurations, whereas it does not address the difficulties associated with scoring functions (28, 29), which represent the major hurdle in accurate docking.###Reliable determination of the most probable ligand orientation, and its ranking within a series of compounds, requires accurate scoring functions and efficient search algorithms (28).",impact-revealing,highlighting the focus of the paper on GBS devices and their impact on docking configurations
44,5b1643ba8fbcbf6e5a9bc5b5,8c1b00128e74f1cd92aede3959690615695d5101,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,5550411c45ce0a409eb3897f,Neural Machine Translation by Jointly Learning to Align and Translate.,"Then we learn the interactions between context and question by standard attentions (Xiong et al., 2016; Seo et al., 2016; Bahdanau et al., 2015).###In this work, we consider attention-based neural machine translation (NMT) models Bahdanau et al. (2015); Luong et al. (2015), which have demonstrated excellent translation quality Wu et al. (2016), as the core models of our data augmentation pipeline.",impact-revealing,describing the use of attention-based models in translation
946,,aa27bdb8708533ad6bd0076dd6ded70c8518ff0d,speaq 2.0: A complete workflow for high-throughput 1D NMR spectra processing and quantification,,,"###The MetaboAnalyst [12] platform is widely used for the analysis of metabolomics data.###0—1.2.3) MetaboAnalyst [10– 12] web Arguably one of the most widely used platforms for metabolomics analysis.###Such a peak list is the starting point of an analysis with for example the often used MetaboAnalyst [12].###The overall aim however, is not to construct yet another all-encompassing package for NMR analysis, but more importantly, to construct a method that can complement established tools for NMR data analysis like MetaboAnalyst [12], by improving performance, analysis quality and reproducibility.###This is achieved by improving the quality of the peak lists which are the starting points for MetaboAnalyst [12] or muma-R [13].",impact-revealing,highlighting the significance and widespread use of MetaboAnalyst in metabolomics analysis
2093,,09f5da9d32856cc31807d84e4253742566aa3da8,Complexity/performance Tradeoos in Multistage Decoders,,,"###It was later extended by Ginzburg [12] and many other authors [4, 7, 15, 16, 19, 21].###C Staged Decoding The multilevel structure of MB codes facilitates the use of staged sub-optimal decoding [4, 19, 21].###A major advantage of GC codes is that they can be decoded by staged decoders, which is a family of decoding algorithms that carries out decoding in multiple stages with decoded information obtained from previous stages passed on to future stages to reduce the decoding complexity [4, 14, 19, 21].",impact-revealing,acknowledge extensions and advantages of decoding methods
2075,,25f2c67780f642ac03767e7fbf1e536ffe8b0137,ST4MP: A Blueprint of Multiparty Session Typing for Multilingual Programming,,,###The main design decisions related to the parser have been: Use a language-independent notation for data types based on JSON ((cid:16)Java-Script Object Notation(cid:17)) [6].###JSON is a domain-speci(cid:28)c language for typed values; it has been widely adopted as a (cid:16)text syntax that facilitates structured data interchange between all programming languages(cid:17) [6].,impact-revealing,describing design decisions for a parser
1176,,32be249764f9ddcffee5b29ab2bf82ee0c467ee6,Competitive target search with multi-agent teams: symmetric and asymmetric communication constraints,,,"###Our higher dimensional extensions build on coverage methods that use lawn-mower sweep patterns (Choset and Pignon 1998).###Our world model shares many of the same assumptions as Choset and Pignon (1998); in particular, an initial uniform prior distribution over target location and perfect sensors.###Using sweep patterns for single agent coverage is studied by Choset and Pignon (1998), while Vincent and Rubin (2004) extends these ideas to a single multi-agent team searching for a moving and possibly evading target.",impact-revealing,building on previous coverage methods for higher dimensional extensions
254,5c8fd41a4895d9cbc66534e9,950aae7979a2faa874b7481179c064d3ad151ea8,Objects Segmentation From High-Resolution Aerial,5550415645ce0a409eb3a69e,Very Deep Convolutional Networks for Large-Scale Image Recognition.,The encoder part of FCN consists of visual geometry group network (VGGNet) [26] that is a famous CNN classification model and the decoder part consists of a deconvolution layer for upsampling.,impact-revealing,describing the architecture of the FCN model
429,5f7fdd328de39f0828397e7f,9fa283d4f9c2ed991383c0434ef6043bee0dc8e2,Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification,5e7495c591e0111c7cee14ab,Spatially Adaptive Inference with Stochastic Feature Sampling and Interpolation,"Recent research has revealed that considerable spatial redundancy occurs when inferring CNNs [10, 62].###The methods proposed in [19] and [62] skip the computation on some less important regions of feature maps.",impact-revealing,highlighting findings on spatial redundancy in CNNs
1635,,4a8a3b61618b8343d378939b214c8d94a918cef8,"Degradation behavior, cytotoxicity, hemolysis of partially unzipped carbon nanotubes/zinc composites as potential biodegradable bone implants",,,"###Recently, some investigations show that a high concentration of extracts in vitro cytotoxicity test is not suitable for biodegradable metals owing to the great difference between in vivo and in vitro conditions [1, 10, 13, 33].###0 cm2 was calculated to be 53–283 μg d−1, which was much lower than the daily intake of Zn in healthy adults (15 mg d−1) [13].###The degradation rate can be calculated based on weight loss method by the following formula [13]:###76× 104 for rate unit ofmillimeter per year [13]),W represents mass loss in gram, A shows the surface area of the sample in cm2, T is on behalf of immersion time in hours, and D represents the density in g (cm3)−1.###Niu et al [13] showed that the addition of Cu to Zn matrix could refine the microstructure of Zn-Cu alloy.",impact-revealing,highlighting the significance of in vitro vs in vivo conditions in cytotoxicity testing
2551,5bbacbad17c44aecc4eb007e,fdb87d305c59e9ab82a96198e34e46581b838c4e,one-shot relational learning for knowledge graphs,5b1643998fbcbf6e5a9bbe0c,Never-ending learning.,"We conduct ablation studies using the model that achieves the best Hits@10 on the NELL-One dataset.###Table 3 shows the decomposed results on NELL-One generated by our best metric model (GMatching-ComplEx) and its corresponding embedding method.###For NELL-One, we use 51/5/11 task relations for training/validation/testing.###Our ﬁrst dataset is based on NELL (Mitchell et al., 2018), a system that continuously collects structured knowledge by reading webs.###Taking MRR as an example, the selected metric model achieves 17.1% on NELL-One and 20.0% on Wiki-One; while the results of KG embedding are 9.3% and 7.2%.",other,reporting results from ablation studies on NELL-One dataset
3520,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,59ae3c262bbe271c4c71f46b,Ego-Splitting Framework: from Non-Overlapping to Overlapping Clusters.,"by the fact that a single node implicitly participates in many different clusters [17], e.###We attribute that to extremely uneven structure of these graphs, as popular products are co-purchased with a lot of other items, so the effects discussed in [17, 32] are prohibiting good cuts.",other,discussing challenges in graph structure affecting clustering
995,,83bec5a7aca2abc2c5ae42c16aba4e6a8f6e5662,The SECI model of knowledge creation: some empirical shortcomings,,,"###The questionnaire comprised 185 items, 38 of which concerned ”the content of organizational knowledge creation”, as measured by the amount of time spent on specific activities ( Nonaka et. al. 1994, pp. 342-3,  350).###The theory of organizational knowledge creation developed by Nonaka and his colleagues (Nonaka 1994;  Nonaka et. al. 1994;  Nonaka & Takeuchi 1995; Nonaka et. al. 2000; 2001a; Nonaka & Toyama 2003) originated in studies of information creation in innovating companies (Imai et. al. 1985; Nonaka 1988a, 1988b, 1990, 1991b, Nonaka & Yamanouchi 1989; Nonaka & Kenney 1991) and appears to have undergone two phases of development.###It is “closely related” to “the traditional notion of learning”, and to “learning by doing” ( Nonaka et. al. 1994:  340-41; Nonaka 1994: 19; Nonaka & Takeuchi 1995: 69) although somewhat confusingly they also say that internalization is ‘triggered’ by learning-by-doing (Nonaka et. al. 1996: 208).###While they cautioned that this was the first time that the survey had been used, that generalization to other cultures remained questionable, and that more qualitative data would have been useful (Nonaka et. al. 1994: 350), they nevertheless concluded that the survey validated the four modes of knowledge conversion (Nonaka & Takeuchi 1995: 91; see  Nonaka et. al. 1994 ).###While they cautioned that this was the first time that the survey had been used, that generalization to other cultures remained questionable, and that more qualitative data would have been useful ( Nonaka et. al. 1994:  350), they nevertheless concluded that the survey validated the four modes of knowledge conversion (Nonaka & Takeuchi 1995: 91; see Nonaka et. al. 1994).###We have to conclude that the notion of ‘combination’ is far from coherent despite having been validated by a survey ( Nonaka et. al. 1994 ).###The SECI components reappear at this level although in a different order ( Nonaka et. al. 1994:  342; Nonaka 1994: 17; Nonaka & Takeuchi 1995: 73, 89-90, 235- 6).###The first, or “epistemological”, dimension is the site of “social interaction” between tacit and explicit knowledge whereby knowledge is converted from one type to another, and new knowledge created ( Nonaka et. al. 1994:  338; Nonaka 1994: 15).###In 1993 Nonaka and his colleagues surveyed a convenience sample of Japanese managers to test the emerging theory of knowledge creation ( Nonaka et. al. 1994 ).",impact-revealing,providing context on the development of organizational knowledge creation theory
2909,5f0d85c69fced0a24be4f019,6817b5d48cf8f665b680de07d23b91814d2923b1,Bouquet of Instruction Pointers: Instruction Pointer Classifier-based Spatial Hardware Prefetching,58d83045d649053542fe853e,Path confidence based lookahead prefetching.,"It is well known that spatial prefetchers fail to improve performance for server workloads like CloudSuite [14], [33], [53], [58], [59] and additional prefetchers [12], [24], [52], [58], [59] can be used on top of IPCP to improve the performance.###Recent prefetching proposals [11], [14], [33] have also been coded and evaluated with ChampSim, helping the community for a fair comparison of techniques.###Prefetchers like variable length delta prefetching (VLDP) [45] and signature path prefetching (SPP) [33] are well known delta prefetchers.###The problem: State-of-the-art spatial prefetchers [45] [33], [11], [14], [13] are designed speciﬁcally for L2’s access patterns.###Well-established and recent spatial L2 prefetchers (prefetchers that prefetch within a spatial region) [33], [13], [14], [38], [11], [45] have pushed the limits of data prefetching.",other,highlighting the limitations and advancements in spatial prefetching techniques
804,5ed12ca69e795e8ab1c11568,b3b60a15502abca31ae7b20ce16bf50049183c5a,knowledge enhanced personalized search,599c7956601a182cd2631f67,Word-Entity Duet Representations for Document Ranking,"[41] consider the bags of entity representations in search model, and the interaction between bags of word representations and bags of entity representations is also studied in [42].###entities and relations from knowledge graphs, in search systems and effectively improves the text representation and ranking accuracy [14, 19, 21, 28, 40, 42, 43].###Then the personalized entity annotations enable KEPS to construct entity enhanced user profiles, using a memory network that represents user’s search preferences in the word-entity duet representation space [42].###Queries are often short and ambiguous[42], making query entity linking a challenging task: A recent study shows that state-of-the-art entity linking techniques only have 50% accuracy onweb queries [41].###Using such noisy query entities in ranking often requires manual annotations [12] or soft linking/diversification [42].",impact-revealing,highlighting challenges in query entity linking and its impact on ranking accuracy
2672,58d82fcbd649053542fd669e,a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c,Deep Variational Information Bottleneck.,58437722ac44360f1082f600,Differential Privacy as a Mutual Information Constraint.,"…real images; using richer parametric marginal approximations, rather than assuming r ( z ) = N (0 , I ) ; exploring the connections to differential privacy (see e.g., Wang et al. (2016a); Cuff & Yu (2016)); and investigating open universe classiﬁcation problems (see e.g., Bendale & Boult (2015)).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2881,5e15adca3a55ac47ab5b0729,97ebd482a78e6e6c1ba51da5e1b2f8e7640cc8b5,hyperbolic graph convolutional neural networks,573696ce6e3b12023e5ce95a,Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift,"In practice, however, optimization is much more stable when the values are normalized [16].",other,providing context on optimization stability
36,5e09a83ddf1a9c0c41685fc3,90820988957e74db0fbb1df79175610e08016ba4,Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling,5a9cb60d17c44a376ffb3c99,Co-PACRR: A Context-Aware Neural IR Model for Ad-hoc Retrieval.,", 2016) and Co-PACRR (Hui et al., 2018), adopt an interaction-based design.###Since relevance matching is fundamentally a matching task, most recent neural architectures, such as DRMM (Guo et al., 2016) and Co-PACRR (Hui et al., 2018), adopt an interaction-based design.",impact-revealing,acknowledge existing neural architectures for relevance matching
991,,703f71c39f254d0fb7845cbe151e95dd37c0c3ac,Knowledge Creation in New Product Development Projects,,,"###This research builds on the conceptualization of knowledge creation as proposed by Nonaka and several coauthors (Nonaka, 1994; Nonaka et al., 1994; Nonaka et al., 2000; Nonaka & Takeuchi, 1995).",impact-revealing,building on the conceptual framework of knowledge creation
2259,5736982b6e3b12023e6fd332,d0b8c5464b582e1a582a4affb270e8b02f8414a2,a scalable architecture for ordered parallelism,53e99ecab7602d97027978f1,"A ""Flight Data Recorder"" for Enabling Full-System Multiprocessor Deterministic Replay.","Third, they are needed in systems that must maintain externally-imposed order constraints, such as geo-replicated databases where transactions must appear to execute in timestamp order [14], or deterministic architectures [17, 45] and record-andreplay systems [36, 77] that constrain the schedule of parallel programs to ensure deterministic execution.",other,providing context for systems requiring order constraints
65,5ee9f15b91e01152af022ce0,bf2174c69f84f4e57813e0bed4571c6dbff123ed,Automatic Validation of Textual Attribute Values in E-commerce Catalog by Learning with Limited Labeled Data,53e99a48b7602d97022a8346,Auto-Encoding Variational Bayes.,"To enable distribution qφc (z |x q c ) differentiable, we follow previous work [2, 3, 19] to use reparameterization trick to parameterize z.###As with variational autoencoders [19], we approximate the objective function using the evidence lower bound (ELBO) on the log likelihood.###We concatenate the two final hidden states from Transformers and then feed them into two fully connected layers with weight matrix W2d×d μ and W2d×d σ to output mean μ and log(σ ) as suggested in [19].",impact-revealing,describing the method for parameterizing distribution in variational autoencoders
2618,5f00587b9fced0a24b1fbbf1,1803c317dbd25d64210026fc4181faa31e521719,USMPep: universal sequence models for major histocompatibility complex binding affinity prediction,59d83c830cf2bed5eb5203ef,NetMHCpan-4.0: Improved Peptide-MHC Class I Interaction Predictions Integrating Eluted Ligand and Peptide Binding Affinity Data.,"NetMHCpan4 [7] Input: 9mer fixed length BLOSUM encoding for peptide, pseudo-sequence for MHC molecule plus additional features; multilayer perceptron with one hidden layer",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
691,599e96ec9c05cae4992b4ef3,95b803d07c37e8349bd7b1318367d8237c76cbc0,Audio-driven facial animation by joint end-to-end learning of pose and emotion,53e9a85cb7602d97031a42f4,Expressive speech-driven facial animation,"They also present a user study rating the level of realism in emotion synthesis, covering several methods [Cao et al. 2005; Liu and Ostermann 2011; Melenchon et al. 2009].###One way to implement emotional states would be to manually label or categorize the training samples based on the apparent emotion [Anderson et al. 2013; Cao et al. 2005; Deng et al. 2006; Wampler et al. 2007].",impact-revealing,acknowledge methods for emotion synthesis and user study
1882,,8561ad2d222f0c89798a20686c3ba28a3d597001,Behavioral Activation and Inhibition as Moderators of the Relationship Between Music Video-Viewing and Joyriding Attitudes,,,"###Research indicated that the BAS fun-seeking subscale was associated with an increased alcohol consumption and smoking status (Franken & Muris, 2006; O’Connor, Stewart, & Watt, 2009; Voigt et al., 2009).###Sensitivity to rewards and punishments were assessed with the Dutch version of Carver and White’s (1994) BIS/BAS questionnaire, which was
validated by Franken and colleagues (Franken & Muris, 2006; Franken, Muris, & Rassin, 2005).###…in BIS and BAS are not only believed to result in individual differences in personality characteristics, such
as extraversion, but the relative strength of these two brain systems also influences whether someone will engage in a particular behavior or not (Franken & Muris, 2006; Gray, 1987, 1994).###BAS drive appeared to be a positive predictor of heavy and frequent alcohol consumption (O’Connor et al., 2009) and the number of illicit substances college students use (Franken & Muris, 2006).###Mixed results have been found with regard to the correlates of the BIS (Franken & Muris, 2006; O’Connor et al., 2009; Voigt et al., 2009) and the BAS reward subscale.###…unexpectedly, found reward responsiveness (BAS reward) to be negatively related to engagement in sex, taking safety precautions, and to the use of alcohol, drugs, and tobacco, other studies reported no relationship between BAS reward and risk-taking (Franken & Muris, 2006; O’Connor et al., 2009).###Following RST, Franken and Muris (2006) have argued in connection with drug and alcohol use, that individuals with a high BAS profile are expected to be more likely to exhibit approach behavior and to experience positive affect when exposed to stimuli that are associated with reward (Carver &…",impact-revealing,highlighting the relationship between BAS sensitivity and substance use behaviors
378,573696106e3b12023e5227c8,f5a7da72496e2ca8edcd9f9123773012c010cfc6,Neural Architectures for Named Entity Recognition,53e99beab7602d970249335e,Natural Language Processing (almost) from Scratch,"For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top.###For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Zhou and Xu (2015) also used a similar model and adapted it to the semantic role labeling task. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features.###For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features.###For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Zhou and Xu (2015) also used a similar model and adapted it to the semantic role labeling task. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers.###For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Zhou and Xu (2015) also used a similar model and adapted it to the semantic role labeling task.###As in Collobert et al. (2011), we use pretrained word embeddings to initialize our lookup table.",impact-revealing,acknowledge existing models and methods in sequence labeling
3932,5cede10dda562983788ed645,690edf44e8739fd80bdfb76f40c9a4a222f3bba8,BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer,5b67b45517c44aac1c860885,STAMP: Short-Term Attention/Memory Priority Model for Session-based Recommendation.,"Recently, some works try to employ the attention mechanism to improve recommendation performances and interpretability [28, 33].###Other than recurrent neural networks, various deep learning models are also introduced for sequential recommendation [3, 22, 33, 49].###STAMP captures both users’ general interests and current interests using an MLP network with attention [33].",other,acknowledge recent advancements in recommendation systems using attention mechanisms
882,5ddcf7f53a55ac1c5e8cce13,1f2577071ca2aa1086f4b1c12cd911061aeea960,meta-learning of neural architectures for few-shot learning,599c7974601a182cd263f01c,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.,"Prior work has proposed meta-learning methods for this problem that are model-agnostic [16, 37] and allow meta-learning weights of fixed neural architectures (see Figure 1, top).###This is in contrast to prior work where the architecture is always fixed during meta-testing [16, 37].###We show that model-agnostic, gradient-based metalearning methods (such as [16]) can very naturally be combined with recently proposed gradient-based NAS methods, such as DARTS [33].###Similar to MAML’s meta-learning strategy in the weight space [16], our goal is to meta-learn an architecture with corresponding weights which is able to quickly adapt to new tasks.###In our work, we choose DARTS for neural architecture search because of conceptual similarities to gradient-based meta-learning, such as MAML [16], which will allow us to combine the two kinds of methods.###In this work, we focus on a particular class of approaches denoted as model-agnostic meta-learning [16, 17, 37, 1, 18].###Prior work [16, 37, 25] considered a fixed, predefined architecture αfixed and chose Φ to be an optimizer like SGD for the weights:###Top: gradientbased meta-learning with fixed architecture such as MAML [16] or REPTILE [37].###As an example, one could use MAML [16] as a meta-learning algorithm, which runs SGD on the meta-objective, yielding meta-updates###Prior work [40, 16, 18, 19] often approaches few-shot learning via meta-learning or learning to learn [46, 50, 21, 22], where one aims at learning from a variety of learning tasks in order to learn new tasks much faster than otherwise possible [51].",impact-revealing,acknowledging existing meta-learning methods and their applications
2019,,0a7ac3d04f872889ae0804d78ddd30988b2f215b,The Prognostic Value of the Neutrophil–Lymphocyte Ratio (NLR) in Acute Pancreatitis: Identification of an Optimal NLR,,,"###The scoring system currently regarded as the gold standard for assessment of AP, namely the Acute Physiology and Chronic Health Evaluation (APACHE II), is labor intensive and is not widely adopted for patients with acute pancreatitis outside of the intensive care setting.(2,3) Other scoring systems such as the Sequential Organ Failure Assessment (SOFA) have been developed but are still suitable only in the intensive care setting and not for routine use in all patients presenting with AP.",impact-revealing,highlighting limitations of existing scoring systems for acute pancreatitis
2133,,5deb12b97917a2a4db322a29db12e63f44533858,Determining the Breakpoints of Fundamental Diagrams,,,"###The triangular FD is widely used for modeling and analyzing the important features of the kinematic wave model such as Daganzo [46] and Jabari and Liu [47].###The triangular FD has the advantage of simplifying mathematical and numerical analyses, as developed in the theoretical works of Newell [44] and Daganzo [45].",impact-revealing,highlighting the significance and advantages of the triangular FD in kinematic wave modeling
2882,5edcbb9a91e0110f1d6db1c2,c99e2877d4b1f171746a3aa73d5fd5fd680c982d,Hierarchical Bipartite Graph Neural Networks: Towards Large-Scale E-commerce Applications,5b67b4b917c44aac1c867dbc,Hierarchical Graph Representation Learning with Differentiable Pooling.,"2) Compared Algorithms and Settings: To the best of our knowledge, no existing algorithm can deal efﬁciently with hierarchical user preferences and hierarchical item attractiveness to predict real-world e-commerce tasks of such large scale, including [30] and [20].###On one hand, it demonstrates in [20] that hierarchical representations of graphs can be combined with various graph neural network architectures in an end-to-end fashion to achieve prevailing results on graph classiﬁcation benchmarks.###In particular, the recently proposed approach DIFFPOOL [20], a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion.###In [20], authors make some efforts in effectively co-training two embeddings by decomposing user information into two orthogonal spaces, each of which represents information captured by community level and individualized user preference respectively.",other,highlighting the lack of existing algorithms for hierarchical user preferences in e-commerce tasks
1304,,c4cd1bc6b7861d8135a23e247bcc53c5fe25e9a9,Atmospheric levels of BTEX compounds during the 2008 Olympic Games in the urban area of Beijing.,,,"###…1998; Rappenglück and Fabian, 1999; Rappenglück et al., 1999; Chan et al., 2002; Lee et al., 2002; Hellén et al., 2003; Na et al, 2003; Ho et al., 2004; Liu et al., 2005; Lu et al., 2007; Ras-Mallorqí et al., 2007; Song et al., 2007; Gros et al., 2007; Barletta et al., 2008; Xie et al., 2008).###…in the urban atmosphere (Singh et al., 1985; Chan et al., 2002; Gros et al., 2007; Barletta et al., 2008; Xie et al., 2008; Derwent et al., 1995; Rappenglück and Fabian, 1999; Lee et al., 2002; Na et al., 2003; Hellén et al., 2003; Ho et al., 2004; Ras-Mallorquí et al., 2007; Song et al., 2007).###However, the few measurements focused on August– September in the campus of Peking University revealed that the concentration of BTEX decreased about 50% from 2004 to 2006 (Gros et al., 2007; Liu et al., 2005; Lu et al., 2007; Song et al., 2007; Xie et al., 2008).###To evaluate the impacts of atmospheric BTEX on air quality, the ambient levels of BTEX at rural, suburban, city-center and industrial sites in many nations have been investigated (Singh et al., 1985; Derwent et al., 1995; Gee and Sollars, 1998; Rappenglück and Fabian, 1999; Rappenglück et al., 1999; Chan et al., 2002; Lee et al., 2002; Hellén et al., 2003; Na et al, 2003; Ho et al., 2004; Liu et al., 2005; Lu et al., 2007; Ras-Mallorqí et al., 2007; Song et al., 2007; Gros et al., 2007; Barletta et al., 2008; Xie et al., 2008).###…3.97 1.92 3.51 1.90 13.7 This work
Beijing 8–9/ 2006 6.87 9.38 3.26 4.69 1.91 26.1 Xie et al. (2008) Beijing 8/ 2005 5.62 11.4 4.26 8.86 4.04 34.2 Song et al. (2007) Beijing 8/ 2004 13.4 16.1 5.4 12.2 2.6 49.7 Gros et al. (2007)
Other cities in China Guangzhou 9/
2005 6.55 22.1 5.38 6.68 2.47…###The sampling sites of the three previous studies in Beijingwere all in the campus of Peking University where is only about 3.3 km away from our sampling site (Gros et al., 2007; Song et al., 2007; Xie et al., 2008).###The benzene/toluene (B/T) ratio is widely used as an indicator for vehicle exhaust in the urban areas because of the significant difference of the ratios between the vehicle exhaust and other sources, such as painting and gasoline etc (Sweet and Vermette, 1992; Baldasano et al., 1998; Song et al., 2007).###conducted in the urban atmosphere (Singh et al., 1985; Chan et al., 2002; Gros et al., 2007; Barletta et al., 2008; Xie et al., 2008; Derwent et al., 1995; Rappenglück and Fabian, 1999; Lee et al., 2002; Na et al., 2003; Hellén et al., 2003; Ho et al., 2004; Ras-Mallorquí et al., 2007; Song et al., 2007).###…(B/T) ratio is widely used as an indicator for vehicle exhaust in the urban areas because of the significant difference of the ratios between the vehicle exhaust and other sources, such as painting and gasoline etc (Sweet and Vermette, 1992; Baldasano et al., 1998; Song et al., 2007).###…sources, and vehicle exhaust has been recognized as the dominant source in the atmosphere, followed by gasoline evaporation, emissions from the use of solvents and paintings, leakage from natural gas and liquefied petroleum gas etc. (Liu et al., 2005; Lu et al., 2007; Song et al., 2007).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2599,5b1642388fbcbf6e5a9b54be,b3dae9529f3caeeec9cc6872e94aa690418acb22,Reinforcement Learning for Relation Classification from Noisy Data,53e9b89bb7602d97044763cc,Distant supervision for relation extraction without labeled data.,"…et al. 2014; dos Santos, Xiang, and Zhou 2015; Mooney and Bunescu 2005; Yang et al. 2016) including convolutional neural networks, recursive neural network (Ebrahimi and Dou 2015; Liu et al. 2015), and long short-term memory network (Miwa and Bansal 2016; Xu et al. 2015; Miwa and Bansal 2016).",other,acknowledge various neural network approaches in research
2809,5ef0816891e0112aee042b88,5c126ae3421f05768d8edd97ecd44b1364e2c99a,denoising diffusion probabilistic models,5bdc31b817c44a1f58a0bcb4,FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models,"While diffusion models might resemble ﬂows [9, 43, 10, 30, 5, 14, 21] and VAEs [31, 44, 34], diffusion models are designed so that q has no parameters and the top-level latent x T has nearly zero mutual information with the data x 0 .",other,providing context on diffusion models and their design
1758,,e2fe20e37e14851ad65c6154a55bee996c2cbc2c,Accelerating CNN Inference on ASICs: A Survey,,,"###Finally, the all-in-one reuse architecture called the Row stationary (RS) dataflow [26] (see Figure 13(d)) reuses all the types of data – weights, activations and partial sums – at the register file.###Because of architectural considerations, only four such combinations are widely used as described by Chen et al. [26] (refer to Figure 13).",impact-revealing,describing a specific dataflow architecture and its components
2949,5f03f3b611dc830562231f99,6b65f02a0b4826b5b26ea0ed5fdefebeda76c597,A Framework for Recommending Accurate and Diverse Items Using Bayesian Graph Convolutional Neural Networks,5dcbd5da3a55ac789b0dbdc8,Bayesian Graph Convolutional Neural Networks using Node Copying,"These limitations were addressed in the follow-up works [23, 24], where [23] uses a non-parametric model for the graph generative model and [24] proposes a node copying model to achieve flexibility in the generative model and improve computational efficiency.###The proposed BGNN incorporates a random graph generative model based on node-copying [24].###In [24], Pal et al. introduce the node copying model for 𝑝 (G) .###Bayesian GNNs have not previously been used for the task of recommendation, but it has been shown that they can produce significant performance improvements in semi-supervised node classification when there are very few training labels [23, 24, 30, 39].###As an alternative, we use a more general generative model for graphs based on copying nodes, as introduced in [24].",other,highlighting advancements in graph generative models and their application in recommendation systems
390,5db929e147c8f766461fc024,e03b5bc5edeb44d4b47d225c0c26ac54088fe528,An Improved Neural Baseline for Temporal Relation Extraction,5c0495e417c44a2c747043c5,CogCompTime: A Tool for Understanding Time in Natural Language.,"3.2), on the TCR dataset (Ning et al., 2018a).###Moreover, we further improve the LSTM system by injecting knowledge from an updated version of T EM P ROB , an automatically induced temporal common sense knowledge base that provides typical TempRels between events 1 (Ning et al., 2018b).###Temporal relation (TempRel) extraction has been considered as a major component of understanding time in natural language (Do et al., 2012; Uz-Zaman et al., 2013; Minard et al., 2015; Llorens et al., 2015; Ning et al., 2018a).###Recently, Ning et al. (2018c) introduced a new dataset called Multi-Axis Temporal RElations for Start-points (MATRES).###…2014; Cassidy et al., 2014; Mostafazadeh et al., 2016; O’Gorman et al., 2016), structured inference (Chambers and Juraf-sky, 2008a; Do et al., 2012; Chambers et al., 2014; Ning et al., 2018a), and structured machine learning (Yoshikawa et al., 2009; Leeuwenberg and Moens, 2017; Ning et al., 2017).###Both systems achieved better scores (suggesting that TCR is easier than MATRES), while the proposed sys-tem still outperformed CogCompTime by roughly 8% under the three-metric-average metric, consistent with our improvement on MATRES.###Ning et al. (2018b) was an initial attempt to acquire such knowledge, by aggregating automatically extracted TempRels from a large corpus.###Note that in Table 2, CogCompTime performed slightly different to Ning et al. (2018d): Cog-CompTime reportedly had F 1 =65.9 (Table 2 Line 3 therein) and here we obtained F 1 =66.6.###A recent annotation scheme, Ning et al. (2018c), introduced the notion of multi-axis to represent the temporal structure of text, and iden-tiﬁed that one of the sources of confusions in human annotation is asking annotators for TempRels across different axes.###This paper uses MATRES to show that a long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) system can readily outperform the previous state-of-the-art system, CogCompTime (Ning et al., 2018d), by a large margin.###Finally, since Concat+CSE improved over CogCompTime by a large margin either on MATRES or on TCR, it was not surprising to see that the proposed Concat+CSE is signiﬁcantly better than CogCompTime with p < 0 .###We also show our performance on another dataset, TCR 6 (Ning et al., 2018a), which contains both temporal and causal relations and we only need the temporal part.###Note that the T EM P ROB we use is reconstructed using the same method described in Ning et al. (2018b) with the base method changed to CogCompTime.###Table 3 furthermore applies CogCompTime and the proposed Concat+CSE system on a different test set called TCR (Ning et al., 2018a).###In addition, Ning et al. (2018d) only reported F 1 scores, while we also use another two metrics for a more thorough comparison: classiﬁcation accuracy (acc.) and temporal awareness F aware , where the awareness score is for the graphs represented by a group of related TempRels (more details in the…",impact-revealing,highlighting the improvement of the proposed system over previous state-of-the-art methods in temporal relation extraction
3998,5e5e18ba93d709897ce2b48e,04f3203f1214063436d81ce0c2ad7623204da488,Geom-GCN: Geometric Graph Convolutional Networks,56d8c494dabfae2eee42d895,Query-driven Active Surveying for Collective Classification,"Cora, Citeseer, and Pubmed are standard citation network benchmark datasets (Sen et al., 2008; Namata et al., 2012).",other,reporting standard benchmark datasets in citation networks
2933,53e9b54ab7602d97040825b6,e4fc3adca44206ecb5dc2c4960e578fe2d0994fe,Secure Untrusted Data Repository (SUNDR),53e9bab5b7602d97046e5ab2,Building secure file systems out of Byzantine storage.,"1 There remains the possibility of a malicious server entirely concealing some users’ actions from others, if neither collection of users expects anyone from the other to have accessed the ﬁle system.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3534,5bbacbad17c44aecc4eb007e,fdb87d305c59e9ab82a96198e34e46581b838c4e,one-shot relational learning for knowledge graphs,53e9ab78b7602d970351a734,Freebase: a collaboratively created graph database for structuring human knowledge.,"Large-scale knowledge graphs (Suchanek et al., 2007; Vrandečić and Krötzsch, 2014; Bollacker et al., 2008; Auer et al., 2007; Carlson et al., 2010) represent every piece of information as binary relationships between entities, usually in the form of triples i.###Large-scale knowledge graphs (Suchanek et al., 2007; Vrandeˇci´c and Kr¨otzsch, 2014; Bollacker et al., 2008; Auer et al., 2007; Carlson et al., 2010) represent every piece of information as binary relationships between entities, usually in the form of triples i.e. (subject, predicate, object) .",other,providing context on large-scale knowledge graphs
2542,53e9980eb7602d9702022371,d73cbcb051c3ba30bd037816fbc91852194dd8ca,continual flow pipelines,53e9ba64b7602d970468183c,Execution-based prediction using speculative slices,", a backward slice of a cache miss) is pre-executed [19, 23] on idle contexts of a multithreaded processor prior to encountering the blocked operation.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2476,5f75aa6a9fced0a24b64599d,69fb130409d48478fab0c4545dc2ff102a2630ef,The forward slice core microarchitecture,53e9ab7eb7602d9703526dce,An instruction set and microarchitecture for instruction level distributed processing,"A similar steering policy is used by Kim et al. [9] in their Instruction-Level Distributed Processing (ILDP) work, which proposes an ISA with in-order accumulator-based execution units.",other,acknowledge related work in steering policy
2500,5efb0d5691e011063336d27d,2b364917b0c51e91fcf2ab9c1d66a14ed4b44c03,BERTology Meets Biology: Interpreting Attention in Protein Language Models,5f68917f9fced0a24b340640,Accelerating Protein Design Using Autoregressive Generative Models,"In [46, 61], autoregressive generative models were trained to predict the functional effect of mutations and generate natural-like proteins.",other,reporting prior findings on autoregressive generative models
1295,,d5513ae4aee1f0b27dc260585ec05211f1dce274,A General Framework for Implicit and Explicit Debiasing of Distributional Word Vector Spaces,,,"###It is inspired by the work on semantic specialization of word embeddings (Mrkšić et al. 2017; Glavaš and Vulić 2018): but instead of
using linguistic constraints (e.g., synonyms), we “specialize” the vector space by leveraging debiasing constraints.###We therefore extract nearest neighbours of initial terms from an embedding space specialized to accentuate true semantic similarity and attenuate other types of semantic association (Faruqui et al. 2015; Wieting et al. 2015; Mrkšić et al. 2017; Glavaš and Vulić 2018, inter alia).###We first define two types of bias specifications – implicit and explicit – and propose a method of augmenting bias specifications with the help of embeddings specialized for semantic similarity (Mrkšić et al. 2017; Ponti et al. 2018).###10We evaluate word similarities for DE, IT, RU, and HR on their respective SimLex datasets (Leviant and Reichart 2015; Mrkšić et al. 2017); there is no ES and TR SimLex.###We first define two types of bias specifications – implicit and explicit – and propose a method of augmenting bias specifications with the help of embeddings specialized for semantic similarity (Faruqui et al. 2015; Mrkšić et al. 2017; Ponti et al. 2018).###We evaluate word similarities for DE, IT, RU, and HR on their respective SimLex datasets (Leviant and Reichart 2015; Mrkšić et al. 2017); there is no ES and TR SimLex.",impact-revealing,drawing inspiration from previous work on semantic specialization of word embeddings
321,5ede0553e06a4c1b26a8419c,1f3c381eedfe8914b81e93070bfdb00cf86ac943,Contrastive Multi-View Representation Learning on Graphs,5e5e18b993d709897ce2ad5c,On Mutual Information Maximization for Representation Learning,"Nevertheless, it is shown that success of these models cannot only be attributed to the properties of MI alone, and the choice of encoder and MI estimators have a signiﬁcant impact on the performance (Tschannen et al., 2020).",impact-revealing,highlighting the importance of encoder and MI estimators in model performance
2486,53e99967b7602d97021ac42b,41721de035c15528a7e35d3ab4d79b053633d763,Feedback-directed memory disambiguation through store distance analysis,53e9bd6fb7602d9704a0cd33,Pin: Building Customized Program Analysis Tools With Dynamic Instrumentation,"However, tools like ATOM 
[22] and PIN [12] may be used on many architectures to reduce the pro.ling cost over simu­lation.###However, tools like ATOM [22] and PIN [12] may be used on many architectures to reduce the profiling cost over simulation.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
570,5a73cb3517c44a0b303556bf,eac48f406c46527f5ca821de7fe8d62d6db56a27,"Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer",53e9bd8cb7602d9704a3269b,Sequence Transduction with Recurrent Neural Networks,"During inference, the most likely label sequence is computed using beam search as described in [13], with a minor alteration which was found to make the algorithm less computationally intensive without degrading performance: we skip summation over prefixes in pref(y) (see Algorithm 1 in [13]), unless multiple hypotheses are identical.###The RNN-T was proposed by Graves [13] as an extension to the connectionist temporal classification (CTC) [17] approach for sequence labeling tasks where the alignment between the input sequence, x, and the output targets y is unknown.###The RNN-T model, depicted in Figure 1, consists of an encoder (referred to as the transcription network in [13]), a prediction network and a joint network; as described in [15], the RNN-T model can be compared to other encoder-decoder architectures such as “listen, attend, and spell” [7], if we view the combination of the prediction network and the joint network as a decoder.###For example, in our previous work [15] we evaluated a number of end-to-end models including attention-based models [7] and RNN-T [13, 14] trained on ∼12,500 hours of transcribed training data; although end-to-end approaches were found to be comparable to a state-of-the-art context-dependent phone-based baseline on dictation test sets, these models were found to be significantly worse than the baseline on voice-search test sets.###The entire network is trained jointly to optimize the RNN-T loss [13], which marginalizes over all alignments of target###recurrent neural network transducer (RNN-T) [13, 14].",impact-revealing,describing the RNN-T model and its components
1418,,d352df83b48a221867b2bcd42e72f6491b6445d9,Data-Driven Offline Optimization For Architecting Hardware Accelerators,,,"###Like several offline reinforcement learning algorithms [36], our method, PRIME and COMs are based on the key idea of learning a conservative surrogate of the desired objective function, such that it does not overestimate the value of unseen data points, which prevents the optimizer from finding accelerators that appear promising under the learned model but are not actually promising under the actual objective.###[36] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.###PRIME builds on “conservative” offline RL and offline MBO methods that train robust surrogates [36, 57].###While a straightforward approach for learning such a mapping is to train it via supervised regression, by minimizing the mean-squared error Exi,yi∼D[(fθ(xi)− yi)(2)], prior work [35, 36, 57] has shown that such predictive models can arbitrarily overestimate the value of an unseen input xi.",impact-revealing,describing the method and its foundation in offline reinforcement learning
3280,5eede0b091e0116a23aafc15,9a75cb455b4e70c66f3b72e6bb1498d8cab72fb2,Big Self-Supervised Models are Strong Semi-Supervised Learners,5c75715bf56def9798776d22,Large Batch Training of Convolutional Networks,We use the LARS optimizer [32] (with a momentum of 0.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3056,58437722ac44360f1082efeb,36eff562f65125511b5dfab68ce7f7a943c27478,Semi-Supervised Classification with Graph Convolutional Networks,53e9a727b7602d970305fa6d,Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples,"…where label information is smoothed over the graph via some form of explicit graph-based regularization (Zhu et al., 2003; Zhou et al., 2004; Belkin et al., 2006; Weston et al., 2012), e.g. by using a graph Laplacian regularization term in the loss function: (1) Here, L 0 denotes the…###…against the same baseline methods as in Yang et al. (2016), i.e. label propagation (LP) (Zhu et al., 2003), semi-supervised embedding (SemiEmb) (Weston et al., 2012), manifold regularization (ManiReg) (Belkin et al., 2006) and skip-gram based graph embeddings (DeepWalk) (Perozzi et al., 2014).###Methods based on graph-Laplacian regularization (Zhu et al., 2003; Belkin et al., 2006; Weston et al., 2012) are most likely limited due to their assumption that edges encode mere similarity of nodes.###Prominent examples for graph Laplacian regularization include label propagation (Zhu et al., 2003), manifold regularization (Belkin et al., 2006) and deep semi-supervised embedding (Weston et al., 2012).",other,highlighting limitations in graph-based regularization methods
2871,5c8c52bc4895d9cbc6ddad8d,76e4d56d712d64ec2f77fd5b2fcb504888c07eab,Island loss for learning discriminative features in facial expression recognition,53e9aa73b7602d97033ebd77,Au-Aware Deep Networks For Facial Expression Recognition,"Benefiting from the advance in feature learning, features can be learned either unsupervised by sparse coding [25], [59], [35], [29] or supervised by deep learning [36], [44], [24], [5], [8], [30], [21], [15], [22], [58], [9], [51], [3], [49],",other,acknowledge advancements in feature learning methods
1089,,fbd2b61c998cb3ad8427759a45370a02d9338c31,"Task analysis and human-computer interaction: approaches, techniques, and levels of analysis",,,"###Furthermore, a GOMS analysis can be used to predict the quality of an existing system or prototype (Preece et al., 1994).###…of keystroke level interaction _x0083_ Improves productivity _x0083_ Not applicable to broader problems _x0083_ Ignores contextual factors Card et al., 1983 Preece et al., 1994 John and Kieras, 1996 Conceptual CTA _x0083_ Defines a coherent knowledge representation for the domain being studied _x0083_ Requires deep…###Decomposes complex tasks into subtasks _x0083_ Complex activities demand extensive hierarchy construction/charting _x0083_ Improves problem diagnosis and useful for concurrent operations _x0083_ Does not account for system dynamics MacLean et al., 1991 Annet and Stanton, 2000 Hollan et al., 2000 Shepherd 2001 Technical GOMS _x0083_ Requires detailed analysis of keystroke level interaction _x0083_ Improves productivity _x0083_ Not applicable to broader problems _x0083_ Ignores contextual factors Card et al., 1983 Preece et al., 1994 John and Kieras, 1996 Conceptual CTA _x0083_ Defines a coherent knowledge representation for the domain being studied _x0083_ Requires deep engagement with a particular knowledge domain _x0083_ Increases the understanding of cognitive aspects of the task _x0083_ Captures task expertise _x0083_ Fails to fully incorporate learning, contextual and historical factors Barnard and May, 2000 Chipman et al., 2000 Dubois and Shalin, 2000 Work-Process###Although some argue that the sociotechnical approach failed to fully integrate technology with user needs to improve the tasks being performed (Preece et al., 1994), it is clear that in some cases organizational performance was generally enhanced.###Contrary to some criticisms of HTA (e. g. Preece et al. 2002), these plans can describe quite complex activities, including contingencies, decision points, concurrent operations and cycling (Shepherd, 2001).###Ideally, these predictions are based on "" parameters that are robust and reliable across tasks and can be used without further empirical validation "" (John and Kieras, 1996a:3).###The downside is that its "" higher degree of formality and precision "" make it harder to learn and use (John and Kieras, 1996b).###Practitioners and researchers routinely advocate building user-centered systems which enable people to reach their goals, take account of natural human limitations, and generally are intuitive, efficient and pleasurable to use (Preece, Rogers and Sharp, 2002).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2222,5bdc315017c44a1f58a05e13,1717255b6aea01fe956cef998abbc3c399b5d7cf,AMC: AutoML for Model Compression and Acceleration on Mobile Devices,58d82fced649053542fd7299,Designing Neural Network Architectures using Reinforcement Learning.,"Following [57, 4, 54], we use the following reward:###The discount factor γ is set to 1 to avoid over-prioritizing short-term rewards [4].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3380,5ede0553e06a4c1b26a841e6,9a772646ef9ed9c917f45fa592d5f89f7d5f8542,bayesian graph neural networks with adaptive connection sampling,5a260c0c17c44a4ba8a1e088,Concrete Dropout.,"Following the variational interpretation in Gal et al. (2017), GDC can be seen as an approximating distribution q θ ( ω ) for the posterior p ( ω | A , X ) when considering a set of random weight matrices ω = { ω e } |E| e =1 in the Bayesian framework, where ω e = { W ( l ) e } Ll =1 is the set of…###W ( l ) , are considered random to enable Bayesian inference based on predictive posterior given training data (Gal et al., 2017; Boluki et al., 2020).###We impose a concrete distribution relaxation (Jang et al., 2016; Gal et al., 2017) for the Bernoulli random variable z uv , leading to an efﬁcient optimization by sampling from simple sigmoid distribution which has a convenient parametrization where u ∼ Unif[0 , 1] and t is temperature parameter of…###To be able to evaluate the KL term analytically, the discrete quantised Gaussian can be adopted as the prior distribution as in Gal et al. (2017).",other,providing context for Bayesian inference in GDC
1800,,9a6aafcac7cb0cb00edc2a8d88736dd2a41c5885,Livelihood alternatives model for sustainable rangeland management: a review of multi-criteria decision-making techniques,,,"###Each weighting method differs with regard to accuracy, ease of use and theoretical foundations and creates different weights for the criteria (Zardari et al. 2014).###Equal weighting methods have been applied in many decision-making problems as the simplest way to weigh criteria and distribute equal weights among all the proposed criteria (Wang et al. 2009; Zardari et al. 2014).###…Mendoza and Martins 2006; Ananda and Herath 2009), ecosystem management (Prato 1999, 2003), water resource management (Hajkowicz and Collins 2007; Zardari et al. 2014), renewable energy (Taha and Daim 2013), flood (Levy 2005; de Brito and Evers 2016), wetland ecosystems (Malekmohammadi and…###In general, the depth and type of available information largely determines which techniques could be used for a particular multi-criteria problem (Zardari et al. 2014).###2009), agriculture (Hayashi 2000), fishery (Mardle and Pascoe 1999), forest (Ananda and Herath 2003; Kangas and Kangas 2005; Mendoza and Martins 2006; Ananda and Herath 2009), ecosystem management (Prato 1999, 2003), water resource management (Hajkowicz and Collins 2007; Zardari et al. 2014), renewable energy (Taha and Daim 2013), flood (Levy 2005; de Brito and Evers 2016), wetland ecosystems (Malekmohammadi and Blouchi 2014) and natural recourses management as a whole (Herath and Prato 2006).###To avoid such obstacles, the reasonable and manageable amounts of criteria should be contained in the model (Zardari et al. 2014).###The AHP technique, as a subjective method, builds on the pairwise comparison model to determine the weights for every unique criterion to be completed by the experts (Wang et al. 2009; Zardari et al. 2014).###Then, 11 attributes (natural, human, social, financial, physical capital and seasonality shocks, trends, policy, institution and processes) were Table 4 Pairwise comparison scale for AHP preference (Zardari et al. 2014) Numerical rating Verbal judgments of preference 1 Equally preferred 2 Equally to moderately 3 Moderately to preferred 4 Moderately to strongly 5 Strongly preferred 6 Strongly to very strongly 7 Very strongly preferred 8 Very strongly to extremely 9 Extremely preferred Livelihood alternatives model for sustainable rangeland.###The rank-order weights have been categorized as objective, subjective and combined methods (Wang et al. 2009; Zardari et al. 2014).",impact-revealing,reporting on various weighting methods and their applications
22,58d82fcbd649053542fd669e,a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c,Deep Variational Information Bottleneck.,53e99ae2b7602d970236ab1d,The information bottleneck method,"3 This approach is known as the information bottleneck (IB), and was ﬁrst proposed in Tishby et al. (1999).###There are two notable exceptions: the ﬁrst is when X , Y and Z are all discrete, as in Tishby et al. (1999); this can be used to cluster discrete data, such as words.",impact-revealing,providing context for the information bottleneck approach
2621,5f7aeb7691e011983cc81e80,572c12e81319ccd47cc0c637c82efadd03fd05ab,Autoregressive entity retrieval,58437725ac44360f1082ffd7,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,"Concretely, we use the objective that is typically used for neural machine translation (NMT, Wu et al., 2016), that is maximizing log p θ ( y | x ) with respect to model’s parameters θ which, due to the factorized formulation, can be calculated exactly.",other,describing the objective used in neural machine translation
2164,,ef30d8035b0420143a5f9a094967422b03d0b404,A Formal Language toward the Unification of Model Checking and Performance Evaluation,,,"###To express dependability related properties, traditional model checking logics such as linear temporal logic (LTL) [58] and computation tree logic (CTL) [20] have evolved into quantitative model checking logics; the original temporal logics are extended by adding or modifying operators (e.###From the previous chapter, we learned that temporal logics such as LTL [58] and CTL [19] are powerful, widely used logics for properties that depend only on which states can be reached, such as “the system never reaches a deadlocked state”, and not on the time or probability to reach them.###3 Linear-time Temporal Logic (LTL) Linear-time temporal logic [58], or LTL for short, is designed to express properties related to linear time paths.###For such properties, traditional logic formalisms such as linear-time temporal logic (LTL) [58], computation tree logic (CTL) [20], and computation tree logic star (CTL*) [31] are powerful, widely-used logics 2.",impact-revealing,providing context on traditional model checking logics
1816,,3357d8cc9767688a7299ff8829c8c8e298db7cd7,The Complex Media Effects on Civic Participation Intention Amid COVID-19 Pandemic: Empirical Evidence from Wuhan College Students,,,"###The conceptualization of civic participation intention was inspired by the concept of community participation [4], which refers to a narrow concept such as voting in the earlier work [5], and then a broad concept including a wide range of activities in the later research, as defined by Zimmerman and Rappaport [6] (p.###The conceptualization of civic participation intention was inspired by the concept of community participation [4], which refers to a narrow concept such as voting in the earlier work [5], and then a broad concept including a wide range of activities in the later research, as defined by Zimmerman and Rappaport [6] (p. 726) as “any organized activities in which the individual participates without pay
Int.",impact-revealing,drawing inspiration from prior concepts of civic participation
3297,5d3ed25a275ded87f97deb36,37f7eaf57b88ef03efd4196f2f57ae0b4c657a79,Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommendation,57a4e91dac44365e35c9830c,Learning Convolutional Neural Networks for Graphs,"Several GNN methods also operate on the local neighbors of a node directly [6, 7, 15].",other,acknowledge existing GNN methods
2694,5aed148b17c44a4438154fae,fa54b47df8641dff1579b5e8e0f18f057de68e73,DRN: A Deep Reinforcement Learning Framework for News Recommendation,57d063b4ac4436735428dc8f,Online Learning To Rank For Information Retrieval Sigir 2016 Tutorial,"Therefore, rather than doing random exploration, we apply a Dueling Bandit Gradient Descent algorithm [16, 17, 49] to do the exploration.###Third, we propose to applyDueling Bandit Gradient Descent exploration strategy [16, 49] to our algorithm which can both improve recommendation diversity and avoid the harm to recommendation accuracy induced by classical exploration strategies like ε-greedy [31] and Upper Confidence Bound [23].###Then, the agent G will do a probabilistic interleave [16] to generate the merged recommendation list L̂ using L and L̃.###Third, we propose to apply a Dueling Bandit Gradient Descent (DBGD)method [16, 17, 49] for exploration, by choosing random item candidates in the neighborhood of the current recommender.",other,describing the application of a specific algorithm for exploration in recommendations
2899,5f3e44b791e011c0de1c29bc,61325245e98920a69b40e18c069fda0c1cf00f21,MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings for Sequential Recommendation,5ce2d08dced107d4c638fb4e,Multi-order Attentive Ranking Model for Sequential Recommendation,"In order to validate the effectiveness of our method, we compare it with the state-of-the-art baselines: MARank [32], SASRec [10], TiSASRec [14], and BERT4Rec [19].",other,comparing the proposed method with state-of-the-art baselines
874,5edf5ddc91e011bc656defd7,c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87,linformer: self-attention with linear complexity,599c7987601a182cd2648373,Attention Is All You Need.,"Transformer models (Vaswani et al., 2017) have become ubiquitous for wide variety of problems in natural language processing (NLP), including translation (Ott et al., 2018), text classiﬁcation, question answering, among others (Raffel et al., 2019; Mohamed et al., 2019).###Complexity per Layer Sequential Operation (Vaswani et al., 2017) O ( n 2 ) O (1) Sparse Tansformer, (Child et al., 2019) O ( n √ n ) O (1) Reformer, (Kitaev et al., 2020) 2 Backgrounds and Related works",impact-revealing,highlighting the widespread application and significance of transformer models in NLP
1870,,4f5cc09b76438ea61313307c3562cb73ddd494a2,Designing Education for Professional Expertise Development,,,"###” Following Whittemore and Kna ﬂ (2005), relevant empirical as well as theoretical papers are included in the review.###This method builds on the updated integrative review method described by Whittemore and Kna ﬂ (2005).",impact-revealing,acknowledge the methodology used in the review
3667,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e9aa24b7602d970339167e,Heuristics in Monte Carlo Go,"Evaluation functions are problematic for several reasons: • a piece placed early in the game may have a strong influence later in the game, even if it will eventually be captured [76]; • it can be impossible to determine whether a group will be captured without considering the rest of the board; • most positions are dynamic, i.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
780,599c797a601a182cd2642797,668db48c6a79826456341680ee1175dfc4cced71,Get To The Point: Summarization with Pointer-Generator Networks,57d063b9ac4436735428ea8e,Modeling Coverage For Neural Machine Translation,"Originating from Statistical Machine Translation (Koehn, 2009), coverage was adapted for NMT by Tu et al. (2016) and Mi et al. (2016), who both use a GRU to update the coverage vector each step.###We propose a novel variant of the coverage vector (Tu et al., 2016) from Neural Machine Translation, which we use to track and control coverage of the source document.###We adapt the coverage model of Tu et al. (2016) to solve the problem.###Repetition is a common problem for sequence-to-sequence models (Tu et al., 2016; Mi et al., 2016; Sankaran et al., 2016; Suzuki and Nagata, 2016), and is especially pronounced when generating multi-sentence text (see Figure 1).",impact-revealing,highlighting the adaptation of coverage models in neural machine translation
3291,5e2ac357df1a9c0c41e7fa48,73bc990757d54f2161eceacd330e0aeb189393bf,Building attention and edge message passing neural networks for bioactivity and physical–chemical property prediction,5ce2d07eced107d4c63856f3,A survey of multi-task learning methods in chemoinformatics,"The higher the y-axis is, the better the model performs effectively pick up on common structural features and learn them as reported in other studies [62, 63].",other,acknowledging prior findings on model performance
3109,5cede10fda562983788ef75c,2a69ddbafb23c63e5e22401664bea229daaeb7d6,Res2Net: A New Multi-Scale Backbone Architecture,5b67b4b417c44aac1c867290,Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition,"Since the proposed Res2Net module does not have specific requirements of the overall network structure and the multi-scale representation ability of the Res2Net module is orthogonal to the layer-wise feature aggregation models of CNNs, we can easily integrate the proposed Res2Net module into the state-ofthe-art models, such as ResNet [23], ResNeXt [56], DLA [60] and Big-Little Net [5].###Big-Little Net [5] is a multi-branch network composed of branches with different computational complexity.###Different from some concurrent works [5], [9], [11] that improve the multi-scale ability by utilizing features with different resolutions, the multi-scale of our proposed method refers to the multiple available receptive fields at a more granular level.###For fair comparisons, we use the Pytorch implementation of ResNet [23], ResNeXt [56], DLA [60] as well as bLResNet50 [5], and only replace the original bottleneck block with the proposed Res2Net module.###One common operation in [5], [9], [11], [48], [49] is that they all use pooling or up-sample to re-size the feature map to 2 times of the original scale to save the computational budget while maintaining or even improving performance.###Note that the ResNet [23], ResNeXt [56], SE-Net [25], bLResNet [5], and DLA [60] are the state-of-the-art CNN models.###bLResNet-50 [5] 22.###Recently, there are some concurrent works aiming at improving the performance by utilizing the multi-scale features [5], [9], [11], [49].###For experiments on the ImageNet [44] dataset, we mainly use the ResNet-50 [23], ResNeXt-50 [56], DLA-60 [60], and bLResNet-50 [5] as our baseline models.",other,describing the integration of the Res2Net module into existing models
3489,5db1765a3a55ac101c887e97,6c4b76232bb72897685d19b3d264c6ee3005bc2b,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,5390985d20f70186a0e0854d,BLEU: a method for automatic evaluation of machine translation,"For all translation tasks, we report the BLEU score [Papineni et al., 2002] as provided by SacreBLEU v1.",other,reporting evaluation metric used
3536,5cb06564ced107d4c6006f1c,19351711295bddc627f761d59a1ef58ab2fa7e2c,Identifying SDC-Causing Instructions Based on Random Forests Algorithm,53e997c6b7602d9701fb7074,Random Forests,We build the trees compliance with the random forest framework [27].,other,describing the method used for building trees
718,5e539eca3a55ac4db70a53f1,c529f5b08675f787cdcc094ee495239592339f82,learning to simulate complex physics with graph networks,5e5e18e193d709897ce38629,Lagrangian Fluid Simulation with Continuous Convolutions,"We also created WATER-3D, a high-resolution 3D water scenario with randomized water position, initial velocity and volume, comparable to Ummenhofer et al. (2020)’s containers of water.###Ummenhofer et al. (2020), for example, train with two-step predictions.###To be consistent with Ummenhofer et al. (2020), we used their batch size of 16, learning rate decay of 10−3 to 10−5 for 50k iterations, and connectivity radius of 4.5x the particle radius.###…Sanchez-Gonzalez et al. (2018)’s GNbased model which was applied to various robotic control systems, Li et al. (2018)’s DPI which was applied to fluid dynamics, and Ummenhofer et al. (2020)’s Continuous Convolution (CConv) which was presented as a non-graph-based method for simulating fluids.###The full CConv update as described in Ummenhofer et al. (2020) is, f ′i = 1 ψ(xi) ∑ j∈N (xi,R) a (xj ,xi) fj g (Λ (xj − xi)).###Ummenhofer et al. (2020) reported CConv outperformed DPI, so we quantitatively compared our GNS model to CConv.###We implemented the CConv model, loss and training procedure as described by Ummenhofer et al. (2020).###Continuous convolution (CConv)
Recently Ummenhofer et al. (2020) presented Continuous Convolution (CConv) as a method for particle-based fluid simulation.###While Ummenhofer et al. (2020) state that “Unlike previous approaches, we do not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors.”###While previous learning simulation approaches (Li et al., 2018; Ummenhofer et al., 2020) have been highly specialized for particular tasks, we found our single GNS model performed well across dozens of experiments and was generally robust to hyperparameter choices.###This it implicitly what happens when the loss is defined directly on the positions, regardless of whether the inputs are perturbed with noise (Sanchez-Gonzalez et al., 2018) or the inputs have noise due to model error (Ummenhofer et al., 2020).###Ummenhofer et al. (2020)’s CConv propagates information across particles5, and uses particle update functions and training procedures which are carefully tailored to modeling fluid dynamics (e.g., an SPH-like local kernel, different sub-networks for fluid and boundary particles, a loss function…",impact-revealing,acknowledge and compare methods in fluid simulation
2844,5aed14d617c44a4438158d78,7d89abfe87ed7d1b40391d37364560656d208117,learning memory access patterns,5aed147c17c44a4438153ecf,Memory Hierarchy for Web Search,"However, the working sets of modern datacenter workloads are orders of magnitude larger than those of traditional workloads such as SPEC CPU2006 and continue to grow (Ayers et al., 2018; Ferdman et al., 2012; Gutierrez et al., 2011; Hashemi et al., 2016).",other,highlighting the increasing scale of modern datacenter workloads compared to traditional workloads
1574,,3c10dbaec2085c7ee536f5ec84e11934f2abeb7f,T-cell Co-inhibitory Molecules in Sepsis-induced Immunosuppression: From Bench to Bedside,,,"###Later, Monneret et al.[11] measured cytokines T‐cell Co‐inhibitory Molecules in Sepsis‐induced###Later, Monneret et al.[11] measured cytokines
T‑cell Co‑inhibitory Molecules in Sepsis‑induced Immunosuppression: From Bench to Bedside
Jian‑Feng Xie, Hai‑Bo Qiu, Yi Yang
Department of Critical Care Medicine, Nanjing Zhongda Hospital, School of Medicine, Southeast University, Nanjing, Jiangsu 210009, China
Address for correspondence: Prof. Yi Yang, Department of Critical Care Medicine, Nanjing Zhongda Hospital, School of Medicine, Southeast University, Nanjing, Jiangsu 210009, China
E-Mail: yiyiyang2004@163.com
Key words: Co‑inhibitory Molecules; Immunosuppression; Sepsis; T‑cell
Access this article online
Quick Response Code: Website: www.cmj.org
DOI: 10.4103/0366‑6999.205867
This is an open access article distributed under the terms of the Creative Commons Attribution‑NonCommercial‑ShareAlike 3.0 License, which allows others to remix, tweak, and build upon the work non‑commercially, as long as the author is credited and the new creations are licensed under the identical terms.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3066,5a9cb65d17c44a376ffb83f3,e06357ac23811054acb23e2ea30d087f5beaef90,an interpretable reasoning network for multi-relation question answering,5a260c2817c44a4ba8a23761,Recovering Question Answering Errors via Query Revision.,"Our work follows the line of Embedding-based models (Bordes et al., 2014b; Dong et al., 2015; Xu et al., 2016; Hao et al., 2017; Yavuz et al., 2017) which are recently introduced into the QA community where questions and KB entities are represented by distributed vectors, and QA is formulated as a problem of matching between vectors of questions and answer entities.###In contrast, embedding-based models (Bordes et al., 2014b; Hao et al., 2017; Yavuz et al., 2017) can be trained end-to-end with weak supervision, but existing methods are not adequate to handle multi-relation QA due to the lack of reasoning ability.",other,highlighting the limitations of existing embedding-based models in multi-relation QA
544,5e5e189993d709897ce1e202,2bf7c350a8280e7c593d46a60127f99b21517121,on the variance of the adaptive learning rate and beyond,5550415745ce0a409eb3a739,Adam: A Method for Stochastic Optimization.,"In particular, Adagrad (Duchi et al., 2010) and its variants, e.g. , RMSprop (Hinton et al., 2012), Adam (Kingma & Ba, 2014), Adadelta (Zeiler, 2012) and Nadam (Dozat, 2016), stand out due to their fast convergence, and have been considered as the optimizer of choice in many applications.",impact-revealing,highlighting the significance and popularity of specific optimization algorithms
3229,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,53e9a58bb7602d9702eb2bd4,cSamp: A System for Network-Wide Flow Monitoring.,"[117] presented a system-wide approach that samples as a router primitive.###[117] proposed cSamp, a monitoring tool based on a coordinating mechanism for flow sampling, hash-based packet selection, and workload distributed.",other,reporting prior findings on system-wide approaches
3624,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,558c1d30e4b0cfb70a1bb744,Identifying Key Features for P2P Traffic Classification,"entropy, kernel function, mutual information and Hellinger distance [134], or data fusion with other log files such as Snort, DNS related requests [7] (number of DNS requests, response, normals, and anomalies for each host over a certain period of time).###(%) Feature Application 2007 [57] NBKE 91 Basic and derived P2P, email, Multimedia 2009 [17] DT 90 Basic P2P, VoIP, DNS, email, FTP 2010 [19] Clustering 90 Application SNMP, email, DNS, IRC 2010 [113] SVM 90 Advanced P2P 2010 [116] SVM 94 Application Webmail 2010 [9] DT 90 Basic and derived P2P, HTTP, VoIP, DNS, FTP, email, games 2011 [134] SVM 70 Advanced P2P 2012 [81] BN 95 Derived BULK, email, P2P",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1347,,cac2475bcbb52641ac02e7fe046e808d01e4eaaf,Motion Tuned Spatio-Temporal Quality Assessment of Natural Videos,,,"###Our approach to VQA represents an evolution, as we have sought to develop principles for VQA that were inspired by the structural similarity and information theoretic approaches to IQA proposed in [12]‐[ 15 ].###Spatial quality measurement is accomplished by a method loosely inspired by the SSIM index and the information theoretic methods for IQA [13]‐[ 15 ].",impact-revealing,highlighting the evolution of VQA principles inspired by IQA approaches
497,57d063e8ac443673542950ad,0509facb30efa81c4730f98bfc28cdbd6178822e,"Image sentiment analysis using latent correlations among visual, textual, and sentiment views",53e9b18fb7602d9703c1b31b,SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining,"For this, we use an exter-nal knowledge base, called SentiWordNet [10].###To highlight the sentiment information in the text, we introduce an external sentiment knowledge base, Senti-WordNet [10], which forms the sentiment view.###…feature-based method [4] (denoted as Low ), a mid-level visual feature-based method [5] (denoted as SentiBank ), a method that concatenates low-level visual features with the mid-level features (denoted as Low&SentiBank ), and a textual feature-based method [10] (denoted as SentiStrength 3 ).",impact-revealing,describing the use of an external knowledge base for sentiment analysis
462,5f88146591e0118ce8f040a7,3cb9c274a1a087be7040675b744b2fd0d579e55f,Are all negatives created equal in contrastive instance discrimination?,58437725ac44360f1082fa21,Hard Negative Mining For Metric Learning Based Zero-Shot Classification,"In this paradigm, selecting the hardest (Bucher et al., 2016) or harder (Schroff et al.###In this paradigm, selecting the hardest (Bucher et al., 2016) or harder (Schroff et al., 2015) negatives has improved both the rate of learning and final performance.",impact-revealing,acknowledging the effectiveness of selecting harder negatives in learning
533,5cede10fda562983788ef75c,2a69ddbafb23c63e5e22401664bea229daaeb7d6,Res2Net: A New Multi-Scale Backbone Architecture,5a260c8117c44a4ba8a30771,Squeeze-and-Excitation Networks.,"3, we can easily integrate the cardinality dimension [43] and SE block [19] with the proposed Res2Net module.###3: The Res2Net module can be integrated with the dimension cardinality [43] (replace conv with group conv) and SE [19] blocks.###Numerous neural network modules have been proposed in recent years, including cardinality dimension introduced by Xie et al. [43], as well as squeeze and excitation (SE) block presented by Hu et al. [19].###…a more efﬁcient network architecture is the key to further improving the performance of CNNs. Inthe past few years, several backbone networks, e.g. , [6], [9], [17], [19], [20], [22], [37], [39], [43], [47], have made signiﬁcant advances in numerous vision tasks with state-of-the-art performance.###Similar to [19], we add the SE block right before the residual connections of the Res2Net module.###An SE block adaptively re-calibrates channel-wise feature responses by explicitly modeling interdependencies between channels [19].",impact-revealing,acknowledge advancements in neural network architectures
2406,5a260c8117c44a4ba8a30ec9,5fdd40601a2ccdaa2d2ade27872bd8b3f43b2c1c,MemNet: A Persistent Memory Network for Image Restoration,53e9a508b7602d9702e2bcf5,Rectified Linear Units Improve Restricted Boltzmann Machines,"Specifically, each residual function contains two convolutional layers with the pre-activation structure [13],
F(Hr−1m ,Wm) = W 2mτ(W 1mτ(Hr−1m )), (6)
where τ denotes the activation function, including batch normalization [16] followed by ReLU [30], and W im, i = 1, 2 are the weights of the i-th convolutional layer.###where τ denotes the activation function, including batch normalization [16] followed by ReLU [30], and W i m, i = 1, 2 are the weights of the i-th convolutional layer.",other,providing context for the architecture of a model
131,5efb0d5691e011063336d27d,2b364917b0c51e91fcf2ab9c1d66a14ed4b44c03,BERTology Meets Biology: Interpreting Attention in Protein Language Models,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Transformers are the backbone of state-of-the-art pre-trained language models in NLP including BERT [19].###We study the BERT Transformer model [19], though this analysis could be applied to other Transformers as well.###, the subspecialty of “BERTology” [65], which studies the BERT [19] Transformer model specifically.",impact-revealing,highlighting the significance of the BERT Transformer model in NLP
1069,,bbdeae12a2a5e760cfacfa91b5a183396d749b24,"A Phrase-Level User Requests Mining Approach in Mobile Application Reviews: Concept, Framework, and Operation",,,"###The phrase-level frequency is inspired by a classical phrase extraction algorithm—PMI (Point-wise Mutual Information) [35] formulated in (4):###The correlations between the five features are shown in (3):
f _io = f _ix − f _grams, f _oi = f _xi − f _grams (3)
The phrase-level frequency is inspired by a classical phrase extraction algorithm—PMI (Point-wise Mutual Information) [35] formulated in (4):
PMI(w1, w2) = ln p(w1, w2)
p(w1)p(w2) (4)
where p(w1, w2) means the probability of the co-occurrence of two words (in the same phrase) whereas p(w1) and p(w2) means the probability of the two words appearing independently.",impact-revealing,providing context for a phrase extraction algorithm
1974,,cab9a47fddd9570a3b188f9c1edda362302942dc,Engaging Organizational Voice: A Phenomenological Study of Employees' Lived Experiences of Silence in Work Group Settings,,,"###…show when differences of opinion exist in a group; pressures are exerted to achieve uniformity by attempting to change the opinion of others, to change one’s own opinion, and to redefine the boundaries of the group to exclude the deviate group member (Back, 1951; Festinger, 1950; Schachter, 1951).###Second, managers’ fear of negative feedback is based on the theory that people are often afraid of negative feedback whether the information is about them personally or about an intitiative or idea that they endorse or advocate. Argyris and Schon (1978) theorize that managers fear of embarrassment and negative information may cause perceptions of inadequacy among their constituents.###Management-by-exception involves corrective criticism, negative feedback, and negative reinforcement (Northouse, 2007). There are two types of management-by-exception, passive and active. Northouse asserts that the active type of management-by-exceptions involves watching for mistakes committed by the subordinate and then taking corrective action. Passive management-by-exception involves management intervention only after standards are not met. In contrast to transactional leaders, transformational leaders stimulate and inspire followers to achieve higher performance levels and in the process develop their own leadership capacity (Bass & Riggio, 2006). Bass (1985) further emphasized the importance of the leader’s emotions and argued that transformational leaders motivate followers to do more than what is expected by elevating followers’ consciousness about the value of goals, by getting followers to think beyond their own interests to those of the team, and by addressing###According to Peters (1989), Locke stated that “Words are the promissory notes that have no value unless they are backed by ideas on deposit in peoples minds...” (p.390). The sharing of individual thoughts is often done at the collective level within organizations. Morrison and Milliken (2000) posit that silence is a collective level phenomenon and that in most organizations, employees remain silent “en masse” (p.###Second, managers’ fear of negative feedback is based on the theory that people are often afraid of negative feedback whether the information is about them personally or about an intitiative or idea that they endorse or advocate. Argyris and Schon (1978) theorize that managers fear of embarrassment and negative information may cause perceptions of inadequacy among their constituents. Lastly, Morrison and Milliken’s (2000) theory of organizational silence proposes that individual employee variables contribute to a climate of silence which results in organizational silence.###Studies show when differences of opinion exist in a group; pressures are exerted to achieve uniformity by attempting to change the opinion of others, to change one’s own opinion, and to redefine the boundaries of the group to exclude the deviate group member (Back, 1951; Festinger, 1950; Schachter, 1951).###This study uses Baer and Frese’s (2003) conceptualization of organizational climate as “the property of individuals and how they generally perceive the organization” (p.",impact-revealing,discussing theories related to organizational behavior and management
3347,5f0277e911dc830562231dab,6deae79dec438eaaa524bca3b82c6b8d93553b20,A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data,5b67b46f17c44aac1c8632a4,Should I Follow the Crowd?: A Probabilistic Analysis of the Effectiveness of Popularity in Recommender Systems.,"Recommender Systems as a feedback loop system may suffer from the bias problems such as popularity bias [1, 6], previous model bias [9, 16, 17] and position bias [3, 28].",other,highlighting bias issues in recommender systems
2450,5fae6dced4150a363cec41f7,71c97e7d4a529a21f32d98aad72fbbf32c9ee32e,Anomaly-Based Intrusion Detection From Network Flow Features Using Variational Autoencoder,53e9bb7fb7602d97047c21fa,"Imbalanced Learning Foundations, Algorithms, And Applications Preface","For a binary classiﬁcation, the results can be separated into four groups [49], [50] Receiver Operating Characteristics (ROC): The ROC curve [51], [52] is utilized as a standard criterion in the assessment of classiﬁers in the case of a class imbalance problem encountered in the dataset [53].###The ROC curves offer an illustrated approach for deciding the efﬁciency of a classiﬁer [53].",other,providing context on ROC curves in binary classification
1964,,d34369c769b402c9b139d7056f2b6b882ef1f6c9,11C-Choline PET Guided Salvage Radiation Therapy for Isolated Pelvic and Paraortic Nodal Recurrence of Prostate Cancer After Radical Prostatectomy: Rationale and Early Genitourinary or Gastrointestinal Toxicities,,,"###Importantly, these data represent a significant improvement in the AEs reported in historical series evaluating the utility of 2- or 3-dimensional conformal pelvic RT with or without PA nodal RT for GU or gynecologic malignancies (Table 4), for which grade 2 þ GI AEs may range from 2% to 40%.(12,13,15-18) We contend these differences are attributable to an improved understanding of RT dose-volume relationships that predict for GI AEs, relatively strict planning parameters, and the use of volumetrically modulated arc therapy with daily image guidance, which allowed for reductions in RT exposure to GI OARs.###Historical studies evaluating the role of 2-dimensional or 3-dimensional conformal pelvic PA nodal RT in genitourinary (GU) or gynecologic malignancies have reported rates of grade 2 GI AEs as high as 40% and grade 3 or higher rates of 1% to 5% with pelvic RT or 1% to 10% with pelvic and PA RT (Table 1).(12-18) However, with improved understanding of GI tissue tolerances and advancements in treatment techniques such as image guided intensity modulated RT, it is reasonable to expect that contemporary pelvic or PA nodal RT would be associated with lower rates of AEs compared with historical series.###Multiple cooperative group trials have evaluated the role of elective pelvic and/or PA LN irradiation in the management of patients with prostate cancer and a high risk of occult lymph node metastases and have not found a significant oncologic advantage with such treatment.(12,13,15,16) However, these trials do not provide guidance on how best to manage patients with clinical or pathologic LN recurrences of prostate cancer after initial treatment.",impact-revealing,highlighting improvements in treatment techniques and their impact on adverse event rates
1379,,37e8fd317d6336ca08509291e367af3e91d0d95c,Differences in cerebral activation during perception of optokinetic computer stimuli and video clips of living animals: An fMRI study,,,"###Significant signal changes for each contrast were assessed by means of tstatistics on a voxel-by-voxel basis and maximum likelihood estimation, as commonly used for fMRI (Friston et al., 1995b; Friston et al., 1999b).###…results are thought to be valid, because the sample size of 12 subjects exceeds that recommended in fMRI studies; individual variations are within the statistical model, and spatial solution is highly sensitive (compare Sections 4.3 Image acquisition and 4.4 Image analysis; Friston et al., 1999a).###Following single subjects analyses, second level random effects were calculated with the single subject contrast images (Friston et al., 1999a).",impact-revealing,describing the methodology for assessing signal changes in fMRI studies
2789,5550415645ce0a409eb3a69e,eb42cf88027de515750f230b23b1a057dc782108,a very deep convolutional networks for large-scale image recognition,53e99d57b7602d9702610216,Improving The Fisher Kernel For Large-Scale Image Classification,"In particular, an important role in t he advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) [1], which has served as a testbed for a few generations of large-s cal image classification systems, from high-dimensional shallow feature encodings [13] (the winner of ILSVRC-2011) to deep ConvNets [10] (the winner of ILSVRC-2012).",other,highlighting the significance of the ImageNet challenge in advancing deep visual recognition
1853,,c4f2481ec893ea9fe9897488eefa0ce287a9d4b5,Responding to Uncertainty on Approach in Hazardous Situations,,,###Conversation Analysis (CA) [20-24] builds upon the rich work in Speech Acts [25] to provide a theoretical framework for analysing verbal interactions.,impact-revealing,providing context for Conversation Analysis and its theoretical foundations
633,5736960c6e3b12023e51ee06,492f57ee9ceb61fb5a47ad7aebfec1121887a175,Gated Graph Sequence Neural Networks,53e9b5bcb7602d97041070ad,The graph neural network model.,"Several variants are discussed in Scarselli et al. (2009) including positional graph forms, node-speciﬁc updates, and alternative representations of neighborhoods.###More closely related to our goal in this work are methods that learn features on graphs, including Graph Neural Networks (Gori et al., 2005; Scarselli et al., 2009), spectral networks (Bruna et al., 2013) and recent work on learning graph ﬁngerprints for classiﬁcation tasks on graph representations of chemical molecules (Duvenaud et al., 2015).###A secondary contribution is highlighting that Graph Neural Networks (and further extensions we develop here) are a broadly useful class of neural network model that is applicable to many problems currently facing the ﬁeld.###We will focus in this work on directed graphs, so ( v, v (cid:48) ) represents a directed edge v → v (cid:48) , but we note that the framework can easily be adapted to undirected graphs; see Scarselli et al. (2009).###Scarselli et al. (2009) focus on outputs that are independent per node, which are implemented by mapping the ﬁnal node representations h To handle graph-level classiﬁcations, they suggest to create a dummy “super node” that is connected to all other nodes by a special type of edge.###Our main contribution is an extension of Graph Neural Networks that outputs sequences.###We now describe Gated Graph Neural Networks (GG-NNs), our adaptation of GNNs that is suitable for non-sequential outputs.###Here, (1) is mostly achieved by previous work on Graph Neural Networks (Scarselli et al., 2009); we make several minor adaptations of this framework, including changing it to use modern practices around Recurrent Neural Networks.###Concretely, Scarselli et al. (2009) suggest decomposing f ∗ ( · ) to be a sum of per-edge terms: where f ( · ) is either a linear function of h v (cid:48) or a neural network.###More closely related to our goal in this work are methods that learn features on graphs, including Graph Neural Networks (Gori et al., 2005; Scarselli et al., 2009), spectral networks (Bruna et al., 2013) and recent work on learning graph ﬁngerprints for classiﬁcation tasks on graph representations…###GNNs have been applied in several domains (Gori et al., 2005; Di Massa et al., 2006; Scarselli et al., 2009; Uwents et al., 2011), but they do not appear to be in widespread use in the ICLR community.###In this section, we review Graph Neural Networks (GNNs) (Gori et al., 2005; Scarselli et al., 2009) and introduce notation and concepts that will be used throughout.",impact-revealing,highlighting the relevance and application of Graph Neural Networks in various problems
1549,,304b27cbdcf98f9e267674792d6d96e4fb548e29,Knowledge Transfer From Business Schools to Firms Through Academics: An AMO Perspective in an Emerging Economy,,,"###The proposed framework provides valuable insights grounded in the AMO theory (Blumberg and Pringle 1982; Waldman and Spangler 1989).###Opportunity “consists of the particular configuration of the forces surrounding a person and his or her task that enables or constrains that person's task performance and that is beyond the person's direct control” (Blumberg and Pringle 1982, 565).###This capacity encompasses physiological and cognitive capabilities, such as knowledge and skills, that enable an individual to execute a task effectively (Blumberg and Pringle 1982).###The ability and motivation dimensions represent personal factors, while opportunity describes environmental factors (Blumberg and Pringle 1982).###This framework originates from social and industrial psychology (Blumberg and Pringle 1982; Waldman and Spangler 1989).###It , thus, contributes to the existing academic engagement literature (Perkmann et al. 2021) by following a proposed framework based on the ability– motivation– opportunity (AMO) theory (Blumberg and Pringle 1982).",impact-revealing,highlighting the contribution of the AMO theory to academic engagement literature
1914,,1f6075d588e1e591603493d95145d42c13b837ea,An Examination of Cognitive Presence and Learning Outcome in an Asynchronous Discussion Forum,,,"###Teaching presence is also the ability of the instructor to develop close proximity to the learners in an online course while overcoming the lack of physical presence associated with the online learning medium (Garrison et al., 2000).###The coders were first trained to code online discussion messages using a rubric based on the model developed by Garrison et al. (2000) (see Appendix B).###To enable comparison between the set of constructed guidelines used in this study
and those of Garrison et al. (2000), Cohen‟s κ values were calculated among pairs of raters.###From Garrison et al (2000) community of inquiry model, we can examine the research to draw correlations between these three components to determine their impacts on learners‟ performance or student learning outcomes.###The first element in the community of inquiry model, teaching presence is the ability of the instructor to develop a close relationship with the learners in an online course while overcoming the lack of physical presence associated with the online learning medium (Garrison et al., 2000).###Garrison et al. (2000) discuss a practical inquiry model comprised of four phases in depth.###The third element in the community of inquiry model, cognitive presence is defined as the extent to which the participants in any particular configuration of a community of inquiry are able to construct meaning through sustained negotiation (Garrison et al., 2000).###Cognitive presence is defined as the extent to which the participants in any particular configuration of a community of inquiry are able to construct meaning through sustained negotiation (Garrison et al., 2000).###Teaching presence is defined as the design, facilitation, and direction of cognitive and social processes for the purpose of realizing personally meaningful and educational worthwhile learning outcomes (Garrison et al., 2000).###This is the phase where students come up with the
resolution of the dilemma or problem (Garrison et al., 2000).###This is the phase where students come up with the resolution of the dilemma or problem (Garrison et al., 2000).###The first element in the community of inquiry model, teaching presence is the
ability of the instructor to develop a close relationship with the learners in an online course while overcoming the lack of physical presence associated with the online learning medium (Garrison et al., 2000).###The set of constructed guidelines for the study builds on a series of content
analyses described by Garrison et al. (2000, 2001), who analyzed online discussions based on a community of inquiry model that splits community-based learning into three overlapping areas: social presence, cognitive…###I want to also thank the Chair of my dissertation committee, Dr. Steve Harmon for his help, guidance and leadership during my pursuit of this doctoral degree.
iii
TABLE OF CONTENTS
Page
List of Tables .......................................................................................................................v List of Figures .................................................................................................................... vi Abbreviations .................................................................................................................... vii Chapter
1 INTRODUCTION ...................................................................................................1
Purpose .....................................................................................................................5 Research Questions ..................................................................................................5 Statement of the Problem .........................................................................................6
2 REVIEW OF THE LITERATURE .........................................................................8
Contextual Framework.............................................................................................8 Social Learning Theories .........................................................................................9 Virtual Learning Communities/Asynchronous Discussion Forums ......................12 Communities of Inquiry .........................................................................................15 Learning Effectiveness and Student Performance .................................................17 Levels of Cognitive Presence.................................................................................25
3 RESEARCH METHODOLOGY...........................................................................29
Course Details ........................................................................................................30 Characteristics of the Participants ..........................................................................33 Instructional Components ......................................................................................37 Data Preparation.....................................................................................................38 Steps for Coding ....................................................................................................41 Data Analyses ........................................................................................................47 Steps for Standardizing the Data............................................................................49 Summary ................................................................................................................50
4 RESULTS ..............................................................................................................52
Quantitative Discourse (Using standardized data) .................................................52 Qualitative Discourse (Examples of Students‟ Posts) ...........................................70 Cohen‟s K Results..................................................................................................73
5 DISCUSSION ........................................................................................................77
Research Question 1 ..............................................................................................77 Research Question 2 ..............................................................................................82 Research Question 3 ..............................................................................................83 Research Question 4 ..............................................................................................85 Additional Findings ..............................................................................................86 Summarizing the Findings .....................................................................................89 Limitations of the Study.........................................................................................92 Recommendations for Future Research .................................................................94 Conclusions ............................................................................................................96
iv
References ........................................................................................................................100 Appendixes ......................................................................................................................105
v
LIST OF TABLES
Table Page
1 Previous Literature on Cognitive Presence in Various Phases ..............................26 2 Landis and Koch Reliability Figures .....................................................................47 3 Minimums and Maximum of the Population (standardized data) .........................53 4 Means and Standard Deviations of the Population I (standardized data) ..............53 5 Means and Standard Deviations of the Population II (standardized data) .............55 6 Summary of students that reached the “Resolution” stage ....................................59 7 Ranges of Pearson Correlations .............................................................................59 8 Pearson Correlations and p values of the Population (standardized data) .............60 9 Pearson Correlations and p values of the Population (original data) .....................61
10 Correlations between Maximum Levels of Cognitive Presence and Student
Performance (standardized data) ............................................................................64
11 Correlations between Message Lengths and Student Performance
(standardized data) .................................................................................................64
12 Correlations between Maximum Levels of Cognitive Presence and
Message Lengths (standardized data) ....................................................................65
13 Correlations between Number of Messages and Message Lengths
(standardized data) .................................................................................................65
14 Correlations between Student Performance and Number of Messages
(standardized data) .................................................................................................66
15 Correlations between Maximum Levels of Cognitive Presence and
Number of Messages (standardized data) ..............................................................66
16 Inter-Rater Reliability (Cohen‟s κ) ........................................................................74 17 Landis and Koch Reliability Figures ....................................................................75 18 Levels of Cognitive Presence in Online Environments Cited
in Previous Studies .................................................................................................79
vi
LIST OF FIGURES
Figure Page
1 Sample IT 2010 Course Timeline for Summer2007 semester ...............................31 2 Computer Ethics Modules offered during 1-week or 2-week ................................32 3 Computer Ethics Modules with student distribution .............................................33 4 Standardizing the Number of Messages and Message Lengths .............................49 5 Levels of Cognitive Presence at the “Resolution” or “Integration” level ..............56 6 Occurrence at the “Resolution” level by the same student ....................................57 7 Distribution of students that reached the “Resolution” level/section ....................57 8 Correlations for standardized and original Populations .........................................68 9 Restriction of Range on Maximum Levels of Cognitive Presence .......................83
vii
ABBREVIATIONS
5076_020 Summer 2007, section 020 5076_035 Summer 2007, section 035 5079_030 Fall 2007, section 030 5079_035 Fall 2007, section 035 5081_025 Spring 2008, section 025 5089_005 Fall 2008, section 005 5089_015 Fall 2008, section 015 5089_020 Fall 2008, section 020 5089_025 Fall 2008, section 025 5089_030 Fall 2008, section 030 ALN Asynchronous Learning Network CAI Computer Assisted Instruction DSL Digital Subscriber Line SCCI Sense of Classroom Community Index SLN SUNY Learning Network
1
CHAPTER 1
INTRODUCTION
Education occurs at any time and at any place in our daily lives, whether we are at
home with our families, or while we‟re at a market in a public setting.###Later on, Garrison et al. (2000) modified Henri‟s model by breaking it into three components: cognitive presence, social presence, and teaching presence).###The third element in the community of inquiry model, cognitive presence is
defined as the extent to which the participants in any particular configuration of a community of inquiry are able to construct meaning through sustained negotiation (Garrison et al., 2000).###Existing research on online learning communities and cognitive presence prior to
this study focused on determining the mean levels of cognitive presence (Garrison et al., 2000); this study focused on the maximum levels of cognitive presence.###Communities of Inquiry
Garrison et al. (2000) developed the community of inquiry model, which
constitutes three elements essential to an educational transactioncognitive presence,
social presence, and teaching presence.###The third element in the community of inquiry model, cognitive presence is defined as the extent to which the participants in any particular configuration of a community of inquiry are able to construct meaning through sustained negotiation (Garrison et al., 2000). This third element also comes with it some depth and presents an area of interest for further research in this study. For this reason, I will spend a more time describing this third element in the community of inquiry model. Garrison et al. (2000) discuss a practical inquiry model comprised of four phases in depth.###In terms of teaching presence, this phase is the most important for the instructor to assert his or her presence because students might have ideas that need the teacher‟s input (Garrison et al., 2000).###Additionally, in the online
16
learning environment, any group member can add a triggering event to the learning discourse (Garrison et al., 2000).###This exploration can take place in the community of inquiry and can be characterized by brainstorming, questioning, and exchanging information (Garrison et al., 2000).###Cognitive presence concerns the process of both reflection and discourse in the initiation, construction, and confirmation of meaningful learning outcomes (Garrison et al, 2000).###Henri (1992) and Garrison et al (2000) provided the content analysis framework that is particularly useful when attempting to measure the levels of cognitive presence, which transitions to the next section of discussion in this chapter.###In summary, the practical inquiry model of Garrison et al. (2000) reflects the critical thinking process and the means to create cognitive presence.###Social presence is the ability of learners to project their personal characteristics into the community of inquiry, thereby presenting themselves as real people (Garrison et al., 2000).",impact-revealing,discussing the community of inquiry model and its components
1505,,5fe8556a3dfc7f0e90d18ec29fcb7d1ae6fe6adb,Role of Advanced Glycation End Products in the Progression of Diabetes Mellitus,,,"###At present, OS has been widely recognized as a key component in the development of diabetes and diabetic complications [50].",impact-revealing,highlighting the significance of OS in diabetes research
3535,58d82fd2d649053542fd75bc,e0b207e96351671453aa8bf05b7225c8a340a0b2,towards end-to-end speech recognition with deep convolutional neural networks,56d8d43cdabfae2eeebde226,Supervised Sequence Labelling,"(8)
A dynamic programming algorithm similar to the forward algorithm for HMMs [29] is used to compute the sum in Equation 8 in an efficient way.###After these successes on phoneme recognition, similar systems have been proposed in which multiple layers of RNNs were combined with CTC to perform large vocabulary continuous speech recognition [7, 16].###Inspired by the strengths of both CNNs and CTC, we propose an end-to-end speech framework in which we combine CNNs with CTC without intermediate recurrent layers.###The convolutional layers are followed by multiple fully connected layers and, finally, CTC is added on the top of the model.###We show promising results on the TIMIT dataset and conclude that the model has the capacity to learn the temporal relations that are required for it to be integrated with CTC.###The only previous attempt to combine CNNs with CTC that we know about [21], led to results that were far from the state-of-the-art.###Our results showed that convolutional architectures with CTC cost can achieve results comparable to the state-of-the-art by adopting the following methodology: (1) Using a significantly deeper architecture that results in a more non-linear function and also wider receptive fields along both frequency and temporal axes; (2) Using maxout non-linearities in order to make the optimization easier; and (3) Careful model regularization that yields better generalization in test time, especially for small
datasets such as TIMIT, where over-fitting happens easily.###While RNNs can learn these kind of dependencies and have been combined with CTC for this very reason, it was not known whether CNNs were able to learn the required temporal relationships.###For example, a model with multiple layers of bi-directional LSTMs and CTC on top which is pre-trained with the transducer networks [12, 13] obtained the state-of-the-art on the TIMIT dataset.###To generate predictions from a trained model using CTC, we use the best path decoding algorithm.###Two popular neural network sequence models are Connectionist Temporal Classification (CTC) [10] and recurrent models for sequence generation [8, 11].###At test time, simple best path decoding (at the CTC frame level) is used to get the predicted sequences.###In the context of Automatic Speech Recognition, CNNs are usually combined with HMMs/GMMs [5, 6], like regular Deep Neural Networks (DNNs), which results in a hybrid system [2, 3, 4].###We conjecture that the convolutional CTC model might be easier to train on phoneme-level sequences rather than the character-level.###Connectionist Temporal Classification (CTC) [29] specifies a distribution over latent sequences by applying a softmax function to the output of the network for every time step, which provides a probability for emitting each label from the alphabet of output symbols at that time step Pr(Ot|X).###A dynamic programming algorithm similar to the forward algorithm for HMMs [29] is used to compute the sum in Equation 8 in an efficient way.",other,highlighting the proposed method's efficiency and potential improvements in speech recognition
650,5aed14d117c44a4438158a8d,36db98d8604df17e7ab68952c6f7a6cb70cff2e7,a deep learning approach for multimodal deception detection,53e99fe4b7602d97028bf016,A Convolutional Neural Network For Modelling Sentences,"We use Convolutional Neural Networks (CNN) [18, 19] to extract features from the transcript of a video, v .",impact-revealing,method use for feature extraction from video transcripts
1531,,c580355d9e3a8b9036ab9c1b5ee258df556eb7d1,"Two New Books Rethinking Antiracist Intragroup and Intergroup DialogueIntellectual Empathy: Critical Thinking for Social Justice, by Linker, M.Intergroup Dialogue: Engaging Difference, Social Identities and Social Justice, by Zúñiga, X., Lopez, G., Ford, K. A.",,,"###These recent books build on work by Dewey (1916/2012), Buber (1970/1996), Freire (1970/2000), Collins (1990/2014), hooks (1994), and others, while often integrating more contemporary theories of unconscious racial aversion and aggression through disengagement, microaggressions (Sue et al., 2007), and other acts of racialized power.###…Dewey (1916/2012), Buber (1970/1996), Freire (1970/2000), Collins (1990/2014), hooks (1994), and others, while often integrating more contemporary theories of unconscious racial aversion and aggression through disengagement, microaggressions (Sue et al., 2007), and other acts of racialized power.",impact-revealing,acknowledging foundational works and contemporary theories in racial studies
1047,,38cc302b9204c222316a2ed34813f16d42e5c7b4,Missing Data Imputation Based on Low-Rank Recovery and Semi-Supervised Regression for Software Effort Estimation,,,"###In this paper, we use three measures, namely Median Magnitude of Relative Error (MdMRE), PRED(25) [1, 11, 46, 56] and effect size[62], which are commonly used for evaluate the effort estimation accuracy of estimators.###Considering the noisy, redundant, or unreliable information in dataset, like in [17], we employ the z-score normalization [56] to preprocess data.",impact-revealing,describing the evaluation measures used for effort estimation accuracy
395,5b1642388fbcbf6e5a9b54be,b3dae9529f3caeeec9cc6872e94aa690418acb22,Reinforcement Learning for Relation Classification from Noisy Data,573696006e3b12023e5142e1,Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning.,"To address these issues, we propose a new framework which ﬁrst selects correct sentences in the framework of reinforcement learning (Sutton and Barto 1998; Narasimhan, Yala, and Barzilay 2016) and then predicts relations from each sentence in the cleansed data.",impact-revealing,proposing a new framework to address existing issues
25,5a9cb60d17c44a376ffb3c4c,fc3384d631f5e2b2a9d66623d4d3e1d28b96dee7,Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search,599c7968601a182cd263a485,End-to-End Neural Ad-hoc Ranking with Kernel Pooling,"K-NRM uni_x0080_ed the progress of IR customized embeddings and interaction based model [29].###Interaction based models thrive with encoding word-word translations using word embeddings, and utilizing new pooling methods to be_x008a_er summarize the word translations into ranking signals [11, 13, 29].###Such counting-based pooling methods have shown be_x008a_er performance than score-based ones like mean-pooling or max-pooling [13, 29].###_x008c_is part extends K-NRM [29] to n-grams.###A recent success of neural methods in information retrieval (neural IR) is the development of interaction based models [13, 21, 29].###Learned end-to-end from user feedbacks [23, 29], the word embeddings can encode so_x0089_ matches tailored for relevance ranking, which has signi_x0080_cant advantages over traditional feature-based methods [29, 30].###When trained with user feedback in a search log, K-NRM outperforms both neural IR methods and feature-based learning-to-rank by a large margin [29].###Conv-KNRM adds the ability of so_x0089_ matching n-grams to the recent state-of-the-art K-NRM model [29] with convolutional neural networks (CNNs).###_x008c_e current state-of-the-art kernel pooling and learningto-rank techniques are then used to combine the n-gram so_x0089_matches to the _x0080_nal ranking score [29].",impact-revealing,describing advancements in information retrieval models
1500,,dac1541f9052ec74637cf8556d9fd7da05f8cf7c,A Pathway Towards Responsible AI Generated Content,,,"###Google, as a rival to OpenAI, presented two text-to-image models that can generate photorealistic images: the diffusion-based model Imagen [ Saharia et al. , 2022a ] , and the Pathways Autoregressive Text-to-Image model (Parti) [ Yu et al. , 2022 ] .###Due to the fact that AIGC models are trained on large-scale web-scraped data [ Rombach et al. , 2022a; Ramesh et al. , 2022; Saharia et al. , 2022a ] , the issue of overﬁtting and privacy leakage becomes especially relevant.###Diffusion models have been used not only for text-to-image tasks, but also for image-to-image [ Saharia et al. , 2022b; Whang et al. , 2022 ] and text-to-video models, such as Run-way [ Runway, 2022 ] , Make-A-Video [ Singer et al. , 2022 ] , Imagen Video [ Ho et al. , 2022 ] , and Phenaki […###Although some AIGC models like Imagen [Saharia et al. , 2022a] try to ﬁlter out undesirable data, such as pornographic imagery and toxic language, the ﬁltered data can still contain sexually explicit or violent content.###In addition to text-to-image tasks, diffusion models had been widely used for image-to-image [Saharia et al., 2022b; Whang et al., 2022] and text-to-video models, such as Runway [Runway, 2022], Make-A-Video [Singer et al.###Google’s Ima-gen [ Saharia et al. , 2022a ] also encodes several social biases and stereotypes, such as generating images of people with lighter skin tones and aligning with Western gender stereotypes.",impact-revealing,Highlighting the relevance and implications of Google's text-to-image models in the context of AIGC
1580,,905d01d2f835e09f3e545d8cd55d40508840a80d,An examination of special focus facility nursing homes.,,,"###Castle, Sonon, and Antonova (2009) found that the SFF initiative may have influenced aggregate market level nursing facility quality.###Staffing levels are frequently used as quality indicators (Castle, 2008).",impact-revealing,highlighting the influence of the SFF initiative on nursing facility quality
3687,5f0277e911dc830562231dac,72a5feb4835b3e6cab3754437bb033371c5df154,DVGAN: A Minimax Game for Search Result Diversification Combining Explicit and Implicit Features,53e99ebdb7602d9702786088,Expected reciprocal rank for graded relevance.,"Among all the evaluation metrics [2–4, 20, 21], we use ERR-IA [2], 𝛼 -NDCG [4], and NRBP [3] as our diversity evaluation metrics.###(4) using Plackett-Luce model and the 𝐸 is the diversification metrics such as 𝛼 -NDCG and ERR-IA [2].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
149,5d9edc8347c8f76646042a37,7e71eedb078181873a56f2adcfef9dddaeb95602,simplifying graph convolutional networks,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,"…the coefficients θ0 and θ1 such that θ = θ0 = −θ1, we attain the basic GCN convolution operation
g ∗ x = θ(I + D−1/2AD−1/2)x. (12)
In their final design, Kipf & Welling (2017) replace the matrix I + D−1/2AD−1/2 by a normalized version D̃−1/2ÃD̃−1/2 where Ã = A + I and consequently D̃ = D…###We follow Kipf & Welling (2017) to introduce GCNs (and subsequently SGC) in the context of node classification.###Kipf & Welling (2017) empirically observe that the “renormalization trick”, i.e. adding self-loops to the graph, improves accuracy, and we demonstrate that this method effectively shrinks the graph spectral domain, resulting in a low-pass-type filter when applied to
ar X
iv :1
90 2.###For citation networks, we compare against GCN (Kipf & Welling, 2017) GAT (Velickovic et al., 2018) FastGCN (Chen et al., 2018) LNet, AdaLNet (Liao et al., 2019) and DGI (Velikovi et al., 2019) using the publicly released implementations.###…GCNs and subsequent variants have achieved state-of-the-art results in various application areas, including but not limited to citation networks (Kipf & Welling, 2017), social networks (Chen et al., 2018), applied chemistry (Liao et al., 2019), natural language processing (Yao et al., 2019; Han…###To tackle potential numerical issues associated with the first-order Chebyshev filter, Kipf & Welling (2017) propose the renormalization trick.###Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017) employ an affine approximation (k = 1) of Equation 11 with λmax = 2 to define graph convolutions.###Graph Convolutional Networks (GCNs) (Kipf & Welling, 2017) are an efficient variant of Convolutional Neural Networks (CNNs) on graphs.###In addition, we show that adding self-loops to the original graph, i.e. the renormalization trick (Kipf & Welling, 2017), effectively shrinks the underlying graph spectrum.###GCNs (Kipf & Welling, 2017) further simplify graph convolutions by stacking layers of first-order Chebyshev polynomial filters with a redefined propagation matrix S. Chen et al. (2018) propose an efficient variant of GCN based on importance sampling, and Hamilton et al. (2017) propose a framework…",impact-revealing,acknowledging the foundational work on Graph Convolutional Networks (GCNs) and their applications
3411,5c8d1d8b4895d9cbc63cdd4f,dec56bd20137a1076751c9d7190b685a01a08885,5G Ubiquitous Sensing: Passive Environmental Perception in Cellular Systems,56d81e12dabfae2eeea833ff,Complex Activity Recognition Using Time Series Pattern Dictionary Learned from Ubiquitous Sensors,"Walking speed is also used in a number of relevant applications for the IoT, such as fitness tracking, attention monitoring, and health [14], [15].",other,acknowledging applications of walking speed in IoT
2080,,8c542f159d1afe785b3aa9b9aa12845b912ce4dd,Speech recognition using autosegmental representation of phonological units with interface to the trended HMM,,,"###The algorithm is motivated by and is an extension of the segmental K-means algorithm developed in the past for training conventional . HMMs  Juang and Rabiner, 1990  . Like the segmental K-means algorithm, the algorithm developed here also involves two iterative steps the segmentation . step and the regression step , which will not be described further here.",impact-revealing,describing the algorithm's motivation and iterative steps
1336,,eaa3a77b199eb89a664750f13f293a71566665ed,A Subjective Quality Study for Video Frame Interpolation,,,"###SIM [25], LPIPS [12], VIF [15], VMAF [16], FRQM [13] and ST-GREED [14].###In order to better predict the visual quality of interpolated videos, perceptual quality metrics including LPIPS [12], FRQM [13], ST-GREED [14], VIF [15] and VMAF [16], which were designed for various application scenarios, can also be employed.###While PSNR, SSIM, MSSSIM, and LPIPS are commonly used for assessing video frame interpolation methods, VIF and VMAF are included due to their superior correlation with perceptual quality in other application scenarios (e.g. video compression [21]).###These DMOS values were then used to evaluate their correlation with eight quality metrics: PSNR, SSIM [11], MSS-
SIM [25], LPIPS [12], VIF [15], VMAF [16], FRQM [13] and ST-GREED [14].",impact-revealing,reporting various perceptual quality metrics for video evaluation
1180,,3f3bc11d17d516a29ac866485bfc87f9c764e1b5,Quantum Inspired Training for Boltzmann Machines,,,"###the rejection step and amplitude estimation quadratically reduces the number of training vectors (at the price of quadratically worsening the scaling with E) [1].###We present the general theory of IRS here and apply it to training DBMs in Section 3.###Although IRS has asymptotic advantages over existing methods for training DBMs in the presence of sufficiently strong regularization, our work only shows that it can provide accurate and efficient approximations to the gradients of the training objective under such circumstances.###This language not only proves to be useful for representing rejection sampling based algorithms, but also is useful because it provides a clear method for dequantizing the method of [1].###Recent work in quantum computing has revealed a class of training algorithms that use a quantum form of rejection sampling to overcome these problems [1].###Similarly, the GEQAE algorithm in [1] can also be efficiently simulated but IRS is not a natural analog of it since GEQAE uses a manifestly quantum method (known as amplitude estimation) for inferring the expectation values needed to train the DBM.###This consequently shows that the GEQS algorithm for training deep networks given in [1] can be efficiently simulated and has a direct classical analog in the IRS algorithm.###Our algorithm is inspired by a recent quantum algorithm for training DBMs [1].###Unfortunately, quantum computers are currently limited to tens of quantum bits, which means that [1] can only be used to train impractically small Boltzmann machines using present-day hardware.###Second, it does not explicitly depend on the interaction graph used: it can be used to train full Boltzmann machines rather than just DBMs.###Such differences are expected to be even more striking for deep restricted Boltzmann machines because IRS does not greedily optimize the weights [1].###The main insight behind our result, and stemming from [1], is that variational approximations to the Gibbs state can be used to make training with rejection sampling much more efficient than it would initially appear.###Due to the emergence of CD, Boltzmann machines, specifically restricted Boltzmann machines (RBMs) and their deep layered counterparts (DBMs), have become standard tools for solving problems in vision and speech recognition [3, 4].###The result of [1] chooses Q to be a product distribution that minimizes KL(Q||P/Z) known as the mean–field distribution; however, this choice only provides a polynomial penalty for using an exponentially poor approximation to the tail probability.",impact-revealing,discussing the implications and limitations of the IRS algorithm in the context of quantum computing and training deep networks
2765,5f0d85c69fced0a24be4f052,0dd3e9f581c617eb826bc0fabac5ae1394f9cef1,Data Compression Accelerator on IBM POWER9 and z15 Processors : Industrial Product,53e9ac2eb7602d97035f0e0f,To Zip or not to Zip: effective resource usage for real-time compression.,"Compression is standard on DS8000 and Storewize SAN storage systems, and TS7700 tape storage systems [16]–[18], [36], [37].",other,reporting standard practices in storage systems
253,5eccb534e06a4c1b26a83a46,e582444763f8b1e3e2c725aab1f37c5d64b69123,Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations,59ae3bf12bbe271c4c71be4d,Fast Feature Fool: A data independent approach to universal adversarial perturbations.,"Our proposed approach is more practical because the training data is generally inaccessible to the attacker [33].###To overcome this limitation, Mopuri et al . propose to generate universal perturbation without training data [33].###However, in practice the attacker often has no access to the training data [33].###The existing attacks are commonly categorized under image-dependent attacks [42, 14, 23, 31, 5] and universal ( i.e . image-agnostic) attacks [28, 19, 33, 27, 36, 46, 35] which devise one single perturbation to attack most images.",impact-revealing,highlighting the categorization of existing attacks in the context of accessibility to training data
1985,,c75521cd6604ce73017e377cd19d5a5433ea2831,HIV Care Utilization: A Theory-Based Approach to Retention in Care,,,"###However, the available observational studies (as discussed in Chapter 1) and intervention efforts (as discussed in Chapter 5) to understand and promote retention in HIV care are limited in their abilities to speak to retention as a health behavior enacted at the individual level (Cheever, 2007; Horstmann et al., 2010; L. R. Smith et al., 2012).###pertinent to HIV care; to optimize the health and well-being of people living with HIV (PLWH) and HIV-affected communities (Cheever, 2007; S. Cohen et al., 2011; Higa et al., 2012; Horstmann et al., 2010; Mayer, 2011; Mugavero, 2008; Thompson et al., 2012).###A cross-disciplinary commitment to this integrated approach is needed to effectively retain PLWH across the continuum of HIV care and prevention; and invest in the long-term health and well-being of individuals and communities affected by HIV/AIDS in the US and abroad (Cheever, 2007; Mugavero et al., 2011; Rosen & Fox, 2011).###Over the past six years, the field of HIV treatment adherence has increasingly emphasized the importance of retention in HIV care, as a unique health behavior; in need of targeted intervention to optimize the health and wellbeing of people living with HIV (PLWH) and HIV-affected communities (Cheever, 2007; Higa et al., 2012; Horstmann et al., 2010; Mayer, 2011; Mugavero, 2008; Thompson et al., 2012).###, 2010), bringing more focused attention to the various challenges of supporting full engagement in HIV care (Cheever, 2007; M. S. Cohen & Gay, 2010; Morin et al., 2011; Mugavero, 2008; B. G. Wagner & Blower, 2009).###, facilitating behavioral determinants of maintaining ≥ 90% adherence), with relatively limited focus on patients’ overall engagement in HIV care (Cabral et al., 2007; Cheever, 2007).###An individual’s actual engagement in HIV care may vary over time ranging from not at all engaged to fully engaged in HIV care (Cheever, 2007).###, retention in HIV care) has been formally evaluated (Cheever, 2007; Horstmann et al., 2010; L. R. Smith et al., 2012).###, retention in HIV care) has been evaluated (Cheever, 2007; Horstmann, Brown, Islam, Buck, & Agins, 2010; L. R. Smith, Fisher, Cunningham, & Amico, 2012).###Yet, since the introduction of ART as a longterm treatment option in the mid-1990s, the majority of HIV treatment interventions have focused predominantly on ART adherence (Cabral et al., 2007; Cheever, 2007).",impact-revealing,highlighting the need for a comprehensive approach to retention in HIV care
2850,5ee8986891e011e66831c556,73366d75289c5e37481639fb54fdee28a664e2b3,GNNGUARD: Defending Graph Neural Networks against Adversarial Attacks,53e9ac18b7602d97035d9131,Collective Classification In Network Data,We use two citation networks with undirected edges and binary features: Cora [48] and Citeseer [49].,other,reporting data sources used in the study
845,53e9b350b7602d9703e268f6,333da42f4369dff8ca905ad21ea6ee2f5dc99d55,"Utility-Based Cache Partitioning: A Low-Overhead, High-Performance, Runtime Mechanism to Partition Shared Caches",53e9b6e8b7602d9704277d3c,A Case for MLP-Aware Cache Replacement,"The sampled sets for UMON-DSS are chosen using the simple static policy [12], which means set 0 and every 33rd set is selected.###The key idea behind DSS is that the behavior of the cache can be approximated by sampling only a few sets.###We can use DSS to approximate the hit counter information of UMON-global by sampling few sets in the cache.###Unless stated otherwise, we use 32 sets for UMON-DSS.###For most of the workloads studied, the value of variance ( (cid:27) 2 ) is less than 3, indicating that even with the pessimistic bounds, as few as 32 sets are suf(cid:2)cient for UMON-DSS to approximate UMON-global.###Let u s be the number of ways allocated to application A by UMON-DSS.###This happens because a set of accesses with high memory-level parallelism (MLP) now fits in the cache which reduces the average MLP and increases the average mlp-based cost[12] of each miss.###To reduce the overhead of UMON, we use Dynamic Set Sampling (DSS) [12].###4DSS was used in [12] to choose between two replacement policies.###We compare UMON-DSS to UMON-global in Section 5.4.###Fortunately, on-chip caches are set-associative which makes them amenable to dynamic set sampling (DSS).###Therefore the bounds derived in [12] are not applicable to our mechanism.###Then if a ( i ) does not vary across sets then even with a single set UMON-DSS can approximate UMON-global.###Our mechanism uses DSS to propose a cost-effective partitioning framework which requires less than 1% storage overhead.###Thus, Let n be the number of randomly selected sets sampled by UMON-DSS.###For the remainder of the paper UMON by default means UMON-DSS.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3353,5e15adcb3a55ac47ab5b0b8c,aa63ac11aa9dcaa9edd4c88db18bec87e0834328,Graph Transformer Networks,5736986c6e3b12023e73037b,Spatial transformer networks,GTNs can be viewed as a graph analogue of Spatial Transformer Networks [16] which explicitly learn spatial transformations of input images or features.,other,providing context for graph transformation networks
2776,573696076e3b12023e51a63f,819167ace2f0caae7745d2f25a803979be5fbfae,The Limitations Of Deep Learning In Adversarial Settings,53e9a3cdb7602d9702ce0a8c,Evading network anomaly detection systems: formal reasoning and practical techniques.,"We believe this makes adversarial crafting much easier for input domains like malware executables, which are not as easy to perturb as images [10], [15].",other,highlighting the ease of adversarial crafting in certain input domains
770,5736982b6e3b12023e6fd21d,0344f9ac92006155f6fa464cab19925ac481dea4,Microarchitectural implications of event-driven server-side web applications,53e99ae2b7602d97023691f2,Temporal instruction fetch streaming,We propose to use the Temporal Instruction Fetch Streaming (TIFS) prefetcher [51] to prefetch recurring missing instructions.###We classify instruction cache misses into three categories as originally defined in [51].,impact-revealing,describing the proposed method for instruction prefetching
200,5eccb534e06a4c1b26a83514,a9682a89b2fef793507c365a577f1521745db96c,Boosting the Transferability of Adversarial Samples via Attention,5bdc315017c44a1f58a05dc3,Transferable Adversarial Perturbations,"We abandon the other term in TAP for simplicity because we do not have the issue of vanishing gradients.###One is query-based [24, 2, 10], and the other one is transfer-based [41, 39, 8, 21, 23].###More related to our work is the regularization-based approach: transferable adversarial perturbation (TAP) introduced by [41].###Contrarily, by applying only one regularization term to maximize attention-weighed internal feature distances, our method outperforms TAP in almost all cases.###Unfortunately, the malicious images synthesized by such a scheme are prone to overfit to the exclusive blind spots of the source model [39, 8, 41, 7].###Since the original C&W implementation cannot strictly meet the l∞ budget, we employ the modified l∞ version of C&W as introduced by [41], which can explicitly satisfy the l∞ norm constraint.###TAP injects two regularization terms into the vanilla training loss function of the model to guide the search of adversarial manipulations, which alleviates the issue of vanishing gradient and reduces the variations of resultant adversarial samples.###We follow the protocol of the baseline method [41] to curate experimental datasets and target models for fair comparisons.###Therefore, we also include TAP in the competing benchmarks.###We note that TAP employs two regularization terms, one for maximizing internal feature distances and the other for smoothing resultant perturbations.###One is the ensemble-based translationinvariant attack (TI) developed by [8], and the other one is the regularization-based transferable adversarial perturbation (TAP) proposed by [41].###Moreover, we defeat all the other benchmark methods with a significant margin, except for two cases, where we only lag a little behind TAP.###Following [41], we fix the perturbation budget ǫ to 16 for all methods.###In black-box settings, our strategy promotes the average attack success rate of TAP and TI by 6.8% and 4.6%, respectively.###For fair comparisons, we adopt default parameters as recommended in benchmark approaches and Foolbox [41, 28].###For fair comparisons with the baseline approach [41], we stick to employing undefended models as local source models.###The integration of TAP and ATA only adds the following term into the attack object function J (Eq.###Second, our ATA remarkably outperforms all the other benchmarks except for two cases, where we only slightly lag behind TAP.###Similar to our strategy, TAP [41] boosts adversarial transferability through two regularization terms and is the state-of-the-art approach under this category.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3384,5dbebb7447c8f766462c2328,b0d941cfa0a3c43703e5222221addb1ad0f9e68d,Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks,573696056e3b12023e51964e,Order Matters: Sequence to sequence for sets,"In contrast, Set2Set [34] is a graph level feature extractor which learns a query vector indicating the order of reading from the memory for an undirected graph.###Set2Set [34] assigns each node in the graph a descriptor as the order feature and uses this re-defined order to process all nodes.",other,providing context on the Set2Set feature extractor
3205,5dbebb7447c8f766462c21c0,3d9baf7e87ec43f0ad486e2077824a346a58118e,Emotion-aware Chat Machine: Automatic Emotional Response Generation for Human-like Emotional Interaction,53e9ad18b7602d97036fa99c,Data-Driven Response Generation in Social Media.,", retrievalbased, matching-based, or statistical machine learning based approaches [13, 30, 42, 43].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
600,5ac1827b17c44a1fda915855,f96a5a9cfa1dbb01df6df749e093c0bad20240a8,Control Flow Checking at Virtual Edges.,558aca08e4b0b32fcb38dfd0,Control-flow checking by software signatures,"The basic idea of software control flow checking is to partition the program into basic blocks (branch-free parts of code) [14].###This solution is inspired by [13, 14, 15, 16, and 17] and incorporates their advantages.###…solutions proposed in the literature are the techniques called Enhanced Control Flow Checking Using Assertions (ECCA) [13] and Control Flow Checking by Software Signatures (CFCSS) [15] and On-line control flow error detection using relationship signatures among basic blocks (RSCFC) [14].###These techniques are suggested in literature that would fall into two general classes, hardware [10, 11, and 12] or software [13, 14, 15, 16, and 17] redundancy.###A variety of defense mechanisms are proposed to detect and correct control flow errors (e.g., [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, and 20]).###…four versions for each benchmark:  the original code,  a safe one, obtained by applying the CFCSS [15] technique to the original code,  a safe one, obtained by applying the RSCFC [14] technique to the original code,  a safe one, obtained by applying the CFCVE technique to the original code.",impact-revealing,describing software control flow checking methods and their inspirations
2914,5d3ed2653a55ac61d998598b,077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,roberta: a robustly optimized bert pretraining approach,56d81308dabfae2eee5eff4b,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,", 2018), Stanford Sentiment Treebank (SST) (Socher et al., 2013), Microsoft Research Paragraph Corpus (MRPC) (Dolan and Brockett, 2005), Semantic Textual Similarity Benchmark (STS) (Agirre et al.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2052,,4a0d8ea6aa465219723531e142927be8a619caae,RUNX3 modulates hypoxia-induced endothelial-to-mesenchymal transition of human cardiac microvascular endothelial cells,,,"###Due to rising prevalence, poor clinical outcome, and high health care costs, cardiovascular disease (CVD) is becoming a major public health concern and is the leading cause of death worldwide.###According to the report on Cardiovascular Disease in China (2014), approximately 290 million individuals in China suffer from CVD each year, and CVD accounts for two in every five deaths (1).###RUNX3 is a common downstream target of TGF-β and Notch signaling, and may be a novel therapeutic target for treating CVD mediated by EndMT.###This study was designed to investigate the role of RUNX3 in EndMT as well as its signaling pathways, and to gain insight into the underlying molecular mechanisms so that novel therapeutic strategies can be ultimately developed for CVD.###In addition, our study provided a novel molecular mechanism for CVD and further underscored the importance of RUNX3 during EndMT.###Clinical and experimental evidence suggests that the rarefaction of cardiac capillaries, infiltration of myofibroblasts, and the deposition of collagen and other extracellular matrix proteins contribute to the progression of cardiac fibrosis and heart failure, two common clinical outcomes of CVD (2-4).###While exisiting therapies for CVD, such as pharmacological and invasive therapies, can relieve symptoms and attenuate disease progression, there is still an urgent need for novel therapies to effectively treat or even cure CVD.
Endothelial-mesenchymal transition (EndMT), one subgroup of cellular phenotypic switching, can induce transcription factors, which can affect gene expression and promote the loss of cell-cell adhesion and the change from endothelial morphology and physiology to a mesenchymal phenotype, a change characterized by low expression of endothelial markers such as CD31 (Pecam-1) and vascular endothelial (VE)-cadherin and increased expression of mesenchymal markers such as α-smooth muscle actin (α-SMA) and fibroblast-specific protein (FSP)-1.",impact-revealing,highlighting the significance of RUNX3 in cardiovascular disease and its potential as a therapeutic target
818,5ecb57199e795ec6f2ba59cc,512f34906ddaefe885af2e5eec9b2b3b50ffd377,deep entity matching with pre-trained language models,5e427c893a55acbff4c4074d,Snippext: Semi-supervised Opinion Mining with Augmented Data,"Data augmentation has been extensively studied in computer vision and has recently received more attention in NLP [24, 48, 50].###The last operator, entry_swap, swaps the order of the pair ( e, e (cid:48) ) with probability 1 / 2 . decisions (i.e., F ( e, e (cid:48) ) = F ( e (cid:48) , e ) ) and helps double the size of MixDA: interpolating the augmented data Unlike DA operators###Following [24], we applyMixDA with the interpolation parameter λ sampled from a Beta distribution Beta(0.###To address this issue, Ditto applies MixDA, a recently proposed data augmentation technique for NLP tasks [24] illustrated in Figure 4.###These two operators are used in NLP tasks [48, 24] and shown to be effective for text classification.###We designed a set of data augmentation operators suitable for EM and apply them with MixDA [24], a recently proposed DA strategy based on convex interpolation.###We omit the technical details and refer the interested readers to [24].###The last technique, data augmentation, is adapted from [24] for EM to help Ditto learn “harder” to understand the data invariance properties that may exist but are beyond the provided labeled examples and also, reduce the amount of training data required.",impact-revealing,highlighting the significance of data augmentation techniques in NLP
309,5b67b46f17c44aac1c86329e,ef5cb5d49716bcd754b971a9a7303d7da2cd1036,From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks,59a030cdb161e8ad1a7b6f1e,Adapting Markov Decision Process for Search Result Diversification,"The utility of a document is estimated based on the MDP state, which consists of the query, the preceding documents, and the remaining candidates [33].###From the results we can see that, in terms of the four diversity evaluation metrics, “M 2 Div (with MCTS)” and “M 2 Div (without MCTS)” outperformed all of the baseline methods, including the heuristic method of MMR, xQuAD, PM-2 and learning methods of SVM-DIV, R-LTR, PAMM( α -NDCG), NTN-DIV( α -NDCG), and MDP-DIV( α -DCG).###M 2 Div, and the baselines of MDP-DIV and NTN-DIV need prelim-inary representations of the queries and the documents as their inputs.###From the results we can see that “M 2 Div (without MCTS)”, which did not conduct MCTS at the online time, still outperformed all of the baselines including “MDP-DIV( α -DCG)”, indicating that the MCTS conducted at the training time can generate better episodes to estimate the model parameters, achieving better raw policy p for ranking.###[33] and [30] propose to model the process of constructing a document rankingwithMDP, for the ranking tasks of search result diversi cation and relevance ranking, respectively.###MDP-DIV [33]: a state-of-the-art learning approach which uses an MDP for modeling the diverse ranking process.###[33] proposed to model the dynamics of the document utility with MDP and learning the model parameters with policy gradient.###Inspired by the success and methodology of the AlphaGo [27] and AlphaGo Zero [28] for the Game of Go, in this paper we propose to enhance the MDP model for diverse ranking [33] with the Monte Carlo tree search (MCTS), for alleviating the suboptimal ranking problem.###In our experiments, for e ective training of the model parameters and following the practices in [33], we combined four TREC datasets and constructed a new dataset with 200 queries and in total about 45,000 labeled documents.###Experimental results based on the TREC benchmark datasets show that M 2 Div can signi cantly outperform the state-of-the-art baselines including the heuristic methods of MMR, xQuAD and learning methods of R-LTR, PAMM, and MDP-DIV.###We also compared MDP-DIV with the learning methods: SVM-DIV [36]: a learning approach which utilizes structural SVMs to optimize the subtopic coverage.###MDP-DIV adapted the Markov decision process (MDP) to model the document ranking process.###Following the practice in [33], we con gured the reward function in MDP-DIV as α -DCG and the discounting parameter γ = 1.###The experimental results showed that M 2 Div can signi cantly outperform the state-of-the-art diverse ranking approaches that using greedy sequential decision making, including the heuristic based diverse ranking methods of MMR and xQuAD, and the machine learning based diverse ranking methods of PAMM and MDP-DIV.###Following the practices in [33], in the experiments we used the query vector and document vector generated by the doc2vec [17] to represent the document.###Following the practice in [33], we con gured the reward function in MDP-DIV as α-DCG and the discounting parameter γ = 1.###We conducted significant testing (t-test) on the improvements of our approaches over the best baseline MDP-DIV( α -DCG).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3894,5a260c0917c44a4ba8a1e00e,93f9607034c9b7b7693c60e9d2631adc15a2a524,learning to model the tail,5a260c8117c44a4ba8a30e59,The Devil is in the Tails: Fine-grained Classification in the Wild.,"pre-training on the head is quite similar to training on the unbalanced long-tailed dataset (which is dominated by the head) [10].###1, in which the number of training examples per class varies significantly from hundreds or thousands for head classes to as few as one for tail classes [8, 9, 10].###Long-tail: Minimizing the skewed distribution by collecting more tail examples is a notoriously difficult task when constructing datasets [11, 6, 12, 10].",other,highlighting challenges in constructing long-tailed datasets
791,5dc9327d3a55acc1042498de,2cf3bd0cc1382f35384e259d99e4f9744eeaed28,Blockwise Self-Attention for Long Document Understanding,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Most of them focus on the sparsiﬁcation of attention matrix, such as Star Transformer (Guo et al., 2019), Sparse Transformer (Child et al., 2019), Adaptive Sparse Transformer (Correia et al., 2019; Sukhbaatar et al., 2019), Log-Sparse Transformer (Li et al., 2019), etc.###Compared to the previous method that also enforces sparsity (e.g., Child et al. (2019)), our approach is much simpler mathematically and very easy to implement.###The attention masks used for Sparse Transformer encoder are illustrated in Figure 5.###SparseB ERT We pre-train BERT models with its Transformer encoder replaced by a Sparse Trans-former encoder (Child et al., 2019).",impact-revealing,acknowledge existing methods in attention matrix sparsification
2035,,94250e6762661ec3d587429e9b0950e2553bace4,Myocardial wall thickening from gated magnetic resonance images using Laplace's equation,,,"###These methods are based on volume element approaches and, in some cases, require additional tagged MR imaging acquisitions [14-16].###The use of MR tagging to assess wall thickening [14-16] is limited by poor spatial resolution if an insufficient number of tags are used.###3D wall thickness quantification techniques have been explored by several groups [7-19].###Currently, the methods for automatically determining the myocardial wall motion measurements in vivo using MRI are based on straight line distance techniques [6-16] and assume a 2D model for each short-axis slice.",impact-revealing,acknowledge limitations and variations in myocardial wall motion measurement methods
2745,5db929ff47c8f766461fd7e4,a73051e08af289a50ef8ed53e69f91c189dd01e5,Induction Networks for Few-Shot Text Classification,53e99822b7602d97020455c1,Transforming Auto-Encoders.,"Eq 5 encodes important invariant semantic relationships between lower level sample features and higher level class features (Hinton et al., 2011).###Given input vector x, squash is defined as:
squash(x) = ‖x‖2
1 + ‖x‖2 x ‖x‖ (6)
Eq 5 encodes important invariant semantic relationships between lower level sample features and higher level class features (Hinton et al., 2011).",other,providing context for semantic relationships in feature encoding
3674,53e9af67b7602d97039a85ee,529e8c6e6b5a6cb4f1cf202c47d9d42f5889ec1d,Last-Touch Correlated Data Streaming,53e998a3b7602d97020deda2,Microlib: A Case For The Quantitative Comparison Of Micro-Architecture Mechanisms,"We corroborate prior results [9,15] showing that GHB, an advanced stride/delta correlating predictor, can eliminate a large fraction of L2 cache misses in many applications, attaining on average 31% performance improvement across the applications we studied.###GHB uses 256-entry index and history tables, as recommended for SPEC applications [9,15].###Our results corroborate prior work showing that DBCP with practically-sized storage is ineffective [9].###The delta-correlating Global History Buffer (GHB) prefetcher [15] was recently shown to outperform a variety of other hardware prefetching schemes [9].",other,corroborating prior findings on GHB's performance improvement
2535,573698426e3b12023e70bf13,4e9dbca4218d32a9f92d58c340f3f8f3c5020a44,best-offset hardware prefetching,53e9bd6fb7602d9704a0cd33,Pin: Building Customized Program Analysis Tools With Dynamic Instrumentation,The microarchitecture simulator used for this study is an in-house simulator based on Pin [20].###Pin : building customized program analysis tools with dynamic instrumentation.,other,describing the tool used for simulation
3159,5cede104da562983788e3653,05dc5fb3a3bdefdf181aafcc42cd80ff6b7704e7,TuckER: Tensor Factorization for Knowledge Graph Completion,5cede105da562983788e48c5,RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space.,"The RotatE (Sun et al., 2019) results are reported without their self-adversarial negative sampling (see Appendix H in the original paper) for fair comparison.###414 − − − − RotatE (Sun et al., 2019) no − − − − .",other,reporting results for fair comparison
1483,,1662aabfb331ccf6b91343ee9e59c73757cd5c7c,Cationic phosphorus-containing dendrimers reduce prion replication both in cell culture and in mice infected with scrapie.,,,"###This property decreases the likelihood of rapid elimination as observed with regular cationic systems, due to electrostatic interactions with negatively-charged cellular membranes and extracellular matrices (Padilla De Jesus et al., 2002).###Interestingly, high molecular mass polymers, like dendrimers, have been widely used as soluble drug carriers to improve drug targeting and therapeutic efficacy (Padilla De Jesus et al., 2002).",impact-revealing,highlighting the significance of high molecular mass polymers in drug delivery
411,5b8c9f5317c44af36f8b778e,16ba65426ed5e1e3367eff5bd507dcf6d99bd7c2,Wide Activation for Efficient and Accurate Image Super-Resolution,59ae3be32bbe271c4c71ba2e,Enhanced Deep Residual Networks for Single Image Super-Resolution,"Thus, in recent image SR networks [7, 19, 42], batch normalization is abandoned.###Compared with vanilla residual blocks used in EDSR [19], we introduce WDSR-A which has a slim identity mapping pathway with wider ( 2 × to 4 × ) channels before activation in each residual block.###…super-resolution, commonly only small image patches (e.g. 48 × 48 ) and small mini-batch size (e.g. 16) are used to speedup training [7, 14, 17, 19, 33, 36, 42], thus the mean and variance of small image patches differ a lot among mini-batches, making theses statistics unstable, which is…###However, with the increasing depth of neural networks for SR (e.g. MDSR [19] has depth around 180), the networks without batch normalization become difﬁcult to train.###It is also experimentally proved in single image super-resolution task [7, 14, 17, 19, 33, 36, 42].###Two-layer residual blocks are speciﬁcally studied following baseline EDSR [19].###Deep convolutional neural networks (CNNs) have been successfully applied to the task of single image super-resolution (SISR) [14, 19, 20, 42].###Previous works including EDSR [19], BTSRN [7] and RDN [42] found that batch normalization [12] deteriorates the accuracy of image super-resolution, which is also conﬁrmed in our experiments.###They are inferior in accuracy compared with later proposed deep SR networks (e.g., VDSR [14], SRResNet [17] and EDSR [19]).###Normalization layers As image super-resolution networks going deeper and deeper (from 3-layer SRCNN [4] to 160-layer MDSR [19]), training becomes more difﬁcult.",impact-revealing,highlighting challenges in training deep neural networks for super-resolution
2428,5f75aa6a9fced0a24b64599c,10f61ec5c6c822325a91bd7d718a81b93e9628ca,Opportunistic Early Pipeline Re-steering for Data-dependent Branches,558c6c66e4b02b9f07a703d0,Control-Flow Decoupling,"Control Flow Decoupling (CFD) [36], is another approach to hoist the load out the loop to create the slack between the load and branch, which then feeds the dependent branches using a prediction queue.",other,providing context for Control Flow Decoupling method
2257,558b3e9384ae84d265c24c24,fed9ac8dc6ea64141e5526894a146e476b8fea52,SCD: A scalable coherence directory with flexible sharer set encoding,53e9b802b7602d97043b3500,Virtual Tree Coherence: Leveraging Regions And In-Network Multicast Trees For Scalable Cache Coherence,"Alternatively, prior work has proposed coarse-grain coherence tracking [4, 9, 34].",other,acknowledge existing methods for coherence tracking
57,5edf5dd891e011bc656ded70,374f36c9081ab5dc686ab833c42a7297235cd13f,Denoising Implicit Feedback for Recommendation,5a260c8117c44a4ba8a30b08,Focal Loss for Dense Object Detection.,"Inspired by the success of Focal Loss [22], we estimate ω(u, i) with a function of f (ŷui ) that takes the prediction score as input.",impact-revealing,reporting a method inspired by prior work
2,5b67b45517c44aac1c860884,9fa3e53b5937a0ec92499ed415e339ede6c92010,DeepInf: Social Influence Prediction with Deep Learning,53e9bab4b7602d97046e48fb,Social influence locality for modeling retweeting behaviors,"Weibo [53, 54] Weibo 6 is the most popular Chinese microblogging service.###The dataset is from [53] and can be downloaded here.###Data Preparation We process the above four datasets following the practice in existing work [53, 54].###The above observations inspire a lot of user-level influence prediction models, most of which [27, 53, 54] consider complicated hand-crafted features, which require extensive knowledge of specific domains and are usually difficult to generalize to different domains.###Social Influence Locality[53] Social influence locality models the probability of v ’s action status conditioned on her r -ego network G rv and the action states of her r -neighbors.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1732,,669f6a4e55db08b9939466357cf84ffb9f5028b9,Parietal modules for reaching,,,"###The interpretation of OA as a pecific visuomotor deficit has been reinforced by the careful study f Vighetto in the 80s (Perenin & Vighetto, 1983, 1988).###In their study, Perenin and Vighetto (1988) reported that lesions of the right hemisphere resulted in a field effect only whereas lesions of the left hemisphere induced an additional hand effect.###The interpretation of OA as a specific visuomotor deficit has been reinforced by the careful study ofVighetto in the 80s( Perenin & Vighetto, 1983, 1988 ).###Hand effect and field effect in OA
The most famous OA patient population study is presented n the Perenin and Vighetto (1988) paper.###Perenin and Vighetto 1988) overlaid the lesions of two right and six left brain-damaged atients and found that the IPS was involved in 100% of the patient ases and that the SPL (mainly the pre-cuneus) was involved in six atients out of eight.###The lesion site responsible for OA was classically attributed to the superior lobule of the posterior parietal cortex (SPL) and/or the intraparietal sulcus (IPS) (Ratcliff & Davies-Jones, 1972; Perenin & Vighetto,1988; JeannerodandRossetti,1993).PereninandVighetto (1988)overlaid the lesions of two right and six left brain-damaged patients and found that the IPS was involved in 100% of the patient cases and that the SPL (mainly the ...",impact-revealing,reporting findings on visuomotor deficits and lesion studies
3984,5ec49a639fced0a24b4de7d4,9eda533cf0badf8dbed5c8240bb828b622328183,Fine-grained Fact Verification with Kernel Graph Attention Network,5550417445ce0a409eb3b699,Convolutional Neural Network Architectures For Matching Natural Language Sentences,"The recent development of neural information retrieval models, especially the interaction based ones, have shown promising effectiveness in extracting soft match patterns from query-document interactions (Hu et al., 2014; Pang et al., 2016; Guo et al., 2016; Xiong et al., 2017; Dai et al., 2018).",other,highlighting the effectiveness of recent neural information retrieval models
1691,,b8551038a1c73976c7f9f4ddfa5a0d04528126a3,Constructing and communicating an ethical consumer identity: A Social Identity Approach,,,"###According to Tajfel and Turner (1979), when an individual identifies himself as part of a social group, his membership is reinforced by viewing the group as superior to other groups.",impact-revealing,providing context on social identity theory
524,5f7fdd328de39f0828397e22,21e33bd0ad95ee1f79d8b778e693fd316cbb72d4,Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs,5e5e18ba93d709897ce2b48e,Geom-GCN: Geometric Graph Convolutional Networks,"For all benchmarks (except Cora-Full), we use the feature vectors, class labels, and 10 random splits (48%/32%/20% of nodes per class for train/validation/test2) provided by [26].###The “*” results are obtained from [26] and “N/A” denotes non-reported results.###The “*” denotes ranks based on results reported in [26].###Geom-GCN [26] precomputes unsupervised node embeddings and uses neighborhoods defined by geometric relationships in the resulting latent space to define graph convolution.###Some of these works [1, 26, 12] acknowledge the challenges of learning from graphs with heterophily.###We used the preprocessed version in [26].###[26] claims that the ratios are 60%/20%/20%, which is different from the actual data splits shared on GitHub.###• Actor is a graph representing actor co-occurrence in Wikipedia pages, processed by [26] based on the film-director-actor-writer network in [35].###Additional model comparison In Table 4, we also report the best results among the three recentlyproposed GEOM-GCN variants (§ 4), directly from the paper [26]: other models (including ours) outperform this method significantly under heterophily.###For the classification task, we utilize the class labels generated by [26], where the nodes are categorized into 5 classes based on the amount of their average traffic.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1973,,ec5fb774023a4e791d8b0e989e9e015afb3ee619,Intergenerational Solidarity in Families: Untangling the Ties That Bind,,,"###A great deal of empirical and theoretical work done by social psychologists in the 1950's and 1960's was devoted to understanding group dynamics (Back, 1951; Collins & Raven, 1969; Deutsch, 1968; Deutsch & Krauss, 1965; Festinger, Schachter, & Back, 1950; Turner, 1957). Much of the work focussed on identifying charactcristics of interaction related to group cohesion and, in many ways, derived from and extendcd the classical formulations of solidarity. One of the more cogent theoretical taxonomies of the elements of group solidarity was developed by Homans (1950) and later amplified and extended by Fritz Heider (1958) (McChesney and Bengtson, 1988). Homans emphasized the importance of four characteristics of human interaction in determining group solidarity. The first component, ""interaction,"" referred to the degrce of interconnectedness between the actions of one group member and another member. Homans' use of ""interaction"" was consistent with the notion of functional interdependence in Durkheim's organic solidarity. A second related component of group solidarity was the extensivity of ""activity"" involving group members. This construct subsumed the breadth of activities in which group members mutually engaged. Homan's third component-""sentiment""-referred to the degree of mutual affection obtaining between members of the group. The fourth dimension in Homans' scheme was indexed by group members' ""norms"" toward group membership and interaction. More cohesive groups were those in which members interacted often, liked each other, and shared similar normative commitments to group activities. Heider's (1959) balance theoretical approach was similar to Homans' in that he emphasized the importance of interaction (""contact"") and sentiment (""liking"").###A great deal of empirical and theoretical work done by social psychologists in the 1950's and 1960's was devoted to understanding group dynamics (Back, 1951; Collins & Raven, 1969; Deutsch, 1968; Deutsch & Krauss, 1965; Festinger, Schachter, & Back, 1950; Turner, 1957).###A great deal of empirical and theoretical work done by social psychologists in the 1950's and 1960's was devoted to understanding group dynamics (Back, 1951; Collins & Raven, 1969; Deutsch, 1968; Deutsch & Krauss, 1965; Festinger, Schachter, & Back, 1950; Turner, 1957). Much of the work focussed on identifying charactcristics of interaction related to group cohesion and, in many ways, derived from and extendcd the classical formulations of solidarity. One of the more cogent theoretical taxonomies of the elements of group solidarity was developed by Homans (1950) and later amplified and extended by Fritz Heider (1958) (McChesney and Bengtson, 1988).",impact-revealing,acknowledging historical contributions to understanding group dynamics
886,5fae6daad4150a363cec035c,413117826eecb3fd1491e0665e4b644a521d3bc3,Spanet: Spatial Pyramid Attention Network for Enhanced Image Recognition,5a260c8117c44a4ba8a30771,Squeeze-and-Excitation Networks.,"However, attention based CNNs (e.g., [5], [6], [7], etc.) apply global average pooling on each feature map.###Compared to SENet [5], our structure only modiﬁes the ﬁrst fully-connected layer to tackle the large input size.###Most of the existing self-attention based networks follow a path de-sign pattern: they learn an attention map from a feature map and then apply the learned attention map to the original feature map [5,18] .###To address this problem, we leverage the excitation block [5] to encode v and generate a 1D attention map ˜ v .###Without bells and whistles, SPANet outperforms related state-of-art work [2,5,10,11].###More recently, attention based networks such as SENet [5] and CBAM [6] provide an independent attention path to learn the weight of each channel and achieve state-of-the-art performance.###In [5], a Squeeze-and-Extraction block was proposed to learn the channel-wise attention for each convolutional layer, which provides an end-to-end training paradigm for attention learning.###As an example, SENet [5] introduces Squeeze-and-Excitation (SE) blocks to study the channel dependencies in a CNN architecture.###Many existing attention based networks [5,6,6,17] aggregate input feature maps into a 1D vector using global average pooling.###Like in SENet [5], we set r to 16 .",impact-revealing,discussing variations in attention-based CNN architectures
3801,5dbebb7447c8f766462c2328,b0d941cfa0a3c43703e5222221addb1ad0f9e68d,Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks,5cfa5b985ced2477cb3c50be,AIR: Attentional Intention-Aware Recommender Systems,"recurrent neural networks [1, 2, 10, 29, 36], which is naturally designed for processing sequential data.",other,providing context for recurrent neural networks
4021,5e15adcb3a55ac47ab5b0b8c,aa63ac11aa9dcaa9edd4c88db18bec87e0834328,Graph Transformer Networks,53e9afe8b7602d9703a3c50f,Node Classification in Social Networks,"Conventionally, hand-crafted features have been used such as simple graph statistics [2], graph kernel [34], and engineered features from a local neighbor structure [23].",other,acknowledge traditional feature engineering methods
609,5dce78623a55ac93ec834bf7,1a55ab1849eccc03801852e1cf445e15d17f8020,Limago: An FPGA-Based Open-Source 100 GbE TCP/IP Stack,5736977d6e3b12023e66448a,Scalable 10Gbps TCP/IP Stack Architecture for Reconfigurable Hardware,"It is divided into three parts, the incoming data path (Rx Engine), the outgoing data path (Tx Engine), and the state-keeping data structures [11], [29].###Limago explores the changes needed to upgrade an existing open-source TCP/IP stack from 10 Gbit/s [11] to 100 Gbit/s, but maintaining the same high-productivity design methodology, based on Vivado-HLS, that was utilized in the previous design.",impact-revealing,describing the structure and purpose of the system
2985,5ca600ae6558b90bfa4d76e9,7aa38b85fa8cba64d6a4010543f6695dbf5f1386,Towards Deep Learning Models Resistant to Adversarial Attacks,5b3d98a217c44a510f7ff2b1,Understanding adversarial training: Increasing local stability of supervised models through robust optimization.,"Indeed, this is the conclusion reached by prior work which then resorted to linearizing the inner maximization problem (Huang et al., 2015; Shaham et al., 2015).",other,acknowledging conclusions from prior work
603,53e9984bb7602d970207d617,04d90a08d6fe2bea9a247fd01dbc90c86a8a5722,virtual-physical registers,53e9adffb7602d970380b2f0,Register File Design Considerations in Dynamically Scheduled Processors,"In addition, larger register files have a longer access time, and this may increase the critical path length and penalize performance [1].",impact-revealing,highlighting the impact of larger register files on performance
178,5bdc31b817c44a1f58a0c039,abfa95058fa50c55a0b923a6c35830f470c125ad,Adaptive sampling towards fast graph representation learning,5a9cb66717c44a376ffb8667,FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling.,"For FastGCN, we directly make use of the provided results by [20].###More recently, two kinds of sampling-based methods including GraphSAGE [3] and FastGCN [20] were developed for fast representation learning on graphs.###The codes of GraphSAGE [3] and FastGCNN [20] provided by the authors are implemented inconsistently; here we re-implement them based on our framework to make the comparisons more fair.###We contrast our approach with GraphSAGE [3] and FastGCN [20] regarding the following aspects:###Following the supervised learning scenario in FastGCN [20], we use all labels of the training examples for training.###For FastGCN, we adopt the Independent-Identical-Distribution (IID) sampler proposed by [20] in Eq.",impact-revealing,acknowledge existing methods and their inconsistencies
3821,5e5e18b693d709897ce29a22,53a77e8f73f2ca422d6e38fa9ecc490231ac044c,Neural Text Generation with Unlikelihood Training,53e99e61b7602d9702727399,Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms,"Some representative algorithms include structured perceptron (Collins, 2002), energy-based models (LeCun et al.###Some representative algorithms include structured perceptron (Collins, 2002), energy-based models (LeCun et al., 2006) and more recently reflective likelihood (Dieng et al., 2018). A particular variant in this family of algorithms, called negative training, was recently used by He and Glass (2019) to prevent generic and malicious responses in dialogue models.",other,reporting existing algorithms and their applications
4033,5bbacb9e17c44aecc4eaff64,5b1516c87818084dc5d195cc274e1ee8923210d2,Neural Cross-Lingual Named Entity Recognition with Minimal Resources,53e99c52b7602d970250265c,Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure.,"This transfer can be performed using a variety of resources, including parallel corpora (T¨ackstr¨om et al., 2012; Ni et al., 2017), Wikipedia (Nothman et al., 2013), and large dictionaries (Ni et al., 2017; Mayhew et al., 2017).###This transfer can be performed using a variety of resources, including parallel corpora (Täckström et al., 2012; Ni et al., 2017), Wikipedia (Nothman et al.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
209,599c797a601a182cd2641df7,63a010c69f00e65c946a68b546bbd42cbed03564,MagNet: A Two-Pronged Defense against Adversarial Examples,53e9a93eb7602d97032928b5,Intriguing properties of neural networks.,"Current defenses against adversarial examples follow three approaches: (1) Training the target classifier with adversarial examples, called adversarial training [34, 5]; (2) Training a classifier to###Since the discovery of adversarial examples for neural networks in [34], researchers have found adversarial examples on various network architectures.###However, recent research showed that an attacker could generate adversarial examples to fool classifiers [34, 5, 24, 19].###More specifically, researchers showed that it was possible to generate adversarial examples to fool classifiers [34, 5, 24, 19].###For example, one may use a mixture of normal and adversarial examples in the training set for data augmentation [34, 22], or mix the adversarial objective with the classification objective as regularizer [5].###Researchers developed several methods for generating adversarial examples, most of which leveraged gradient based optimization from normal examples [2, 34, 5].",impact-revealing,describing current defenses against adversarial examples
1641,,f745c8d633109e4547cc32089e04dd884b5a40a6,Between two worlds: East Asian sojourners’ negotiation of identity in the United States,,,"###CTI builds on earlier theories of social identity (e.g., Tajfel & Turner, 1979), which argue that individuals form their identity through learning social norms and values of groups.",impact-revealing,acknowledging foundational theories in social identity
510,5bdc317017c44a1f58a08086,6062d8f5cc50014e5ff0f9aa467e1b044d33c051,Learning Cloud Dynamics to Optimize Spot,573697c96e3b12023e6a9ac3,How to Bid the Cloud,"[10] assumes bids are drawn from U [⇡,⇡]) as well in the simple bidding strategy of bidding the on-demand price since the user only pays the spot price anyway, as is commonly advocated (e.###Other works have derived more in-depth bidding strategies for resources in the cloud spot market by explicitly considering job deadlines [16], cost minimization [10], [17], and task dependency [18], [19].###Many works [10], [23], [24] assume that the CP’s objective is to maximize its own profit under some capacity constraint, which we show is inconsistent with empirical spot price distributions in Section III-A.###bt instead of Bt), we follow [10] and assume that all bids b 2 Bt are drawn independently from the uniform distribution on [⇡,⇡].###The authors of [10] used a profit-maximization model to understand spot price distributions; however, they considered only the asymptotic (time-invariant) user demand regime and made fairly restrictive assumptions on spot user arrival and bid distributions.###A better strategy would be to consider the collective behavior of the spot prices over time [10], which we do in this section by accounting for their temporal dynamics.###Thus, unlike prior works on bidding optimization [10], our model not only explicitly accounts for the interplay between the on-demand and spot markets under a resource capacity constraint but also handles time correlations between spot prices.",impact-revealing,highlighting the limitations of prior bidding strategies and proposing a more comprehensive model
3068,5db9294247c8f766461f1f4a,79c93274429d6355959f1e4374c2147bb81ea649,lxmert: learning cross-modality encoder representations from transformers,5b1643998fbcbf6e5a9bc3b6,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.,"…progress towards building a universal back-bone model with large-scale contextualized language model pre-training (Peters et al., 2018; Rad-ford et al., 2018; Devlin et al., 2019), which has improved performances on various tasks (Ra-jpurkar et al., 2016; Wang et al., 2018) to signiﬁcant levels.",other,highlighting the advancements in universal backbone models through large-scale pre-training
281,5edcbb9a91e0110f1d6db1c2,c99e2877d4b1f171746a3aa73d5fd5fd680c982d,Hierarchical Bipartite Graph Neural Networks: Towards Large-Scale E-commerce Applications,5b67b4b917c44aac1c867dbc,Hierarchical Graph Representation Learning with Differentiable Pooling.,"2) Compared Algorithms and Settings: To the best of our knowledge, no existing algorithm can deal efﬁciently with hierarchical user preferences and hierarchical item attractiveness to predict real-world e-commerce tasks of such large scale, including [30] and [20].###On one hand, it demonstrates in [20] that hierarchical representations of graphs can be combined with various graph neural network architectures in an end-to-end fashion to achieve prevailing results on graph classiﬁcation benchmarks.###In particular, the recently proposed approach DIFFPOOL [20], a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion.###In [20], authors make some efforts in effectively co-training two embeddings by decomposing user information into two orthogonal spaces, each of which represents information captured by community level and individualized user preference respectively.",impact-revealing,highlighting the limitations of existing algorithms in hierarchical user preference prediction
2462,5c8bd2e54895d9cbc6af9826,d14afc470cd90521147130e153c0d3e1324cd104,Learning Language Representations for Typology Prediction,53e9b53cb7602d9704072ce2,Learning to Map into a Universal POS Tagset.,"Typological information from sources like the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013), has proven useful in many NLP tasks (O’Horan et al., 2016), such as multilingual dependency parsing (Ammar et al., 2016), generative parsing in low-resource settings (Naseem et al., 2012; T¨ackstr¨om et al., 2013), phonological language modeling and loan-word prediction (Tsvetkov et al., 2016), POS-tagging (Zhang et al., 2012), and machine translation (Daiber et al., 2016).###, 2016), POStagging (Zhang et al., 2012), and machine translation (Daiber et al.###…dependency parsing (Ammar et al., 2016), generative parsing in low-resource settings (Naseem et al., 2012; T¨ackstr¨om et al., 2013), phonological language modeling and loan-word prediction (Tsvetkov et al., 2016), POS-tagging (Zhang et al., 2012), and machine translation (Daiber et al., 2016).",other,highlighting the usefulness of typological information in various NLP tasks
3730,5ef0816891e0112aee042b88,5c126ae3421f05768d8edd97ecd44b1364e2c99a,denoising diffusion probabilistic models,5550411145ce0a409eb38586,NICE: Non-linear Independent Components Estimation.,"While diffusion models might resemble ﬂows [9, 43, 10, 30, 5, 14, 21] and VAEs [31, 44, 34], diffusion models are designed so that q has no parameters and the top-level latent x T has nearly zero mutual information with the data x 0 .",other,providing context on the design of diffusion models
3632,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,53e9b91eb7602d9704502adf,Fusion of AV features and external information sources for event detection in team sports video,"Similar approach was used by Xu and Chua [149].###[13] and Xu and Chua [149] attempted to synchronize video based on the time extracted from the time overlays in the video and web-casted text, respectively; Xu et al.###For example, fusion of audio-visual features along with other textual information have become more effective in detecting events from a team sports video [149], which would otherwise not be possible by using a single medium.###Meyer et al. [85] and Xu and Chua [149] have used the Bayesian inference method at the decision level for spoken digit recognition and sports video analysis, respectively; while Atrey et al. [8] employed this fusion strategy at both the feature as well as the decision level for event detection in the multimedia surveillance domain.###Therefore, many researchers ([16, 88, 149], etc.###In the context of news video analysis, Chua et al. [31] have emphasized on the need to utilize multimodal features (text with audio-visual) for segmenting news video into story units.###[85] and Xu and Chua [149] have used the Bayesian inference method at the decision level for spoken digit recognition and sports video analysis, respectively; while Atrey et al.###Similar to this work, Xu and Chua [149] also used the Bayesian inference fusion method for integrating the probabilistic decisions about the offset and non-offset events detected in a sport video.",other,acknowledge existing methods and their applications in multimedia analysis
1726,,0f592f2bf9621d04b68d1cb76d0fd55134d9d366,Reflective Hierarchical State-Machines for Self-Adaptable Distributed Transaction Coordination,,,"###Each mobile transaction coordination model is implemented as a HLS in the Activity Service, and Beanlet framework is responsible for adaptations between models between KT and TTR.###Consequently, the processing catches this exception and re-starts from localPro in the TTR refinement and perform local commit on a local copy of data.###• LocalPro in the TTR refinement:
MH is still or moving within a cell; MH has appropriate resource (memory, disk space, etc.) for local transaction execution;
A local copy of data is available at the MH.###In this case, since the TTR model does not consider hand-offs, the bridge beanlet is used to “remedy” the otherwise inconsistent coordination state.###We adopt two independent mobile transaction models, Two-Tier Replication (TTR) [10] and the Kangaroo Transaction model (KT) [5].###While TTR uses a lazy replication mechanism to cope with disconnections, KT focus more on mobile host (MH) movement during the execution of transactions.###In TTR, data is kept in a master version and several replicated copies.###In this way, the current POST condition of the bridge beanlet satisfies the PRE condition of LocalPro, and the coordination switches to the TTR refinement and proceeds forward.###Rather than proposing a new mobile transaction model, we use the existing TTR and KT coordination models, and incorporate them into our Beanlet framework using the concepts introduced in sections 2.1 and 2.2.###For example, the Processing beanlet have two different refinements: the KT processing (figure 2) and the TTR processing (figure 3).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2965,5aed14d617c44a4438159123,921196c32213a229245a9705ee4768bc941e7a26,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,53e99bffb7602d97024ac0e6,Generating Sequences With Recurrent Neural Networks.,"This family of architectures has gained tremendous popularity due to prominent applications to language modeling (Sutskever et al., 2011; Graves, 2013; Hermans & Schrauwen, 2013) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015).",other,highlighting the significance and applications of a family of architectures in language modeling and machine translation
964,,799fc4a5bf72c6e65d74769572ba4b1b338f236c,Rumination and symptom reports in children and adolescents: Results of a cross-sectional and experimental study,,,"###This was already shown in previous studies by Jose and Brown (2008) and Abela et al. (2002). The previous results are, however, extended by this study, because it is related to a broader scope of symptoms than the previous studies which focused on depression. However, the sex differences were only partially mediated by rumination, which means that there are additional mediating variables. For example, catastrophising may be a possible variable, which may lead to an explanation of additional variance. Catastrophising refers to the individual tendency to focus on and exaggerate the threat value and negative outcomes of pain (Hermann, Hohmeister, Zohsel, Ebinger, & Flor, 2007; Rosenstiel & Keefe, 1983). In contrast to rumination, catastrophising explicitly emphasises the negative feelings elicited by actual and anticipated painful experiences (Sullivan et al., 2001). As is the case of rumination, there are substantial sex differences with regard to catastrophising (Hermann et al., 2007). If not only rumination is involved, but also an exaggeration of elicited negative feelings occurs, this may additionally contribute to the sex differences in symptom reports. As a consequence, it would be interesting to include both rumination and catastrophising as mediators to be able to analyse whether this would lead to an additional increase in explained variance and to a complete mediation of sex differences. The main question addressed in this study is, however, related to the contribution of rumination to the explanation of symptom reports in children and adolescents. As the correlational analyses show, there are substantial associations between rumination and symptom reports. It is, however, unclear how these relations can be appropriately interpreted. It is possible that rumination leads to increased symptom frequency reports, but it is also possible that increased experiences of somatic and psychological symptoms lead to rumination (Nolen-Hoeksema et al., 2007). A previous longitudinal study by Broderick and Korteland (2004) supports the first notion as their results indicated that rumination predicted a later increase of depressive symptoms across a threeyear interval. This is also supported by the study of Jose and Brown (2008), which showed that sex differences in rumination preceded sex differences in stress and depression.###This was already shown in previous studies by Jose and Brown (2008) and Abela et al. (2002). The previous results are, however, extended by this study, because it is related to a broader scope of symptoms than the previous studies which focused on depression.###This was already shown in previous studies by Jose and Brown (2008) and Abela et al. (2002).###This is also underlined by a short-term longitudinal study across an interval of six weeks which showed that children of grades 3 and 7 with a ruminative response style exhibited increases in depressive symptoms across the six-week period (Abela et al., 2002).###This was already shown in previous studies by Jose and Brown (2008) and Abela et al. (2002). The previous results are, however, extended by this study, because it is related to a broader scope of symptoms than the previous studies which focused on depression. However, the sex differences were only partially mediated by rumination, which means that there are additional mediating variables. For example, catastrophising may be a possible variable, which may lead to an explanation of additional variance. Catastrophising refers to the individual tendency to focus on and exaggerate the threat value and negative outcomes of pain (Hermann, Hohmeister, Zohsel, Ebinger, & Flor, 2007; Rosenstiel & Keefe, 1983). In contrast to rumination, catastrophising explicitly emphasises the negative feelings elicited by actual and anticipated painful experiences (Sullivan et al., 2001). As is the case of rumination, there are substantial sex differences with regard to catastrophising (Hermann et al., 2007). If not only rumination is involved, but also an exaggeration of elicited negative feelings occurs, this may additionally contribute to the sex differences in symptom reports. As a consequence, it would be interesting to include both rumination and catastrophising as mediators to be able to analyse whether this would lead to an additional increase in explained variance and to a complete mediation of sex differences. The main question addressed in this study is, however, related to the contribution of rumination to the explanation of symptom reports in children and adolescents. As the correlational analyses show, there are substantial associations between rumination and symptom reports. It is, however, unclear how these relations can be appropriately interpreted. It is possible that rumination leads to increased symptom frequency reports, but it is also possible that increased experiences of somatic and psychological symptoms lead to rumination (Nolen-Hoeksema et al., 2007). A previous longitudinal study by Broderick and Korteland (2004) supports the first notion as their results indicated that rumination predicted a later increase of depressive symptoms across a threeyear interval.",impact-revealing,highlighting the contribution of rumination to symptom reports in children and adolescents
596,5ce3a7b6ced107d4c654a979,a15cd4ac38a38fa977c912c9c360d380dfcccfee,Research on Trustiness of Software Behavior Based on Cross-References,53e9a8d3b7602d9703220fcd,"Fast, accurate call graph profiling","Remarkable works include Forrest, et al., [1], Giffin, et al., [23], Spivey [24], Bond and McKinley [25].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
667,5f50ba4291e01182e69239cb,20454697ea082975db2503a52418efb8f65b8ae6,clocs: camera-lidar object candidates fusion for 3d object detection,5bdc315817c44a1f58a05e9d,Deep Continuous Fusion For Multi-Sensor 3d Object Detection,"sensor modalities with better correspondence, MMF [26] adopts continuous convolution [14] to build dense LiDAR BEV feature maps and do point-wise feature fusion with dense image feature maps.###[13], [14], and typically require pixel-level correspondences of sensor data.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1819,,d52ff331ecb5df6cffe0509da9bfbfb2a3ec03cd,Learning Together in Highland Park to Build Civic Capacity,,,"###…competence and mastery, a greater desire for control, more civic duty, and a general belief that their success is a result of internal rather than external factors -Zimmerman & Rappaport 1988, 745-746 Focusing on personal empowerment is limited by individual bias (Peterson & Zimmerman 2003).",impact-revealing,acknowledging prior findings on personal empowerment and bias
2905,5dbab2523a55acea3c05b02b,395de0bd3837fdf4b4b5e5f04835bcc69c279481,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Bidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classification decisions.###Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.###Masked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.###Figure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al.###BERT (Devlin et al., 2019) introduced masked language modelling, which allows pre-training to learn interactions between left and right context words.###Token Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.###Bidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classiﬁcation decisions.###Figure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).###Self-supervised methods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).",other,highlighting the importance of bidirectional encoders in NLP tasks
1022,,df5267525f3414cf6216f83d84722fb8ee35c225,Recognizing the usage of analgesics among Saudi women with primary dysmenorrhea,,,"###The sample size was determined from the following formula that is commonly used for cross-sectional studies [17] in which n is the sample size, Z is the Z value for 95% confidence limits ( α = 0.05), p is the predictable prevalence of dysmenorrhea (80%), and d is the anticipated accuracy (4%).",impact-revealing,describing the sample size determination method
1814,,ad7a3de15cc9021ef162f0ed29b9556418213c17,Passive Immunization against HIV/AIDS by Antibody Gene Transfer,,,"###In addition to its transient expression, which is not suited for antibody therapy requiring sustained delivery, clinical utilization of Ads is hindered by the inflammatory and immune response they evoked after in vivo administration [38,42,43].",impact-revealing,highlighting challenges in the clinical utilization of Ads
4035,5da2f8aa3a55ac3402d8c092,fd2a0a326db4f034fe22340c20b7bacd9a14c3d6,second-order attention network for single image super-resolution.,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"Deep convolution neural networks (CNNs) have recently achieved unprecedented success in various problems [7, 25].",other,highlighting the success of deep convolution neural networks in various problems
3676,5a4aef9e17c44a2190f7a4bd,1deb7f96fc92d5c9e04d2cbb277473fee878e144,Cascaded Pyramid Network for Multi-person Pose Estimation,573696106e3b12023e5236ff,DeepCut: Joint Subset Partition and Labeling for Multi Person Pose   Estimation,"DeepCut [33] interprets the problem of distinguishing different persons in an image as an Integer Linear Program (ILP) problem and partition part detection candidates into person clusters.###DeeperCut [20] improves DeepCut [33] using deeper ResNet [17] and employs image-conditioned pairwise terms to get better performance.###Bottom-up approaches [5, 26, 30, 19] directly predict all keypoints at first and assemble them into full poses of all persons.###DeepCut [30] interprets the problem of distinguishing different persons in an image as an Integer Linear Program (ILP) problem and partition part detection candidates into person clusters.###DeeperCut [19] improves DeepCut [30] using deeper ResNet [16] and employs image-conditioned pairwise terms to get better performance.",other,acknowledge existing methods in person detection
3961,5f88146591e0118ce8f040a7,3cb9c274a1a087be7040675b744b2fd0d579e55f,Are all negatives created equal in contrastive instance discrimination?,53e9a16ab7602d9702a5dfae,A survey on instance selection for active learning,"In active learning, for example, it is common to favor examples on which the model is most uncertain (Fu et al., 2013).",other,providing context for active learning methods
387,58437725ac44360f108302aa,03a5b2aac53443e6078f0f63b35d4f95d6d54c5d,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,5550472145ce0a409eb64ae3,Learning A Deep Convolutional Network For Image Super-Resolution,"We first evaluate the power of the sub-pixel convolution layer by comparing against SRCNN’s standard 9-1-5 model [6].###However, the use of Set14 on a single CPU core is selected here in order to allow a straight-forward comparison with results from previous published results [31, 6].###[6], there are no efficient implementations of a convolution layer whose output size is larger than the input size and well-optimized implementations such as convnet [21] do not trivially allow such behaviour.###Here, we follow the approach in [6], using relu as the activation function for our models in this experiment, and training a set of models with 91 images and another set with images from ImageNet.",impact-revealing,evaluating the effectiveness of a specific model in comparison to previous work
3011,5bdc315017c44a1f58a05e7e,7a71941e60894ae7e1f5af8e79c37cec6cd6c6ad,Multi-scale Residual Network for Image Super-Resolution,5a260bfb17c44a4ba8a1c65a,NTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study.,"We train our models on the DIV2K [11] dataset without special weight initialization method or other training tricks.###In our work, we choose DIV2K [11] as our training dataset, a new high-quality image dataset for image restoration challenge.",other,reporting the choice of training dataset for image restoration
951,,50d13618ad5b7034395e81ddd747e2aecbf68dd6,Neuromuscular Junction Defects in a Mouse Model of Charcot-Marie-Tooth Disease Type 2O,,,"###Concerns about a valid environment are validated by data suggesting interactions between genes and external factors in the environment (Gottlieb, 1998).",impact-revealing,highlighting the significance of gene-environment interactions
3108,5fc61cdb91e0118947381abc,c9d736dd9f967844d2391bb13c4cb477576ab373,On Disambiguating Authors: Collaboration Network Reconstruction in a Bottom-up Manner,5736972a6e3b12023e61b7be,CNL: Collective Network Linkage Across Heterogeneous Social Platforms,"IUAD adopts the EM algorithm to learn the MLEs of parameters [38].###To solve the problem, we propose a generative probabilistic model to calculate the probabilities Pr(rj ∈ M |γj ,Θ) and Pr(rj ∈ U |γj ,Θ) to make the decision [38], where Θ denotes the parameters of the generative model.",other,reporting a proposed generative probabilistic model
735,5cede0e5da562983788c40d8,e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b,Unsupervised Embedding Learning via Invariant and Spreading Instance Feature,563e04d40cf219a1e1f70071,Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks.,"2) Estimating Between-image Labels, it usually estimates between-image labels using the clustering technique [3, 9, 26] or kNN-based methods [41], which provide label information.",impact-revealing,reporting methods for estimating between-image labels
958,,9742067ae66987d6dd9954e1d2c7bf5522e88d83,Relationships between childhood adversity and life functioning in US college students: Risk and resilience,,,###Research on resilience following adult traumas 43 has been criticized for defining and measuring resilience in terms of single outcomes.,impact-revealing,highlighting a critique in resilience research methodology
939,5d1eb9e4da562961f0b1eb04,f80be25edf309ab595fc76fddd8cefe8eb2e5a54,GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation,5a260c8417c44a4ba8a310e2,FiLM: visual reasoning with a general conditioning layer,"Recently, “feature-wise linear modulation” (FiLM) was introduced in the visual question answering domain (Perez et al., 2017).###In the extreme case in which the dimen-sion of each chunk is 1, this method coincides with the ideas of Perez et al. (2017), who propose to use layers of element-wise afﬁne transformations to modulate feature maps in the visual question answering setting; there, a natural language question is the…",impact-revealing,highlighting the introduction of a method in visual question answering
1947,,753caee1d1ee83ded99b8534922f9f525248108e,Efficient flow scheduling in distributed deep learning training with echelon formation,,,"###The networking community has a long history of resolving bandwidth contentions with flow scheduling, from individual flow scheduling [8, 9, 20, 58] to Coflow scheduling [13, 14, 23, 34, 60].###EchelonFlow is motivated by Coflow [7, 13, 14, 23, 34, 60, 64], but deviates from Coflow’s assumption that all flows in a collective group have a common finish time.###As discussed in § 3.3, we expect the coordinator to run a heuristic algorithm adapted from Coflow scheduling, e.g., MADD [14].###In MADD [14], for example, in intra-EchelonFlow scheduling, we estimate the latest flow that has the largest tardiness, rather than the longest flow completion time as for Coflow; in inter-EchelonFlow scheduling, we rank EchelonFlows by each EchelonFlow’s tardiness (Eq.###Coflow scheduling is NP-hard [14], so the superset problem EchelonFlow scheduling is also NP-hard.",impact-revealing,highlighting the evolution and challenges in flow scheduling within the networking community
714,5b67b45517c44aac1c86084b,af8a8dcb74561d52d904f7bc4afcc747e079b702,modeling task relationships in multi-task learning with multi-gate mixture-of-experts,53e997b2b7602d9701f91200,Multitask Learning,"Researchers have reported multi-task learning models can improve model predictions on all tasks by utilizing regularization and transfer learning [8].###Prior works [4, 6, 8] investigated task di erences in multi-task learning by assuming particular data generation processes for each task, measuring task di erences according to the assumption, and then making suggestions based on how di erent the tasks are.###Doing so can result in both improved e ciency and model quality for each task [4, 8, 30].###One of the widely used multi-task learning models is proposed by Caruana [8, 9], which has a shared-bottom model structure, where the bottom hidden layers are shared across tasks.###The backbone of MMoE is built upon the most commonly used Shared-Bottom multi-task DNN structure [8].###We rst introduce the shared-bottom multi-task model in Figure 1 (a), which is a framework proposed by Rich Caruana [8] and widely adopted in many multi-task learning applications [18, 29].",impact-revealing,acknowledge existing research on multi-task learning models
3422,5e3be3c33a55ac29c4ae7e18,6dbdc34000b034b75b8ff70872fc7c35549e273a,Interpretable & Time-Budget-Constrained Contextualization For Re-Ranking,58437713ac44360f1082cc52,Quality versus efficiency in document scoring with learning-to-rank models.,"This includes applying a temporal constraint on the number of features that are selected for a re-ranking model [35], incorporating an efficiency metric in the training of linear rankers [34], and comparing the effectiveness and efficiency of various learning-to-rank algorithms [4].###In traditional learning-to-rank the trade-off between effectiveness and efficiency has been thoroughly studied [34, 35, 37, 4].",other,acknowledge existing research on learning-to-rank algorithms
2236,5ec49a639fced0a24b4de7ed,1eed0659d561354d5c471a723cf3381430561d04,Graph Neural News Recommendation with Unsupervised Preference Disentanglement,53e9a5cdb7602d9702ef59bb,Personalized News Recommendation Based On Click Behavior,"In order to tackle the problem of information over-load and meet the needs of users, news recommendation has been playing an increasingly important role for mining users’ reading interest and providing personalized contents (IJntema et al., 2010; Liu et al., 2010).###load and meet the needs of users, news recommendation has been playing an increasingly important role for mining users’ reading interest and providing personalized contents (IJntema et al., 2010; Liu et al., 2010).",other,highlighting the significance of news recommendation in addressing information overload
1370,,7f0707794c374c9908acdc8b449db7f32db89596,Optimization of Fenton Oxidation Process for Biodegradability Enhancement of a Mature Landfill Leachate Using Response Surface Methodology,,,"###Therefore biological treatment methods (anaerobic and aerobic) are commonly applied for treatment of young leachate (De Morais and Zamora, 2005).",impact-revealing,highlighting the application of biological treatment methods for leachate
1706,,27e7805d538336aa6178f7549b2d6de390435961,Linguistic Synchrony in Parasocial Interaction,,,"###It is an extension of social identity theory (Tajfel, 1982; Tajfel & Turner, 1979) and assumes behavior is motivated by the desire to be viewed favorably by socially desirable others.",impact-revealing,providing context for social identity theory
2799,5a9cb60d17c44a376ffb35be,b8bbef8e62b4ef342243666f39b205de9f20eb8c,MSTM: A novel map matching approach for low-sampling-rate trajectories,53e99a2bb7602d9702284f85,Discovering Similar Multidimensional Trajectories,"Usually, when object moves, it tend to be close to the max speed limit of the road[18].###Secondly, we use the algorithm IVMM to generate the ptrue and calculate the value of LCSSivmm(Ptrue, Pivmm) and LCSSmstm(Ptrue, Pmstm) again.###We calculate the LCSS [18] similarity of LCSSst(Ptrue, Pst) and LCSSmstm(Ptrue, Pmstm) as the accuracy.###Obviously, the large the value of LCSS is, the similar the path to the true path is.###Finally, we use the mean value of LCSSmstm(Ptrue, Pmstm) as the final accuracy.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2699,58d82fced649053542fd692f,29e944711a354c396fad71936f536e83025b6ce0,categorical reparameterization with gumbel-softmax,58d82fc8d649053542fd5ae7,Regularizing Neural Networks by Penalizing Confident Output Distributions.,"The practical outcome of this paper is a simple, differentiable approximate sampling mechanism for categorical variables that can be integrated into neural networks and trained using standard back-propagation.###If τ is a learned parameter (rather than annealed via a ﬁxed schedule), this scheme can be interpreted as entropy regularization (Szegedy et al., 2015; Pereyra et al., 2016), where the Gumbel-Softmax distribution can adaptively adjust the “conﬁdence” of proposed samples during the training process.",other,describing a practical outcome and its implications for neural networks
2169,,1c1628938e56417042fa3c9f37ca29767203ff89,A Visualization Review of Cloud Computing Algorithms in the Last Decade,,,"###Vouk [5] discussed the concept of cloud computing from the aspects of service-oriented architecture, components, workﬂows, virtualization, and users.###Issues Needed to Be Solved Vouk [5] ... embraces cyberinfrastructure, and builds upon... research in virtualization, distributed computing, “grid computing”, utility computing, and, more recently, networking, Web and software services.###In the literature, various aspects on cloud computing have been reported, such as service availability [5,6,16], computing schemes and algorithms [17,18], data security [10–12, 19 ],###To summarize the extant studies and ﬁnd potential research directions, some review works related to cloud computing have been consecutively reported [5–15].",impact-revealing,summarizing various aspects and research directions in cloud computing
2523,5dbebb7447c8f766462c2328,b0d941cfa0a3c43703e5222221addb1ad0f9e68d,Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks,5a260c8117c44a4ba8a30f54,Graph Attention Networks.,"In recent years, some baseline methods on GNN, for example, GCN [12] and GAT [33], are demonstrated to be capable of extracting features of the graph.###In recent years, many GNN methods [6, 12, 18, 33, 39] work under the mechanism that is similar to message passing network [3] to compute the information flow between nodes via edges.###, GCN [12], GAT [33] and gated graph networks [18, 38].###As suggested in previous work [32, 33], the multi-head attention can help to stabilize the training of the self-attention layers.",other,acknowledge advancements in GNN methods and their feature extraction capabilities
34,5e09a83ddf1a9c0c41685fc3,90820988957e74db0fbb1df79175610e08016ba4,Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling,57fdf41c654a3f2774eccd6b,A Deep Relevance Matching Model for Ad-hoc Retrieval,"Base-lines include the classic query likelihood (QL) method, RM3 query expansion (Abdul-Jaleel et al., 2004), learning to rank (L2R), as well as a number of neural ranking models: DRMM (Guo et al., 2016), DUET (Mitra et al., 2017), K-NRM (Xiong et al., 2017b), and PACRR (Hui et al., 2017).###It has been observed that existing approaches for textual similarity modeling in NLP can produce poor results for IR tasks (Guo et al., 2016), and vice versa (Htut et al., 2018).###Inspired by DRMM, Xiong et al. (2017b) proposed K-NRM, which introduced a differentiable kernel-based pooling technique to capture matching signals at different strength levels.###Since relevance matching is fundamentally a matching task, most recent neural architectures, such as DRMM (Guo et al., 2016) and Co-PACRR (Hui et al., 2018), adopt an interaction-based design.###Speciﬁcally, Guo et al. (2016) point out three distinguishing characteristics of relevance matching: exact match signals, query term importance, and diverse matching requirements.###For example, DRMM (Guo et al., 2016) introduced a pyramid pooling technique to convert the similarity matrix into histogram representations, on top of which a term gating network aggregates weighted matching signals from different query terms.###It’s worth noting that term importance modeling can be important for some search tasks (Guo et al., 2016); therefore, we inject external weights as priors to measure the relative importance of different query terms and phrases.",impact-revealing,acknowledge existing methods and their limitations in textual similarity modeling
554,5c2c7a9217c44a4e7cf3161b,e6926981ef9c1d06d6c075cdae7b298d3dbf3a7d,Learning Latent Representations for Style Control and Transfer in End-to-end Speech Synthesis,5aed14d617c44a4438159517,Expressive Speech Synthesis Via Modeling Expressions With Variational Autoencoder,"[12] which combines an autoregressive speech synthesis model with VAE for expressive speech synthesis.###Paper submitted to IEEE ICASSP 2019 ? Work done during internship at Microsoft STC Asia text generation [9], image generation [10, 11] and speech generation [12, 13] tasks.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2604,5b67b45517c44aac1c86084b,af8a8dcb74561d52d904f7bc4afcc747e079b702,modeling task relationships in multi-task learning with multi-gate mixture-of-experts,58d82fc8d649053542fd5c5a,PathNet: Evolution Channels Gradient Descent in Super Neural Networks.,"Similarly, PathNet [17], which is designed for arti cial general intelligence to handle di erent tasks, is a huge neural network with multiple layers and multiple submodules within each layer.",other,providing context on PathNet architecture
336,5a73cb7417c44a0b3035a202,c1cb7a1efb1a47348e3a25c21ff0a3ff192d7058,Image Super-Resolution Using Dense Skip Connections,558c4a2384ae6766fdf2358f,Image Super-Resolution Using Deep Convolutional Networks,"In addition, in previous works [2, 11], only high-level features at top layers were used in the reconstruction of HR images.###Instead of using interpolation for upscaling as in [2, 11], recent studies [3, 21] have demonstrated that the SR performance can be further improved both in terms of accuracy and speed by learning the upscaling filters.###Recent works [11, 12] have successfully used very deep convolutional neural networks (CNN) to perform single image super-resolution (SISR), and significant improvements over shallow CNN structures [2] have been observed.###In previous SR methods such as SRCNN [2] and VDSR [11], bicubic interpolation is used to upscale LR images to the HR space.###Dataset Bicubic Aplus [24] SRCNN [2] VDSR [11] DRCN [12] SRDenseNet All###0 dB using the proposed method was achieved over SRCNN [2] with 3-layer CNN and an increase of about 0.###A network with three layers was first developed in [2] to learn an end-to-end mapping for SR.###As in previous methods [2, 11], only the feature maps at the top layer are used as input for reconstructing the HR output.###We compared the results using the proposed method and those using other SISR methods, including bicubic, Aplus [24], SRCNN [2], VDSR [11] and DRCN [12].###In addition to sparsity-based methods, other sophisticated learning techniques have been developed to model the mapping from LR to HR space, including neighbor embedding [4], random forest [20] and convolutional neural network [2].",impact-revealing,acknowledge advancements in super-resolution methods and their comparative performance
386,5e9ef9b69fced0a24b1b65a2,69f1ab7fd22c3df3c9900430566be890e1529b4e,NetTaxo: Automated Topic Taxonomy Construction from Text-Rich Network,58d82fd2d649053542fd7486,Higher-Order Organization Of Complex Networks,"Meta-paths [31] and motif patterns [5, 18] have been widely adopted to extract useful structural information from networks.###Network motifs are higher-order subgraph structures that are critical in complex networks across various domains, such as neuroscience [30], bioinformatics [18], and information networks [5].",impact-revealing,highlighting the significance of network motifs in various domains
2772,5d0b00ed8607575390fedd4e,493d5f344eea1468260946b29a80dc81b2be409c,HopSkipJumpAttack: A Query-Efficient Decision-Based Attack,57a4e91aac44365e35c975d0,Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples.,"Papernot et al. [12, 13] proposed to train a substitute model by querying the target model.",other,reporting prior findings on substitute model training
724,5a260c3b17c44a4ba8a26007,d0034ccbf6d5929242de043b39acf48249d99db0,Load Value Prediction via Path-based Address Prediction: Avoiding Mispredictions due to Conflicting Stores,53e9adcdb7602d97037d40f9,Value Locality and Load Value Prediction,"For the loads that belong to the ﬁrst sequence, a conventional value predictor (e.g., Last-Value-Predictor, LVP [20]) might mispredict the second load’s value because the value has been changed by the interleaving store.###1 Value Prediction Since the introduction of value prediction [12, 20], there has been a plethora of work on this subject.###Value prediction was proposed to address this limitation [12, 20].###, Last-Value-Predictor, LVP [20]) might mispredict the second load’s value because the value has been changed by the interleaving store.",impact-revealing,acknowledge the development and challenges in value prediction
1876,,bf9927d91da1b4d035a3d3fc745529e2ed094b78,Emotionally intelligent nurse leadership: a literature review study.,,,"###The methodology employed in this literature review is inspired by Whittemore and Knafl (2005), Akerjordet and Severinsson (2007a) and Høye and Severinsson (2007) and the result of the study is primarily a qualitative synthesis (Fink 2005).",impact-revealing,describing the methodology for the literature review
3483,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",558c6eb5e4b00c3c48e23f5a,Reduced dimension policy iteration for wireless network control via multiscale analysis,"example, several papers have explored the use of GSP techniques to improve the efficiency of value function estimation in a reinforcement learning scenario [24], [25].",other,acknowledge prior research on GSP techniques in reinforcement learning
3993,5d3ed25a275ded87f97deaab,025ea689e6ab3b544101df17233e87536a1e578a,Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation,573695fe6e3b12023e5125f8,A Survey of Heterogeneous Information Network Analysis,"In this paper, we propose to model the intent recommendation system with a HIN, through which we can flexibly exploit its rich interaction information.###In order to solve the challenges in intent recommendation, we model objects and interactions in intent recommendation system with a HIN and propose a novel metapath-guided GNN method for intent recommendation, called MEIRec.###It demonstrates that we should consider heterogeneity of objects in HIN for better performances.###As a general information modeling method, Heterogeneous Information Network (HIN) [18], consisting of multiple types of objects and links, has been widely applied to many data mining tasks [10, 17, 18].###As shown in Figure 2(a), obviously, HIN clearly demonstrates objects in intent recommendation (e.g., users, items and queries) and their interaction relations, such as “user click item”, “user search query” and “query guide item”.###More-over, the recommended query may be not previous queries Figure 2(a) shows a toy example of HIN and Figure 2(b) is the corresponding network schema.###Although, some HIN based recommendation methods have been proposed [8, 19, 23], they mainly employ metapath based features through exploiting the interaction relations between users and items, which makes them hardly interactions in intent recommendation. queries with heterogeneous Graph Neural Network (GNN).###Through modeling modeling intent recommendation system as a HIN , MEIRec utilizes metapath-guided neighbours to exploit rich interaction information in HIN.###3.1 Overview The basic idea of the proposed model MEIRec is to design a heterogeneous GNN for enriching the representations of users and queries With the help of HIN built from intent recommendation system, MEIRec leverages metapaths to guide the selection of different-step neighbors and designs a heterogeneous GNN to obtain the rich###They only utilize attribute and statistic information of users and queries, and fail to take full advantage Information Network (HIN) [18], consisting of multiple types of objects and links , has been widely applied to many data mining tasks [10, 17, 18].###MEIRec utilizes metapath-guided neighbours to exploit rich structural structural information in HIN.",other,introducing a novel intent recommendation system using Heterogeneous Information Networks
3506,5c04967517c44a2c74709321,54c4642d017830e1faddbb49f0377228d2b01493,HAQ: Hardware-Aware Automated Quantization With Mixed Precision,5a260c8617c44a4ba8a32308,Learning Efficient Convolutional Networks through Network Slimming,"However, they are not able to directly reflect the latency, since there are many other factors influencing the hardware performance, such as memory access cost and degree of parallelism [25, 21].###For instance, the coarse-grained channel pruning methods [12, 19, 21] prune away the entire channel of convolution kernels to achieve speedup.",other,highlighting limitations in existing methods for reflecting latency
974,,9246100b598369863f476d584a39fa88ea7fddad,What really is going on? Review of situation awareness models for individuals and teams,,,"###…rarely performed entirely independently of others, especially in complex situations and when critical decision making is required (Artman and Garbis 1998) – these activities tend to require coordinated activity between several individuals (Cannon-Bowers and Salas 1990 cited in Salas et al. 1995).###Based on a review of the literature, Salas et al. (1995) proposed a framework of team SA, suggesting that it comprises two critical, but poorly understood, processes, individual SA and team processes.###Salas et al. (1995) point out that there is a lot more to team SA than merely combining individual team member SA.###According to Salas et al. (1995), team SA depends on communications at various levels.###Salas et al. (1995) also highlight the importance of team processes such as communication, assertiveness and planning, all of which they suggest contribute to the acquisition and maintenance of team SA.###Salas et al. (1995) also highlighted the importance of communication in team SA acquisition.###Salas et al. (1995) suggest that schema limitations can be offset by information exchange and communication, the information to support this being provided by communication and coordination between team members.",impact-revealing,highlighting the importance of communication and team processes in situational awareness
1967,,cfa1f783ea47aa318800b1be10f4d0ad1b4d0e53,A systematic review of state-of-the-art noise removal techniques in digital images,,,"###This filter cannot preserve the edges properly at high densities because this filter does not consider the local details while performing its operations [12, 20, 29, 31].###Noise detection algorithm used in the filter decides corrupted pixel, uncorrupted pixel and how the correction algorithm use median equation [12, 20, 29, 31].",impact-revealing,highlighting limitations of the filter in preserving edges at high densities
1036,,26c160bcc24ff1b7d5901dd513a65e140a0a6321,"Socio-cultural adaptation, academic adaptation and satisfaction of international higher degree research students in Australia",,,"###To build on the semi-structured interviews, a participant solicited diary method was employed (Jacelon & Imperio, 2005).",impact-revealing,describing the method used for data collection
2355,558c9cd8e4b0cfb70a1eab9c,beb69e174c75a19299c446a427425edfd55209f2,sampling dead block prediction for last-level caches,53e9bd92b7602d9704a386c6,Cache Bursts: A New Approach For Eliminating Dead Blocks And Increasing Cache Efficiency,"In keeping with the methodology of recent cache papers [12], [13], [20], [19], [15], [9], [11], [7], [8], we choose a memory-intensive subset of the benchmarks.###3) Poor Performance for Trace-Based Predictor: Note that the reftrace predictor performs quite poorly compared with its observed behavior in previous work [15].###3) Cache Burst Predictor: Cache bursts [15] can be used with trace, counting, or time based dead block predictors.###Previous work introduced several dead block predictors and applied them to problems such as prefetching and block replacement [13], [15], [11], [5], [1].###The experiments model a 16-way set-associative last-level cache to remain consistent with other previous work [12], [15], [19], [20].###Dead blocks lead to poor cache efficiency [15], [4].###techniques for counting predictors as well as cache-bursts predictors [15] at all levels of the memory hierarchy.",other,acknowledge existing methodologies in cache performance evaluation
2869,599e96ec9c05cae4992b4ef3,95b803d07c37e8349bd7b1318367d8237c76cbc0,Audio-driven facial animation by joint end-to-end learning of pose and emotion,573696ce6e3b12023e5ce95a,Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift,"Drawing on the idea of batch normalization [Ioffe and Szegedy 2015], we remove this trivial solution by normalizing R′(x) with respect to the observed magnitude of e(x):
R(x) = R′(x) / ©« 1EB E∑ i=1 B∑ j=1 e(i)(x j )2ª®¬ (4) Normalization.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1712,,bbe4b6d372d40c41397519784e3f47bae8aeba48,Conceptualizing a Secure Wireless Cloud,,,"###CLOUD COMPUTING “Cloud” computing is a relatively recent term and builds on decades of research in virtualization, distributed computing, utility computing, and more recently networking, web and software services [4, 32].",impact-revealing,providing context on the evolution of cloud computing
1170,,7d9aac6f4ee16d2e93f4243de70e9bec9285a0ca,Multikernel Adaptive Filters Under the Minimum Cauchy Kernel Loss Criterion,,,"###However, the higher order error based loss is not suitable for Gaussian and heavy-tailed noises [5], and the lower order error based loss may provide slow convergence rate in Gaussian noises [6].###The higher order error [4], [5] and lower order error [6], [7] based loss functions are the simplest two types of non-quadratic loss functions.",impact-revealing,highlighting limitations of higher and lower order error based loss functions
1831,,bebb9cd9c0200f6db96d45703a81b93d082ab5f5,A school-based program implemented by community providers previously trained for the prevention of eating and weight-related problems in secondary-school adolescents: the MABIC study protocol,,,"###The two-subscale structure was supported by means of confirmatory factorial analysis, and revealed a good fit, excluding Item 1 and 6 from the POTS-WT subscale (CFI = 0.97; TLI = 0.96; RMSA = 0.038).###Internal consistency was .86 and .76 for the POTS-S-WT and POTS-S-CT subscales, respectively.###The POTS [110] is also the most widely used scale for determining changes in appearance-related teasing (B of MABIC) in this preventive field [47].###As was the case with the Spanish version of the SATAQ-3, the POTS-S version showed invariance by sex and grade, giving support for its use with both boys and girls throughout a large portion of adolescence.###The 9-item Spanish version of the POTS (POTS-S), also validated by our research team, was used for this study [111].###The original scale yields a six-item Weight-Related Teasing subscale (POTS-WT) (e.g., “People made jokes about you being too heavy”) and a five-item Competency-Related Teasing subscale (POTS-CT) (e.g., “People said you acted dumb”).###The POTS-S showed good correlation indexes with other, related variables.",impact-revealing,reporting validation and findings of the POTS scale
1409,,566528fd93eef379efa27031a6bd520488a551a3,Distributed Offline Reinforcement Learning,,,"###Along the direction of offline RL, off-policy RL such as Qlearning can in principle enable an agent to learn from large and fixed datasets, which however usually lead to overfitting to the dataset and thus result in large extrapolation errors [2]–[4].###In order to account for distributional shift in offline RL, one introduces the following term as in [4]###Offline RL by learning from large, previously collected datasets without further interaction with the environment offers an alternative to classical reinforcement learning [2], [4]–[6].###In practice, such interactions can be costly and sometimes dangerous since it might lead to exploration of unsafe actions [2]–[4].###As such, this term attempts to prevent learning a policy that is biased towards actions that are not in the dataset [4], since we do not know if these actions actually lead to higher rewards.",impact-revealing,highlighting the challenges and considerations in offline reinforcement learning
3749,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,599c7a82601a182cd26bf962,Inside 6th-Generation Intel Core: New Microarchitecture Code-Named Skylake.,"We perform our characterization on 18- and 20-core Intel Skylake processor platforms [31], Skylake18 and Skylake20.",other,reporting the specific hardware used for characterization
2282,5f7fdd328de39f0828397e7f,9fa283d4f9c2ed991383c0434ef6043bee0dc8e2,Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification,5c75721bf56def97987de7ad,Multi-Scale Dense Networks for Resource Efficient Image Classification,"We consider two settings to evaluate our method: (1) budgeted batch classification [20], where the network needs to classify a set of test samples within a given computational budget; (2) anytime prediction [13, 20], where the network can be forced to output a prediction at any given point in time.###We compare our method with another adaptive inference architecture, MSDNet [20], under the Anytime prediction setting in Figure 4 (f), where the GFNet is based on a DenseNet121.###In the context of budgeted batch classification [20], the model needs to classify a set of samples Dtest within a given computational budget B>0, leading to the constraint |Dtest| ∑ tqtCt≤B.###) in the budgeted batch classification setting [20], where the test set comes with a given computational budget, and the anytime prediction setting [13, 20], where the network can be forced to output a prediction at any given point in time.###, maxj ptj , (treated as confidence following earlier works [20, 64]) is compared to a pre-defined threshold ηt.###For example, MSDNet [20] and its variants [32, 64] introduce a multi-scale architecture with multiple classifiers that enables it to adopt small networks for easy samples while switch to large models for hard ones.###In our implementation, following [20], we let qt = z(1 − q)t−1q, where z is a normalizing constant to ensure ∑ tqt=1, and 0<q<1 is an exit probability to be solved.###, MnasNets [49], ShuffleNets-V2 [36], MobileNets-V2 [43], CondenseNets [21], FBNets [59], ProxylessNAS [5], SkipNet [55], SACT [10], GoogLeNet [48] and MSDNet [20].###The plot shows that GFNet achieves ∼ 4− 10% higher accuracy than MSDNet when the budget ranges from 5× 108 to 2.2× 109 Multiply-Adds.###For fair comparison, here we hold out 50,000 training images, following [20] (we do not do so in Figure 4 (e), and hence the Budgeted batch classification comparisons between GFNet and MSDNet are deferred to Appendix B.2).###We also compare GFNet with a number of highly competitive baselines, i.e., MnasNets [49], ShuffleNets-V2 [36], MobileNets-V2 [43], CondenseNets [21], FBNets [59], ProxylessNAS [5], SkipNet [55], SACT [10], GoogLeNet [48] and MSDNet [20].###As discussed in [20], these two settings are ubiquitous in many real-world applications.",other,describing evaluation settings for the proposed method
3684,5d3ed25a275ded87f97deae9,2c6097792ed9e4e8a664ce2dc7492377bfd57139,LightNet: A Dual Spatiotemporal Encoder Network Model for Lightning Prediction,573696f46e3b12023e5f12ae,Learning Spatiotemporal Features with 3D Convolutional Networks,"To better model spatiotemporal information, 3D ConvNets [29] extend conventional 2-dimensional convolution, which slides only in the spatial dimension, to 3-dimensional convolution, which moves in both spatial and temporal dimensions.###There are also some deep learning models focusing on spatiotemporal prediction tasks such as 3D ConvNets [29], PredCNN [27], DCRNN [26] and StepDeep [20].###StepDeep [20] is a prediction framework based on 3D ConvNets (3-dimensional convolutional networks), which are proposed for spatiotemporal feature learning using 3-dimensional (introducing time dimension) convolutional kernels.",other,providing context on 3D ConvNets and their applications
1710,,6dd88882815542120ff011068d6d99488d7c5888,Group Identity and the Moral Hazard Problem: Experimental Evidence,,,"###…builds upon a large literature in social psychology on how the perceptions of in-group and out-group originate (Tajfel et al., 1971; Tajfel and Turner, 1979).5 According to the group identity theory, members of one group compare their group (in-group) with other groups (out-group) with a…",impact-revealing,highlighting the foundational theories in social psychology related to group identity
2132,,a7fac5d8b6d830393186540210e8bec0da10405e,Multicommodity Eulerian-Lagrangian Large-Capacity Cell Transmission Model for En Route Traffic,,,"###It can be seen that the CTM(L) works very well: for 51% of the sectors, the mean errors are below one aircraft; for 99.65% of the sectors, the mean errors are below two aircraft.###The CTM(L) is based on the graph-theoretic multicommodity network model constructed from historical ASDI/ETMS traffic data, as described in Sec.###This paper presents a new model, large-capacity cell transmission model, or CTM(L), in which the two benefits outlined above are demonstrated.###Sector counts predicted by the CTM(L) are first compared with sector counts obtained from the recorded ASDI/ETMS data.###Several approaches have been proposed to solve this problem, in particular split coefficients [9] inspired by the highway transportation literature [13,30].###This model is motivated by a discretized version of the Lighthill–Whitham– Richards (LWR) partial differential equation (PDE) [16,17] and inspired by the Daganzo cell transmission model [12,13].###11 Comparison of the predictions of aircraft sector counts with the CTM(L) and ASDI/ETMS.###The multicommodity flow model makes it straightforward to incorporate different graph topologies into the CTM(L).###The terminology CTM(L) is in reference to the seminal Daganzo cell transmission model, commonly used for highway traffic [12,13].###The terminology CTM(L) is in reference to the seminal Daganzo cell transmission model, commonly used for highway traffic [12] [13].###In this section, the new Eulerian–Lagrangian model, the largecapacity cell transmission model, or CTM(L), is inspired by the Lighthill–Whitham–Richards theory [16,17] and by the Daganzo cell transmission model [12,13] commonly used in highway traffic.###Themodel presented in this paper could be viewed as an extension of the seminal Daganzo cell transmission model (widely used in the highway transportation literature [12,13]) for the air traffic control problem.###In the CTM(L) derived next, a link is understood in the graphtheoretic sense, that is, an edge of a graph [27].###2) Because the model takes into account the OD information, it does not have split parameters (usually called in the literature [9,13]) and eliminates the diffusion problem.###Note that this is very close to the approach taken by Daganzo in his definition of the original CTM [12,13].###Controllability and observability are important in CTM(L) for TFM.###As will be seen in the subsequent sections describing the proposed CTM(L), each link is divided into several cells.###5 Illustration of the CTM(L) at link level; everywhere inside the link, x p_x0087_1 i _x0085_k _x0087_ 1_x0086_ _x0088_ x p i _x0085_k_x0086_, unless some control action was applied.",impact-revealing,describing the performance and characteristics of the CTM(L) model
3704,599c7ea4601a182cd28b81a7,f323036340ad0ec30cafacd6b09a3e28379e24f1,maximizing cache performance under uncertainty,53e99aa6b7602d970232214c,Planning with Markov Decision Processes: An AI Perspective,"In particular, policy iteration [23] alternates between computing the expected total reward for each state and updating the policy.",other,providing context for policy iteration method
645,5c8b99db4895d9cbc69c7956,add350d0c5605c98d285b87493fc77c1d68281df,architectural support for server-side php processing,53e99924b7602d9702160b04,Thin servers with smart pipes: designing SoC accelerators for memcached,"In recent years, numerous research efforts have been devoted to optimizing warehouse-scale(WSC) and big data workloads [24, 36, 47, 53, 55, 56, 69] developed in C++-like compiled languages.###A hash table that supports only GET operation has been deployed in hardware before for memcached workloads [55].###CPU-like workloads contrary to the instruction cache behavior observed in prior works with other server-side applications (serversside Javascript applications [73] or memcached workloads [55]).###First, in contrast to the most large-scale memcached deployments [23, 55] where the GET requests vastly outnumber the SET and other request types, these PHP applications observe relatively higher percentage of SET requests (ranging from 15−25%) when generating dynamic contents for web pages.###As a result, we store the keys in the hash table itself, unlike the hash table designed for memcached deployments [55].###hardware to satisfy the unique access pattern of these PHP applications, contrary to prior works deploying a hash table that supports only GET requests in a memcached environment [55].",impact-revealing,acknowledge existing research on optimizing warehouse-scale workloads
3763,5d0b00ed8607575390fedd4e,493d5f344eea1468260946b29a80dc81b2be409c,HopSkipJumpAttack: A Query-Efficient Decision-Based Attack,58437722ac44360f1082f4d8,Towards Evaluating The Robustness Of Neural Networks,"The same target class and a common initialization image are used for all attacks. d) Metrics: The ﬁrst metric is the median (cid:96) p distance between perturbed and original samples over a subset of test images, which was commonly used in previous work, such as Carlini and Wagner [6].###While relying neither on training data nor on the assumption of transferability, this attack method achieves comparable performance with state-of-the-art white-box attacks such as C&W attack [6].###…adversarial training work most effectively. b) Baselines: We compare our algorithm with state-of-the-art attack algorithms that require access to gradients, including C&W Attack [6], DeepFool [4] for minimizing (cid:96) 2 -distance, and FGSM [2], and BIM [7, 41] for minimizing (cid:96) ∞ -distance.###Under this setting, the generation of adversarial examples is often formulated as an optimization problem, which is solved either via treating misclassiﬁcation loss as a regularization [1, 6] or via tackling the dual as a constrained optimization problem [2, 3, 7].###Standard choices of d studied in past work [2, 5, 6] include the usual (cid:96) p -norms, for p ∈ { 0 , 2 , ∞} .###We use the implementaion provided by Carlini and Wagner [6] for defensive distillation.###Changing the loss function allows for switching between two types of attacks [3, 5, 6].###As a comparison with state-of-the-art white-box targeted attacks, C&W attack [6] achieves an average (cid:96) 2 -distance of 0 .###As a comparison between data sets and models, we see that adversarial images often have a larger distance to their corresponding original images on MNIST than on CIFAR-10 and CIFAR-100, which has also been observed in previous work (e.g., [6]).###For defensive distillation, we include the (cid:96) ∞ -optimized C&W Attack [6].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2711,5b67b4b917c44aac1c867dbc,d18b48f77eb5c517a6d2c1fa434d2952a1b0a825,hierarchical graph representation learning with differentiable pooling,5a260c0917c44a4ba8a1df82,Predicting Organic Reaction Outcomes With Weisfeiler-Lehman Network,"GNNs have been applied to a wide variety of tasks, including node classification [16, 21], link prediction [31], graph classification [7, 11, 39], and chemoinformatics [14, 19, 27, 28, 32].",other,acknowledge applications of GNNs in various tasks
99,5550411c45ce0a409eb3897f,fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5,Neural Machine Translation by Jointly Learning to Align and Translate,558a6aa684ae84d265bd7b55,Bidirectional recurrent neural networks,"With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.###Hence, we propose to use a bidirectional RNN (BiRNN, Schuster and Paliwal, 1997), which has been successfully used recently in speech recognition (see, e.g., Graves et al. , 2013).",impact-revealing,describing a proposed method for information retrieval
830,5e54f1813a55acae32a25da5,76b8d5f2ef97d71167aa78309918bf3f7d633c96,Author Name Disambiguation on Heterogeneous Information Network with Adversarial Representation Learning,5b67b45517c44aac1c86078b,"Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop.","…similarity between each pair of papers using
the carefully designed pairwise features, including author names, titles, institute names etc.
AMiner (Zhang et al. 2018): This model designs a supervised global stage to fine-tune the word2vec result, and designs an unsupervised local stage based on…###The dataset is released by (Zhang et al.
2018), which contains 500 author names for training and 100 author names for testing.###And Zhang et al. (2018) actually transform the academic network into a homogeneous paper network after a complicated feature engineering.###, paper network (Zhang et al. 2018), paper-author network (Zhang and Al Hasan 2017).###Thus constructing the network becomes the critical part of these methods, e.g., paper network (Zhang et al. 2018), paper-author network (Zhang and Al Hasan 2017).###However, either complicated feature engineering or the supervision (Zhang et al. 2018) is required.###AMiner (Zhang et al. 2018): This model designs a supervised global stage to fine-tune the word2vec result, and designs an unsupervised local stage based on the first stage.###In the experiment on AMiner-AND, we use 100 names for testing and compare the result with the results of other models reported in (Zhang et al. 2018).###Zhang et al. (2018) construct paper networks, where the weights of edges are decided by a supervised model based on the sharing information between two papers.###The dataset is released by (Zhang et al. 2018), which contains 500 author names for training and 100 author names for testing.",impact-revealing,reporting on the methodology and dataset used in academic network construction
2713,5dce788a3a55ac9580a162f8,56cafbac34f2bb3f6a9828cd228ff281b810d6bb,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,5b1642d68fbcbf6e5a9b7edc,Accurate Text-Enhanced Knowledge Graph Representation Learning.,"…Zhong et al., 2019) and dialogue system (Madotto et al., 2018), but also some early works use text as additional information (Xie et al., 2016; An et al., 2018) or jointly train the knowledge and text embedding in the same space (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016;…",other,acknowledge existing research in dialogue systems and knowledge embedding
3064,53e9b413b7602d9703f0894b,c40d176e8c55399c45ac18f90d254438a26829e1,dynamic data dependence tracking and its application to branch prediction,53e9a3abb7602d9702cbe7eb,Focusing Processor Policies Via Critical-Path Prediction,"Other researchers, including Bodik [11], have proposed techniques for identifying critical instructions.###For instance, Bodik’s random sampling approach may unintentionally miss critical sequences.###These include dynamic scheduling, selective value prediction [6], criticality measures and their application [11, 29, 30], and decoupled architectures [3, 33] to name a few.",other,acknowledge existing techniques for identifying critical instructions
1700,,0e27a95894c23c11f8bbfff70d15a643ad18a14f,Partnership Formation: The Role of Social Status,,,"###In his study,
of group (or social) identity builds upon a large literature in social psychology on how the perceptions of in-group and out-group originate (Tajfel and Turner 1979, Tajfel et al. 1971).###If we had not included more than two groups, we would confound the effect of the caste divide among players with the in-group favoritism effect as evident in Tajfel and Turner (1986). For each of the above treatments, we subtly revealed each player’s social status, via showing a player’s last name to the coplayer in the instructions.###…in-group favoritism, a result repeatedly documented by studies on social identity in psychology and economics (Chen and Chen 2011; Tsutsui and Zizzo 2010; Chen and Li 2009; Charness et al. 2007; Bernhard et al. 2006; Goette et al. 2006; Brown 2000; Tajfel and Turner 1979, 1986; among others).",impact-revealing,acknowledging foundational theories in social psychology
1722,,f7f3f44d9a82d59de255d45ec4b4a0ddf35da4ae,Coordination via cooperative advertising and pricing in a manufacturer-retailer supply chain,,,"###Moreover, the classic model of Xie and Neyret (2009) was extended into a new form in which the special assumptions of linear price demand function and restrictive identical margins in the Nash and Stackelberg retailer games were relaxed.###It reveals that the cooperation is strongly infeasible for the most part,
17
which is significantly different from the results reported previously (Aust & Buscher, 2012; SeyedEsfahani et al., 2011; Xie & Neyret, 2009; Xie & Wei, 2009).###…aimed at developing a model that comprises most of the above mentioned factors in order to examine the manufacturer-retailer relationships in a supply chain (Aust & Buscher, 2012; Huang et al., 2002; SeyedEsfahani et al., 2011; Szmerekovsky & Zhang, 2009; Xie & Neyret, 2009; Yue et al., 2006).###This is contrary to the findings of Xie and Neyret (2009) and SeyedEsfahani et al. (2011).###An approach similar to Xie and Neyret (2009) but more precise is adopted here to present the numerical simulations and to illustrate the previous results.###The static models study the co-op advertising in a single period; examples include Berger (1972); Dant and Berger (1996); Bergen and John (1997); Huang and Li (2001); Huang et al. (2002); Karray and Zaccour (2006); and Xie and Neyret (2009).###However, we found that the cooperation can be feasible merely for
very low values of and , which is also different from the findings of Xie and Neyret (2009) who considered the special case of .###The work of Xie and Neyret (2009) is extended by considering a general price demand function and relaxing the assumption of equal margins in order to understand pricing impacts on channel members’ profits.###A similar approach was adopted by Xie and Neyret (2009).###very low values of and , which is also different from the findings of Xie and Neyret (2009) who considered the special case of .###Similar to the retailer margin, it can be seen again that the behavior of the manufacturer’s price curve is entirely stable compared to the one reported in Xie and Neyret (2009).###However, Xie and Neyret (2009) and SeyedEsfahani et al. (2011) found the lowest retail price concurrently in the cooperation game and in the case when the retailer was the leader.###N, SM and Co Xie and Wei (2009) ---- SM and Co Xie and Neyret (2009) Assumed N, SR, SM and Co SeyedEsfahani et al. (2011) Assumed N, SR, SM and Co Aust and Buscher (2012) Relaxed N, SR, SM and Co Proposed Model Relaxed N, SR, SM and Co
The rest of the paper is organized as follows.###6 Comparison of retail prices
This Subsection examines the assumption of equal margins in the Nash and Stackelberg retailer games as employed by Xie and Neyret (2009) and SeyedEsfahani et al. (2011).###Following Xie and Neyret (2009), we will employ an appropriate change of variables to handle the problem in an equivalent but more convenient way shown in Table 3.###Numerical simulations An approach similar to Xie and Neyret (2009) but more precise is adopted here to present the numerical simulations and to illustrate the previous results.###In Xie and Neyret (2009) , both retail price and relationship
between and are roughly estimated based on results of two games and extended for others.###While for , the wholesale price is consistent with the equivalent one in the classical model (Xie & Neyret, 2009), for
the results depict a substantial difference indicating that will decrease as the effectiveness of the
retailer’s local advertising increases.###This is a distinct result compared to those of Xie and Neyret (2009) and SeyedEsfahani et al. (2011) who assumed equal margins and those reported by Aust and Buscher (2012) who found either a higher, equal, or lower ratio between the manufacturer’s and the retailer’s margins in this game.###In contrast, we address the advertising-sales response function by utilizing the model proposed by Huang and Li (2001) which is very popular in the literature (Huang et al., 2002; Li et al., 2002; Szmerekovsky & Zhang, 2009; Xie & Ai, 2006; Xie & Neyret, 2009; Yue et al., 2013; Yue et al., 2006).###The current study is not only closely connected to the three papers cited above, namely Xie and Neyret (2009), SeyedEsfahani et al. (2011), and Aust and Buscher (2012), but also extends beyond by generating a number of insights.",impact-revealing,highlighting the extension and insights gained from previous models in supply chain relationships
1312,,ddc6ae299050ae1434727bc19cafdc3ba0417336,Emotional manifestations of PD: Neurobiological basis,,,"###Although a randomized, controlled study comparing paroxetine and nortriptyline showed superiority of nortriptyline in alleviating anxiety and depression,(43) another randomized, controlled study comparing desipramine and citalopram found improvement with both treatments.(48) A placebocontrolled study reported a suggestion of effectiveness of atomoxetine in PD.###studies on depression have shown that noradrenergic and serotonergic antidepressants also have a positive effect on anxiety.(41,43,48) Although a randomized, controlled study comparing paroxetine and nortriptyline showed superiority of nortriptyline in alleviating anxiety and depression,(43) another randomized, controlled study comparing desipramine and citalopram found improvement with both treatments.###When comparing selective serotonin uptake inhibitors with nonselective serotonin uptake inhibitors or a tricyclic antidepressants, the response to the latter seems to be faster, although their use is limited by their side effects.(41-45,48) A placebocontrolled study on atomoxetine, a selective noradrenaline reuptake inhibitor, for depressed PD patients, showed a trend toward greater improvement with atomoxetine.",impact-revealing,reporting findings from multiple studies on antidepressant effectiveness
277,5fd3404791e01161cf73952c,e988e15d200faf64bb71e155b8354c4e127f7dab,Bipartite Graph Embedding via Mutual Information Maximization,5c8dd94c4895d9cbc6a7d918,BiNE: Bipartite Network Embedding.,"k pretty well in the settings of homogeneous and heterogeneous graphs, most of them are not tailored for modeling bipartite graphs. As a result, they are suboptimal to learn bipartite graph embedding [7, 8]. To remedy such a problem, several studies have been specifically proposed for modeling bipartite graphs. They can be roughly divided into two branches: random walk-based and reconstruction-based met###it, we can observe that ML-10M is much larger than other datasets, since it is used to test whether our model can be deployed to large-scale bipartite graphs. 5.1.1 DataPreprocessing. As used in BiNE [7], we select 60% edges for training and remaining edges for test in both of DBLP and ML10M.WeusethesamedivisioninIGMC[42]forML-100K.Following experimental settings in the previous work [8], we split Wi###by paired t-test. learn node embeddings. DMGI [41] also follows the principle of MI maximization, and it uses the same infomax objective in DGI [36]. •Bipartite graph embedding: PinSage [40] and BiNE [7]. PinSage integrates random walk into GNN architectures for high-scalable performances. BiNE jointly optimizes explicit and implicit relations in a unified framework. •Matrix completion: GC-MC [34] an###re Metapath2vec [6] and DMGI [41]. But they are not tailored for bipartite graphs, and the structural characteristics of bipartite graph are hard to be preserved by them. IGE [44], PinSage [40], BiNE [7] and FOBE [32] are specially designed for bipartite graphs. However, as mentioned in the introduction, they mainly focus on how to model local graph structures in the latent space. Matrix completion [###thods achieve promising results to some extent, but they mainly focus on learning local graph structures with the assumption that nodes within the sliding window or neighborhoods are closely relevant [7, 37, 41]. We argue that they lack the capability of better modeling the global properties of bipartite graph including community structures of homogeneous nodes and longrange dependencies of heterogeneous nod",impact-revealing,acknowledge limitations in existing bipartite graph embedding methods
182,5550470045ce0a409eb63934,00dd475a3966857498b4404e48c118d88c8838f4,Psychological stress detection from cross-media microblog data using Deep Sparse Neural Network,53e99dabb7602d970266b3ae,Extracting and composing robust features with denoising autoencoders,We propose a Cross-media Auto-Encoder (CAE) to learn the joint representation using Denoising Auto-Encoder (DAE) style learning [15].,impact-revealing,introducing a new model for joint representation learning
1316,,6ac028a13ac729064ba3c29713f891cd9abe4f39,On the evolution of protein–adenine binding,,,"###Our focus on hydrogen bonds for characterizing binding patterns is motivated by the fact that such bonds, which are very common in proteins (8-11), are prevalent in ligand-binding sites and are important for the specificity of protein-ligand interactions (12-14).",impact-revealing,highlighting the significance of hydrogen bonds in protein-ligand interactions
2178,,118438829e7c8d18b4348676526d114b54303be6,Online C/C++ compiler using cloud computing,,,"###Cloud computing builds on decades of research in virtualization, distributed computing, utility computing, and more recently networking, web and software services.[1]###It implies a service oriented architecture, reduced information technology overhead for the end-user, great flexibility, reduced total cost of ownership and on-demand services among other advantages.[1] The National Institute of Standards and Technology (NIST) defines ‘Cloud Computing’ as ‘a model for enabling easy, on-demand network access to a shared pool of configurable computing resources (e.",impact-revealing,providing context and advantages of cloud computing
2083,,1c993ae5c9f8c05fdc34ea154420545bdd9742e5,Human Body Model based ID using Shape and Pose Parameters,,,###Our approach can be extended by using more sophisticated human body models (such as STAR [16] as compared to SMPL).,impact-revealing,suggesting potential improvements to the approach
3170,5a260c0917c44a4ba8a1e00e,93f9607034c9b7b7693c60e9d2631adc15a2a524,learning to model the tail,58d83020d649053542fe37a4,Learning Deep Representation For Imbalanced Classification,"An alternative practice is to introduce additional weights for different classes, which, however, makes optimization of the models very difficult in the large-scale recognition scenarios [28].###(3) Cost-sensitive [28], which introduces additional weights in the loss function for each class with inverse class frequency.###Method Plain [4] Over-Sampling [16, 17] Under-Sampling [27] Cost-Sensitive [28] MetaModelNet (Ours) Acc (%) 48.",other,discussing challenges in model optimization for large-scale recognition
2477,5dc5488edf1a9c0c41511e7e,59ce117f1c290075ee7bb67b8928344b37f788cc,the impact of cache inclusion policies on cache management techniques,53e9b3e2b7602d9703ed13b0,"Ncid: A Non-Inclusive Cache, Inclusive Directory Architecture For Flexible And Efficient Cache Hierarchies",proposed a non-inclusive cache with an inclusive directory to keep the positive features of both inclusive and non-inclusive policies [39].,other,reporting prior findings on cache policies
195,5ee8986891e011e66831c3b6,38f93092ece8eee9771e61c1edaf11b1293cae1b,Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,5dcd263a3a55ac58039516c5,Momentum Contrast for Unsupervised Visual Representation Learning,"Similarly, we evaluate on object detection by reproducing the setup in [9] using a Faster R-CNN architecture [82], as detailed in Appendix E.###Method AP50 mIoU
Supervised-IN [9] 74.4 74.4
MoCo [9] 74.9 72.5 SimCLR (repro) 75.2 75.2 BYOL (ours) 77.5 76.3
(a) Transfer results in semantic segmentation and object detection.###Contrastive methods often require comparing each example with many other examples to work well [9, 8] prompting the question of whether using negative pairs is necessary.###This sheds new light on the use of the target network in MoCo [9], where the target network is used to provide more negative examples.###Among discriminative methods, contrastive methods [9, 10, 32, 33, 34, 11, 35, 36] currently achieve state-of-the-art performance in self-supervised learning [37, 8, 38, 12].###These methods need careful treatment of negative pairs [13] by either relying on large batch sizes [8, 12], memory banks [9] or customized mining strategies [14, 15] to retrieve the negative pairs.###At the end of training, we only keep the encoder fθ; as in [9].###Among them, state-of-the-art contrastive methods [8, 9, 10, 11, 12] are trained by reducing the distance between representations of different augmented views of the same image (‘positive pairs’), and increasing the distance between representations of augmented views from different images (‘negative pairs’).###Finally, in self-supervised learning, MoCo [9] uses a slow-moving average network (momentum encoder) to maintain consistent representations of negative pairs drawn from a memory bank.",impact-revealing,acknowledge existing methods and their performance in self-supervised learning
2339,5db9298647c8f766461f8ed6,784b018c87c7dcbbe772374e45d5191bae9938ee,Hyperbolic Graph Neural Networks,53e9aa1db7602d970338a8a2,Hyperbolic geometry of complex networks.,"Typical properties such as heterogeneous degree distributions and strong clustering can often be explained by assuming an underlying hierarchy which is well captured in hyperbolic space [28, 35].###Furthermore, [27, 6, 28] exploited the property of hyperbolic embeddings to embed tree-like graphs with low distortion, for greedy-path routing in large-scale communication networks.###Hyperbolic geometry has also shown great promise in network science: [28] showed that typical properties of complex networks such as heterogeneous degree distributions and strong clustering can be explained by assuming an underlying hyperbolic geometry and used these insights to develop a geometric…",other,highlighting the significance of hyperbolic geometry in understanding complex networks
220,5dcbd5da3a55ac789b0dbc7f,f0efc23ecb6d4fb9745d555450b2c4a97e8ac4d5,Robust Design of Deep Neural Networks Against Adversarial Attacks Based on Lyapunov Theory,5c04967517c44a2c74708fc4,Generalizable Adversarial Training via Spectral Normalization.,"[11] uses PAC-Bayes generalization analysis to estimate the robustness of DNNs trained by spectral regularization against adversarial attacks.###Previous works [11, 28, 37, 8] have used a penalty on the spec-###[11] for instance, obtained an accuracy of 62% at ✏ = 0.###All the previous works on this subject [11, 28, 37, 8], keep constant across layers.###The closest to our work are the results given in [11, 28, 37, 8].",impact-revealing,reporting prior findings on DNN robustness against adversarial attacks
3202,5ee7495191e01198a507f7ae,09bda461aa4911d0513e8e46dd39a4113947e450,Ansor : Generating High-Performance Tensor Programs for Deep Learning,5e5e19c093d709897ce87ab9,Automatically scheduling halide image processing pipelines,"Halide has three versions of auto-scheduler based on different techniques [2, 31, 36].",other,reporting on different versions of auto-scheduler in Halide
2878,5da2f8a647c8f76646083cd9,b789abc47b7a92596050f6055a93c8fe1929db2a,Dynamic Multicontext Segmentation of Remote Sensing Images Based on Convolutional Networks,5fffb05f14528aee3382cadb,A REVIEW ON IMAGE SEGMENTATION TECHNIQUES WITH REMOTE SENSING PERSPECTIVE,"Given the importance of such task, several methods [9], [10] have been proposed for the semantic segmentation of remote sensing images.",other,acknowledging the significance of methods for semantic segmentation in remote sensing
3880,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,53e9a33cb7602d9702c4637e,Stager A Web Based Application for Presenting Network Statistics,"2006 [101] Network Network statistics of protocols, chart Network statistics",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1516,,3586df8e049cecdfc9afa03bf82afd7054e650f0,Comparison of survival outcomes among standard radiotherapy regimens in limited-stage small cell lung cancer patients receiving concurrent chemoradiation.,,,"###Accepted doses range from 45 to 70 Gy with daily or twice-daily fractionation [4–6].###Despite different approaches to dosentensification, the Intergroup, CALGB, and RTOG trials yielded imilar response rates (87%, 92%, and 82%, respectively), with early-identical failure-free survival (FFS) rates at two years of 9% in Intergroup 0096 and 31% in CALGB 39808 (FFS not reported n RTOG 9712) [4–6].###[4] As a result, the BID fractionation schema became a tandard regimen, although the trial has been criticized for the se of an inferior standard arm.###Primary analysis—comparison of standard CRT regimens The primary aim of this study was to compare OS between ublished CRT regimens from three published trials, Intergroup 096, CALGB 39808, and RTOG 9712 [4–6].###Available trial data has employed a wide range of doses and fractionation schedules, yielding similar median survival durations of approximately 23 months in patients receiving 45 Gy in 30 fractions delivered twice daily (BID) or 70 Gy in 35 fractions delivered on a daily basis [4,6].",impact-revealing,reporting on accepted doses and trial outcomes in radiation therapy
1615,,16436b1d9a512ba90da69af2622f3ee975902684,A Regularized Contrast Statistic for Object Boundary Estimation-Implementation and Statistical Evaluation,,,"###This is true for the problem of estimation of the head boundary in CT and ECT scans of the brain [ 26 ].###Since heads usually appear as roughly elliptical objects in emission computed tomography scans [ 26 ], our approach is to use an elliptical coordinate transformation to define the initial curve.###In medical imaging, for example, the most widely used approach for registration of brain images from X-ray computerized tomography (CT), magnetic resonance (MR), and emission computed tomography (ECT) is based on correlation head boundaries derived from the different imaging modalities [ 26 ].###There are several applications where this assumption is reasonable [3], [ 26 ].###We are motivated by applications in emission computed tomography, wherein the target object is to a first approximation star shaped [ 26 ].",impact-revealing,highlighting the significance of elliptical coordinate transformation in medical imaging
1852,,57abf865600e4bbdeb26964460a39871acd6ab71,Understanding in counselling: a preliminary social constructionist and conversation analytic examination,,,"###Two theoretical ideas served as backdrop for the present study: Wittgenstein’s (1958) notion that what is needed are conversations that move forward satisfactorily for the speakers involved, and Schegloff’s (1991) notion that a search for understanding is often cued up by one or another speaker’s feeling that the conversation is not moving forward satisfactorily. In CA terms, Schegloff is talking about the idea of repair, as in when speakers try to settle things between them. But here one can see how such repairs are more than conceptual; they can be about restoring a sense of mutuality or rapport between those speaking. To many social constructionists, conversation is where some things are talked into significance while others are not. This holds for how words, in endeavours such as understanding, are signified through their acknowledged use in conversation. An example of this, from Vygotsky (1967), occurs in how mothers and infants develop shared (or coordinated) words and language through what the mother deliberately attends to, acknowledges and builds on in the course of their communicative interactions.###Schegloff (1991), one of the principal figures in CA, suggested we abandon trying to know conceptually what goes on inside another’s thinking or feeling and instead attend to how people know (or show) when they are not being understood, and what they do to get back to feeling understood by each…###Two theoretical ideas served as backdrop for the present study: Wittgenstein’s (1958) notion that what is needed are conversations that move forward satisfactorily for the speakers involved, and Schegloff’s (1991) notion that a search for understanding is often cued up by one or another speaker’s feeling that the conversation is not moving forward satisfactorily.###In broader terms, Michael Billig (1999, a discourse analyst) and Emmanuel Schegloff (1999, a conversation analyst) took up these themes in a classic debate in the journal Discourse & Society in 1999.###In broader terms, Michael Billig (1999, a discourse analyst) and Emmanuel Schegloff (1999, a conversation analyst) took up these themes in a classic debate in the journal Discourse & Society in 1999. Having taken up the conversation analytic position, I acknowledge that concerns about power can be a feature of talk, but following Schegloff (1999) what matters analytically for the conversation analyst is how speakers make such power differentials evident and consequential on their talk.",impact-revealing,acknowledging theoretical frameworks in conversation analysis
3713,5e524da993d709897cb69fde,09e48adf1a3f012ec56fbb4f92acae975ee43d87,a deep learning approach to antibiotic discovery,53e99ed1b7602d970279a7fe,Efficient algorithms for accurate hierarchical clustering of huge datasets: tackling the entire protein space.,"The average linkage uses the algorithm called unweighted pair group method with arithmetic mean (UPGMA), which is the most popular and preferred algorithm for hierarchical data clustering (Jaskowiak et al., 2014; Loewenstein et al., 2008).",other,describing a widely used algorithm for hierarchical data clustering
1375,,01a8e5893fe7610ed2fd5e8c3d69b155bf0297dc,Correlation Matrix Analysis Identifies Gene Signatures of Immune Cell Subsets and Their Interactions in Follicular Lymphoma,,,"###In another approach to cross compared to those of T cell signatures in the NB fraction; other signatures were not considered, since the methods used to prepare the fractions did n potential presence of non-B, non approach also differed in that the metric for comparison was not based solely on the Pearson r value, but also considered signature as described in the
T FH cells support proliferation of FL B
previously reported to be
FL.(85, 86) The other group of NB fraction
with the hyperge
-correlated group of B fraction genes were
genes in FL tumors
-correlation, levels of individual B fraction genes were
-###This unusual behavior for a malignancy is related to the complex interaction between the benign immune system and FL.###(87) Other investigators have previously found
similarities between T FH cells and subsets of T reg cells (Follicular T reg , T FR ), both which express CD4 + /CXCR5 + /ICOS + cells in FL.(91, 93) We found substantial differences in our CMA results of the whole tumor Dave dataset and of the separated 43 FL samples from our dataset.###Eventually, escape mechanisms develop resulting in progression of FL. Numerous recent studies support the hypothesis that the immune system plays a significant role in the control of FL.###Third, the FLIPI had a primary endpoint of OS, which is challenging to study in an indolent disease like FL.###(6) In addition, these experiments identified mutations in another histone-modifying enzyme, CREBBP, that appear to be early events in the clonal evolution of FL.###Although Dave et al. observed that “the immune-response 1 signature is not merely a surrogate for the number of T cells in the tumor-biopsy specimen, since many other standard T-cell genes…were not associated with survival,” to date GEP has not revealed specific mechanisms affecting the immune response in FL.###We recently reported that pidilizumab, a monoclonal antibody directed against programmed death receptor 1 (PD1), a co-inhibitory receptor expressed by activated T cells, B cells, NK cells, and myeloid cells, had minimal toxicity and impressive efficacy when combined with rituximab in relapsed FL.(42) Pidilizumab is a humanized IgG1kappa recombinant monoclonal antibody that blocks the interaction of PD-1 with its ligands.###(7) Up to 50% of healthy individuals harbor circulating cells that contain t(14;18) translocation, and rarely progress to FL. (8-12) Peripheral blood cells which harbor the t(14;18) translocation in healthy individuals were thought to represent naïve B cells, but this assumption has been challenged by the discovery of many similarities to FL cells, including class-switch recombination and surface expression of IgM and IgD.###Other immunomodulating agents have shown preliminary impressive activity against FL.###Of interest, plasmacytoid dendritic cells (pDC) manifested a small self-correlating gene signature, despite their typically only representing ~0.1% of all cellular events by flow cytometry (FACS) in FL.###Further improvements to cancer vaccine therapy are needed if it is to be a viable treatment strategy for patients with FL.###It is unclear if this potential lead time bias will ultimately have any impact on the natural history of FL. Follicular lymphoma is characterized by the Ann Arbor staging system, originally devised nearly 40 years ago to account for radiation field size.###(67) Finally, tumor-specific T cells can be isolated from the tumor microenvironment and
peripheral blood in patients with FL.(68, 69) Together, these data suggest that an endogenous antitumor immune response is present in patients who harbor FL, but eventually becomes ineffective at controlling the tumor.###The prognostic significance of immune-response signatures has largely been validated by subsequent studies using microarrays and comparable techniques to measure mRNA (qRT-PCR),(64) including in patients treated with Rituximab,(65) and is among the evidence that the immune environment is an important determinant of outcome in FL.###The Food and Drug Administration (FDA) approved immunomodulatory drug lenalidomide has shown promising efficacy in both newly diagnosed and relapsed FL.(36-39) A phase II trial in newly diagnosed FL patients found that lenalidomide and rituximab had an ORR of 98% and CR of 87%.###The work in this thesis demonstrates that the novel approach of CMA can discern signatures consisting of genes attributable to various immune cell types in FL.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2227,5f02f17c91e011ee5e0258c8,3da4626411d83c19c9919bb41dba94fff88da90e,Scaling Graph Neural Networks with Approximate PageRank,53e9bc4eb7602d97048c2071,Fast and Exact Top-k Algorithm for PageRank.,"Luckily, given the broad applicability of PageRank, many such algorithms have been developed [4, 5, 19, 20, 23, 35, 45, 46, 48].###Another family of approaches [20] are based on the idea of maintaining upper and lower bounds on the PageRank scores which are then used for early termination with certain guarantees.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2465,5c234870da562935fc1d4db3,0b6a531754e67379518a946f3a3cf685f59358cc,CritICs Critiquing Criticality in Mobile Apps,558ac993e4b0b32fcb38dd52,Dynamic speculative precomputation,"Different optimizations can be employed upon fetching a critical instruction - prioritizing CPU resources [26]–[28], caches [8], [9], [18], memory requests [11], predicting instruction results [14], [29]–[31], issuing prefetches [18], etc.",other,reporting various optimization techniques
3346,5eede0b091e0116a23aafbd3,91fb815361fdbf80ff15ce4d783a41846bd99232,GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training,59ae3bf12bbe271c4c71bf14,graph2vec: Learning Distributed Representations of Graphs,"Examples include DGK [60], Struc2vec [43], GraphWave [12], graph2vec[33] and InfoGraph [46].###We compare GCC with several recent developed graph classification models, including Deep Graph Kernel (DGK) [60], graph2vec [33], InfoGraph [46], DGCNN [66] and GIN [59].",other,reporting existing graph classification models for comparison
615,5d04eeba8607575390f83f3a,b67fd0612e3f72faee0fed9b1e930b69ed7ee98d,sparse reram engine: joint exploration of activation and weight sparsity in compressed neural networks,5e86c7ee9fced0a24b3fc29d,SNrram: an efficient sparse neural network computation architecture based on resistive random-access memory,"SNrram [44] is another ReRAMbased sparse DNN accelerator that compresses the model at a finer###SNrram [44] seeks to enable fine-grained column compression at the cost of high output indexing overhead.###with SNrram [44], as SNrram uses model-based compression and its crossbar architecture is highly model-dependent.###Existing sparsity solutions [24, 44] are based on an over-idealized ReRAM crossbar architecture [9, 40].###In Figure 20, we also use arrows to indicate the weight compression ratio that can be obtained from SNrram [44].###SNrram [44] compresses the model at a finer level, i.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
978,,953c1364fc825acaa5e3899a6f12a80962df546b,Count Your Life by Smiles and Tears: An Integrative Review on Resilience and Growing Older,,,"###” For example, Aburn et al. (2016) and Fisher et al. (2019) recognize inclusion of adversity in defining and understanding resilience as fundamental. Although adversity is often defined in relation to major life events or traumatic situations (Holston & Callen, 2021; Wild et al., 2011, Wiles et al., 2019), Power et al. (2019) argue that adversity need not only arise in extraordinary circumstances but can also occur in everyday life. They define adversity as particular (often extraordinary) embodied and emplaced circumstances in people’s lives that cause pain, disruption, exhaustion, disorientation, loneliness, and grief. Understood in this way, adversity may refer to mundane, everyday experiences such as feeling weaker when getting dressed, no longer being able to drive a car or having difficulties with cooking dinner (Wright St-Clair et al., 2011). Another concept of importance in research on resilience in later life is the generic and often vaguely defined concept of “aging” (Hicks & Conner, 2014). Drawing upon the dictionary of Medicine, Nursing Allied Health (2003), Hicks and Conner (2014, p.###“Resilience” is widely recognized as complex to define (Aburn et al., 2016; Windle, 2011). Based on a systematic review of 100 articles, Aburn et al. (2016) identify five key themes underpinning ranging definitions of resilience.###” For example, Aburn et al. (2016) and Fisher et al. (2019) recognize inclusion of adversity in defining and understanding resilience as fundamental.###For example, Aburn et al. (2016) and Fisher et al. (2019) recognize inclusion of adversity in defining and understanding resilience as fundamental.###“Resilience” is widely recognized as complex to define (Aburn et al., 2016; Windle, 2011).###” For example, Aburn et al. (2016) and Fisher et al. (2019) recognize inclusion of adversity in defining and understanding resilience as fundamental. Although adversity is often defined in relation to major life events or traumatic situations (Holston & Callen, 2021; Wild et al., 2011, Wiles et al., 2019), Power et al. (2019) argue that adversity need not only arise in extraordinary circumstances but can also occur in everyday life.###Based on a systematic review of 100 articles, Aburn et al. (2016) identify five key themes
underpinning ranging definitions of resilience.",impact-revealing,highlighting the complexity and varying definitions of resilience in research
1868,,3bfcd3e764c00ad790125c6d6fad9459d2b349fb,Data Governance in the Health Industry: Investigating Data Quality Dimensions within a Big Data Context,,,"###This method of data evaluation was inspired by integrative review discussions in nursing, where reports were coded on a 2-point scale [34].",impact-revealing,drawing inspiration from nursing review discussions for data evaluation
675,5f576c1591e011f4c3d5dd7e,c36571ac50808c75fa8a5d37f1041af22e89e6ee,GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training,573696ce6e3b12023e5ce95a,Batch normalization: accelerating deep network training by reducing internal covariate shift,"Hence, normalization decouples the optimization of direction and length of the parameters (Kohler et al., 2019), implicitly tunes the learning rate (Ioffe & Szegedy, 2015;
Hoffer et al., 2018; Arora et al., 2018b; Li & Arora, 2019), and smooths the optimization landscape (Santurkar et al., 2018).###[16] Sergey Ioffe and Christian Szegedy.###For example, Batch normalization (BatchNorm) is a standard component in computer vision (Ioffe & Szegedy, 2015); Layer normalization (LayerNorm) is popular in natural language processing (Ba et al., 2016; Xiong et al., 2020); Instance normalization (InstanceNorm) has been found effective for style…###We visualize the statistics from BatchNorm layers under different settings of batch sizes [8, 16, 32, 64], as in Figure 9.###We apply the normalization after the linear transformation as in previous works (Ioffe & Szegedy, 2015; Xiong et al., 2020; Xu et al., 2019).###In computer vision, batch normalization [16] is a standard component.###During testing, the estimated dataset-level statistics (running mean µD and standard deviation σD) are used instead of the batch-level statistics (Ioffe & Szegedy, 2015).###For example, in computer vision, BatchNorm [16] is the de facto method that normalizes the feature values in the same channel across different samples in the batch.###Normalization methods shift and scale the hidden representations and are shown to help the optimization for deep neural networks (Ioffe & Szegedy, 2015; Ulyanov et al., 2016; Ba et al., 2016; Salimans & Kingma, 2016; Xiong et al., 2020; Salimans et al., 2016; Miyato et al., 2018; Wu & He, 2018;…###Using this property, [20] suggests that normalization decouples the optimization of direction and length of the parameters; [1, 13, 16, 21] show that the normalization implicitly tunes the learning rate.###In batch normalization (BatchNorm), the mean and standard deviation in a sampled batch are random variables which try to provide accurate estimations for the mean and standard deviation over the whole dataset [16, 23, 32].###1 Related Work Normalization is important in optimizing deep neural networks, and different normalization techniques have been proposed to improve the training process in different applications [4, 16, 24, 26, 27, 33, 36].###the normalization after the linear transformation as in previous works [16, 36, 37].###Each step in this branch can boost the performance of GNNs: subtracting graph mean has preconditioning effect; introducing a learnable shift avoids the expressiveness degradation; further scaling to unit norm enjoys “scale-invariant” property [1, 13, 16].###During testing, the estimated dataset-level statistics are used instead of the batch-level statistics [16].",impact-revealing,highlighting the importance and effects of normalization techniques in optimizing deep neural networks
3723,57a4e91aac44365e35c975d0,78aa018ee7d52360e15d103390ea1cdb3a0beb41,Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples,573696076e3b12023e51a63f,The Limitations of Deep Learning in Adversarial Settings,"Many classes of machine learning algorithms have been shown to be vulnerable to adversarial samples [22, 12, 19]; adversaries subtly alter legitimate inputs (call input perturbation) to induce the trained model to produce erroneous outputs.###Building on previous work [22, 12, 19] describing how adversaries can eﬃciently select perturbations leading deep neural networks to misclassify their inputs, we introduce new crafting algorithms for adversaries targeting Support Vector Machines (SVMs) and Decision Trees (DTs).###Learning substitute models approximating the decision boundaries of targeted classiﬁers alleviates the need of previous attacks [22, 12, 19] for knowledge of the target architecture and parameters.###To craft adversarial samples misclassiﬁed by DNNs, an adversary with knowledge of the model f and its parameters θ can use the fast gradient sign method introduced in [12] or the Jacobian-based iterative approach proposed in [19].###In one such attack, Papernot et al. trained a local deep neural network (DNN) using crafted inputs and output labels generated by the target “victim” DNN [19].",other,highlighting vulnerabilities of machine learning algorithms to adversarial samples and introducing new crafting algorithms
1725,,3c8526709a9e2a8a3d3b8fb6d17dd3b36b9a70be,A Probabilistic Analysis of Snapshot Isolation with Partial Replication,,,"###As in [11], we suppose that, in average, each transaction is about half way complete, thus the number of resources locked by executing transactions is at most###In lock-based systems transactions executing over multiple replicas will acquire locks on remote data items and thus, may increase the likelihood of distributed deadlocks [11], a problem that group communication protocols can mitigate [1].###Besides the assumptions considered throughout our probabilistic modelling, the work in [11] does not account for read operations — all transactions are composed of updates only.###in [11], where the authors analyze the deadlock rate of fully replicated database systems based on locking only.###We do not consider deadlocks that involve more than two transactions: deadlocks composed of cycles of three or more transactions are very unlikely to occur [11].###[11], where the deadlock rate rises as the third power of the number of replicas, in our replication model all commits of update transactions are ordered and thus replication does not increase the deadlock rate of the 1SR system — the aborts in Figure 1(b)) are due to the certification test.###In a replicated setting, 2PL may result in high abort rates as aborts grow with the third power of the number of replicas [11].",impact-revealing,acknowledge prior work on deadlock analysis in replicated database systems
2531,5bbacbad17c44aecc4eb007e,fdb87d305c59e9ab82a96198e34e46581b838c4e,one-shot relational learning for knowledge graphs,599c797d601a182cd2643e8a,Modeling Relational Data with Graph Convolutional Networks.,"The neighbor encoder module we propose here is similar to the Relational Graph Convolutional Networks (Schlichtkrull et al., 2017) in the sense that we also use the shared kernel { W c , b c } to encode the neighbors of different entities.###is similar to the Relational Graph Convolutional Networks (Schlichtkrull et al., 2017) in the sense that we also use the shared kernel {Wc, bc} to encode the neighbors of different entities.",other,drawing a comparison to existing graph convolutional networks
313,5b67b45517c44aac1c860885,fd3f489fc0438e500c0473af40dfebe4705df6d2,STAMP: Short-Term Attention Memory Priority Model for Session-based Recommendation,573696c26e3b12023e5c346a,Adaptation and Evaluation of Recommendations for Short-term Shopping Goals,"[7] noted that both the users’ short-term and long-term interests are of great importance for recommendation, but traditional RNN architectures are not designed to distinguish and exploit these two types of interests simultaneously [11].",impact-revealing,highlighting a limitation in traditional RNN architectures for recommendation systems
3388,5d04eeba8607575390f83f4d,9cceaadb580c24d0d5c381fa8e3d4afb32fd88b9,perceptron-based prefetch filtering,5736982b6e3b12023e6fd15e,Efficiently Prefetching Complex Address Patterns,"Shevgoor et al. propose the Variable Length Delta Prefetcher (VLDP) [35], which correlates histories of deltas between cache line accesses within memory pages with the next delta within that page.",other,reporting prior findings on cache line access prediction
3034,5f0d85c69fced0a24be4f019,6817b5d48cf8f665b680de07d23b91814d2923b1,Bouquet of Instruction Pointers: Instruction Pointer Classifier-based Spatial Hardware Prefetching,5c9df4643cb210d271bea0b4,Bingo Spatial Data Prefetcher,"A recent work called Bingo [11], uses multiple signatures (like IP, IP+Offset, and memory region) and fuses them into a single hardware table.###In general, spatial * A major part of the work was done through a remote internship, while the author was at BITS Pilani. prefetchers demand less storage (closer to tens of KBs, except spatial memory streaming (SMS) [47] and Bingo [11]) as compared to the temporal ones (closer to hundreds of KBs).###This is a signiﬁcant improvement compared to lightweight versions of MLOP [44], SPP+PPF [14], and Bingo [11] that###Recent prefetching proposals [11], [14], [33] have also been coded and evaluated with ChampSim, helping the community for a fair comparison of techniques.###We do not implement Bingo at the LLC as it provides low performance (the reason being, Bingo [11] is implemented with 37.5GBps DRAM bandwidth, ﬁxed latency DRAM.###The above points are the primary reasons for DOL’s poor performance compared to state-of-the-art spatial prefetchers [11], [13], [14].###Prefetchers like SMS [47], [49] and Bingo [11] are capable of prefetching at the L1-D.###The problem: State-of-the-art spatial prefetchers [45] [33], [11], [14], [13] are designed speciﬁcally for L2’s access patterns.###IP information has been used extensively at the L2 and the LLC [14] [56], [60], [25], [29], [11], [24], [59].###Well-established and recent spatial L2 prefetchers (prefetchers that prefetch within a spatial region) [33], [13], [14], [38], [11], [45] have pushed the limits of data prefetching.",other,reporting on recent advancements in data prefetching techniques
2843,5f02f17c91e011ee5e0258c8,3da4626411d83c19c9919bb41dba94fff88da90e,Scaling Graph Neural Networks with Approximate PageRank,5a9cb66717c44a376ffb8667,FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling,"etection and graph classification [3, 15, 22, 37]. The success of GNNs on academic datasets has generated significant interest in scaling these methods to larger graphs for use in real-world problems [13, 14, 16, 21, 25, 27, 41, 54]. Unfortunately, there are few large graph baseline datasets available; apart from a handful of exceptions [16, 54], the scalability of most GNN methods has been demonstrated on graphs with fewer than###ically need to performarecursiveneighborhoodexpansiontocomputethehidden representations of a given node. While several approaches have been proposed to improve the efficiency of graph neural networks [13, 14, 16, 21, 25, 27, 41, 49, 54], the scalability of GNNs to massive (web-scale) graphs is still under-studied. As we discussed in § 1 the most prevalent approach to scalability is to sample a subset of the graph, e.g. based on diff###rom thek-hop neighborhood of a given node to generate its prediction [25, 54]. The key differences between many scalable techniques lies in the design of the sampling scheme. For example, Chen et al. [13] directly sample the receptive field for each layer using importance sampling, while Chen et al. [14] use the historical activations of the nodes as a control variate. Huang et al. [27] propose an ada###5.4) •How efficient is the proposed sparse inference scheme? (§ 5.5) 5.1 Large-Scale Datasets The majority of previous approaches are evaluated on a small set of publicly available benchmark datasets [2, 13, 14, 21, 25, 27, 41, 49]. The size of these datasets is relatively small, with the Reddit graph (233K nodes, 11.6M edges, 602 node features) [25] typically being the largest graph used for evaluation.4 Chiang et al. [16] rec###curacy. Unfortunately, for many of the proposed methods sampling does not directly reduce the number of nodes that need to be retrieved, since e.g. we have first have to compute the importance scores [13]. Recent work shows that personalized PageRank [28] can be used to directly incorporate multi-hop neighborhood information of a nodewithoutexplicitmessage-passing[33].Intuitively,propagation based on ",other,highlighting the challenges and limitations in scaling graph neural networks
427,5e5e18ca93d709897ce315f0,68f86237dadcf2f570f0cd5b5e56161693619a74,Residual Energy-Based Models for Text Generation,5d1eb9d6da562961f0b11140,Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting.,"Lastly, this formulation enables efﬁcient evaluation and generation via importance sampling (Horvitz & Thompson, 1952; Grover et al., 2019).###Similar to our work, Grover et al. (2019) propose to use the discriminator to de-bias the pretrained generator using importance sampling.###Concurrently, Grover et al. (2019) proposed a general approach to “de-bias” a generator, by simply training a discriminator and using its output for importance sampling.###In order to generate efﬁciently, we use self-normalizing importance sampling (Owen, 2013; Grover et al., 2019).",impact-revealing,highlighting the efficiency of evaluation and generation methods in relation to prior work
3196,5fd3404791e01161cf73952c,e988e15d200faf64bb71e155b8354c4e127f7dab,Bipartite Graph Embedding via Mutual Information Maximization,5e5e189a93d709897ce1e760,InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization,"x objectives. DGI [36] is the first work that applies the infomax objective to homogeneous graphs. It provides a new approach for the task of unsupervised node classification. Based on DIM, InfoGraph [31] tries to learn unsupervised graph representations via maximizing the MI between the graph-level representation and the representations of substructures. DMGI [41] extends DGI into heterogeneous graph",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
341,5da2f8aa3a55ac3402d8c092,fd2a0a326db4f034fe22340c20b7bacd9a14c3d6,second-order attention network for single image super-resolution.,5a260c8117c44a4ba8a30771,Squeeze-and-Excitation Networks.,"In recent years, several trials have embeded attention processing to improve the performance of CNNs for various tasks, such as image and video classiﬁcation tasks [9, 33].###As explored in [9], the simple sigmoid function can serve as a proper gating function where W D and W U are the weight set of convolution layer, which set channel dimension of features to C/r and C , respectively. f ( · ) and δ ( · ) are the function of sigmoid and RELU.###To utilize such information, SENet [9] was introduced in CNNs to rescale the channel-wise features for image SR.###Channel attention [9, 38] has been shown to be effective for better discriminative representations.###[9] proposed SENet to exploit channel-wise relationships to achieve signiﬁcant performance gain for image classiﬁcation.",impact-revealing,acknowledge advancements in attention processing for CNNs
3480,5c2c7a9217c44a4e7cf317b4,61946177ea5d44ad18ff09de7929556d7b34cd8b,simd-x: programming and processing of graph algorithms on gpus.,599c7d86601a182cd282f6ff,Falcon: Scaling Io Performance In Multi-Ssd Volumes,"The advent of big data [40, 27, 35, 36, 37, 5, 25, 26, 28, 14, 83] exacerbates the need of extracting useful knowledge within an acceptable time envelope.",other,highlighting the significance of big data in knowledge extraction
734,5b3d98cc17c44a510f801acc,155b7782dbd713982a4133df3aee7adfd0b6b304,Unsupervised Feature Learning via Non-parametric Instance Discrimination,5550417745ce0a409eb3b87d,Discriminative Unsupervised Feature Learning with Convolutional Neural Networks.,"Exemplar CNN [5] appears similar to our work.###The results of these methods are reported with AlexNet architecture [18] in their original papers, except for exemplar CNN [5], whose results are reported with ResNet-101 [3].###In recent years, unsupervised learning has received increasing attention from the community [5, 2].",impact-revealing,acknowledging similarities with existing methods and the growing interest in unsupervised learning
558,5a73cb6317c44a0b30358203,a072c2a400f62f720b68dc54a662fb1ae115bf06,tacotron: towards end-to-end speech synthesis,5550411c45ce0a409eb3897f,Neural Machine Translation by Jointly Learning to Align and Translate.,"The backbone of Tacotron is a seq2seq model with attention [7, 14].###The backbone of Tacotron is a seq2seq model with attention (Bahdanau et al., 2014; Vinyals et al., 2015).###In this paper, we propose Tacotron, an end-to-end generative TTS model based on the sequence-to-sequence (seq2seq) (Sutskever et al., 2014) with attention paradigm (Bahdanau et al., 2014).###In this paper, we propose Tacotron, an end-toend generative TTS model based on the sequence-to-sequence (seq2seq) [6] with attention paradigm [7].",impact-revealing,describing the model architecture of Tacotron
3185,5f0d85c69fced0a24be4f04c,a43ba805d13785378fecdb408a571ee50d0afb8e,auto-predication of critical branches,53e9ae22b7602d9703832ecc,Wish Branches: Combining Conditional Branching And Predication For Adaptive Predicated Execution,"This is unlike previous approaches [7], [12]–[14] that were dependent upon compiler analysis and proﬁling.###Popular ISAs support static predication [17], [18] but due to large overheads, the realistic beneﬁts are diminished [7], [12].###Wish Branches [12] rely on the compiler to supply predicated code but applies predication dynamically only on less predictable instances.###Wish Branches [12] relies on the compiler to provide predicated code for every branch PC.",other,highlighting differences between current and previous approaches
2263,5ecb57199e795ec6f2ba59cc,512f34906ddaefe885af2e5eec9b2b3b50ffd377,deep entity matching with pre-trained language models,53e9b923b7602d970450fda1,A Comparison of Fast Blocking Methods for Record Linkage,"EM solutions have tackled the blocking problem [2, 6, 14, 28, 45] and the matching problem with rules [9, 13, 38, 44], crowdsourcing [16, 18, 43], or machine learning [37, 8, 3, 16, 20].",other,acknowledge existing solutions to the blocking and matching problems in entity matching
1604,,1abfc211793c683972ded8d3268475e3ee7a88b0,Adversarial Demonstration Attacks on Large Language Models,,,"###As our advICL is built upon the TextAttack framework, we use certain configurations and functions from TextAttack.###1 Our attack, advICL , builds upon TextAttack (Morris et al., 2020), a standard attack framework.###Specifically, to make our attack easily and flexibly deployable in existing systems, we design our attack under the TextAttack framework but add extra demonstration masking that only allows manipulating demonstration.###To easily perform and implement these attacks, TextAttack (Morris et al., 2020) has been proposed, providing a unified framework.###In Table 7, we list all configurations of advICL in comparison to the baseline TextBugger in TextAttack framework.###To solve the above objective function, we consider a more practical black-box setting and adopt the greedy search in TextAttack framework(Morris et al., 2020).###To ensure easy and flexible deployment of our attack, we implement it in the TextAttack framework using a black-box approach.",impact-revealing,describing the implementation of the advICL attack within the TextAttack framework
979,,a998fbeeb0c1d70e736d28cccb658f966de684c4,Induced sensorimotor cortex plasticity remediates chronic treatment-resistant visual neglect,,,"###…brain activity, likely mediate the enhancement effects of M1 a-tDCS on motor learning and memory formation that can be observed when stimulation is applied during a range of motor tasks (Buch et al., 2017; Galea et al., 2011; Hunter et al., 2009; Panico et al., 2017; Reis et al., 2008).###These physiological mechanisms, interacting with endogenous task-related brain activity, likely mediate the enhancement effects of M1 a-tDCS on motor learning and memory formation that can be observed when stimulation is applied during a range of motor tasks (Buch et al., 2017; Galea et al., 2011; Hunter et al., 2009; Panico et al., 2017; Reis et al., 2008).###This has raised concerns about inter-individual variability and reliability of tDCS effects (Buch et al., 2017; Jamil et al., 2017).",impact-revealing,highlighting concerns about variability and reliability of tDCS effects
2478,5c0f8548da562944ac906a71,5a3da29970d0c3c75ef4cb372b336fc8b10381d7,CNN-Based Real-Time Dense Face Reconstruction with Inverse-Rendered Photo-Realistic Face Images,573696016e3b12023e5156ec,Fitting A 3d Morphable Model To Edges: A Comparison Between Hard And Soft Correspondences,"The reconstruction results of the methods [3], [14], [24], [46] are generated using the source codes provided by the authors.###From the first row to the last row, it respectively shows the input images, and the results of [42], [3], [24], [46], [14] and ours.###[3] presents an algorithm for fully automatically fitting a 3D Morphable Model to a single image using landmarks and edge features.###Note that the method of [3] uses a 3DMM with identity variation only, and thus is not able to handle facial expressions well.###We compare our method with [3], [14], [24], [42], [46] on single-image based face reconstruction.",other,reporting reconstruction results and comparing methods
359,5bdc31b417c44a1f58a0bbc4,27e98e09cf09bc13c913d01676e5f32624011050,U-Net: Machine Reading Comprehension with Unanswerable Questions,5b8c9f5317c44af36f8b742c,Read plus Verify: Machine Reading Comprehension with Unanswerable Questions,We believe that the performance of the U-Net can be boosted with an additional post-processing step to verify answers using approaches such as (Hu et al. 2018).###Hu et al.; Tan et al. (2018; 2018) introduced an answer veriﬁer idea to construct a classiﬁcation layer.###Another approach introduces an answer veri-ﬁer to determine whether the question is unanswerable (Hu et al. 2018; Tan et al. 2018).,impact-revealing,suggesting improvements to U-Net performance through answer verification
2633,58d83045d649053542fe853e,51898a5e35b2646a4e5121ca74f612fab831ad6a,path confidence based lookahead prefetching,53e9a766b7602d970309e323,Friendly fire: understanding the effects of multiprocessor prefetches.,multiple workloads fight for limited shared LLC and DRAM bandwidth [24].,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1903,,5b3b616d803faf3f11e757c645347770599aa7b0,Designing for Engagement: Using the ADDIE Model to Integrate High-Impact Practices into an Online Information Literacy Course.,,,"###Much of the scholarship is explanatory or exploratory; the research that does focus on implementing ADDIE is limited to designing one-shot, subject-based information literacy sessions or emphasizing it as a potential tool for creating a planned program of library instruction (see, for instance, Guder, 2014; Koneru, 2013; Summey & Valenti, 2013). Easter, Bailey & Klages (2014) provided a case study of two embedded librarians working with a faculty member to design IL modules for an online course using ADDIE; the commentary in the article from both the librarians and the faculty member provided a balanced reflection on the process and results.###Importantly, the scholarship emphasized the importance of instructor participation in these smaller communities (Garrison et al., 2000; Murdock & Williams, 2011).",impact-revealing,highlighting the limited focus on implementing ADDIE in library instruction
3111,5d0616bd8607575390f86730,635b079447ee46acc813a08305988e77dfe3d1dd,Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi,619b76ba1c45e57ce97b0659,"Lifs: Low Human-Effort, Device-Free Localization With Fine-Grained Subcarrier Information","In general, the last estimation of location and the last estimation of moving direction can be provided by tracking systems[26, 33, 41], as the location and orientation of the user in Widar3.###…extract various parameters of signals reflected or shadowed by human, including DFS [26, 32, 44], ToF [2–4, 21], AoA / AoD [2 , On the human side, existing model-based works only tracks coarse human motion status, such as location [4, 41], velocity [26, 32], gait [43, 49] and figure [2, 19].###0 can exploit existing sophisticated passive tracking systems, e.g., LiFS [41], IndoTrack [26] and Widar2.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3168,5a73cbcc17c44a0b3035f264,afd76be183d34af3bf944debd73db1c77987a8c6,Covariant Compositional Networks For Learning Graphs,53e99acab7602d970234a328,Weisfeiler-Lehman Graph Kernels,"…artner, 2002; Borgwardt & Kriegel, 2005; Feragen et al., 2013), counting subgraphs (Shervashidze et al., 2009), spectral ideas (Vishwanathan et al., 2010), label propagation schemes with hashing (Shervashidze et al., 2011; Neumann et al., 2016), and even algebraic ideas (Kondor & Borgwardt, 2008).###…the classic Weisfeiler–Lehman test of isomorphism also follows the same logic (Weisfeiler & Lehman, 1968; Read & Corneil, 1977; Cai et al., 1992), and so does the related Weisfeiler–Lehman kernel, arguably the most successful kernel-based approach to graph learning (Shervashidze et al., 2011).",other,acknowledge various graph learning methods and their historical context
1919,,330accc59ffbf2aa7642d932c9cb2e40d422928a,"Disentangling SEL: Advocating for Black sociality, questioning white teachers’ emotionality",,,"###Comer (1988) was well-aware of how white teachers would frustrate the SDP process if they were not trained “to analyze, much less solve, the social-misalignment problems of children outside the mainstream” (p.###model that 50 years later has been successfully applied to over 1,000 schools across the country (Comer & Emmons, 2006). Although there are several elements to the SDP model,(1) in the broadest sense it is a process designed to make schooling into what hooks (1990) describes as homeplace, that is, “the one site where one could freely confront the issues of humanization, where one could resist.###Broderick, 2011), the Comer Model interrogates how settler colonialism negatively impacts the chances BIPOC youth have for academic success. While working with primarily Black children in New Haven, Comer (1988) insisted that these children’s mis-educative experiences are shared by Native Americans and Hispanics whose lived experiences likewise###Comer directed the SDP from 1968 to 1980 and cites Yale psychoanalyst Albert Solnit’s argument that education reforms should be immersive and action-based and his own childhood experiences as generative (Comer, 1988).###This parsing is motivated by a “backward mapping” (Love, 2019, p. 26) of the School Development Program (SDP) developed by James Comer (Comer, 1988, 1992-93-93; Comer & Emmons, 2006; DarlingHammond et al., 2019).###Also echoing Comer, Darling-Hammond (2020) insists that white teachers must willfully learn from students’ lived experiences if education policy reforms are going to succeed in protecting and loving children of color.###In positing the SDP as a process model for education reform that is foundational to SEL, but whose legacy is forgotten amongst an array of current individualist SEL frameworks, this section argues that the Comer Model made schooling a homeplace by intentionally constructing “a safe place where black people could affirm one another and by so doing heal many of the wounds inflicted by racist domination” (hooks, 1990, p. 42). Comer schools also practiced Love’s (2019) insistence that “schools must support the fullness of dark life as a way to justice” (p.###This limitation extends to teacher preparation programs, which Comer (1988) insisted would frustrate the SDP if white teachers were not trained to understand how schooling produces sociocultural misalignments.###Comer (1988) was well-aware of how white teachers would frustrate the SDP process if they were not trained “to analyze, much less solve, the social-misalignment problems of children outside the mainstream” (p. 48).###From its beginning in 1968, the SDP centered the lived experiences of students who attended 2 majority-Black elementary schools in New Haven, Connecticut—Martin Luther King and Katharine Brennan
CONTACT Benjamin Kearl kearlb@pfw.edu Purdue University Fort Wayne, 2101 East Coliseum Boulevard, Fort Wayne, 46805, IN, USA
© 2022 The College of Education and Human Ecology, The Ohio State University
—as well as the sociopolitical contexts of their parents’ lives.###Comer (1988) insisted that this change must begin with teacher preparation.###When emotional does appear, it is used to describe bonds with parents: “Our understanding is based on the fact that a child develops a strong emotional bond to competent caretakers (usually parents) that enable them to help the child develop” (Comer, 1988, p. 44).###Extending Comer’s (1988) original thesis, the incapacity of education to buildout conditions of Black sociality produce sociocultural misalignments within schools and society that result###While working with primarily Black children in New Haven, Comer (1988) insisted that these children’s mis-educative experiences are shared by Native Americans and Hispanics whose lived experiences likewise testify to how racism systematically destroys vital communal institutions (p. 45).###argument that education reforms should be immersive and action-based and his own childhood experiences as generative (Comer, 1988).",impact-revealing,Highlighting the importance of the Comer Model in addressing sociocultural misalignments in education
2603,5e3940c73a55ace46ed436d2,845b4941d8c016aa5f8967da2f86d38ef6c18fa3,a survey on knowledge graphs representation acquisition and applications,58d83051d649053542fe99b8,Towards Time-Aware Knowledge Graph Completion.,"[143], [144] proposed time-aware embedding, a joint learning framework with temporal regularization, to incorporate temporal order and consistency information.",other,reporting prior findings on time-aware embedding
860,5eede0b091e0116a23aafcd3,9898b3d600b3881bde162b7e4b668a3c063cba10,sequential graph convolutional network for active learning,5c7572b7f56def97988385ce,Active Learning for Convolutional Neural Networks: A Core-Set Approach,"CoreSet[31] on learner feature space is one of the best performing geometric techniques to date and it is another competitive baseline for us.###In principle, CoreSet [31] uses risk minimisation between core-sets on the learner feature space while we employ this operation over GCN features.###Although there have been studies exploring the data space through the representations of the learning model ([20, 35, 14]), the first work applying it for CNNs as an active learning problem, CoreSet, has been presented in [31].###This has become a standard in deep learning system due to its successful deployment in recent methods [2, 31, 33, 42].###UncertainGCN is based on the standard AL method uncertainty sampling [31] which tracks the confidence scores of the designed graph nodes.###To integrate geometric information between the labelled and unlabelled graph representation, we approach a CoreSet technique [31] in our sampling stage.###[31] shows how bounding the difference between the loss of the unlabelled samples and the one of the labelled is similar to the k-Centre minimisation problem stated in [37].###Furthermore, we adapt the higher-order graph node information under the CoreSet [31] for a new sampling technique by introducing latent space distancing.###Furthermore, CoreGCN adapts the highly successful CoreSet [31] on the induced graph embeddings by the sequentially trained GCN network.###Random UncertainGCN(Ours) VAAL[33] Learning Loss[42] CoreSet[31] CoreGCN(Ours) FeatProp[38]",impact-revealing,acknowledge the effectiveness of CoreSet in active learning and its application in deep learning
3791,5aed14d617c44a4438158e20,9e788f1530af08a1f2140e6016fd4aeaa8b29033,Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform,59ae3bf12bbe271c4c71bf7f,Photographic Image Synthesis with Cascaded Refinement Networks,"In image generation [20, 5], semantic segments are used as input conditions to generate natural images.",other,providing context on the use of semantic segments in image generation
2382,58437722ac44360f1082f13a,d2e4587744a89bad95fea69e08842cad6c8ff0dd,Temporal ensembling for semi-supervised learning,57a4e91aac44365e35c97e90,Mutual Exclusivity Loss for Semi-Supervised Deep Learning,"The recently introduced transform/stability loss of Sajjadi et al. (2016b) is based on the same principle as our work, and the Π -model can be seen as a special case of it.###We test the Π -model and temporal ensembling in two image classiﬁcation tasks, CIFAR-10 and SVHN, and report the mean and standard deviation of 10 runs using different random seeds.###In purely supervised training the de facto standard way of augmenting the CIFAR-10 dataset includes horizontal ﬂips and random translations, while SVHN is limited to random translations.###The same is probably true for SVHN as well, but there the best published results rely on extra data that we chose not to use.###Sajjadi et al. (2016b) recently introduced a new loss function for semi-supervised learning, so called transform/stability loss, which is founded on the same principle as our work.###official 73257 training examples following Salimans et al. (2016). Even with this choice our error rate with all labels is only 3.###In addition, they employ a mutual exclusivity loss term (Sajjadi et al., 2016a) that we do not use.###In SVHN Sajjadi et al. (2016b) provide results without augmentation, with the caveat that they use fractional max pooling, which is a very augmentation-like technique due to the random, local stretching it introduces inside the network.###The street view house numbers (SVHN) dataset consists of 32 × 32 pixel RGB images of real-world house numbers, and the task is to classify the centermost digit.###official 73257 training examples following Salimans et al. (2016). Even with this choice our error rate with all labels is only 3.05% without augmentation. Table 2 compares our method to the previous state-of-the-art. With the most commonly used 1000 labels we observe an improvement of 2.7 percentage points, from 8.11% to 5.43% without augmentation, and further to 4.42% with standard augmentations. We also investigated the behavior with 500 labels, where we obtained an error rate less than half of Salimans et al. (2016) without augmentations, with a significantly lower standard deviation as well.###Given that in a separate experiment our network matched the best published result for non-augmented SVHN when extra data is used (1.69% from Lee et al. (2015)), this gap is quite surprising, and leads us to conclude that fractional max pooling leads to a powerful augmentation of the dataset, well beyond what simple translations can achieve.###A principled comparison with Sajjadi et al. (2016b) is difﬁcult due to several reasons.###In SVHN we chose to use only the Table 2 compares our method to the previous state-of-the-art.",other,comparing methods and results in image classification tasks
1420,,e991f0b06a47bb88380da1dcfe45466125ddf82e,Provably Efficient Generative Adversarial Imitation Learning for Online and Offline Setting with Linear Function Approximation,,,"###…with optimism (Auer et al., 2002, 2009; Azar et al., 2017; Jin et al., 2018, 2019; Yang and Wang, 2020), offline RL (Fujimoto et al., 2019b; Kumar et al., 2020; Fujimoto et al., 2019a; Duan et al., 2020; Levine et al., 2020; Jin et al., 2021), policy optimization (Beck and Teboulle, 2003;…###Inspired by the spirit of being conservative in offline RL (Fujimoto et al., 2019b; Kumar et al., 2020; Jin et al., 2021), we propose a pessimistic variant of policy optimization in the policy update stage of PGAP (Lines 5–11 of Algorithm 3), which ensures that PGAP utilize the information of the additional dataset D in the sense of minimax optimality.###This problem has been studied widely in offline RL (Fujimoto et al., 2019a; Kumar et al., 2020; Fujimoto et al., 2019b; Levine et al., 2020; Jin et al., 2021) and Wang et al. (2020a) even propose that the lower bound of offline RL can grow exponentially with the horizon under linear approximation…###Inspired by the spirit of being conservative in offline RL (Fujimoto et al., 2019b; Kumar et al., 2020; Jin et al., 2021), we propose a pessimistic variant of policy optimization in the policy update stage of PGAP (Lines 5–11 of Algorithm 3), which ensures that PGAP utilize the information of the…###This problem has been studied widely in offline RL (Fujimoto et al., 2019a; Kumar et al., 2020; Fujimoto et al., 2019b; Levine et al., 2020; Jin et al., 2021) and Wang et al.###The principle pessimism-in-face-of-uncertainty guides the agent to be conservative to visit the states and actions that are less covered by the additional dataset DA (Kumar et al., 2020; Jin et al., 2021; Liu et al., 2020; Yu et al., 2020, 2021; Buckman et al., 2020).###…the uncertainty quantification for estimated model in PGAP is motivated by the pessimism in offline RL (Chen and Jiang, 2019; Xie et al., 2021a,b; Kumar et al., 2020; Jin et al., 2021; Liu et al., 2020; Yu et al., 2020, 2021; Buckman et al., 2020; Rashidinejad et al., 2021; Uehara and Sun, 2021).###The principle pessimism-in-face-of-uncertainty guides the agent to be conservative to visit the states and actions that are less covered by the additional dataset D (Kumar et al., 2020; Jin et al., 2021; Liu et al., 2020; Yu et al., 2020, 2021; Buckman et al., 2020).###, 2018, 2019; Yang and Wang, 2020), offline RL (Fujimoto et al., 2019b; Kumar et al., 2020; Fujimoto et al., 2019a; Duan et al., 2020; Levine et al., 2020; Jin et al., 2021), policy optimization (Beck and Teboulle, 2003; Hazan, 2019; Cai et al.###Specifically, the uncertainty quantification for estimated model in PGAP is motivated by the pessimism in offline RL (Chen and Jiang, 2019; Xie et al., 2021a,b; Kumar et al., 2020; Jin et al., 2021; Liu et al., 2020; Yu et al., 2020, 2021; Buckman et al., 2020; Rashidinejad et al., 2021; Uehara and Sun, 2021).",impact-revealing,highlighting the principle of pessimism in offline reinforcement learning
1200,,b2ff43d7b41eafca47d8b590a74e2e35dfd33905,A New Discriminant Analysis Approach under Decision-Theoretic Rough Sets,,,"###Basic concepts, notations and results of the discriminant analysis and DTRS are briefly reviewed in this section [4, 6, 8, 15].###Positive rules make decisions of acceptance, negative rules make decisions of rejection, and boundary rules make deferred or non-committed decisions [15].###This method does not consider the deferment scenario and it is actually regarded as a two-way decision [15].###For the Bayesian decision procedure, the DTRS model is composed of 2 states and 3 actions [15, 16].###He pointed out that the two-way decision is the special case of a three-way decision which considers the deferment scenario [15].###introduced Bayesian decision procedure to propose decision-theoretic rough sets (DTRS) [15], and the pair of thresholds can be directly calculated by minimizing the decision cost with Bayesian theory.",impact-revealing,reviewing basic concepts and results of discriminant analysis and DTRS
1393,,e36c743bbce9b3495f164e03bb4ea48cdb4a8d00,Stacking of Dependency and Phrase Structure Parsers,,,"###For the stacking of graph-based and transition-based dependency parsers, we used the feature template deﬁnitions from Nivre and McDonald (2008) which was extended by grandchildren factors (similarly to Torres Martins et al. (2008)).###Different parsing approaches have different strengths on distinct linguistic constructions, cf. (Nivre and McDonald, 2008).###Our approach can be regarded as a special stacking procedure (Nivre and McDonald, 2008), speciﬁcally, the stacking of a phrase structure parser with a dependency parser.###This result shows that the stacking idea of Nivre and McDonald (2008) works well on this higher accuracy level obtained with the offspring of their parsers.",impact-revealing,acknowledge and build upon previous parsing approaches
1154,,aecbe351822b77cb36d22e8a43b4fe2bda6ab998,Soft Diffusion: Score Matching with General Corruptions,,,"###4 deﬁnes a general class of diﬀusion processes, that includes (as special cases) the VE, VP and subVP SDEs used in Song et al. (2021b).###For example, we might need to tune the weights w ( t ) since for the ablations (and the state-of-the-art model), we use w ( t ) = 1 /σ 2 t (as in Song & Ermon (2019; 2020); Song et al. (2021b)) which might be causing instabilities for low values of noise (Nichol & Dhariwal, 2021).###Song et al. (2021b) also propose another SDE with bounded variance, the subVP-SDE, that experimentally yields better likelihoods.###Crucially, the corruption processes of interest cannot be described by linear Ito SDEs as in the seminal work of Song et al. (2021b).###Similarly to Song et al. (2021b), we can also consider the Ordinary Diﬀerential Equation (ODE) associated with this SDE: We can approximate ˙ C t E [ x 0 | ˜ x t ] , ∇ ˜ x t log q t (˜ x t ) with our trained network and get a deterministic version of the Momentum Sampler, which we call Probability…###Hence, the Fokker-Planck equations hold and we are guaranteed that (as long as the approximation of the conditional expectation and the approximation of the score function are accurate), we are sampling from the correct distribution (Song et al., 2021b).###We note that diﬀerent samplers can be used to accelerate sampling for all models, e.g. samplers from Karras et al. (2022) or DDIM-type samplers (Song et al., 2021a), as we show in Section B.2.###Inspired by the works of Song et al. (2021b); Maoutsa et al. (2020), we also consider deterministic sampling that is derived by looking at the ODE that describes our diﬀusion.###Typically, ∇ x log q t ( x ) is approximated by s θ ( x t | t ) and samples are generated by solving the Reverse SDE (Song et al., 2021b).###DDPMs usually have a ﬁnal distribution of unit variance and hence their SDE is known as the V ariance P reserving (VP) SDE (Song et al., 2021b).###Speciﬁcally, we compare against DDPM (Ho et al., 2020) that uses the VP SDE, DDIM (Song et al., 2021a) that uses the same model but with a diﬀerent sampler, DDPM++ (Kim et al., 2022b) that is the state-of-the-art model for VP-SDE and the NCSN++ models (Song et al., 2021b) trained with the VE and…###For CIFAR10, we outperform (FID: 3.86) Gaussian diﬀusion with Variance Exploding SDE (Song et al., 2021b).###In their seminal work, Song et al. (2021b) observe that the diﬀusions of both Score-Based models and DDPMs can be expressed as solutions of Stochastic Diﬀerential Equations (SDEs) of the form: where w is the standard Wiener process.###Speciﬁcally, we propose the following DDIM-type sampler: This Equation is very similar to Equation 12, page 5 in the DDIM (Song et al., 2021a) paper.###In the Appendix, we further compare our Momentum Sampler with a DDIM-type sampler (Song et al., 2021a) and Predictor-Corrector (Song et al., 2021b) samplers (see Sections B.2, F.3).###…we compare against DDPM (Ho et al., 2020) that uses the VP SDE, DDIM (Song et al., 2021a) that uses the same model but with a diﬀerent sampler, DDPM++ (Kim et al., 2022b) that is the state-of-the-art model for VP-SDE and the NCSN++ models (Song et al., 2021b) trained with the VE and subVP-SDEs.###Score-based models (Song & Ermon, 2019; 2020; Song et al., 2021b) and Denoising Diﬀusion Probabilistic Models (DDPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021a) are two powerful classes of generative models that produce samples by inverting a diﬀusion process.###In fact, if there is no blur ( C t = I ), our sampler becomes exactly the sampler used for the Variance Exploding (VE) SDE in Song et al. (2021b).###The eﬃciency of DDIM is consistent with what has been observed in (Song et al., 2021a; Karras et al., 2022).###A popular sampling scheme is the DDIM method, introduced in Song et al. (2021a).###Our method requires signiﬁcant less steps to achieve the same or better quality than NCSN++ (VE SDE) (Song et al., 2021b), using the same architecture and training hyperparameters (FID values taken from (Ma et al., 2022)).###Predictor-Corrector Samplers Finally, we perform experiments with Predictor-Corrector samplers, as proposed in Song et al. (2021b).###These two classes have been uniﬁed under a single framework (Song et al., 2021b) and are widely known as diﬀusion models.###C t = I , we recover the DSM objective used in (Song & Ermon, 2019; 2020; Song et al., 2021b).###We use the standard geometric scheduling for the noise (Song & Ermon, 2019; 2020; Song et al., 2021b) and use the methodology described in Section 3.3 to select the blur levels.###The two general classes of diﬀusion models are Score-Based Models (Song & Ermon, 2019; 2020; Song et al., 2021b) and Denoising Diﬀusion Probabilistic Models (DDPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020).###(Song et al., 2021b) sampler for Celeba-64 for our blurring models.###In the Appendix, Section B.2, we also present a DDIM-type (Song et al., 2021a) sampler for which the momentum term also appears.###Formally, let { q t } 1 t =0 be the (noisy) distributions used in Song et al. (2021b) for the Variance Exploding (VE) SDE and let { q t } 1 t =0 the blurry (and noisy) distributions we want to select.###…the Ordinary Diﬀerential Equation (ODE) associated with this SDE: We can approximate ˙ C t E [ x 0 | ˜ x t ] , ∇ ˜ x t log q t (˜ x t ) with our trained network and get a deterministic version of the Momentum Sampler, which we call Probability Flow Momentum Sampler, as in Song et al. (2021b).###Model FID DDPM (VP SDE) (Ho et al., 2020) 3.26 DDIM (VP SDE) (Song et al., 2021a) 3.51 DDPM++ (VP SDE) (Kim et al., 2022b) 1.90 NCSN++ (subVP-SDE) (Song et al., 2021b) 3.95 NCSN++ (VE SDE) (Song et al., 2021b) 3.25 Ours (VE SDE + Blur) 1 .###Hence, the corresponding SDE is named V ariance E xploding (VE) SDE (Song et al., 2021b).###Our method is inspired by the continuous formulation of diﬀusion models that is introduced in Song et al. (2021b).###Particularly, the ODE: has the same marginal distributions (Anderson, 1982; Maoutsa et al., 2020; Song et al., 2021b; Chen et al., 2018) with the Backward SDE: For our case, Eq.###We use the architecture and the training hyperparameters from Song et al. (2021b) (full details can be found in the Appendix).###NFEs (Song et al., 2021a) sampler for CelebA-64 dataset using our blurring model.",impact-revealing,providing context on diffusion models and their applications
942,5f5f378a91e0117a861e8942,59c9ded17008ca35290551fdf8784f240bd808c0,Transfer Learning of Graph Neural Networks with Ego-graph Information Maximization,5bdc31b417c44a1f58a0b4e9,Deep Graph Infomax.,"Moreover, existing mutual information based GNNs such as DGI [54] and GMI [41] explicitly measure the mutual information between node features x and GNN output Ψ.###The self-supervised GNN baselines are GVAE [28], DGI [54] and two latest mutual information estimation methods GMI [41] and MVC [17].###Interestingly, direct application of GVAE, DGI and MVC that do not capture the input k-hop graph jointly, leads to rather limited and even negative transferrability (through comparison against the untrained GIN encoders).###To this end, several unsupervised GNNs are presented, such as the auto-encoder-based ones like VGAE [28] and GNFs [35], as well as the deep-infomax-based ones like DGI [54] and InfoGraph [50].###While training of the DGI can further improve the performance on the source graph, EGI shows the best performance there with the structure-relevant node degree features, corroborating the claimed effectiveness of EGI in capturing the essential graph information (i.e. recover the k-hop ego-graph distributions) as we stress in §3.###We randomly generate 40 graphs with the Forest-fire model (F) [32] and 40 graphs with the Barabasi model (B) [1], The GNN model is GIN [60] with random parameters (baseline with only the neighborhood aggregation function), VGAE[28], DGI [54], and EGI with GIN encoder.",impact-revealing,acknowledge existing mutual information based GNNs and their limitations
2838,5fbe5cf091e011e6e11b3cf5,15a84047e5145891d0c7ee054c00a00f2f5d38a1,Boosting Contrastive Self-Supervised Learning with False Negative Cancellation,5736960b6e3b12023e51e01a,Unsupervised Learning Of Visual Representations By Solving Jigsaw Puzzles,"Early work on self-supervised learning employs proxy tasks to guide the learned embeddings, such as predicting the angle of a rotated image [19], the relative location of patches [15], or organizing shuffled patches to recover the original image much like solving a jigsaw puzzle [36].###While conventional approaches use labeled data to pretrain visual representations, there has been a recent surge in self-supervised representation learning [19, 15, 36, 45, 38, 8, 50, 30].",other,highlighting the evolution and significance of self-supervised learning in representation learning
1400,,8be4fdf24d57bb78e8ef5fb126e17cc6aac1db3c,Decision Mamba: A Multi-Grained State Space Model with Self-Evolution Regularization for Offline RL,,,"###• Conservative Q-Learning (CQL) [29]: it encourages policies that are less likely to choose actions with high Q-value estimates that are uncertain or unreliable, thus expecting to address overestimation bias.###Offline Reinforcement Learning (RL) [13, 27, 29, 36] has attracted great attention due to its remarkable successes in the fields of robotic control [5, 34] and games [3, 48].###Offline Reinforcement Learning (RL) [7, 13, 22, 27, 29, 36, 54, 55, 65] is widely used for robotic control and decision-making.###We compare Decision Mamba with existing SOTA offline RL approaches including Behavioral Cloning (BC), Conservative Q-Learning (CQL) [29], Decision Transformer (DT) [8], Reinforcement Learning via Supervised Learning (RvS) [11], StARformer (StAR) [42], Graph Decision Transformer (GDT) [23], Waypoint…",impact-revealing,acknowledge existing offline reinforcement learning approaches
4024,599c797a601a182cd2641eda,d65ce2b8300541414bfe51d03906fca72e93523c,on calibration of modern neural network,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"We train state-of-the-art convolutional networks: ResNets (He et al., 2016), ResNets with stochastic depth (SD) (Huang et al., 2016), Wide ResNets (Zagoruyko & Komodakis, 2016), and DenseNets (Huang et al., 2017).###Recent research suggests that these normalization techniques have enabled the development of very deep architectures, such as ResNets (He et al., 2016) and DenseNets (Huang et al.###This is visualized in Figure 1, which compares a 5-layer LeNet (left) (LeCun et al., 1998) with a 110-layer ResNet (right) (He et al., 2016) on the CIFAR-100 dataset.###, 1998) with a 110-layer ResNet (right) (He et al., 2016) on the CIFAR-100 dataset.###Recent advances in deep learning have dramatically improved neural network accuracy (Simonyan & Zisserman, 2015; Srivastava et al., 2015; He et al., 2016; Huang et al., 2016; 2017).###On the other hand, the ResNet’s accuracy is better, but does not match its confidence.###Figure 4 contains reliability diagrams for 110-layer ResNets on CIFAR-100 before and after calibration.###It is now common to see networks with hundreds, if not thousands of layers (He et al., 2016; Huang et al., 2016) and hundreds of convolutional filters per layer (Zagoruyko & Komodakis, 2016).###We train state-of-the-art convolutional networks: ResNets (He et al., 2016), ResNets with stochastic depth (SD) (Huang et al.###The top performing ImageNet models of 2015 all use an order of magnitude less weight decay than models of previous years (He et al., 2016; Simonyan & Zisserman, 2015).###Recent research suggests that these normalization techniques have enabled the development of very deep architectures, such as ResNets (He et al., 2016) and DenseNets (Huang et al., 2017).",other,acknowledge advancements in deep learning architectures
3182,5ee7495191e01198a507f7ae,09bda461aa4911d0513e8e46dd39a4113947e450,Ansor : Generating High-Performance Tensor Programs for Deep Learning,5c757c35f56def9798a66484,Augmented Reality Meets Deep Learning for Car Instance Segmentation in Urban Scenes,"Low-latency execution of deep neural networks (DNN) plays a critical role in autonomous driving [14], augmented reality [3], language translation [15], and other applications of AI.",other,highlighting the importance of low-latency execution in various AI applications
3902,5db929e947c8f766461fd005,11ead744b2edbdabe174258a8dfe4023be8e501d,Hierarchical Attention Prototypical Networks for Few-Shot Text Classification,5550412045ce0a409eb38b4c,Convolutional Neural Networks for Sentence Classification.,"The neural network based methods like Kim (2014) applies convolutional neural networks for sentence classification.###The dominant text classification models in deep learning (Kim, 2014; Zhang et al., 2015a; Yang et al., 2016; Wang et al., 2018) require a considerable amount of labeled data to learn a large number of parameters.",other,highlighting the reliance on labeled data in deep learning models for text classification
1482,,c00cc15464e400ada89da6a743c89fc81195b7e0,Conditional Computation in Deep and Recurrent Neural Networks,,,"###, 2015) f (xi) = { xi xi > 0 αix else ELU (Clevert et al., 2015) f (x) = { x x > 0 α (e − 1) else Figure 2.###While the statistics induced by the ELU aren’t strictly zero-mean unit-variance, they are likely closer to the assumption than ReLU statistics, so dependence on proper weight initialization is less important.###Exponential-linear units (ELUs) (Clevert et al., 2015) are motivated by the observation that the mean of a ReLU activation over the samples in a batch can never be zero unless all of the activations are zero.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2033,,b61d365792f6d5b46cf01f4857f233f4e501e580,Long-Term Survival and Pattern of Recurrence After Resection of Small Hepatocellular Carcinoma in Patients With Preserved Liver Function: Implications for a Strategy of Salvage Transplantation,,,"###With the experience accumulated since our first adult-to-adult living-donor liver transplantation in 1996, we have shown that this procedure is safe for both the recipients and the donors, provided that patients are carefully selected.(39,40) In places such as Hong Kong and Japan, where living donor is the main source of graft for liver transplantation, primary resection backed up by salvage transplantation is a particularly appealing strategy for patients with small HCC and preserved liver function.",impact-revealing,highlighting the safety and effectiveness of living-donor liver transplantation based on accumulated experience
947,,396d04a630b626f55ce5fa42d4ee33176b2820ff,The Effect of Odor Valence on Facial Attractiveness Judgment: A Preliminary Experiment,,,"###The selection of different valence odors was motivated by those used in previous studies [7,15,29].###It is generally believed that facial attractiveness is evaluated mainly through visual cues [5,6], while olfactory cues can also modulate the judgment of facial attractiveness [7].###For example, odor valence strongly influences subjects’ likeability rating of neutral expression faces when the olfactory stimuli are delivered unconsciously [7], while this effect disappears when participants become aware of the odors [7].###This may partially explain why previous studies on the influence of odor on facial attractiveness have not been able to reach a consensus [7,13].",impact-revealing,highlighting the influence of previous studies on odor and facial attractiveness
3163,5ed7796e91e011e6e91120f0,8ade0f3bbe8d8251c5de9ef39886a13055f463e5,TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding,573696d96e3b12023e5d8f0a,Tea Party In The House: A Hierarchical Ideal Point Topic Model And Its Application To Republican Legislators In The 112th Congress,"Ideology detection in general could be naturally divided into two directions, based on the targets to predict: of the politicians [7, 24, 28], and of the ordinary citizens [1, 2, 5, 8, 13, 15–17, 20, 23, 29].",other,acknowledging different directions in ideology detection
1837,,40f3a71a125aabf015e8448b13c7c2f1f8a83f4f,On the relationships between models in protocol verification,,,"###The second difference is that [24] uses a combination of send and receive events, denoted a→ b : m, which at first sight looks like a synchronous communication model (where, in particular, the intruder cannot intercept messages), but this is not the case.###The second model is inspired by Paulson’s approach of message traces based on the Isabelle theorem prover [24].###There are further minor differences: in [24], we have two functions synth and analz that represent each a part of the DY closure, namely the synthesis and analysis rules, and instead of DY(IK ) the closure synth(analz (IK )) is employed, which in general is a proper subset of DY(IK ) (only when the protocol uses only atomic keys is executed in a typed model, these closures are equal).###Also, the way we use the set of permitted instances of agent names is not present in the rules of [24]; this is due to the fact that it does not need to be repeated in each step when given a concrete protocol, but necessary when we have an arbitrary protocol as a message pattern.###This formalization is close to the one of [24], but there are some differences.###However, there exists a wide variety of empirically succesful analysis techniques based on the models presented in this paper [7, 9, 10, 11, 13, 18, 25, 24, 2].###Besides a better understanding of the employed models, these results also pave the way for combining methods based on these models, in particular, connecting automated verification procedures with a formalization in the theorem prover Isabelle in the style of [24].",impact-revealing,highlighting differences and improvements in communication models
878,5da1a6d447c8f7664606888c,404d82a8da658f16c46714442155490d79a413f1,temporal prefetching without the off-chip metadata,53e9bdfdb7602d9704aafae1,Using a user-level memory thread for correlation prefetching,"Therefore, early temporal prefetchers explore designs that reduce the traffic and latency costs of accessing an off-chip Markov table [10, 43].",impact-revealing,highlighting the significance of early temporal prefetchers in reducing traffic and latency costs
551,5aed14d117c44a4438158af2,30f86d38f0660af5ea2e16d996434c72eee8c5ee,espnet: end-to-end speech processing toolkit,5736986b6e3b12023e72ff37,Attention-Based Models for Speech Recognition,"ESPnet uses a location-aware attention mechanism [35], as a default attention.###, [33] does not use any language models, while [35] and [11] use a word-based language model through FST), we cannot directly compare them.###5 seq2seq + FST word LM [35] CER N/A 3.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
416,59ae3c262bbe271c4c71e9e8,718e1b453fe9dce79458e0db035091db603775fb,Deep Pyramid Convolutional Neural Networks for Text Categorization,5736986c6e3b12023e730a63,Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding,"Note that ShallowCNN enhanced with unsupervised embeddings (row 2) was originally proposed in (Johnson and Zhang, 2015b) as a semi-supervised extension of (Johnson and Zhang, 2015a), and then it was tested on the large datasets in (Johnson and Zhang, 2016).###In (Johnson and Zhang, 2015b), we showed theoretical conditions on views and labels under which###In (Johnson and Zhang, 2015b unsupervised embeddings obtained this way are useful for classiﬁcation.###• Text region embedding enhanced with unsupervised embeddings (embeddings trained in an unsupervised manner) (Johnson and Zhang, 2015b) for improving accuracy.###Enhancing region embedding with unsupervised embeddings In (Johnson and Zhang, 2015b, 2016), it was shown that accuracy was substantially improved by extending ShallowCNN with unsupervised embeddings obtained by tv-embedding training (‘tv’ stands for two views ).###We take a more general viewpoint as in (Johnson and Zhang, 2015b) and consider text region embedding – embedding of a region of text covering one or more words.###• Text region embedding enhanced with un-supervised embeddings (embeddings trained in an unsupervised manner) (Johnson and Zhang, 2015b) for improving accuracy.###While simple and shallow convolutional neural networks (CNNs) (Kim, 2014; Johnson and Zhang, 2015a) were proposed for this task earlier, more recently, deep and more complex neural networks have also been studied, assuming availability of relatively large amounts of training data (e.g., one million…###Moreover, DPCNN can be regarded as a deep extension of ShallowCNN, which we proposed in (Johnson and Zhang, 2015b) and later tested with large datasets in (Johnson and Zhang, 2016).###A region embedding layer with the sequential input is equivalent to a convolution layer applied to a sequence of one-hot vectors representing a document, and this viewpoint was taken to describe the ﬁrst layer of ShallowCNN in (Johnson and Zhang, 2015a,b).",impact-revealing,reporting the evolution and testing of ShallowCNN with unsupervised embeddings
2165,,9fce35d05b3aa259e0ee03356fa2d2aacd5572b3,Temporal Logic Programming,,,"###Because the concept of time is directly built into the formalism, temporal logic has been widely used as a specification language for programs where the notion of time is central (e.g., Pnueli, 1981; Lamport, 1983).",impact-revealing,providing context for the use of temporal logic in program specification
3886,53e99ae2b7602d97023691f2,f71f02c06192cd8ec06c30f5d373dcf509edd8ba,temporal instruction fetch streaming,53e999cbb7602d9702212df6,Efficient Representations And Abstractions For Quantifying And Exploiting Data Reference Locality,"Like similar studies of repetitive streams in L1 data accesses [6], off-chip data misses [35, 36], and program paths [16], we use the SEQUITUR [10] hierarchical data compression algorithm to identify repetitive sub-sequences within the miss-address traces.",other,drawing parallels with similar studies using SEQUITUR for data analysis
192,5cf48a45da56291d582a8448,f937d6482ad162f55022c7dce5857855ece27c1e,Knowledge Graph Convolutional Networks for Recommender Systems with Label Smoothness Regularization,53e9b7eeb7602d970439e002,Hyperparameter Learning for Graph Based Semi-supervised Learning Algorithms,"Moreover, edge weights do play an essential role in learning tasks on graphs, as highlighted by a large amount of prior works [9, 19, 20, 34, 37].###To solve the issue, we propose minimizing the leave-one-out loss [34].###We propose taking label smoothness (LS) [34, 37] as the additional regularization, which assumes that adjacent entities in the KG are likely to have similar labels.###Based on different settings of edge weights in the input graph, these methods are classified as: (1) Edge weights are assumed to be given as input and therefore fixed [1, 36, 37]; (2) Edge weights are parameterized and therefore learnable [9, 20, 34].",impact-revealing,highlighting the importance of edge weights in graph learning tasks
3114,5cede104da562983788e4508,6303bac53abd725c3b458190a6abe389a4a1e72d,deep high-resolution representation learning for human pose estimation,58d82fced649053542fd7355,Pyramid Scene Parsing Network,"There are related multi-scale networks for classification and segmentation [5, 8, 72, 78, 29, 73, 53, 54, 23, 80, 53, 51, 18].",other,acknowledge related multi-scale networks
835,53e99822b7602d9702041bcd,5e4c84225338263e74c0a41eac4938f9a18fedb5,spatial memory streaming,53e99d7ab7602d9702635311,Exploiting spatial locality in data caches using spatial footprints,"patterns with the code and/or data address that initiates the pattern [4,17].###The precise interval over which a spatial region generation is defined can significantly impact the accuracy and coverage of spatial patterns [17].###Whereas existing spatial pattern prefetching designs are effective for desktop/engineering applications [4], the only practical implementation evaluated on server workloads provides less than 20% miss rate reduction [17].###Our result contradicts a prior study of uniprocessor OLTP and web traces [17], which indicated that PC+address provides superior coverage.###Contrary to previous findings [17], address-based correlation is not needed to predict the access stream of commercial workloads.###We show that the cache-coupled structures used in previous work ([4,17]) are suboptimal for observing spatial correlation.###We formalize our notion of spatial correlation similar to prior studies of spatial footprints [4,17].###For SPEC CPU 2000 applications, PC+address indexing can be approximated by combining the PC with a spatial region offset [4,17].###Prior studies of spatial predictors [4,17] advocate predictor indices that include address information.###Past predictors [4,17] couple the predictor training structure to a sectored (i.",impact-revealing,highlighting the limitations of existing spatial pattern designs and their impact on performance
3627,5c2c7a9217c44a4e7cf3189c,9ae43e25b04f5c35173b0bf490612015bd86c08f,Face-Focused Cross-Stream Network for Deception Detection in Videos,5a4aef9e17c44a2190f7a794,Temporal Relational Reasoning in Videos,"Various improvements such as temporal segment network (TSN) [47] and its variants [60, 62] have been designed by capturing the long-range temporal structure and learning the ConvNet models with limited training samples.",other,acknowledge advancements in temporal segment networks
908,5f6f14c49fced0a24bb647ec,c980c5ab5944260c95891fbc8c1e556171333442,DPDDI: a deep predictor for drug-drug interactions,599c782b601a182cd25a765e,Predicting drug-drug interactions through drug structural similarities and interaction networks incorporating pharmacokinetics and pharmacodynamics knowledge.,"Most methods utilize a single predictor [2, 5-8, 13-16], while some of them integrate multiple predictors to obtain the final prediction [10, 12].###The feature extractor represents drugs in a form of feature vector according to drug properties, such as chemical structure[2, 6-14], targets[2, 8-11],###The supervised predictor is usually implemented by classifier algorithms, such as KNN[12], SVM[12], logistic regression[2, 8, 10], decision tree[10], naïve Bayes[10]), and network propagation methods, such as reasoning over drug-drug network structure[6-8], label propagation[13], random walk[11, 15], probabilistic soft logic[9, 10]) or matrix factorization[14].###alert to potential DDIs before a combinational treatment is made [2].",impact-revealing,acknowledge variations in methods for drug prediction
1672,,594b02d980b2d4f0faad4e231a6299cc255942c4,Social Moral Licensing,,,"###Conversely, they ascribe negative attributes to members of outgroups (Tajfel and Turner 1979).###First, we build on social-psychological theories and concepts to discuss moral licensing from a social perspective (e.g., social identity theory: Tajfel and Turner 1979; theory of collective self-esteem: Crocker and Luhtanen 1990; focus theory of normative conduct: Cialdini et al.###Social identity theory (Tajfel and Turner 1979) postulates that, when the group context is relevant, individuals derive their self-concept partly from group identification (social identity).",impact-revealing,providing context for moral licensing from a social perspective
1912,,5a7ea558e7b1cbc36a80ffeaedf9906f054d7d67,The role of the tutor in supporting online engagement in higher education,,,"###As noted previously, for Garrison et al. (2000), Direct Instruction involves
201
the instructor providing intellectual and scholarly leadership in part through sharing subject matter knowledge with their students.###73 Research Design The previously outlined COI framework of online learning (Garrison et al., 2000) guided this research.###This phase of the study was specifically related to the COI framework (Garrison et al., 2000) used to investigate the discussion board posts of four selected tutors.###The chapter begins with a summary of the data presented in Chapters Five to Eight, and then presents quantitative information about tutor posts within and across the teaching presence indicators as defined within the COI framework (Garrison et al., 2000).###Significance of the Study
This research complements other research using the Teaching Presence
element of the COI framework (Garrison et al., 2000).###Significance of the Study This research complements other research using the Teaching Presence element of the COI framework (Garrison et al., 2000).###, 2005; Salmon, 2013), with students constructing their knowledge through interactive engagement with content, their peers and guided through the process by their tutor (Garrison et al., 2000; Pawan et al., 2003).###For Garrison et al. (2000), direct instruction was contextualised as the
instructor providing intellectual and scholarly leadership through the sharing of their subject matter knowledge with their students.###Garrison et al. (2000) claimed that many students are reluctant to move out of their comfort zone where they can continue to explore without ever having to advance to critical thinking and cognitive development.###…proposition that learning is done best through collaboration with others (Bento et al., 2005; Salmon, 2013), with students constructing their knowledge through interactive engagement with content, their peers and guided through the process by their tutor (Garrison et al., 2000; Pawan et al., 2003).###When Garrison et al. (2000) were
conceptualising their framework, they originally named this element “building understanding” (Swan & Ice, 2010).###The examples given for individual indicators are those suggested by Garrison et al. (2000).###Garrison et al. (2000) identified teaching presence as being essential in the development of learning communities, with the social aspects of learning being fundamental prerequisites to the process.###According to Garrison et al. (2000) three key elements are required in an interactive learning environment: cognitive presence, social presence and teaching presence.###The previously outlined COI framework of online learning (Garrison et al.,
2000) guided this research.###Specific Contribution of this Research
This research complements other research using the Teaching Presence
element of the COI framework (Garrison et al., 2000).###Cognitive presence is defined as “the extent to which learners are able to construct and confirm meaning through sustained reflection and discourse in a critical community of inquiry” (Garrison et al., 2000).###Garrison et al. (2000) also argued that teaching presence is the glue that holds their model together to sustain learning.###The comments used to identify the indicators within the table are those suggested by Garrison et al. (2000).###Garrison et al. (2000) conceptualised this element as the means by which
students are encouraged to engage and interact in ways that build upon the information that is provided, so one could expect this to be a large component of the discussion posts of these online tutors.###A Framework of Technology-Mediated Interaction for Education 37 Figure 2.3 - Community of Inquiry Model 48 Figure 2.4 - Modes of Interaction in Distance Education 53
1
CHAPTER ONE
Introduction and Context
This chapter introduces the study and lays the foundation for this thesis.###…component of online learning environments, interactions by themselves were considered to be insufficient in ensuring effective online learning, with cognitive engagement and critical discourse by students identified as needing an emphasis (Garrison et al., 2000; Garrison & ClevelandInnes, 2005).###Key findings The findings from this phase of the research confirm the work of many
previous authors (Anderson & Dron 2011; Conceicao & Lehman, 2011; Dowson & McInerney, 2003; Garrison et al., 2000; Northcote, 2009; Palloff & Pratt, 2011; Reeves et al., 2005; Reushle, 2005).###The chapter begins with a summary of the data presented in Chapters Five to
Eight, and then presents quantitative information about tutor posts within and across the teaching presence indicators as defined within the COI framework (Garrison et al., 2000).###Key findings The findings from this phase of the research confirm the work of many previous authors (Anderson & Dron 2011; Conceicao & Lehman, 2011; Dowson & McInerney, 2003; Garrison et al., 2000; Northcote, 2009; Palloff & Pratt, 2011; Reeves et al., 2005; Reushle, 2005).###Although interactions between participants were acknowledged as being a necessary component of online learning environments, interactions by themselves were considered to be insufficient in ensuring effective online learning, with cognitive engagement and critical discourse by students identified as needing an emphasis (Garrison et al., 2000; Garrison & ClevelandInnes, 2005).###Garrison et al. (2000) hypothesised that high levels of social presence along
with high levels of learner commitment and participation are essential prerequisites for the development of higher order thinking.###Community of Inquiry Model (Garrison et al., 2000) Reprinted with permission from authors.###I adopted the teaching presence element of the COI framework (Garrison et al., 2000) as a lens to identify the issues involved in being a tutor in an online teaching degree.###Cognitive presence is defined as “the extent to which learners are able to
construct and confirm meaning through sustained reflection and discourse in a critical community of inquiry” (Garrison et al., 2000).###I utilised the Teaching Presence element of the Community of Inquiry (COI) framework (Garrison et al., 2000) as a lens to view the issues involved in being a tutor in an online teaching degree.",impact-revealing,highlighting the significance of the Community of Inquiry framework in online learning
278,5d1eb9dbda562961f0b15d56,1e9c4b836c2693732961017a226780785b3612ba,Adversarial Representation Learning on Large-Scale Bipartite Graphs,57aa28de0a3ac518da9896d5,node2vec: Scalable Feature Learning for Networks,"Compared to the Node2Vec C++ high-performance library, ABCGraph model’s training time is comparable.###Traditional graph embedding approach such as DeepWalk[24]and Node2Vec[10] can not incorporate the various feature dimensions into learning.###Our model does not need to perform random walk preprocessing like GraphSAGE and Node2Vec, which also saves the training time.###So far, graph convolutional networks (GCNs) offers a promise for graph embedding using convolution operators to learn over graph structures [17, 25], which can perform better than Node2Vec.###The pioneer work DeepWalk [24] and Node2vec [10] extend the idea of Skip-gram [21] to model homogeneous network.###The Node2Vec is a high performance version (C++), so its running time is compariable to ours.###We evaluate the performance of our ABCGraph algorithm against four unsupervised graph embedding baselines: random walk[7, 24] based Node2Vec, GCN, GraphSAGE, variational inference [15] based variational graph auto-encoder (VGAE).###We evaluate the performance of our ABCGraph algorithm against four unsupervised graph embedding baselines: Node2Vec[10], standard GCN[17], VGAE[16], and GraphSAGE[11].",impact-revealing,comparing the performance and efficiency of different graph embedding methods
3080,5c04966a17c44a2c74708959,6b98bef930182a848c027dece1bfb58ca706449d,Improving End-to-end Speech Recognition with Pronunciation-assisted Sub-word Modeling,5aed14d617c44a4438158cf1,On Modular Training of Neural Acoustics-to-Word Model for LVCSR,"Although modular training of those components is possible [1], an end-to-end model is usually jointly optimized during training.",other,acknowledge training approaches in model optimization
934,53e99c53b7602d9702504c04,5d29583818f4b3c7a22de39cd82046bb582adcef,LLVA: a low-level virtual instruction set architecture,53e9b01db7602d9703a7942b,Dynamo: a transparent dynamic optimization system,"Unlike other trace-driven runtime optimizers for native binary code, such as Dynamo [4], we have both the rich V-ISA and a cooperating code generator.###Transmeta’s CMS [11] and Dynamo [4] identify and optimize hot traces at runtime, similar to our reoptimization strategy but without the benefits of a rich VISA.###t y p e d e f s t r u c t QuadTree { double Data ; s t r u c t QuadTree ∗ C h i l d r e n [ 4 ] ; } QT;",impact-revealing,highlighting the advantages of their approach over existing trace-driven runtime optimizers
2568,5db92aec47c8f76646216865,146128184cd585b5a1298d3e75e15fb8fbbbd7a5,Improved low-resource Somali speech recognition by semi-supervised acoustic and language model training,5bbacb3717c44aecc4eabdc4,Semi-Orthogonal Low-Rank Matrix Factorization For Deep Neural Networks,"Index Terms: speech recognition, Somali, semi-supervised, TDNN-F, under-resourced language###In comparison with our previous ASR system [9], the improvement afforded by TDNN-F is clear (rows 1 and 2).###A baseline TDNN-F acoustic model was trained using this multilingual data and semi-supervised training was carried out in three passes.###It has recently been shown that, when semi-orthogonal lowrank matrix factorisation is applied to the parameter matrices of TDNN layers, ASR performance can be improved in lowresource situations [13].###We make use of factorised time-delay neural networks (TDNN-F) for acoustic modelling, since these have recently been shown to be effective in resourcescarce situations.###Hence, the TDNN-F models are faster to train.###This factorisation allows the TDNN-F model to use fewer parameters than hybrid architectures such as TDNN-LSTM and TDNN-BLSTM (bidirectional LSTM).###We make use of TDNN-F acoustic models and experiment with the incorporation of additional but unannotated Somali speech data by semisupervised training, an approach which has been applied successfully in some other low-resource settings [8, 14–16].###Consequently, a TDNN-F acoustic model (10 time-delay layers followed by a rank reduction layer) was trained using the Librispeech recipe for Kaldi (version 5.2.164).###Even though TDNN-F uses only half the number of parameters as CNNTDNN-BLSTM, it is able to offer better performance.###Our TDNN-F was trained using the lattice-free maximum mutual information objective criterion [22].###The recently-introduced factorised time-delay neural networks (TDNN-F) [13] utilise half the number of parameters than the hybrid networks with comparable performance, in particular in a low-resource setting.",other,highlighting the effectiveness of TDNN-F models in low-resource ASR settings
1449,,21173941ac551b517b8867204b3840d840ee708b,Multi Scale Attention Network for Crowd Counting,,,"###Here we only summarize the works which are research on similar problems: multi scale counting network, multi learning objectives counting, structural similarity in crowd counting and subway crowd counting Multi Scale counting network: PACNN [12] proposes multi scale perspective-aware network for crowd counting, which is inspired by the perspective geometry of a pinhole camera.",impact-revealing,acknowledge existing research on crowd counting methods
3428,5bdc318017c44a1f58a08780,5ab5658a1666e26c66f0319a469228dbe19598a2,An e-learning recommendation approach based on the self-organization of learning resource,53e9a2ddb7602d9702bea8fc,Adaptive Learning Resources Sequencing in Educational Hypermedia Systems,"Currently, recommender systems obtain the changes in learner preferences by analyzing learners’ behaviors or their ability tests, then adjust recommendation strategies to make appropriate recommendations [37, 38].",other,acknowledge existing methods in recommender systems
1526,,9729bbfe3f46bc30036dcd4654a871993f514316,A Training Model for Addressing Microaggressions in Group Psychotherapy,,,"###Microassaults are similar to “oldfashioned racism” and are defined as “explicit derogation meant to hurt the intended victim” (Sue et al., 2007, p. 274).###Finally, microinvalidations are generally unconscious and covert and are motivated by egalitarian values, antiminority feelings, and implicit bias (Sue et al., 2007).###For the target of a microaggression, initial reactions can include feelings of shock, frustration, or denial; reliving the event; and attempts to force the event out of memory (Smith, Allen, & Danley, 2007; Sue et al., 2007).###Examples of microinsults include assumptions that people of color are not qualified (e.g., “How did you get your job?”) or a teacher failing to acknowledge a student of color (Sue & Capodilupo, 2008; Sue et al., 2007).###, “How did you get your job?”) or a teacher failing to acknowledge a student of color (Sue & Capodilupo, 2008; Sue et al., 2007).###Microaggressions are described as subtle forms of discrimination, often unintentional and unconscious, that send negative and denigrating messages to a person or group based on an identity that has historically has been marginalized (Pierce et al., 1977; Smith & Shin, 2008; Sue & Capodilupo, 2008; Sue et al., 2007).###…described as subtle forms of discrimination, often unintentional and unconscious, that send negative and denigrating messages to a person or group based on an identity that has historically has been marginalized (Pierce et al., 1977; Smith & Shin, 2008; Sue & Capodilupo, 2008; Sue et al., 2007).###Over time, the nature of its expression has shifted from overt behaviors toward more covert manifestations, which are commonly described with terms such as “aversive racism,” “modern racism,” and “racial microaggressions” (Pierce, Carew, Pierce-Gonzalez, & Wills, 1977; Sue et al., 2007).###…that patients of color who are exposed to microaggressions may prematurely terminate psychotherapy, view their psychotherapists as less multiculturally competent, feel misunderstood by their psychotherapists, and have strong feelings of frustration and anger (Constantine, 2007; Sue et al., 2007).###The target has to weigh whether to call attention to those lenses, which is a difficult decision, especially if the perpetrator does not recognize that they have a lens to begin with (Sue et al., 2007).###Other authors have theorized that patients of color who are exposed to microaggressions may prematurely terminate psychotherapy, view their psychotherapists as less multiculturally competent, feel misunderstood by their psychotherapists, and have strong feelings of frustration and anger (Constantine, 2007; Sue et al., 2007).###Microaggressions usually take place in “private” (micro) situations, which allows for some degree of anonymity to remain for the perpetrator (Sue et al., 2007).###Those who do respond are often seen as hypersensitive or overreactive (Sue et al., 2007).",impact-revealing,providing context and definitions for microaggressions
2234,5aed148b17c44a4438154fae,fa54b47df8641dff1579b5e8e0f18f057de68e73,DRN: A Deep Reinforcement Learning Framework for News Recommendation,53e99a92b7602d9702306a53,Introduction to Information Retrieval,CTR = number of clicked items number of total items (9) • Precision@k [10].###• CTR. [10] Click through rate is calculated as Equation 9.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3477,5736982b6e3b12023e6fd099,684a466028785c39911770b96fb0c814e75b5b6d,DynaMOS: Dynamic schedule migration for heterogeneous cores,53e9bbf5b7602d9704851d5f,Single-ISA Heterogeneous Multi-Core Architectures for Multithreaded Workload Performance,"To address this disparity, researchers have designed heterogeneous multi-core processors [2] in which an application is mapped to the most efficient core that meets its performance requirements.###Higher switching costs of coarse-grained heterogeneous systems [1, 2] enforce switching granularities of the order of milli-seconds.",other,highlighting advancements in heterogeneous multi-core processors
2188,,fa7f970d8c3013cd0f64d94fe2ee37205f0968e1,CYBER-PHYSICAL SYSTEMS ( DCPS ),,,"###Cloud computing embraces cyber-infrastructure, and builds upon decades of research in virtualization, distributed computing, utility computing, and more recently networking, web and software services, Vouk (2008).",impact-revealing,providing context on the evolution and foundation of cloud computing
1863,,ffd33b33295a0b488ce9c9f41a1bba939f0cc77d,Measuring Resilience Engineering: An Integrative Review and Framework for Bench-Marking Organisational Safety,,,"###The specific methods used for this review, adapted from [37], comprised of five stages, which are discussed below.###This research utilized an integrative review, a method commonly used for evaluating strengths of evidence, identifying gaps in research, connecting related areas of published research, generating research question(s), identifying theoretical or conceptual frameworks, and exploring research methods [37].###Key authorities, such as Whittemore [37] and Soares, Hoga, Peduzzi, Sangaleti, Yonekura and Silva [39] have suggested a five stage approach—comprised of problem identification, literature search, data evaluation, data analysis, and presentation—be utilised.",impact-revealing,describing the review methodology and its stages
831,5e54f1813a55acae32a25da5,76b8d5f2ef97d71167aa78309918bf3f7d633c96,Author Name Disambiguation on Heterogeneous Information Network with Adversarial Representation Learning,62376b385aee126c0f09faaf,Generative Adversarial Nets,"GraphGAN (Wang et al. 2018a) combines a designed generative model called Graph Softmax which tries to approximate the underlying true connectivity distribution and a discriminative model which predicts whether the edge exists between two nodes.###IRGAN (Wang et al. 2017) unifies generative model and discriminative model in information retrieval, where the discriminative model provides guidance to the generative model, and the generative model generates difficult examples for the discriminative model.###KBGAN (Cai and Wang 2018) implements the similar motivation in knowledge embedding task, which uses one compositional model as a generator to generate high-quality negative samples for the discriminative model.###Inspired by generative adversarial networks (Goodfellow et al. 2014), we may combine the two categories in an adversarial way.###Recently, generative adversarial nets (GAN) (Goodfellow et al. 2014) has attracted a great deal of attention.###Original purpose of GAN is to generate data from the underlying true distribution, e.g., image (Denton et al. 2015), sequence (Yu et al. 2017), dialogue (Li et al. 2017).",impact-revealing,acknowledge existing generative adversarial network models and their applications
1105,,377d9e78d7df52aef5fe6f27b5832868b1072ac1,Plug-and-Hide: Provable and Adjustable Diffusion Generative Steganography,,,"###(1) is now commonly seen in recent diffusion model [24], [28], [29], and is widely used for image generation.###(2) that is adapted to most existing DMs with different noise schedulers, such as, DDIM [24], VP-SDE [29], and VE-SDE [29], without any training or finetuning. ii) Efficiency.###With the advent of DMs (Deep Models), Wei et al. [23] introduced Generative Steganography Diffusion (GSD), a technique leveraging De-noising Diffusion Implicit Models (DDIMs) [24].",impact-revealing,acknowledge advancements in diffusion models for image generation
702,5ce937225ced2477cb328bef,0cee612a4481859e72d23a611cc359120d0e2cba,PRIMAL: Power Inference using Machine Learning,5534d8ba45cedae85c3795a4,Dynamic power and performance back-annotation for fast and accurate functional hardware simulation,"Later eorts use the HLS tool to perform scheduling and back-annotation, and rely on RTL power analysis [3], gate-level power analysis [21], or a per-control-step ML power model [14] for power estimation.###There exists a rich body of research on power analysis at RTL or a higher abstraction level [3, 4, 6, 7, 14, 20, 21, 23, 24].",impact-revealing,acknowledge existing research on power analysis methods
3341,5f03f3b611dc83056223206d,75c8466a0c1c3b9fe595efc83671984ef95bd679,XGNN: Towards Model-Level Explanations of Graph Neural Networks,5c8a11324895d9cbc6121c34,Neural Message Passing for Quantum Chemistry.,"h as node classification [11, 37], graph classification [39, 47], and link prediction [46]. In addition, extensive efforts have been made towards different graph operations, such as graph convolution [13, 16, 19], graph pooling [20, 44], and graph attention [10, 36, 37]. Since graph data widely exist in different real-world applications, such as social networks, chemistry, and biology, GNNs are becoming incre",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
877,5ea6adfa91e011a546871d52,7066df8fd89cca546d1ef3d66679cb15eba48d50,flat: chinese ner using flat-lattice transformer,5dcbd5da3a55ac789b0dbbdd,TENER: Adapting Transformer Encoder for Name Entity Recognition,"Our model outperforms TENER (Yan et al., 2019) by 1.72 in average F1 score.###We take BiLSTM-CRF and TENER (Yan et al., 2019) as baseline models.###Inspired by Yan et al. (2019), we think commuta-tivity of the vector inner dot will cause the loss of directionality in self-attention.",impact-revealing,highlighting the improvement of the proposed model over a baseline
869,5eede0b791e0116a23aafe75,150f95f9c73820e0a0fa1546140e9f2bdfd25954,temporal graph networks for deep learning on dynamic graphs,5e5e18bb93d709897ce2bace,Inductive representation learning on temporal graphs,"The gap is particularly large on the Twitter dataset, where we outperfom the second-best method (TGAT) by over 25%.###Our experimental setup follows (65) using the same splits.###Differently from the original formulation of this layer (firstly proposed in TGAT (65)) where no node-wise temporal features were used, in our case the input representation of each node h j (t) = sj(t)+vj(t) and as such it allows the model to exploit both the current memory sj(t) and the temporal node features vj(t).###TGAT [66] is a speciﬁc case of TGN when the memory and its related modules are missing, and graph attention is used as the Embedding module.###Differently from the original formulation of this layer (ﬁrstly proposed in TGAT [66]) where no node-wise temporal features were used, in our case the input representation of each node h (0) j ( t ) = s j ( t ) + v j ( t ) and as such it allows the model to exploit both the current memory s j ( t ) and the temporal node features v j ( t ) .###While it is often possible to apply static graph deep learning models (37) to dynamic graphs by ignoring the temporal evolution, this has been shown to be sub-optimal (65), and in some cases, it is the dynamic structure that contains crucial insights about the system.###Our strong baselines are state-of-the-art approaches for continuous time dynamic graphs (CTDNE [47], Jodie [36], and TGAT [66]) as well as state-of-the-art models for static graphs (GAE [34], VGAE [34], DeepWalk [51], Node2Vec [23], GAT [61] and GraphSAGE [27]).###Our results for GAE [34], VGAE [34], DeepWalk [51], Node2Vec [23], GAT [61] and GraphSAGE [27], CTDNE [47] and TGAT [66] are taken directly from the TGAT paper [66].###While in TGAT having 2 layers is of fundamental importance for obtaining good performances (TGAT vs TGAT-1l), in TGN the presence of the memory makes it enough to use 1 layer to obtain very high performances (TGN-attn vs TGN-2l).###Due to the efﬁcient parallel processing and the need for only one graph attention layer (see section 5.3 for the ablation study on the number of layers), our model is up to 3 × faster than Jodie and about 19 × faster than TGAT to complete a single epoch (see Figure 3a), while requiring a similar number of epochs to converge.###Our strong baselines are state-of-the-art approaches for continuous time dynamic graphs (CTDNE (47), Jodie (36), and TGAT (65)) as well as state-of-the-art models for static graphs (GAE (34), VGAE (34), DeepWalk (51), Node2Vec (23), GAT (61) and GraphSAGE (27)).###Here, φ(·) represents a generic time encoding (65), ‖ is the concatenation operator and zi(t) = emb(i, t) = h (L) i (t).",impact-revealing,highlighting performance improvements over existing methods
3435,5ecbc6199fced0a24b4eefa9,3c31c95870824736ecaba2ba01f2ccab145fd91d,Leveraging Unpaired Text Data for Training End-To-End Speech-to-Intent Systems,5bbacb4c17c44aecc4eac62e,Spoken Language Understanding Without Speech Recognition,"In contrast, an end-to-end (E2E) SLU system [1–7] processes speech input directly into intent without going through an intermediate text transcript.",other,highlighting the distinction between end-to-end SLU systems and traditional methods
1165,,79697a392789216834a9bfeeb3a4b00fa391980b,On Achieving Asynchronous Energy-Efficient Neighbor Discovery for Mobile Sensor Networks,,,"###Birthday protocol [25] is one of the most well-known probabilistic neighbor discovery protocols, the main idea of which is inspired by the Birthday Paradox [26].###There are many neighbor discovery protocols [22]–[24] proposed recently, which can be classified into two categories: probabilistic neighbor discovery protocols [25] and deterministic neighbor discovery protocols [19].",impact-revealing,providing context on neighbor discovery protocols
1125,,bbfe431a6601af39735cd8040fe7cdb6f37e4b39,MRPD: Undersampled MRI reconstruction by prompting a large latent diffusion model,,,"###It builds upon a deterministic DDIM sampler [58], with several novel phase-modulated guidance mechanisms to enhance its efficacy for complex-valued MRI.###Table IX shows the performance and inference time of our methods with DDIM acceleration [58] on single-coil FastMRI (R=4 × ).###Denoising diffusion implicit model (DDIM) sampler [58] is the backbone of MRPD.",impact-revealing,describing the method and its enhancements for MRI
3918,5e524da993d709897cb69fde,09e48adf1a3f012ec56fbb4f92acae975ee43d87,a deep learning approach to antibiotic discovery,5c0f76cada562944ac701a7e,A Common Platform for Antibiotic Dereplication and Adjuvant Discovery.,"Natural product discovery is now plagued by the dereplication problem, wherein the same molecules are being repeatedly discovered (Cox et al., 2017).",other,highlighting a significant issue in natural product discovery
2312,5e4672c93a55ac14f595d7f3,7784dd6586ce5d4c8bfd020a23e9ad52378889b6,improving deep learning for airbnb search,5bdc31c217c44a1f58a0c898,Applying Deep Learning To Airbnb Search,"In this paper we capture the major enhancements that followed the launch of the DNN described in [6].###Our account of the journey in [6] brought us in conversation with many industry practitioners, allowing us to exchange insights and critiques.###When tested online in an A/B experiment against the fully connected two layer DNN from [6], the two tower architecture recorded a bookings gain of +0.###When tested online as an A/B experiment against the two hidden layer DNN from [6], average price of search results dropped by −5.###_x008c_e plots suggested that the fully connected two layer DNN from [6] already understood cheaper was be_x008a_er.###Along those lines, we started with the observation that the series of successful ranking model launches described in [6] were not only associated with an increase in bookings, but also a reduction in the average listing price of search results.",other,highlighting the impact of DNN enhancements on bookings and pricing
3793,5bdc315017c44a1f58a05a1d,f9fb0ad85f903e3ecf5d4bafdf7c74f647e75f58,LEMNA: Explaining Deep Learning based Security Applications,5a260c8617c44a4ba8a322df,Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms,5The image is selected from the Fashion-mnist dataset [69].,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1431,,69acc5f67ea7dba13c58d7281b8f0a25ed64f0e8,EVIL: Exploiting Software via Natural Language,,,"###NMT has emerged as a promising machine translation approach, and it is widely recognized as the state-of-the-art method for the translation of different languages [5], [6].",impact-revealing,highlighting the significance of neural machine translation as a state-of-the-art approach
3371,5f7fdd328de39f0828397afd,c841c9704bf35873a051f228a15f67b30d650c2f,Scalable Graph Neural Networks via Bidirectional Propagation,5b67b45517c44aac1c860876,Graph Convolutional Neural Networks for Web-Scale Recommender Systems.,"Recently, the field of Graph Neural Networks (GNNs) has drawn increasing attention due to its wide range of applications such as social analysis [23, 20, 28], biology [10, 26], recommendation systems [36], and computer vision [39, 7, 13].",other,highlighting the growing interest and diverse applications of Graph Neural Networks
2889,5b67b45517c44aac1c860823,7f01c6fe27f57ee6191b51efa18b9199baf7b82a,Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System,53e9ab48b7602d97034dc28f,Recurrent Neural Network Based Language Model,"us parameters are shown to have better effectiveness and good representation power [21]. Recently, with the great impact of neural networks on computer vision [17, 22] and natural language processing [19, 25], a new branch of IR using neural networks has shown strong performances. As neural networks have incredible power to automatically capture features, recent works use neural networks to capture semant",other,highlighting the impact of neural networks on information retrieval
2877,5aed14d117c44a4438158af2,30f86d38f0660af5ea2e16d996434c72eee8c5ee,espnet: end-to-end speech processing toolkit,57d063e8ac44367354294dc9,On Training The Recurrent Neural Network Encoder-Decoder For Large Vocabulary End-To-End Speech Recognition,"1Language modeling is often performed by external language model toolkits, for example SRILM [6] network [13, 14, 15, 16].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2338,5e5e190b93d709897ce4997e,cb4571fa905abb70868d0bb9d4681f0a612c2d0f,Differentiable Reasoning On Large Knowledge Bases And Natural Language,57a4e91aac44365e35c97887,Complex Embeddings for Simple Link Prediction,"In addition, we consider DistMult (Yang et al. 2015) and ComplEx (Trouillon et al. 2016), two state-of-the-art black-box neural link predictors suited for large datasets.###In terms of ranking accuracy, GNTPs is comparable to state-of-the-art models, such as ComplEx and KB LR .###4 performs close to ComplEx (Dettmers et al. 2018) ( 44 .###2 MRR for ComplEx and 93 .",other,comparing ranking accuracy of GNTPs with state-of-the-art models
4032,5ed12ca69e795e8ab1c11568,b3b60a15502abca31ae7b20ce16bf50049183c5a,knowledge enhanced personalized search,5a260c3517c44a4ba8a2533a,JointSem: Combining Query Entity Linking and Entity based Document Ranking.,"entities and relations from knowledge graphs, in search systems and effectively improves the text representation and ranking accuracy [14, 19, 21, 28, 40, 42, 43].",other,highlighting the effectiveness of knowledge graphs in search systems
82,5fdb2e1691e0118a02c4f566,16913a534b1630d33770b392767bb316f4fdb11e,Clinical Temporal Relation Extraction with Probabilistic Soft Logic Regularization and Global Inference,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"2019; Leeuwenberg and Moens 2017) leveraging the ILP optimization; (4) Basic version of our model, CTRL, which only fine-tunes a BERT-BASE (Devlin et al. 2018) language model with one layer of FFN, similar to the implementations in Lin et al.###We follow the experimental settings in (Devlin et al. 2018) to set the dropout rate, and batch size as 10−1 and 8.###BERT (Devlin et al. 2018), to derive the sentence representation vi of ds-dimension to encode the input sequence si including two marked named entities xi,1, xi,2 from the instance I, where i ∈ {1, 2, 3}.###We follow the experimental settings in (Devlin et al. 2018) to use 12 Transformer layers and attention heads and set the embedding size ds as 768.###In the framework of CTRL-PG, any contextualized word embedding method, such as BERT (Devlin et al. 2018), ELMo (Peters et al. 2018), and RoBERTa (Liu et al. 2019b), can be utilized.###In the framework of CTRL-PG, any contextualized word embedding method, such as BERT (Devlin et al. 2018), ELMo (Peters et al.###…method, SP-ILP (Han et al. 2019; Leeuwenberg and Moens 2017) leveraging the ILP optimization; (4) Basic version of our model, CTRL, which only fine-tunes a BERT-BASE (Devlin et al. 2018) language model with one layer of FFN, similar to the implementations in Lin et al. (2019); Guan et al. (2020).###We leverage the pretrained BERT-BASE model (Devlin et al. 2018) to generate the sentence embeddings, which contains 110M parameters to fine-tune.###…ȳ3 with l3; dt ← max{P(y = ŷ1|s1) + P(y = ŷ2|s2)− 1, 0}; dt ← max{dt − P(y = ȳ3|s3), 0}; dr ← min{dr, dt}; IsGround← true;
end if IsGround == false then
dr ← 0;
BERT (Devlin et al. 2018), to derive the sentence representation vi of ds-dimension to encode the input sequence si including two…###We choose BERT (Devlin et al. 2018) to derive contextualized sentence embeddings without loss of generality.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2770,5e5e190893d709897ce48240,76b8d5f2ef97d71167aa78309918bf3f7d633c96,Author Name Disambiguation on Heterogeneous Information Network with Adversarial Representation Learning,5736977f6e3b12023e66632b,LINE: Large-scale Information Network Embedding,LINE (Tang et al. 2015) tries to preserve both of first-order and second-order network structures.,other,reporting prior findings on network structure preservation
3278,5bdc31b417c44a1f58a0b894,510d98681e5e85fb1265513728f16e2543ae1b4b,Hypergraph Neural Networks,53e9a33cb7602d9702c46a22,Visual-Textual Joint Relevance Learning for Tag-Based Social Image Search,"In (Gao et al. 2013), a l 2 regularize on the weights is introduced to learn optimal hyperedge weights.",other,reporting a specific method introduced in prior work
1281,,236986fd9ef38670e26333c7efcdb8f4981986a4,ORYX-MRSI: A Fully-Automated Open-Source Software for Three-Dimensional Proton Magnetic Resonance Spectroscopic Imaging Data Analysis,,,"###While LCModel (Provencher, 1993) does not take into account partial volume effect, Osprey (Oeltzschner et al., 2020), FSL-MRS (Clarke et al., 2021), and MRSpant (Wilson, 2021) provide corrections for it.###LCModel is one of the most popular MRS data quantification tools, which estimates metabolite concentration and metabolite to total creatine ratios for a range of metabolites, including macromolecules and lipids, and it recently became open source (Provencher, 1993).###Each metabolite has a CRLB value provided in TABLE files after LCModel data analysis, indicating the quantification reliability, which were used to exclude spectra based on CRLB.###This module enables the user to visualize 3D 1 H-MRSI data located either in a raw data file or COORD file outputted by LCModel.###The FWHM & SNR module reads the LCModel TABLE files of multivoxel 1 H-MRSI data to retrieve the full width at half maximum (FWHM) and signal-to-noise ratio (SNR) information for each voxel.###The corrected metabolite concentrations are calculated as follows: where C represents the corrected metabolite concentration and C 0 the initial metabolite concentration obtained from the LCModel TABLE files.###The default imaging system for the data order of the raw data and LCModel outputs are left, posterior, and superior (LPS).###The analysis results revealed that the metabolite to Cr+PCr ratio values estimated by LCModel and Oryx-MRSI did not agree particularly well, because LCModel does not consider the chemical shift while Oryx-MRSI recalculates metabolite concentrations to tCr or mI ratio values at every voxel after chemical shift correction.###Although several metabolites, including lipids and macromolecules, are quantified by LCModel, Oryx-MRSI currently creates metabolite maps for total creatine (Cr+PCr), glutamate glutamine complex (Glu+Gln), total choline (GPC+PCh), myoinositol (Ins), lactate (Lac), and lipids (Lip13a, Lip13b, Lip13a+Lip13b).###All LCModel TABLE files were parsed to obtain the concentration values of the metabolites.###…the past few year (Clarke et al., 2021; Crane et al., 2013; Edden et al., 2014; Maudsley et al., 2006, 2009; Naressi et al., 2001; Oeltzschner et al., 2020; Poullet et al., 2007; Provencher, 1993; Reynolds et al., 2006; Simpson et al., 2017; Soher et al., 2011; Wilson et al., 2011; Wilson, 2021).###In this paper, we present an open-source 3D MRSI data analysis software with a user-friendly GUI, named Oryx-MRSI, which reads LCModel outputs as well as raw spectral data and enables visualization and metabolite map generation considering the chemical shift correction while providing automated spectral quality control based on full width at half maximum (FWHM), signal-to-noise ratio (SNR), Cramer–Rao lower bounds (CRLB), and CSF fraction (fCSF).",impact-revealing,describing the features and capabilities of a new software for MRSI data analysis
1669,,4867b08047907a0348bb11d6418a7c126ff62b9e,Emotions in Group Sports: A Narrative Review From a Social Identity Perspective,,,"###More particularly, a central assumption of Social Identity Theory (Tajfel and Turner, 1979) is that individuals are motivated by a desire to maintain a positive view of the self.###Indeed, Tajfel and Turner (1979) explained that social identity is part of the self-concept that derives from the knowledge of, and emotional attachment to the group.###Different authors have also shown that an individual may interact with others without being influenced by social comparison and categorisation (Tajfel and Turner, 1979; Haslam, 2004).###Thus, individuals may attach a value of an emotional significance to their group, which results in some degree of group identification, that then guides their behaviors (Tajfel, 1978; Tajfel and Turner, 1979).",impact-revealing,providing context and background on Social Identity Theory
2997,5fe31b9491e01125d4b5b744,2ad565fb0ce9cda15a9e5ce37b5678ec09b134b9,Graph-Evolving Meta-Learning for Low-Resource Medical Dialogue Generation,5b67b46417c44aac1c8613e4,Commonsense Knowledge Aware Conversation Generation with Graph Attention.,"…is emerging as an important step towards human-like conversational AI, where the knowledge could be derived from or open-domain knowledge graphs (Zhou et al. 2018; Zhang et al. 2020; Moon et al. 2019) or retrieved from unstructured documents (Lian et al. 2019; Zhao et al. 2019; Kim, Ahn, and…",other,highlighting the significance of conversational AI and its development
2175,,9551c10bf2733813499d78f9e85a313620013e9a,Defining Cloud Computing in Business Perspective: A Review of Research,,,"###…about ownership or services) which can be dynamically reconfigured to adjust to a variable load (scale), allowing for an optimum resource utilization’ Vouk (2008) opined that ‘cloud computing embraces cyber-infrastructure, and builds upon virtualization, distributed computing, grid computing,…",impact-revealing,providing context on cloud computing concepts
2989,5bbacb9e17c44aecc4eaff64,5b1516c87818084dc5d195cc274e1ee8923210d2,Neural Cross-Lingual Named Entity Recognition with Minimal Resources,5550418145ce0a409eb3be7e,Improving zero-shot learning by mitigating the hubness problem.,", 2018), which is designed to address the hubness problem common to the shared embedding space (Dinu and Baroni, 2014).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1979,,31e576d6f9d53d6daea44955e0fcfcbe20743f4a,A Convex Optimization Framework for Bi-Clustering,,,"###…under a probabilistic generative model for generalized graphs with labels, and our algorithms are inspired by recent convex optimization approaches to graph clustering (Mathieu & Schudy, 2010; Ames & Vavasis, 2011; Lim et al., 2014; Chen et al., 2012; 2014; Cai & Li, 2014; Vinayak et al., 2014).###To illustrate, we use the example problem posed by Cai & Li (2014).###Here we focus on average-case performance under a probabilistic generative model for generalized graphs with labels, and our algorithms are inspired by recent convex optimization approaches to graph clustering (Mathieu & Schudy, 2010; Ames & Vavasis, 2011; Lim et al., 2014; Chen et al., 2012; 2014; Cai & Li, 2014; Vinayak et al., 2014).",impact-revealing,acknowledge inspiration from prior work in graph clustering
3176,5d0b00ed8607575390fedd4e,493d5f344eea1468260946b29a80dc81b2be409c,HopSkipJumpAttack: A Query-Efficient Decision-Based Attack,5a260c8417c44a4ba8a3179d,Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification,Multiple variants have been proposed to randomize the gradients [35–39].###We adopt the implementation in Cao and Gong [35] with suggested noise levels.,other,reporting prior findings on gradient randomization
2209,5e15adcb3a55ac47ab5b0b8c,aa63ac11aa9dcaa9edd4c88db18bec87e0834328,Graph Transformer Networks,5c88fac74895d9cbc6a5bc3e,Distance Metric Learning using Graph Convolutional Networks: Application to Functional Brain Networks.,"The representation learnt by GNNs has been proven to be effective in achieving state-of-the-art performance in a variety of graph datasets such as social networks [7, 14, 35], citation networks [19, 33], functional structure of brains [20], recommender systems [1, 27, 39].",other,highlighting the effectiveness of GNNs in various applications
3211,5c04967517c44a2c7470927f,b9015d4f1e591eba7ea21c3566c919f80f7c2afe,attentive long short-term preference modeling for personalized product search,53e99ef4b7602d97027c230d,Probabilistic models for personalizing web search.,"Approaches in References [54, 56] apply a probabilistic statistical language modeling technique to discover the relevant context of the current query from the search history.",other,reporting prior findings on probabilistic statistical language modeling techniques
1867,,c88a1f5af724501a7a20b66c2259106f06025f64,Anesthetic Use and Cancer Recurrence in the Surgical Oncological Patient: An Integrative Review,,,"###Whittemore’s stages can be broken down into problem identification stage, literature search stage, data evaluation stage, data analysis stage and presentation stage (Whittemore & Knafl, 2005).###The use of The Integrative Review: Updated Methodology article by Whittemore and Knafl (2005) was used as a guide to help define methodology strategies for integrative reviews in stages.###The analysis portion of the review will include additional steps including data reduction, display, comparisons and potential
29
conclusions (Whittemore & Knafl, 2005).###This stage helps to contribute to an enhanced understanding of the problem of interest (Whittemore & Knafl, 2005).###All applicable research should be included for evaluation and assessment to further support the review (Whittemore & Knafl, 2005).",impact-revealing,describing methodology stages for integrative reviews
1851,,56fd73579ecdedc92047990735e09d284bce0499,Computer Programming in Middle School: How Pairs Respond to Challenges,,,"###In successful collaborations, conversational turns build on each other and the content moves the pair closer to solving the problem (Roschelle & Teasley, 1995; Schegloff, 1991).",impact-revealing,highlighting the importance of conversational turns in successful collaborations
3924,5e7345fd91e011a051ebf85f,9c6dccf7e17221adc3b02bfc202a0e0e061fe28a,deliberation model based two-pass end-to-end speech recognition,5b8c9f4a17c44af36f8b71cb,Toward domain-invariant speech recognition via large scale training.,"Our experiments are conducted using the same training data as in [20, 21], which is from multiple domains such as Voice Search, YouTube, Farfield and Telephony.###For training, we use the same multidomain datasets as in [20, 21] which include anonymized and hand-transcribed English utterances from general Google traffic, far-field environments, telephony conversations, and YouTube.",other,reporting the training data sources used in experiments
2629,59a03016b161e8ad1a7b6ed2,3d1f9a530e710fdd4e2313bda4c8a1f574e60ab6,neural factorization machines for sparse predictive analytics,53e99b26b7602d97023bdaff,Why Does Unsupervised Pre-training Help Deep Learning?,"Inspired by FNN [44], we further explore the use of feature embeddings learned by FM to initialize DNNs, which can be seen as a pre-training step.###Until very recently, some work [6, 9, 16, 31, 44] started to explore DNNs for some scenarios of sparse predictive analytics.###It is known that parameter initialization can greatly a_x0082_ect the convergence and performance of DNNs [11, 16], since gradient-based methods can only _x0080_nd local optima for DNNs.###However, the use of DNNs is not as widespread among the IR and DM community.###Zhang et al. [44] developed a FM-supported Neural Network (FNN), which uses the feature embeddings learned by FM to initialize DNNs. Cheng et al. [9] proposed Wide&Deep for App recommendation, where the deep part is a multi-layer perceptron (MLP) on the concatenation of feature embedding vectors to learn feature interactions.###Although DNNs have exhibited strong ability to learn patterns from dense data [14], the use of DNNs on sparse data has received less scrutiny, and it is unclear how to employ DNNs for effectively learning feature interactions under sparse settings.###These relatively negative results reveal optimization difficulties for training DNNs.###In recent five years, deep neural networks (DNNs) have achieved immense success and have been widely used on speech recognition, computer vision and natural language processing.###To demonstrate optimization difficulties of DNNs empirically, we plot the training and test error of each epoch of Wide&Deep and DeepCross on the Frappe data in Figure 1.",other,highlighting optimization difficulties in training deep neural networks
1340,,f0a23cc320cb4cb263555dc6546589940d7bcd2a,PA VIF: A passive aggressive visual information fidelity for full reference image quality assessment,,,"###As mentioned before, our work is inspired by the information-theoretic framework [6,7].###IFC and VIF utilize an information-theoretic IQA framework where image signal is modelled as a stochastic source, HVS is modelled as a distorted channel, and the perceptual quality is measured by quantifying how much information is shared between the reference and the distorted images.###The remaining parameters of the model (such as , , , ) can be estimated according to [6] and [7].###Natural scene statistics (NSS) based metrics, such as the information fidelity criterion IFC [6], the visual information fidelity (VIF) [7], and the metrics proposed in [8-10], is also a feasible direction of IQA problem.",impact-revealing,acknowledge the information-theoretic framework and its application in image quality assessment
457,5ea2b8c391e01167f5a89e2d,38643c2926b10f6f74f122a7037e2cd20d77c0f1,Supervised Contrastive Learning,5e4672c93a55ac14f595d8b5,A Simple Framework for Contrastive Learning of Visual Representations,"In light of the ﬁndings of [6] that self-supervised contrastive loss requires signiﬁcantly different data augmentation than cross-entropy loss, for the second stage we evaluate three different options: – AutoAugment: [9] – RandAugment: [10] – SimAugment: A variant of the strategy of [6] to sequentially apply random color distortion and Gaussian blurring, where we probabilistically add an additional sparse image warp to the end of the sequence.###We experimented with four different implementations of the Aug(·) data augmentation module: AutoAugment [5]; RandAugment [6]; SimAugment [3], and Stacked RandAugment [49] (see details of our SimAugment and Stacked RandAugment implementations in the Supplementary).###Self-supervised contrastive losses similarly use just one positive pair for each anchor sample, selected using either co-occurrence [18, 22, 48] or data augmentation [3].###We compare cross-entropy training, unsupervised representation learning (SimCLR [3]), max-margin classifiers [32] and SupCon (ours).###The state of the art family of models for self-supervised representation learning using this paradigm are collected under the umbrella of contrastive learning [54, 18, 22, 48, 43, 3, 50].###Dataset SimCLR[3] Cross-Entropy Max-Margin [32] SupCon###Our method is structurally similar to that used in [48, 3] for self-supervised contrastive learning, with modifications for supervised classification.###This property is important for representation learning via self-supervised contrastive learning, with many papers showing increased performance with increasing number of negatives [18, 15, 48, 3].###As in self-supervised contrastive learning [48, 3], we discard Proj(·) at the end of contrastive training.###We also note that AutoAugment is faster to implement than other augmentation schemes such as RandAugment [10] or the data augmentations proposed in [6], which we denote SimAugment.###In recent years, a resurgence of work in contrastive learning has led to major advances in self-supervised representation learning [54, 18, 38, 48, 22, 3, 15].###, [3, 48, 18, 22]), the loss takes the following form.###We compare three augmentations (RandAugment [10], AutoAugment [9] and SimAugment) (left plot); three optimizers (LARS, SGD with Momentum and RMSProp); and 3 learning rates that vary from the optimal rate by a factor of 10 smaller or larger.",impact-revealing,highlighting the importance of data augmentation strategies in self-supervised learning
909,5dc3eb4e3a55ac3c4bb65817,dc52b09089704ebd6f471177474bc29741c50023,Fast Transformer Decoding: One Write-Head is All You Need,599c7987601a182cd2648373,Attention Is All You Need.,"An example is a self-attention layer in an autoregressive language model such as Transformer [Vaswani et al., 2017].###Following [Vaswani et al., 2017], in an autoregressive model, we can prevent backward-information-ﬂow by adding a ""mask"" to the logits containing the value −∞ in the illegal positions.###Following [Vaswani et al., 2017], we evaluate on the WMT 2014 English-German translation task.###The Transformer neural sequence model [Vaswani et al., 2017] has emerged as a popular alternative to recurrent sequence models.###The ""Transformer"" seuqence-to-sequence model [Vaswani et al., 2017] uses h diﬀerent attention layers (heads) in parallel, which the authors refer to as ""Multi-head attention"".###To simplify the performance analysis, we will make several simplifying assumptions: , as suggested by [Vaswani et al., 2017] • n ≤ d The total number of arithmetic operations is Θ( bnd 2 ).###…hdv − >hmv"" , M, P_v) l o g i t s = t f . einsum ( ""hk ,hmk − >hm"" , q , K) weights = t f . softmax ( l o g i t s ) o = t f . einsum ( ""hm,hmv − >hv "" , weights , V) y = t f . einsum ( ""hv , hdv − >d"" , o , P_o) return y Note: [Vaswani et al., 2017] include a constant scaling factor on the logits.###We introduce multi-query Attention as a variation of multi-head attention as described in [Vaswani et al., 2017].",impact-revealing,providing context on the Transformer model and its components
2060,,4a2f04d237c62c08e0f9dcf70eac82fb45f3dd59,Surrogate decision making for unrepresented patients: Proposing a harm reduction interpretation of the best interest standard,,,"###Harm reduction practices are most commonly found in public health initiatives involving marginalized, and often stigmatized, groups,(47) and often, at least implicitly, recognize the ways in which discrimination can contribute to an unjust distribution of harms in these groups.(48,49) Common practices that fall under the domain of harm reduction include “opioid replacement therapy (e.###Although the Belmont Report has been criticized for focusing too much on beneficence to the neglect of nonmaleficence.(49) 52.",impact-revealing,acknowledging harm reduction practices in public health
3414,5f842b5891e01129be18ffbd,7097137596f6755675f6aafcdd80969a747322ae,Contrastive Learning with Hard Negative Samples,5e5e189a93d709897ce1e760,InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization,"We use the state-of-the-art InfoGraph method introduced by Sun et al. (2020) as the baseline, which is suitable for downstream graph-level classiﬁcation.###The proposed hard-sample loss is very simple to implement, requiring only two extra lines of code compared to the standard objective.###Before getting that point, in the interests of completeness we cover some required background details on the InfoGraph method of Sun et al. (2020).###For further information see the original paper (Sun et al., 2020).###In line with Sun et al. (2020) we use the Jensen-Shannon mutual information estimator as formulated by Nowozin et al. (2016).###In other words, if we are currently on β i , then β i +1 = β i − β/(cid:96) , and we switch from β i to β i +1 in epoch number i · e/(cid:96) .###Third, we test hard negative sampling on learning representations of sentences using the quick-thoughts (QT) vectors framework introduced by Logeswaran & Lee (2018), which uses adjacent sentences (before/after) as positive samples.",other,reporting the use of a baseline method and its implementation details
2251,5da2f8aa3a55ac3402d8c2e1,17f2f3f7e58b916175d495109bc74b2757ef952a,Barrage of Random Transforms for Adversarially Robust Defense,599c7951601a182cd262fcce,Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong,"Others before us have looked at building a multicomponent defense, but prior work has reached the conclusion that a combined defense is no stronger than any of its constituent members [6].###conclusions that ensembling defenses are not effective and only as strong as the strongest individual defense in the ensemble [6].###this was not the case [6].",other,highlighting limitations in previous multicomponent defense approaches
221,5dfc9de93a55acedae95f519,bf1a1141a4fcac64a456218207261fcf055323b0,SCAttNet: Semantic Segmentation Network With Spatial and Channel Attention Mechanism for High-Resolution Remote Sensing Images,5b67b4b417c44aac1c866e4f,CBAM: Convolutional Block Attention Module,"In this study, we follow the method of Woo [17]to integrate the two attention mechanisms.###In our work, two lightweight attention mechanisms [17] which contains spatial attention and channel attention",impact-revealing,describing the method used for integrating attention mechanisms
2479,5c0f87a5da562944ac95a190,4d157bec76125b1121a999ab2b79730540ab7aad,"Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases",53e99db1b7602d97026706d3,Tarfisdock: A Web Server For Identifying Drug Targets With Docking Approach,"Once the 3D structural information is obtained, docking can be applied to find interactions between a compound and a target, which predicts compound conformations in the binding site of the target using search algorithms and ranks them via scoring functions representing estimated binding affinities [23, 27].###For this purpose, another type of computational approach, target prediction (also known as the reverse VS), was proposed [27, 28].",other,providing context for docking and target prediction methods
1636,,fed50c54e1e8e9f4e58274ff72675b5b74fd549f,Low birth weight: is it related to assisted reproductive technology or underlying infertility?,,,"###Several authors suggest that infertility due to a cervical disorder or anovulation (109), or prenatal exposure to stress and environmental pollutants may contribute to both infertility and perinatal outcomes (6, 110).###The evidence of an effect of female infertility has been reinforced by a lower rate of LBW in subgroups of male-factor infertility in retrospective cohort studies (14, 16, 109).",impact-revealing,highlighting factors contributing to infertility and perinatal outcomes
884,5d3044363a55ac8b59feaf5b,31f8b4ca13c0b333324044bd96aac20c3d0b67bd,JumpSwitches: Restoring the Performance of Indirect Branches In the Era of Spectre,5eca468e9fced0a24b7aa48b,Meltdown: reading kernel memory from user space,"Further analysis revealed that this was due to the overhead of page-table isolation (PTI), which was introduced to mitigate against Meltdown, a different speculative execution vulnerability [1, 34].",impact-revealing,highlighting the impact of page-table isolation on performance
1713,,18adf963c3c8670c46906336bc35f6706c4bc608,Enabling collaborative MapReduce on the Cloud with a single-sign-on mechanism,,,"###A few Clouds [2,19,35,42,43] supports the creation of a virtual cluster or provisions a MapReduce service, however, in most cases the users have to self-build a cluster for running data analysis jobs with MapReduce.###Cloud MapReduce (CMR) [23] is the only MapReduce framework specifically designed for computing Clouds.###Therefore, it is widely adopted by data centres/Clouds to provide MapReduce services.###3 Software architecture of the MapReduce framework on the Cloud The lower side of the figure presents the MapReduce computing resources, which are a group of individual virtual machines on a Cloud or several Clouds.###For using the MapReduce framework the users have to log onto the Hadoop master node with either their Cloud credentials or a username-password pair in case that they are not a customer of the underlying Clouds.###Computing Clouds deliver on-demand computing resources and large storage capacity for data-intensive applications.",impact-revealing,providing context on MapReduce frameworks in cloud computing
567,5e7345fd91e011a051ebf85f,9c6dccf7e17221adc3b02bfc202a0e0e061fe28a,deliberation model based two-pass end-to-end speech recognition,5a260c0917c44a4ba8a1df77,Deliberation Networks: Sequence Generation Beyond One-Pass Decoding.,"In this work, we propose to combine acoustics and first-pass text hypotheses for second-pass decoding based on the deliberation network [16].###A deliberation model is typically trained from scratch by jointly optimizing all components [16].###Our deliberation model has a similar structure as [16]: An RNN-T model generates the first-pass hypotheses, and deliberation attends to both acoustics and first-pass hypotheses for a second-pass decoding.###1, our deliberation network consists of three major components: A shared encoder, an RNN-T decoder [1], and a deliberation decoder, similar to [10, 16].",impact-revealing,describing the proposed method for decoding
2533,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",558bcd40e4b00c3c48de8dd6,Compact Support Biorthogonal Wavelet Filterbanks for Arbitrary Undirected Graphs,"These types of filterbanks have been designed for bipartite graphs [68], [102], thus requiring the graph to be decomposed into a series of bipartite subgraphs [68], [103].",other,providing context on filterbanks for bipartite graphs
1226,,c452e20d7eb08011e958ccd2a190221442877b77,De Novo Acquisition of BCR-ABL Mutations for CML Acquired Resistance,,,"###…patients prior to imatinib exposure is around 21%, even using high sensitivity
www.intechopen.com
Myeloid Leukemia – Basic Mechanisms of Leukemogenesis 72
methods for detection (Ernst et al., 2008; Roche-Lestienne et al., 2002; Willis et al., 2005), as compared to 40% to 90% in relapsed patients.###This model has been validated by subsequent studies from multiple groups identifying such mutations (Ernst et al., 2008; Roche-Lestienne et al., 2003; Willis et al., 2005).###methods for detection (Ernst et al., 2008; Roche-Lestienne et al., 2002; Willis et al., 2005), as compared to 40% to 90% in relapsed patients.",impact-revealing,highlighting the validation of a model through subsequent studies
410,5f7d893591e011346ad27d16,ff8a988d88ebe5d5ca116340baa34ae00ce011e8,PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation,5dbab2523a55acea3c05b02b,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language  Generation, Translation, and Comprehension","Our work uses BART (Lewis et al., 2020), a state-of-the-art seq2seq model that offers better generalizability and stronger capacity for long text generation.###Our generation model has the same architecture as BART (Lewis et al., 2020) with 406 M parameters.###Large pre-trained language models are the cornerstone of many state-of-the-art models in various natural language understanding and generation tasks (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), yet they are far from perfect.###Next, we propose a content-controlled text generation framework , built upon the pre-trained sequence-to-sequence (seq2seq) Transformer model BART (Lewis et al., 2020).",impact-revealing,describing the use of BART for text generation and its significance in NLP tasks
349,573696046e3b12023e517e10,b5f3e5d2912bedbcd9458952d664b08db6aed962,accurate image super-resolution using very deep convolutional networks,558c4a2384ae6766fdf2358f,Image Super-Resolution Using Deep Convolutional Networks,"Our network is very deep (20 vs. 3 [6]) and information used for reconstruction (receptive ﬁeld) is much larger ( 41 × 41 vs. 13 × 13 ).###SRCNN [6] uses a very large ImageNet dataset.###The public code of SRCNN based on a CPU implementation is slower than the code used by Dong et. al [6] in their paper based on a GPU implementation.###Among them, Dong et al. [6] has demonstrated that a CNN can be used to learn a mapping from LR to HR in an end-to-end manner.###SRCNN [6] fails to show superior performance with more than three weight layers.###9 of [6], it is not easy to say their deeper networks have converged and their performances were saturated.###In [6], Dong et al. attempted to prepare deeper models, but failed to observe superior performance after a week of training.###Lately, random forest [18] and convolutional neural network (CNN) [6] have also been used with large improvements in accuracy.###Moreover, our initial learning rate is 10 4 times higher than that of SRCNN [6].",impact-revealing,comparing network architectures and performance metrics
2790,5eede0b091e0116a23aafb82,1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af,Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,5736960d6e3b12023e520462,Unsupervised Deep Embedding for Clustering Analysis,"Our work is also related to clustering-based methods [2, 3, 6, 7, 17, 25, 50, 53, 54, 59].",other,acknowledge related methods
1004,,c26a151ae970e37696de5ed3a06b3169ea6f1c87,Prevalence of Mental Health Complaints Among Performing Arts Students Is Associated With COVID-19 Preventive Measures,,,"###Earlier empirical research has also highlighted the importance of social networks for health and well-being (Kawachi and Berkman, 2001; Elmer et al., 2017).",impact-revealing,highlighting the significance of social networks for health and well-being
2990,53e9b350b7602d9703e268f6,333da42f4369dff8ca905ad21ea6ee2f5dc99d55,"Utility-Based Cache Partitioning: A Low-Overhead, High-Performance, Runtime Mechanism to Partition Shared Caches",53e9bdd5b7602d9704a85f16,"Communist, utilitarian, and capitalist cache policies on CMPs: caches as a shared resource.","[5] studied different policies, including a utilitarian policy, for partitioning a shared cache between competing applications.",other,reporting prior findings on cache partitioning policies
3941,5e5f7c4791e011df604ecbed,0ca7d8c3250d43d14fdde46bf6fc299654d861ef,Heterogeneous Graph Transformer,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"odet’s vector back to its typespecific distribution, indexed by its node typeτ(t). To do so, we apply a linear projection A-Linear τ(t)to the updated vector He (l)[t], followed by residual connection [8] as: H(l)[t]= A-Linear τ(t)  σ He(l)[t]  +H(l−1)[t]. (5) In this way, we get thel-th HGT layer’s output H(l)[t]for the target node t. Due to the “small-world” property of real-world graphs, stacking",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3251,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,599c7988601a182cd2648d44,HARP: Hierarchical Representation Learning for Networks,"could arise from clusters [9, 31, 66], for example through pooling or trainable attention over edges [59].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2453,599c7ea4601a182cd28b8342,103baca878b17a15d148a684c0b0152e78591be1,A Split Cache Hierarchy for Enabling Data-Oriented Optimizations,53e9b6c4b7602d970424e729,Optimal Bypass Monitor For High Performance Last-Level Caches,") • Cache bypassing to improve performance and energy by only installing a cacheline if it is likely to see reuse [12], [13], [14], [15], [16], [17], [18], [19].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1079,,8dce1098be27f28366b4d93036dc0005e05646af,Exploration of Semi-Structured Data Sources,,,"###Although the separation of user tasks, operations and goals from interface design details has been widely recognized as valuable approach in HCI since the existence of task models, such as the Goals, Operations, Methods and Selection rules (GOMS) family [14], it has not been applied in the context of exploration tools.",impact-revealing,highlighting a gap in applying established HCI approaches to exploration tools
2433,5edcbb9a91e0110f1d6db1c2,c99e2877d4b1f171746a3aa73d5fd5fd680c982d,Hierarchical Bipartite Graph Neural Networks: Towards Large-Scale E-commerce Applications,53e9adffb7602d970380bbb8,Ontology construction for information classification,"Dictionary-based ontology taxonomy [34] is a widely used method to organize items into categorical structures in most existing e-commerce platforms, because the hierarchical conceptual knowledge behind the items can be naturally distilled into the ontology dictionary.",other,highlighting the significance of dictionary-based ontology taxonomy in e-commerce
2118,,a66186b801828ccef5971916fbea9b3f642f2a69,A Comprehensive Review on Traffic Control Modeling for Obtaining Sustainable Objectives in a Freeway Traffic Environment,,,"###Over the last decades, it has been commonly used by the research community [32, 33].",impact-revealing,acknowledging the common use of a method in the research community
1665,,7aa3e0b79ba01d8ae9f5d4ecfec71fade68aeb21,Bridging the gap: Unravelling the identity-to-politics link,,,"###…will set out the concept social identification, to do so this research will build on sociopsychological theories that will provide us with insights on the origins of social identity theory (Tajfel, 1974; Tajfel et al, 1979) and how it played a role in explaining intergroup conflict (Hogg, 2016).",impact-revealing,building on sociopsychological theories to explain social identity
692,5f0d85c69fced0a24be4f04c,a43ba805d13785378fecdb408a571ee50d0afb8e,auto-predication of critical branches,53e9b5edb7602d97041417c7,Diverge-Merge Processor (DMP): Dynamic Predicated Execution of Complex Control-Flow Graphs Based on Frequently Executed Paths,"Prior works [7], [11] have relied on select-micro-op based approaches to handle correctness of data dependencies after the predicated region.###This is unlike previous approaches [7], [12]–[14] that were dependent upon compiler analysis and profiling.###Popular ISAs support static predication [17], [18] but due to large overheads, the realistic benefits are diminished [7], [12].###cessor (DMP) [7], which relies on changes to the compiler, ISA and micro-architecture to perform selective predication on###Policies like Diverge Merge Processor (DMP) [7], [15] use careful compiler profiling to select target H2P branches, and then throttle their application using run-time monitoring of branch prediction confidence.###To overcome this, several prior techniques have tried to predicate only those instances of H2P branches which have low confidence of prediction [7], [11]–[14].###Using these less intrusive micro-architectural changes, we are able to achieve register transparency without resorting to complex RAT recovery mechanisms or re-execution as proposed in [7], [11].###To mitigate this, past approaches have dynamically applied predication only on branch instances having low confidence from branch prediction [7], [13], [14].###We define convergent branches as those branches whose taken and not-taken paths can converge to some later point in the program (using the same convergence criterion as DMP [7]).###Diverge Merge Processor (DMP) [7] improves upon both Wish Branches and DHP.",impact-revealing,acknowledge prior works and their approaches in handling data dependencies
549,5d84a3433a55acc20782ce9e,554d300f00fc14c2e4f48a740019496137d060c1,self-training for end-to-end speech recognition,5736978a6e3b12023e6705e8,Librispeech: An ASR corpus based on public domain audio books,"We demonstrate the effectiveness of self-training on Lib-riSpeech [8], a publicly available corpus of read speech.###All experiments are performed on the LibriSpeech corpus [8].###The standard LM training text in LibriSpeech is derived from 14,476 public domain books [8].",impact-revealing,reporting on the effectiveness of self-training using a specific corpus
1339,,28b3c9bef7f2c0efacc528e54dabbd7a56bb701c,2018 PIRM Challenge on Perceptual Image Super-resolution,,,"###As above, RMSE, SSIM and IFC are anti -correlated with human-opinion-scores, while NIQE and PI are most correlated (especially in the high perceptual quality regime). altogether), where we average only over diﬀerent human raters.###Note that RMSE, SSIM and IFC are anti -correlated with human-opinion-scores, and that our PI is the most correlated.###9, RMSE, SSIM and IFC, which are widely used for evaluating the quality of image reconstruction algorithms, are anti -correlated with perceptual quality and thus inappropriate for evaluating it.###9, we plot the mean-opinion scores of the methods included in the human-opinion study vs. the mean score according to the common full-reference measures RMSE, SSIM [45], IFC [33], and LPIPS [50], as well as the no-reference methods by Ma et al. [22], NIQE [27], BRISQUE [26] and the PI deﬁned by (1).###the mean score according to the common full-reference measures RMSE, SSIM [45], IFC [33], and LPIPS [50], as well as the no-reference methods by Ma et al.###That is, models which excel at minimizing the reconstruction error tend to produce visually unpleasing results, while models that produce results with superior visual quality are rated poorly by distortion measures like PSNR, SSIM, IFC, etc. [18, 13, 24, 31, 4] (see Fig.",impact-revealing,highlighting the correlation between evaluation metrics and human opinion scores in image reconstruction
318,5f1ff7ea91e011d50a621ab3,39b45b0d60807296db97d9eb37b1fb99d244e2ed,MACU-Net for Semantic Segmentation of,5b8c9f5317c44af36f8b775c,Unet Plus Plus : A Nested U-Net Architecture For Medical Image Segmentation,"To test the effectiveness of MACU-Net, we compare the performance of the proposed method with U-Net [11], FGC [19], U-Net++ [12], U-NetPPL [20], WRAU-Net [21], CE-Net [22] and U-Net 3+ [13].###To address this issue, in U-Net++ [12], plain skip connections are substituted by nested and dense skip connections, which enhance the power of the skip connections and narrow the semantic gap between the encoder and decoder.###To evaluate the effectiveness of MACU-Net, the U-Net [11], FGC [19], U-Net++ [12], U-NetPPL [20], WRAU-Net [21], CE-Net [22] and U-Net 3+ [13] methods were used as benchmark comparators.",impact-revealing,comparing the performance of MACU-Net with existing methods
2363,5db9298547c8f766461f8b65,ddb2aecf8777007414b1eb341c6c19ec799280d3,Frame attention networks for facial expression recognition in videos,5bdc315817c44a1f58a064ab,Multi-Feature Based Emotion Recognition for Video Clips.,"However, [32] uses DenseNet161 and pretrains it on both large-scale face datasets and their own Situ emotion video dataset.###Additionally, [32] applies complicated post-processing which extracts frame features and compute their mean vector, max-pooled vector, and standard deviation vector.###For static methods, [32] gets slightly better performance than ours.",other,comparing performance of methods in emotion recognition
1793,,0fb847c46163b7d8963589386e238061698e537c,A synthesis of qualitative research on overweight and obese people's views and experiences of weight management,,,"###Overweight and obese people who stop engaging in weight management behaviours after some weight loss are likely to regain weight, and this experience may negatively affect future attempts to lose weight (9).###The findings of this paper complement and build on quantitative reviews that have examined psychosocial factors associated with weight management in overweight and obese adults (9,37) by identifying and explaining facilitators and barriers to weight management from the viewpoint of overweight and obese people.###Previous reviews of the factors associated with weight management have tended to focus on quantitative studies and has been useful for identifying determinants of weight management (9).",impact-revealing,highlighting the contribution of qualitative insights to existing quantitative reviews on weight management
1839,,9e90ec275b051a3842bf3ad41169bc7dcf7027ee,A Cryptographically Sound Dolev-Yao Style Security Proof of the Otway-Rees Protocol,,,"###, in [36, 24, 37], and various new approaches and formal proof tools for the analysis of security protocols were validated by showing that they can prove the protocol in the Dolev-Yao model (respectively that they can find the well-known type-flaw attack if the underlying model does not provide sufficient typing itself; the model that our proof is based upon excludes this attack).",impact-revealing,reporting validation of new approaches and tools for security protocol analysis
373,5bdc315017c44a1f58a05c5e,5201efab94c9376ef894f6f33cab06a5c5e00073,Learning Named Entity Tagger using Domain-Specific Dictionary,573696106e3b12023e5227c8,Neural Architectures for Named Entity Recognition.,"On the BC5CDR and NCBI-Disease datasets, LM-LSTM-CRF (Liu et al., 2018) and LSTM-CRF (Lample et al., 2016) achieve the state-of-the-art F 1 scores without ex-ternal resources, respectively (Wang et al., 2018).###We follow previous works (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016) to deﬁne the score of the predicted sequence, the score of the predicted sequence ( y 1 , y 2 , . . . , y n ) is deﬁned as: where, Φ y i ,y i +1 is the transition probability from a label y i to its next label where…###We revise the LSTM-CRF model (Lample et al., 2016) to the Fuzzy-LSTM-CRF model to support the modiﬁed IOBES labels.###Typically, NER models are built upon conditional random ﬁelds (CRF) with the IOB or IOBES tagging scheme (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Ratinov and Roth, 2009; Finkel et al., 2005).###However, most existing methods require large amounts of manually annotated sentences for training supervised models (e.g., neural sequence models) (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Finkel et al., 2005).###We follow previous works (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016) to define the score of the predicted sequence, the score of the predicted sequence (y1, y2, .###, 2018) and LSTM-CRF (Lample et al., 2016) achieve the state-of-the-art F1 scores without external resources, respectively (Wang et al.###Therefore, we customize the conventional CRF layer in LSTM-CRF (Lample et al., 2016) into a Fuzzy CRF layer, which allows each token to have multiple labels without sacriﬁcing computing efﬁciency.###(Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018).###We revise the LSTM-CRF model (Lample et al., 2016) to the Fuzzy-LSTMCRF model to support the modified IOBES labels.###Recently, extensive efforts have been made on building reliable named entity recognition (NER) models without handcrafting features (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016).###Therefore, we customize the conventional CRF layer in LSTMCRF (Lample et al., 2016) into a Fuzzy CRF layer, which allows each token to have multiple labels without sacrificing computing efficiency.###2055 fields (CRF) with the IOB or IOBES tagging scheme (Liu et al., 2018; Ma and Hovy, 2016; Lample et al., 2016; Ratinov and Roth, 2009; Finkel et al., 2005).",impact-revealing,highlighting the limitations of existing supervised methods in NER
3511,573696106e3b12023e5227c8,f5a7da72496e2ca8edcd9f9123773012c010cfc6,Neural Architectures for Named Entity Recognition,5736974d6e3b12023e638641,Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation,"They have been found useful for morphologically rich languages and to handle the outof-vocabulary problem for tasks like part-of-speech tagging and language modeling (Ling et al., 2015b) or dependency parsing (Ballesteros et al., 2015).###Embeddings are pretrained using skip-n-gram (Ling et al., 2015a), a variation of word2vec (Mikolov et al., 2013a) that accounts for word order.###…decision for each token (i.e., the Pi,y’s) are defined to be the dot product between the embedding of a wordin-context computed with a bidirectional LSTM— exactly the same as the POS tagging model of Ling et al. (2015b) and these are combined with bigram compatibility scores (i.e., the Ay,y′’s).###To capture orthographic sensitivity, we use character-based word representation model (Ling et al., 2015b) to capture distributional sensitivity, we combine these representations with distributional representations (Mikolov et al., 2013b).###A very simple—but surprisingly effective—tagging model is to use the ht’s as features to make independent tagging decisions for each output yt (Ling et al., 2015b).###The best score reported on this task is by Luo et al. (2015). They obtained a F1 of 91.",other,highlighting the usefulness of embeddings for various NLP tasks
1059,,a9b3d3313e8918541c4c348fb2a95020a5242ac4,Federated Learning for Software Engineering: A Case Study of Code Clone Detection and Defect Prediction,,,"###These choices are motivated by several factors: 1) DP and CCD represent two of the most well-established and extensively researched SE tasks by previous studies [16], [31], [32], [33], [34].",impact-revealing,acknowledge the significance of established software engineering tasks
202,5b1643ba8fbcbf6e5a9bc79b,b3f83e8416010e9c3a705a0b6390d268e5ddf5c0,Black-box adversarial attacks with limited queries and information,5a260c8417c44a4ba8a31c1d,Machine Learning as an Adversarial Service: Learning Black-Box Adversarial Examples.,"In this paper, we use the deﬁnition of black-box access as query access (Chen et al., 2017; Liu et al., 2017; Hayes & Danezis, 2017).###(Goodfellow et al., 2015; Carlini & Wagner, 2017; Moosavi-Dezfooli et al., 2016; Moosavi-Dezfooli et al., 2017; Hayes & Danezis, 2017), where an attacker has full access to the model parameters and architecture.",impact-revealing,defining black-box access in the context of model parameters and architecture
1572,,ff9fe54374ce8aad9a3c5ff520c0de6c970902aa,Management competence and incompetence training: theory and practice,,,"###Many studies report on the role of intuition, common sense, life experience, gut feelings, snap judgements and smart guesses in qualifying decisions (Gigerenzer, 2007, 2008; Gladwell, 2005; Goleman, 1998; Goleman, Boyatzis & McKee, 2002; Simon, 1987; Wilson, 2002, 2011).###Early probability models of human thinking – in which humans attempt to find optimal solutions – were popularised by George Boole (1854-1958) (quoted in Gigerenzer, 2008).###Gigerenzer (2004, 2008, 1999) suggests that a way decision-makers can achieve simplicity is by developing “adaptive tools” in the form of fast and frugal heuristics.###Gigerenzer (2008) explores the misconception that more information and more extensive computation are always better and paradoxically states that “good decisions in an uncertain world require ignoring part of the available information” (p. 22).###This study extends the work of Armstrong (2003), Armstrong and Green (2005), Gigerenzer (2008), Gigerenzer & Brighton (2009b) and Schank, Berhman and Macpherson, (1999).",impact-revealing,acknowledging the role of intuition and heuristics in decision-making
1742,,3f90657f4a23dc9c9fdf39f37f526ea89d8aa70f,Moonrise timing is key for synchronized spawning in coral Dipsastraea speciosa,,,"###Moonlight is considered to be the proximate factor in determining coral spawning day (5, 19).###Synchronization is thought to have evolved in order to increase gamete density and ensure high levels of fertilization (4, 5), while predator satiation acts to increase the likelihood of survival (6, 7).###Thus, these findings directly address a gap in knowledge concerning spawning mechanisms, which has been present for decades (5).",impact-revealing,highlighting the significance of findings related to coral spawning mechanisms and addressing a knowledge gap
2620,53e9b42fb7602d9703f2696f,8c34cdd2bab66623d2831004fbd1fa1cdf8a0366,Improving memory scheduling via processor-side load criticality information,53e9ace8b7602d97036c43b4,A Criticality Analysis Of Clustering In Superscalar Processors,"Salverda and Zilles provide some level of stratiﬁcation for criticality by ranking instructions on their likelihood of criticality, based on their prior critical frequency [23].",other,reporting prior findings on criticality stratification
3019,5f896fa591e01149071e45df,1d16d4cdc3fcce26e2c2097d13896ec09683eee3,Self-training for Few-shot Transfer Across Extreme Task Differences,5de8d54c3a55ac9c4229187d,Self-Supervised Learning Of Pretext-Invariant Representations,"A more recent (and better performing) line of self-supervised learning is contrastive learning (Wu et al., 2018; Misra & Maaten, 2020; He et al., 2020; Chen et al., 2020) which aims to learn representations by considering each image together with its augmentations as a separate class.",other,highlighting advancements in self-supervised learning through contrastive learning
3654,5f03f3b611dc83056223206d,75c8466a0c1c3b9fe595efc83671984ef95bd679,XGNN: Towards Model-Level Explanations of Graph Neural Networks,5c8d279f4895d9cbc6403a3b,Attention-based Graph Neural Network for Semi-supervised Learning,"In addition, extensive efforts have been made towards different graph operations, such as graph convolution [13, 16, 19], graph pooling [20, 44], and graph attention [10, 36, 37].",other,acknowledge existing graph operations
3720,5b67b45517c44aac1c860884,9fa3e53b5937a0ec92499ed415e339ede6c92010,DeepInf: Social Influence Prediction with Deep Learning,53e9a812b7602d9703156ee8,Confluence: conformity influence in large social networks.,"Such global patterns includes various respects of a cascade and their correlation with the final cascade size, e.g., the rise-and-fall patterns [32], external influence sources [33], and conformity phenomenon [43].###Indeed, extensive work has been done on social influence prediction in the literature [26, 32, 42, 43].",other,acknowledging extensive research on social influence prediction
334,5d9ed4a047c8f76646fb6da2,0fd26ed185aaf860f2db491c194884914fc29311,A Neural Multi-digraph Model for Chinese NER with Gazetteers,5736960c6e3b12023e51ee06,Gated Graph Sequence Neural Networks,"However, the traditional GGNN (Li et al., 2016) is unable to distinguish edges with different labels.###Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) (Li et al., 2016) and a standard bidirectional LSTM-CRF (Lample et al.###Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) (Li et al., 2016) and a standard bidirectional LSTM-CRF (Lample et al., 2016) (BiLSTM-CRF), our model learns a weighted combination of the information from different gazetteers and resolves matching conﬂicts based on contextual…",impact-revealing,acknowledge limitations of traditional GGNN in distinguishing edges
3009,53e9aeebb7602d970391ac0a,8681e808a9ebd7f7f155590e75fb63563a8aae6e,performance prediction based on inherent program similarity,53e9b338b7602d9703e0cb1e,Quantifying the Impact of Input Data Sets on Program Behavior and its Applications,This gives equal weight to all of the principal components [5].,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
433,5db9297247c8f766461f6d13,9ec95c1130a6ac4238ac2e5c7b2b66047511ea92,long and diverse text generation with planning-based hierarchical variational model,5aed14d117c44a4438158b17,A Hierarchical Latent Structure for Variational Conversation Modeling.,"Variational Hierarchical Conver-sation RNN (VHCR) (Park et al., 2018) is a most similar model to ours, which also adopts a hierarchical latent structure.###Our method differs from VHCR in two aspects: (1) VHCR has no planning mechanism, and the global latent variable is mainly designed to address the KL collapse problem, while our global latent variable captures the diversity of reasonable planning; (2) VHCR in-jects distinct local latent variables without direct dependencies, while our method explicitly models the dependencies among local latent variables to better capture inter-sentence connections.###Variational Hierarchical Conversation RNN (VHCR) (Park et al., 2018) is a most similar model to ours, which also adopts a hierarchical latent structure.",impact-revealing,comparing and contrasting with a similar model
2930,5dea04309e795e693620e97c,b0c35bf9ddffefb0dab4f76c20b30e00a22b1e0a,unsupervised author disambiguation using heterogeneous graph convolutional network embedding,5b67b45517c44aac1c86078b,"Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop.","[1] use a global metric learning and local linkage graph auto-encoder algorithm to learn the representation of publications, but it requires lots of human labeled data to train the model.###Zhang et al. [1] also use a graph convolutional network based encoder-decoder model but on homogeneous graph that can not extract multi-layer relationship that contains various relation types.###High quality representations play a critical role to quantify distinctions and similarities between publications [1].###For example, [5] need to specify the number of distinct author, [1] need labeled data to estimate the number.###Hierarchical Agglomerative Clustering (HAC) method works well for skewed data and is widely used in many name disambiguation methods [1], [5], [9], [20], [21].###Zhang et al. [1]: This method uses a global metric learning and local linkage learning based on a graph auto-encoder method to learn the publications embeddings, then it propose an end-to-end model to estimate the number of clusters using a recurrent neural network and use HAC to determine the…###Supervised methods [1], The research is supported by the National Key Research and Development Plan (2017YFC1601504), the Natural Science Foundation of China (61836013), the CNTC (China National Tobacco Corporation ) Science and Technology Major Project (110201901027(SJ-06)), and the Guangdong…",other,reporting on various methods and their requirements in publication representation
3389,5d3ed25a275ded87f97deaab,025ea689e6ab3b544101df17233e87536a1e578a,Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation,53e9ae2eb7602d9703842658,On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes.,• LR [15]: It is a linear model with static features.,other,providing context for a model description
3893,5e524da993d709897cb69fde,09e48adf1a3f012ec56fbb4f92acae975ee43d87,a deep learning approach to antibiotic discovery,53e9a79eb7602d97030dafec,Global challenge of multidrug-resistant Acinetobacter baumannii.,"baumannii as one of the highest priority pathogens against which new antibiotics are urgently required (Lee et al., 2017; Perez et al., 2007).###Of note, the World Health Organization has designated A. baumannii as one of the highest priority pathogens against which new antibiotics are urgently required (Lee et al., 2017; Perez et al., 2007).",other,highlighting the urgent need for new antibiotics against A. baumannii
1796,,7ffce5a95947dba9ee680663b1ebad509509b84b,Advancing the ethical use of digital data in human research: challenges and strategies to promote ethical practice,,,"###Issues of privacy, data encryption, storage, access and data governance are ethical challenges that are frequently cited as being of concern for ethics committees in the United States (Buchanan et al. 2011; Buchanan and Hvizdak 2009; Metcalf and Crawford 2016).###In the United States, the Secretary’s Advisory Committee on Human Research Protections (SACHRP) has developed a document that offers guidance about ethical and regulatory considerations for internet-based researchers (Secretary’s Advisory Committee on Human Research Protections (SACHRP) 2013).###However, privacy laws and accepted practices for gaining consent vary between different international jurisdictions, and this document has only limited application in contexts outside the United States.",impact-revealing,highlighting ethical challenges and regulatory considerations for internet-based research
578,5d04e8dbda56295d08db13cf,56e3ce0ff4cbd05e404214d19ae264fe6c457a16,cif: continuous integrate-and-fire for end-to-end speech recognition,5ce3aa43ced107d4c6583051,Triggered Attention For End-To-End Speech Recognition,"Besides, CIF provides a concise calculation process by conducting the locating and integrating at the same time, rather than [11, 12] which need two separate steps of first using a hard monotonic attention to decide when to stop and then performing soft attention to calculate, also rather than [13] which needs a CTC trained model to conduct pre-partition before the attention decoding.###triggered attention [13]), but also matches or surpasses most of the published results of end-to-end models.",impact-revealing,comparing the efficiency of CIF with other methods
169,5cf48a2cda56291d5828e868,c42816f497d663c681df20d48a6e66a5632600d8,Mixmatch: A holistic approach to semi-supervised learning,53e9be09b7602d9704abe386,Semi-supervised Learning by Entropy Minimization,"This is done explicitly in [18] with a loss term which minimizes the entropy of pmodel(y | x; θ) for unlabeled data x.###In much recent work, this loss term falls into one of three classes (discussed further in Section 2): entropy minimization [18, 28]—which encourages the model to output confident predictions on unlabeled data; consistency regularization—which encourages the model to produce the same output distribution when its inputs are perturbed; and generic regularization—which encourages the model to generalize well and avoid overfitting the training data.",impact-revealing,describing different loss term classes in recent work
2702,5d3ed25a275ded87f97deb36,37f7eaf57b88ef03efd4196f2f57ae0b4c657a79,Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommendation,5a260c8117c44a4ba8a30f54,Graph Attention Networks.,"Moreover, edge weights do play an essential role in representation learning on graphs, as highlighted by a large amount of prior works [10, 20, 21, 35, 38].",other,acknowledging the importance of edge weights in representation learning
3456,5b3d98cc17c44a510f801acc,155b7782dbd713982a4133df3aee7adfd0b6b304,Unsupervised Feature Learning via Non-parametric Instance Discrimination,555048ef45ce0a409eb72b8f,Learning Deep Features for Scene Recognition using Places Database.,"Our experimental results show that our method outperforms the state-of-the-art on image classiﬁcation on ImageNet and Places, with a compact 128 -dimensional representation that scales well with more data and deeper networks.###With the same settings, we conduct another large-scale experiment on Places [49], a large dataset for scene classification, which contains 2.###6% for Places 205 [49].",other,highlighting the superiority of the proposed method in image classification
2173,,d81554caba1adf8ca1fa30ffd63bfbd376fc44b0,Application of Mobile Cloud Computing in Operational Command Training Simulation System,,,"###Cloud computing embraces cyber infrastructure, and builds upon decades of research in virtualization, distributed computing, grid computing, utility computing, and, more recently, networking, web and software services [5].",impact-revealing,providing context on the evolution of cloud computing
3078,5ecfae0d9e795eb20a615048,fde4e53ba166567f3b9b977a866020f10a996c02,LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition,5a9cb62217c44a376ffb4da1,Comparison of Grapheme-to-Phoneme Conversion Methods on a Myanmar Pronunciation Dictionary.,"According to [4, 14, 15, 38, 43], the pronunciation lexicon, single-speaker high-quality paired data and single-speaker high-quality unpaired speech data require much higher collection cost than other data such as multi-speaker low-quality unpaired speech data and unpaired text, since they can be…",other,highlighting the cost differences in data collection for speech data types
3622,5c6a37d03a69b1c9e12a9fc4,81b6d24e8f313fd88b0fe5ff6c21dd154fbe32d2,SimGNN: A Neural Network Approach to Fast Graph Similarity Computation,53e9b8a1b7602d970447ac2b,A partition-based approach to structure similarity search,"AIDS is a collection of antivirus screen chemical compounds from the Developmental Therapeutics Program at NCI/NIH 7 3, and has been used in several existing works on graph similarity search [26, 44, 48, 49, 51].###Computing GED is a primitive operator in graph database analysis, and has been adopted in a series of works on graph similarity search [26, 44, 48, 49, 51].###The first category of remedies is the pruning-verification framework [26, 48, 49], under which the total amount of exact graph similarity computations for a query can be reduced to a tractable degree, via a series of database indexing techniques and pruning strategies.###GED has been widely used in many applications, such as graph similarity search [26, 44, 48, 49, 51], graph classification [34, 35], handwriting recognition [10], image indexing [45], etc.",other,reporting the use and applications of GED in graph similarity search
1970,,6f5ff6a15f816949b9d42afe494906f23c086804,Poly (ADP-ribose) polymerase inhibitors: recent advances and future development.,,,"###PARP inhibitors also sensitize cells to certain DNA-damaging agents.(27,28,78,79) Different modes of PARP inhibitor action depicted in###one copy of a gene to serve as a template for restoration of a second copy of the same gene.(28,61,62) PARP1 also poly (ADP-ribosyl)ates###Through its synthesis of pADPr, PARP1 contributes to a number of DNA repair pathways.(27,28) In its most extensively studied role, PARP1 is essential for base excision repair (BER),(54-56) a process that removes a single damaged base and restores DNA integrity.###In its most extensively studied role, PARP1 is essential for base excision repair (BER),(54-56) a process that removes a single damaged base and restores DNA integrity.(28,57) In###Current development of PARP inhibitors as anticancer agents is motivated by the hypersensitivity of HR-deficient cells to PARP inhibition(85,86) and the ability of PARP inhibitors to sensitize cells to certain types of DNA damage.(27,28) There is emerging evidence that these two effects might reflect different aspects of PARP biology.###prompted the initial development of PARP inhibitors as agents to enhance targeted DNA damage.(28,78,79) PARP2 and PARP3 also contribute to DNA repair.",impact-revealing,highlighting the role of PARP inhibitors in DNA repair and their potential in cancer treatment
2446,5b1643ba8fbcbf6e5a9bc79b,b3f83e8416010e9c3a705a0b6390d268e5ddf5c0,Black-box adversarial attacks with limited queries and information,5c8928214895d9cbc6b436ff,Evolution Strategies as a Scalable Alternative to Reinforcement Learning.,"We propose the variant of NES described in Salimans et al. (2017) (inspired by Wierstra et al. (2014)) as a method for generating adversarial examples in the query-limited setting.###Like Salimans et al. (2017), we employ antithetic sampling to generate a population of δ i values: instead of generating n values δ i ∼ N (0 , I ) , we sample Gaussian noise for i ∈ { 1 , . . . , n 2 } and set δ j = − δ n − j +1 for j ∈ { ( n 2 + 1) , . . . , n } .",other,proposing a method for generating adversarial examples
2540,5ef0816891e0112aee042b88,5c126ae3421f05768d8edd97ecd44b1364e2c99a,denoising diffusion probabilistic models,5e68b99493d709897cd373ed,Group Normalization,"To represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ [49, 45] with group normalization throughout [62].###We replaced weight normalization [46] with group normalization [62] to make the implementation simpler.",other,describing the architecture and modifications of a model
2333,5aed14d617c44a4438159123,921196c32213a229245a9705ee4768bc941e7a26,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,5736980d6e3b12023e6e4045,Effective Use of Word Order for Text Categorization with Convolutional Neural Networks.,"More recently, convolutional networks were applied to sentence classiﬁcation (Kalchbrenner et al., 2014; Kim, 2014) and document classiﬁcation (Zhang et al., 2015; Conneau et al., 2017; Johnson & Zhang, 2015; 2017).",other,acknowledge recent applications of convolutional networks in classification tasks
2340,5bdc315017c44a1f58a05e13,1717255b6aea01fe956cef998abbc3c399b5d7cf,AMC: AutoML for Model Compression and Acceleration on Mobile Devices,5a260c8417c44a4ba8a31609,Domain-adaptive deep network compression,"Tensor factorization [30, 15, 27, 35] decomposes weights into light-weight pieces, for example [51, 11, 14] proposed to accelerate the fully connected layers with truncated SVD; Jaderberg et al .",other,reporting prior findings on tensor factorization methods
1364,,8548bea4f6c28b9142b9aebcf1ca6b83d185c53b,"Design, privacy and authentication of challenge questions in online examinations",,,"###It has been widely used for password recovery by corporate email service providers as Google, AOL, Yahoo and Microsoft to name a few [10].",impact-revealing,acknowledging the application of a method in password recovery
40,5da1a6d447c8f7664606888d,91a4a5a1184a12821e7f5ddf5372b259ded96feb,Directed Statistical Warming through Time Traveling,573697ae6e3b12023e691a36,Full Speed Ahead: Detailed Architectural Simulation at Near-Native Speed,"It uses virtualized fast-forwarding (VFF) at near-native speed to advance the execution to the next detailed region, at which point it switches to functional simulation to record the set of key cachelines.###Explorer-1 fast-forwards using VFF and switches to DP 5M instructions before the detailed region.###Functional Warming (FW) on the other hand does not incur any storage overhead and is transferable across both hardware and software changes [26, 34].###[26] propose a method that uses two parallel simulations, pessimistic and optimistic, to bound the maximum error due to warming.###Virtualized Fast-Forwarding (VFF) [26] leverages hardware virtualization to quickly get to the next region, while enabling software changes.###Functional warming (FW) [26, 34] does not incur any storage overhead, allows for software changes, but is slow.",impact-revealing,describing methods for simulation and execution
3820,5a9cb66717c44a376ffb89eb,651adaa058f821a890f2c5d1053d69eb481a8352,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,5c757363f56def9798894fae,Thermometer Encoding: One Hot Way To Resist Adversarial Examples.,"We have found applying BPDA is often necessary: replacing f i(·) with g(·) on both the forward and backward pass is either completely ineffective (e.g. with Song et al. (2018)) or many times less effective (e.g. with Buckman et al. (2018)).###Thermometer encoding (Buckman et al., 2018) is a encoding scheme designed to break the local linearity of neural networks.",other,highlighting the necessity of applying BPDA in certain contexts
2681,5d84a3433a55acc20782ce9e,554d300f00fc14c2e4f48a740019496137d060c1,self-training for end-to-end speech recognition,5550411645ce0a409eb38730,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.,"Our sequence-to-sequence model is an encoder-decoder architecture with attention [12, 13].",other,describing the model architecture used
2951,5aed14e217c44a4438159868,d3707cf521e3596313af1f53acba6413d0d528a6,Training Tips for the Transformer Model,58437725ac44360f1082fb93,On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima.,"This observation goes against the common knowledge in other NMT frameworks and deep learning in general (Keskar et al., 2017) that smaller batches proceed slower (training examples per hour) but result in better generalization (higher test-set BLEU) in the end. In our experiments with the BASE model in T2T, bigger batches are not only faster in training throughput (as could be expected), but also faster in convergence speed, Time Till Score and Examples Till Score. Interestingly, when replicating these experiments with the BIG model, we see quite different results, as shown in Figure 6. The BIG model needs a certain minimal batch size to start converging at all, but for higher batch sizes there is almost no difference in the BLEU curves (but still, bigger batch never makes the BLEU worse in our experiments). In our case, the sharp difference is between batch size 1450, which trains well, and 1400, which drops off after two hours of training, recovering only slowly. According to Smith and Le (2017) and Smith et al. (2017), the gradient noise scale, i.###This observation goes against the common knowledge in other NMT frameworks and deep learning in general (Keskar et al., 2017) that smaller batches proceed slower (training examples per hour) but result in better generalization (higher test-set BLEU) in the end.###This observation goes against the common knowledge in other NMT frameworks and deep learning in general (Keskar et al., 2017) that smaller batches proceed slower (training examples per hour) but result in better generalization (higher test-set BLEU) in the end. In our experiments with the BASE model in T2T, bigger batches are not only faster in training throughput (as could be expected), but also faster in convergence speed, Time Till Score and Examples Till Score. Interestingly, when replicating these experiments with the BIG model, we see quite different results, as shown in Figure 6. The BIG model needs a certain minimal batch size to start converging at all, but for higher batch sizes there is almost no difference in the BLEU curves (but still, bigger batch never makes the BLEU worse in our experiments). In our case, the sharp difference is between batch size 1450, which trains well, and 1400, which drops off after two hours of training, recovering only slowly. According to Smith and Le (2017) and Smith et al.",other,highlighting the unexpected results of batch size on training and generalization in NMT
2936,5bdc315017c44a1f58a05a1d,f9fb0ad85f903e3ecf5d4bafdf7c74f647e75f58,LEMNA: Explaining Deep Learning based Security Applications,58d82fcbd649053542fd5fbc,Gradients of Counterfactuals.,"Recent works [53, 59] attempted to address this problem through approximation.",other,acknowledging prior attempts to solve a problem
3675,5dcd263a3a55ac58039516c5,add2f205338d70e10ce5e686df4a690e2851bdfc,Momentum contrast for unsupervised visual representation learning,5b3d98cc17c44a510f801acc,Unsupervised Feature Learning via Non-Parametric Instance Discrimination,"As the focus of this paper is not on designing a new pretext task, we use a simple one mainly following the instance discrimination task in [61], to which some recent works [63, 2] are related.###Contrastive loss functions can also be based on other forms [29, 59, 61, 36], such as margin-based losses and variants of NCE losses.###Several recent studies [61, 46, 36, 66, 35, 56, 2] present promising results on unsupervised visual representation learning using approaches related to the contrastive loss [29].###Contrastive learning is at the core of several recent works on unsupervised learning [61, 46, 36, 66, 35, 56, 2], which we elaborate on later in context (Sec.###The instance discrimination method [61] is related to the exemplar-based task [17] and NCE [28].###In this paper, we follow a simple instance discrimination task [61, 63, 2]: a query matches a key if they are encoded views ( e.g ., different crops) of the same image.###Another mechanism is the memory bank approach proposed by [61] (Figure 2b).###The input x q and x k can be images [29, 61, 63], patches [46], or context consisting a set of patches [46].###A momentum update is adopted on the memory bank in [61].###With similarity measured by dot product, a form of a contrastive loss function, called InfoNCE [46], is considered in this paper: where τ is a temperature hyper-parameter per [61].",other,describing the use of a simple pretext task in the context of contrastive learning
345,5efb0d5691e011063336d39c,0cee58946a13a5c2845647b4af8b9d2bf52a8b6b,BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,", 2018), BERT (Devlin et al., 2019), XLnet (Yang et al.###These language models have achieved state-of-the-art performance in many popular NLP benchmarks with appropriate fine-tuning (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019; Lan et al., 2020b; Raffel et al., 2019), which demonstrates their strong ability in modeling the text data.###Note that our implementations of the fully supervised NER methods attain very close to the state-of-the-art performance (Devlin et al., 2019; Limsopatham and Collier, 2016).",impact-revealing,highlighting the strong performance of language models in NLP benchmarks
3698,5fc61cdb91e0118947381abc,c9d736dd9f967844d2391bb13c4cb477576ab373,On Disambiguating Authors: Collaboration Network Reconstruction in a Bottom-up Manner,555043bf45ce0a409eb49222,A fast method based on multiple clustering for name disambiguation in bibliographic citations,"introduce a coarse-to-fine multiple clustering framework [34].###When these top-down approaches construct the ego-networks, they treat all authors shared the same name as an identical author [4], [22], [24], [26], [28]–[31], [33], [34].",other,discussing limitations in existing clustering approaches
2700,5ee7495191e01198a507f7ae,09bda461aa4911d0513e8e46dd39a4113947e450,Ansor : Generating High-Performance Tensor Programs for Deep Learning,5c757361f56def9798893f0d,Matrix capsules with EM routing,"We first evaluate Ansor on a set of common deep learning operators, including 1D, 2D, and 3D convolution (C1D, C2D, and C3D respectively), matrix multiplication (GMM), group convolution (GRP), dilated convolution (DIL) [57], depth-wise convolution (DEP) [24], transposed 2D convolution (T2D) [40], capsule 2D convolution (CAP) [23], and matrix 2-norm (NRM).###, capsule conv2d [23], dilated conv2d [57]).",other,reporting evaluation of deep learning operators
3835,5edf5ddc91e011bc656defd7,c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87,linformer: self-attention with linear complexity,5e997e4391e01118b66a5cf3,Training with Quantization Noise for Extreme Fixed-Point Compression,"This technique can be further improved through Quantization Aware Training (Jacob et al., 2018; Fan et al., 2020), where the weights are quantized during training and the gradients are approximated with the Straight-Through Estimator.",other,highlighting a method for improving technique through quantization
481,5db929ff47c8f766461fd7e4,a73051e08af289a50ef8ed53e69f91c189dd01e5,Induction Networks for Few-Shot Text Classification,599c7987601a182cd2648444,A Structured Self-attentive Sentence Embedding.,This module is a bi-direction recurrent neural network with self-attention as shown in Lin et al. (2017).,impact-revealing,describing the architecture of a neural network module
2537,5f058d15dfae54570ec57ea1,fb93ca1e004cbdcb93c8ffc57357189fa4eb6770,Resnest: Split-attention networks,53e9a508b7602d9702e2bcf5,Rectified Linear Units Improve Restricted Boltzmann Machines,Batch Normalization [30] is used after each convolutional layer before ReLU activation [41].,other,providing context for the use of Batch Normalization
746,5f8d00a29e795ea21aee8001,34d6bc3dc0a4811eb262508379fc74f600671687,a collective approach to scholar name disambiguation,53e9ae11b7602d9703820c92,On Graph-Based Name Disambiguation,"For example, coauthors, which are used as a strong evidence in many methods [11], [32], [39], may also be ambiguous.###pared with the state-of-the-art methods CE [7], GHOST [11], CSLR [19], MIX [18], and AM [44].###Most existing methods tackle name disambiguation separately [5], [6], [9], [11], [13], [15], [17], [18], [19], [29], [32], [35], [36], [37], [38], [39], [40], [44].###2) GHOST [11] is a graph-based method employing###, agglomerative clustering [9], [19], [39], affinity propagation [11] and Markov clustering [41], or topic modeling [28], [29] to divide the set of author references into different subsets.###More troubles may appear when multi-hop coauthorships are used as features [11], [32].###chosen baselines, only CE and GHOST analyze the time complexity [7], [11].###To avoid the redundant information, we only consider valid 2-hop coauthorship paths connecting two authors [11].###Using three real datasets, we conduct four sets of experiments to evaluate (1) the effectiveness and efficiency of NDCC versus state-of-the-art methods CE [7], GHOST [11], CSLR [19], MIX [18], and AM [44], (2) the effectiveness of author number estimation, (3) effects of important components in NDCC, and (4) the impacts of parameters on###In general, existing work for scholar name disambiguation can be divided into two classes: supervised [4], [15], [17], [18], [35], [38], [40], [44] and unsupervised [7], [9], [11], [19], [28], [29], [32], [34], [37], [39], [41], [42].",impact-revealing,highlighting challenges in name disambiguation methods
1668,,814d7256b711dbf6c012b3af3efb5a1ac9686519,"Academic careers and parenting: identity, performance and surveillance",,,"###Social Identity Theory (Tajfel and Turner 1979) and Nippert-Eng’s (1996) concept of Boundary Work are used as theoretical foundations for the study to explore how academic parents organise their work and family interface and how academics’ identities, both as a parent and an academic, are…###Social Identity Theory (Tajfel and Turner 1979) and Nippert-Eng’s (1996) concept of Boundary Work are used as theoretical foundations for the study to explore how academic parents organise their work and family interface and how academics’ identities, both as a parent and an academic, are constructed by themselves and by their peers.###In this paper attention is drawn to Self-categorisation Theory (Tajfel and Turner 1979) as it provides a basis for understanding how and when specific social categories are incorporated by the individual, or how and when the individual is ascribed identity by specific social categories.###Social Identity Theory (Tajfel and Turner 1979) builds on the notion that people derive part of their identity from the groups to which they belong.###The paper is structured as follows: a critical overview of what is known about academic parents, the theoretical foundations of the study (Social Identity Theory and
© 2017 Society for Research into Higher Education
CONTACT Candice Harris candice.harris@aut.ac.nz
Boundary Work), followed by a description of the research methodology.",impact-revealing,using established theories to frame the study on academic parents' work and family interface
1587,,71b8bcb4eacc217b1881d1aacaf3a85b8cde962c,Seismic layout optimization of steel braced frames by an improved dolphin echolocation algorithm,,,"###HS (Degertekin 2008) DE (Kaveh and Farhoudi 2013) IDE Present work###The results of optimization of 72-bar spatial truss found by IDE are compared in Table 5 with those of attained by DE (Kaveh and Farhoudi 2013) and GA (Wu and Chow 1995).###…in their hunting process were mimicked to propose Dolphin echolocation (DE) (Kaveh and Farhoudi 2013) metaheuristic and the results presented in (Kaveh and Farhoudi 2013) demonstrated the superiority of DE over the GA, HS, ACO, ABC, PSO, ICA and some hybrid algorithms in solving steel structure…###The proposed IDE algorithm is employed to solve two benchmark steel truss and frame optimization problems and the results are compared with those of the DE reported by Kaveh and Farhoudi (2013).###Recently, the strategies used by Dolphins in their hunting process were mimicked to propose Dolphin echolocation (DE) (Kaveh and Farhoudi 2013) metaheuristic and the results presented in (Kaveh and Farhoudi 2013) demonstrated the superiority of DE over the GA, HS, ACO, ABC, PSO, ICA and some hybrid algorithms in solving steel structure optimization problems.###Recently, the strategies used by Dolphins in their hunting process were mimicked to propose Dolphin echolocation (DE) (Kaveh and Farhoudi 2013) metaheuristic and the results presented in (Kaveh and Farhoudi 2013) demonstrated the superiority of DE over the GA, HS, ACO, ABC, PSO, ICA and some hybrid…###GA (Wu and Chow 1995) DE (Kaveh and Farhoudi 2013) IDE Present work###The main steps of DE algorithm for discrete optimization are as follows (Kaveh and Farhoudi 2013):
1.###For the 24-story steel frame Table 6 compares the results of optimization obtained by IDE with those of DE (Kaveh and Farhoudi 2013) and HS (Degertekin 2008).###It is obvious that echolocation has some similarities with optimization and it is inspired by Kaveh and Farhoudi (2013) for designing DE meta-heuristic algorithm.###The main steps of DE algorithm for discrete optimization are as follows (Kaveh and Farhoudi 2013):###In this case, two examples of truss and frame structures solved by Kaveh and Farhoudi (2013) are tackled here using IDE to illustrate its computational merits.",impact-revealing,highlighting the superiority of the proposed method over existing algorithms
2138,,0bcb046221461b24e6d9235ff7e6361882f32830,Performance evaluation of edge-computing platforms for the prediction of low temperatures in agriculture using deep learning,,,###Recurrent neural network (RNN) architectures have been widely applied for regression [20].,impact-revealing,acknowledge application of RNN architectures in regression
2816,5f842b5891e01129be18ffbd,7097137596f6755675f6aafcdd80969a747322ae,Contrastive Learning with Hard Negative Samples,5dcd263a3a55ac58039516c5,Momentum Contrast for Unsupervised Visual Representation Learning,"In computer vision, unsupervised contrastive learning methods have even outperformed supervised pre-training for object detection and segmentation tasks (Misra & Maaten, 2020; He et al., 2020).###In order to test whether our hard negatives sampling method can help when the negative batch size is very large, we also run experiments using MoCo-v2 with standard negative memory bank size N = 65536 (He et al., 2020; Chen et al., 2020c).###The negative sample distribution q is frequently chosen to be the marginal distribution p, or, in practice, an empirical approximation of it (Tian et al., 2019; Chen et al., 2020a;c; He et al., 2020; Chen et al., 2020c; Oord et al., 2018; Hénaff et al., 2020).###The negative sample distribution q is frequently chosen to be the marginal distribution p , or, in practice, an empirical approximation of it (Tian et al., 2019; Chen et al., 2020a;c; He et al., 2020; Chen et al., 2020c; Oord et al., 2018; Hénaff et al., 2020).###, 2020a;b), which uses augmented views of other items in a minibatch as negative samples, and MoCo (He et al., 2020; Chen et al., 2020c), which uses a momentum updated memory bank of old negative representations to enable the use of very large batches of negative samples.",other,highlighting advancements in unsupervised contrastive learning methods in computer vision
70,5e5e190b93d709897ce4997e,cb4571fa905abb70868d0bb9d4681f0a612c2d0f,Differentiable Reasoning On Large Knowledge Bases And Natural Language,5550411c45ce0a409eb3897f,Neural Machine Translation by Jointly Learning to Align and Translate.,"Wepropose using an attention mechanism (Bahdanau, Cho, and Bengio 2015) for attending over known predicates for deﬁning the rule-predicate embeddings θ p : , θ q : , θ r : .",impact-revealing,introducing a method for rule-predicate embeddings using attention mechanism
620,5ef0816891e0112aee042a73,7e627c774997addd7423361e93d8891dd1de35ad,Attention Mesh: High-fidelity Face Mesh Prediction in Real-time,53e99894b7602d97020cf38c,A Morphable Model For The Synthesis Of 3D Faces,"In contrast to methods that use a parametric model of the human face [1], we directly predict the positions of face mesh vertices in 3D.",impact-revealing,highlighting a methodological difference in face mesh prediction
2921,5fef22c691e0113b265a0289,b5b006dc558cb7fbd532d67e989173b536e8ac80,MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers,5d3ed2653a55ac61d998598b,RoBERTa: A Robustly Optimized BERT Pretraining Approach,"For models distilled from RoBERTa, we use similar pre-training datasets as in Liu et al. (2019).###RoBERTa (Liu et al., 2019) achieves strong performance by training longer steps using large batch size and more text data.###Pretrained Transformers (Radford et al., 2018; De-vlin et al., 2018; Dong et al., 2019; Yang et al., 2019; Joshi et al., 2019; Liu et al., 2019; Bao et al., 2020; Radford et al., 2019; Raffel et al., 2019; Lewis et al., 2019a) have been highly successful for a wide range of natural language…###Following previous pre-training (Devlin et al., 2018; Liu et al., 2019; Conneau et al., 2019) and task-agnostic distillation (Sun et al., 2019b; Jiao et al., 2019) work, we evaluate the English student models on GLUE benchmark and extractive question answering.",other,reporting on pre-training methods and their effectiveness
3135,5a73cb6317c44a0b30358203,a072c2a400f62f720b68dc54a662fb1ae115bf06,tacotron: towards end-to-end speech synthesis,5c86ee2b4895d9cbc6b6aa69,RNN Approaches to Text Normalization: A Challenge,"We perform simple text normalization, though recent advancements in learned text normalization [26] may render this unnecessary in the future.###We perform simple text normalization, though recent advancements in learned text normalization (Sproat & Jaitly, 2016) may render this unnecessary in the future.",other,acknowledging advancements in text normalization
710,53e99e61b7602d97027252c8,e3369553f399fb63b9f24261247afe64fb6c3f65,Virtual Program Counter (VPC) Prediction: Very Low Cost Indirect Branch Prediction Using Conditional Branch Prediction Hardware,53e9b8aeb7602d9704486073,A study of devirtualization techniques for a Java Just-In-Time compiler,"[28] classify the devirtualization techniques into guarded devirtualization and direct devirtualization.###These approaches include the method cache in Smalltalk-80 [11], polymorphic inline caches [23], and type feedback/devirtualization [24], [28].###[28] report only an average 40 percent reduction in the number of virtual method calls on a set of Java benchmarks, with the combined application of aggressive guarded and direct devirtualization techniques.###The VPC prediction algorithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) [11], [24], [21], [6] or devirtualization [28].###Dynamic recompilation can overcome this limitation, but it requires an expensive mechanism called on-stack replacement [28].###Devirtualization based on static analysis requires type analysis, which in turn requires whole program analysis [28], and unsafe languages like Cþþ also require pointer alias analysis.###Devirtualization is the substitution of an indirect method call with direct method calls in object-oriented languages [11], [24], [21], [6], [28].",impact-revealing,providing context on devirtualization techniques and their challenges
474,5e68b99493d709897cd373ed,d08b35243edc5be07387a9ed218070b31e502901,group normalization,53e9a95db7602d97032b5715,Histograms of Oriented Gradients for Human Detection,"Classical features of SIFT [39], HOG [9], and GIST [41] are group-wise representations by design, where each group of channels is constructed by some kind of histogram.###For example, a HOG vector is the outcome of several spatial cells where each cell is represented by a normalized orientation histogram.###We notice that many classical features like SIFT [39] and HOG [9] are group-wise features and involve group-wise normalization.###We notice that many classical features like SIFT (Lowe 2004) andHOG (Dalal and Triggs 2005) are group-wise features and involve group-wise normalization.###However, in addition to orientations (SIFT [39], HOG [9], or [11, 8]), there are many factors that could lead to grouping, e.g., frequency, shapes, illumination, textures.###Classical features of SIFT (Lowe 2004), HOG (Dalal and Triggs 2005), andGIST (Oliva and Torralba 2001) are group-wise representations by design, where each group",impact-revealing,acknowledge characteristics of classical features in image processing
217,5e09caba3a55ac662f721afe,36ad06e6f9b39192e7668634eadd6fcf9593e922,Efficient Adversarial Training With Transferable Adversarial Examples,5cede104da562983788e4012,Theoretically Principled Trade-off between Robustness and Accuracy,"To generate strong adversarial examples, iterative attacks [13], which use multiple attack iterations to generate adversarial examples, are widely adopted in various adversarial training methods [4,18,28,34].###In this section, we integrate ATTA with two popular adversarial training methods: Madry’s Adversarial Training (MAT) [18] and TRADES [34].###For the CIFAR10 dataset, we use the wide residual network WRN-34-10 [31] which is same as [18, 34].###Since adversarial perturbations are usually bounded by the allowed perturbation space S, PGD-k (k-step projected gradient descent [13]) is adopted to conduct iterative attack [18, 23, 32, 34].###Recently, lots of works [2,10,11,18,22,33,34] focus on analyzing and improving adversarial machine learning.###For example, TRADES [34] uses robustness loss (Equation 2) as the loss to generate the adversarial examples.###[34] shows that this loss has a better performance in the TRADES algorithm.###Following the literature [18, 32, 34], we use both MNIST [14] and CIFAR10 dataset [12] to evaluate ATTA.###For the MNIST dataset, the model has four convolutional layers followed by three full-connected layers which is same architecture as used in [18,34].###We apply our technique on Madry’s Adversarial Training method (MAT) [18] and TRADES [34] and evaluate the performance on both MNIST and CIFAR10 dataset.",impact-revealing,acknowledge existing methods in adversarial training
1185,,81d2c990f00fa4e18e1b131332ac624be4fe9899,Field Formulation of Parzen Data Analysis,,,"###We try to build on such intuition in this analysis of the Parzen-window distribution [1], an important tool which has been introduced in 1965, and still serves the goal of pattern recognition [2].",impact-revealing,highlighting the historical significance and ongoing relevance of the Parzen-window distribution in pattern recognition
3861,5da1a6d447c8f766460688bc,9570147aee646e15affffc30a6b18be0568cfa82,SWQUE: A Mode Switching Issue Queue with Priority-Correcting Circular Queue,558bfa7e84ae6766fdf05aeb,40-Entry unified out-of-order scheduler and integer execution unit for the AMD Bulldozer x86_x0096_64 core,"The age matrix is used in parallel with the select logic [11], and selects only the single oldest ready instruction.###For example, the CAM type is used in the AMD Bulldozer [11], whereas the RAM type is used in the IBM POWER8 [26].###Although all processor vendors do not publish their IQ organizations, AGE is generally used in modern processors [11, 22, 26].###A circuit called the age matrix [22, 24], which selects the single oldest ready instruction, is used together with RAND in current processors [11, 22, 26].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1544,,17163c55f6a9627f7e3510b2d0417fc205c29227,Roles of the hippocampal formation in pain information processing,,,"###The hippocampal formation (HF), an integral component of the limbic system [17-18], has long been implicated in several functions such as arousal and attention [19], learning and memory [20-23], emotion and affect [24,25], sensory motor integration [17,26] and so on.###The involvement of HF in learning and memory has been validated by a tremendous variety of previous experiments and emerging evidence suggests that the HF is likely to participate in multiple forms of memory, including spatial navigation, declarative memory, recognition memory and so on[20-23,250,251].",impact-revealing,highlighting the significance of the hippocampal formation in various cognitive functions
2490,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,53e9b601b7602d97041573df,Properties And Prediction Of Flow Statistics From Sampled Packet Streams,[34] investigated the resource usage of NetFlow formation and exportation as well as statistical properties of original traffic from sampled traffic data.,other,reporting prior findings on resource usage and statistical properties
3452,5edf5dd891e011bc656deb7d,14b65a86c82e38fce0eb3506e0d4084ad5cdb583,deberta- decoding-enhanced bert with disentangled attention,5b1643998fbcbf6e5a9bc3b6,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.,"The diversity of the tasks makes GLUE very suitable for evaluating the generalization and robustness of NLU models.###The General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding (NLU) tasks.###A.1 Dataset
‚ GLUE.###We summarize the results on eight GLUE [30] tasks in Table 1, which compares DeBERTa with previous models with around 350M parameters: BERT, RoBERTa, and XLNet.",other,providing context for the GLUE benchmark and its relevance in evaluating NLU models
358,5bdc31b417c44a1f58a0bbc4,27e98e09cf09bc13c913d01676e5f32624011050,U-Net: Machine Reading Comprehension with Unanswerable Questions,53e997b2b7602d9701f91200,Multitask Learning,"Multi-task models Different from existing work, we regard the MRC with unanswerable questions as a multi-task learning problem (Caruana 1997) by sharing some meta-knowledge.",impact-revealing,highlighting a novel approach to multi-task learning in MRC
1823,,9460d6c1efb661f3144ee019bcccaded70cd7176,Tissue Engineering and Organ Structure: A Vascularized Approach to Liver and Lung,,,###This theoretical model defines scaling laws of a vascular network and has been validated by comparing it to morphologic data on several native human and animal vascular networks (16).,impact-revealing,highlighting the validation of a theoretical model through comparison with empirical data
2112,,575c2b8784e0b3b80a8418a554ddeda852aa64b4,"On statistics, computation and scalability",,,"###In recent work, Kleiner et al. (2013) have explored a new procedure, the “Bag of Little Bootstraps” (BLB), which targets computational efficiency, but which also alleviates some of the difficulties of subsampling, the m-out-of-n bootstrap and the bootstrap, essentially by combining aspects of these procedures.###An example taken from Kleiner et al. (2013) serves to illustrate the very substantial computational gains that can be reaped from this approach.###…it can be viewed as a novel statistical procedure to be compared to the bootstrap and subsampling according to more classical criteria; indeed, Kleiner et al. (2013) present experiments that show that even on a single processor BLB converges faster than the bootstrap and it is less sensitive…###The material in this section summarizes research described in Kleiner et al. (2013).###Moreover, although the development of BLB was motivated by the computational imperative, it can be viewed as a novel statistical procedure to be compared to the bootstrap and subsampling according to more classical criteria; indeed, Kleiner et al. (2013) present experiments that show that even on a single processor BLB converges faster than the bootstrap and it is less sensitive to the choice of m than subsampling and the m-out-of-n bootstrap.###In recent work, Kleiner et al. (2013) have explored a new procedure, the “Bag of Little Bootstraps” (BLB), which targets computational efficiency, but which also alleviates some of the difficulties of subsampling, the m-out-of-n bootstrap and the bootstrap, essentially by combining aspects of these…",impact-revealing,highlighting the computational efficiency and advantages of the Bag of Little Bootstraps procedure
280,5ece0f029e795ebde7de1cce,d9c99592667c92d01cded2b7ca25cba4fdf83729,Bipartite Graph Neural Networks for Efficient Node Representation Learning,5bdc31b817c44a1f58a0c039,Adaptive sampling towards fast graph representation learning,"To deal with this problem, node-wise sampling like GraphSAGE (Hamilton, Ying, and Leskovec, 2017) directly samples neighbor nodes, while layer-wise sampling method like AS-GCN (Huang et al., 2018) uses adaptive sampling to fix the number of nodes in each layer further.###Our work is also relevant to those GCNbased methods that deal with the scalability issue on largescale graphs, like the node-wise sampling method GraphSAGE (Hamilton, Ying, and Leskovec, 2017) and the ayerwise sampling method AS-GCN (Huang et al., 2018).###We contrast the performance of our algorithm against several unsupervised graph learning counter-parts: Node2Vec (Grover and Leskovec, 2016), VGAE (Kipf and Welling, 2016), GraphSAGE (Hamilton, Ying, and Leskovec, 2017), and ASGCN (Huang et al., 2018).",impact-revealing,acknowledge existing methods for scalability in graph learning
815,53e99c66b7602d9702514d51,8e8e622d5fab4c1d2a5bc7783db84e62cc570f9a,disaggregated memory for expansion and sharing in blade servers,53e999adb7602d97021efd76,Understanding and Designing New Server Architectures for Emerging Warehouse-Computing Environments,"Our prior work [8] employs a variant of this two-level memory organization as part of a broader demonstration of how multiple techniques, including the choice of processors, new packaging design, and use of Flash-based storage, can help improve performance in warehouse computing environments.###, [8][9][11]), it is important for these approaches to require at most minor changes to ensure that the low-cost benefits of commodity solutions not be undermined.###At the same time, several studies show that the contribution of memory to the total costs and power consumption of future systems is trending higher from its current value of about 25% [6][7][8].###Estimates for the memory contributions towards power and cooling are calculated using the same methodology as in [8].",impact-revealing,reporting prior work on memory organization in computing environments
2407,5ca72ae8181a2f3597ce6e45,b1051e81e527d841f0936c604aa6966c719e876d,TANGRAM: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators,573696106e3b12023e523461,EIE: Efficient Inference Engine on Compressed Deep Neural Network,"Recent research has shown that domain-specific NN accelerators can achieve more than two orders of magnitude improvements over CPUs and GPUs in terms of performance and energy efficiency [1, 4, 5, 7, 9, 11, 12, 15, 28, 33, 36].###Recent work either dynamically pruned zero and small values [1, 33], or statically compressed the NN structures into sparse formats [10, 15, 47, 49].",other,highlighting significant advancements in domain-specific neural network accelerators
601,599c7ec9601a182cd28c8607,79dd6b45903407759307f83f4f4a718c4fbb9581,Control-Flow Checking Using Branch Sequence Signatures.,558b20dee4b0b32fcb3ae0c6,Architecture Support for Dynamic Integrity Checking,"The cores of trusted computing technology are trusted computing base and trusted chain [4, 5],and trusted measurement is a key problem of this technology [6, 7].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1289,,b7300ab256e9cced78c3e0b49ee6bfc7c39a271f,Quantification of Metabolites in Swine Brain by ^1H MR Spectroscopy Using LCModel and QUEST: A Comparison Study,,,"###This basis set is used as prior knowledge that introduced by Provencher from in vitro measurements of pure metabolite solutions.###As a commonly used commercial metabolite quantification tool in vivo proton MRS, LCModel was introduced by Provencher [1].###In order to reduce the effect of attenuation behavior, Provencher suggested that more accurate results should be acquired under the conditions that TE<30 ms and TR>3000 ms at least [5].###Water is commonly used as an inner reference to acquire metabolite absolution concentrations [1].",impact-revealing,acknowledge the foundational work and recommendations by Provencher in metabolite quantification
2427,5fc4cfdf91e011abfa2faf94,633e2fbfc0b21e959a244100937c5853afca4853,score-based generative modeling through stochastic differential equations,599c7950601a182cd262f003,Learning to Generate Samples from Noise through Infusion Training.,"Score-based generative models, and related techniques (Bordes et al., 2017; Goyal et al., 2017; Du & Mordatch, 2019), have proven effective at generation of images (Song & Ermon, 2019; 2020; Ho et al.###Score-based generative models, and related techniques (Bordes et al., 2017; Goyal et al., 2017; Du & Mordatch, 2019), have proven effective at generation of images (Song & Ermon, 2019; 2020; Ho et al., 2020), audio (Chen et al., 2020; Kong et al., 2020), graphs (Niu et al., 2020), and shapes…",other,reporting effectiveness of score-based generative models in various domains
2230,555048d345ce0a409eb71be1,eba36ac75bf22edf9a1bfd33244d459c75b98305,Recurrent convolutional neural networks for text classification,56d81308dabfae2eee5eff4b,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,"(2013) is approximately 3-5 hours.###Socher et al. (2013) introduced recursive neural tensor network to analyse sentiment of phrases and sentences.###Socher et al. (2011a; 2011b; 2013) proposed the Recursive Neural Network (RecursiveNN) that has been proven to be efﬁcient in terms of constructing sentence representations.###In contrast to the most positive and most negative phrases in RNTN, our model does not rely on a syntactic parser, therefore, the presented n-grams are not typically “phrases”.###For comparison, we also list the most positive/negative trigram phrases extracted by the RNTN (Socher et al. 2013).",other,comparing different sentiment analysis models and their methodologies
1406,,f5e1993f3f505e8fbb9cac9231285c8c9f1712a7,Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning,,,"###While analysis in prior work [45] notes challenges for a subset of offline RL methods, in Figure 2, we evaluate the fine-tuning performance of a variety of prior offline RL methods (CQL [32], IQL [30], TD3+BC [11], AWAC [45]) on a particular diagnostic instance of a visual pick-and-place###In practice, Cal-QL can be implemented on top of the conservative Q learning (CQL) [32] for offline RL within a one-line code change.###To this end, Cal-QL builds on CQL [32] and then constrains the learned Q-function to produce Q-values larger than the Q-value of a reference policy μ per Definition 4.###This includes naïvely fine-tuning offline RL methods such as CQL [32] and IQL [30], as well as fine-tuning with AWAC [45], O3F [42] and online decision transformer (ODT) [65], methods specifically designed for offline RL followed by online fine-tuning.###In practice, Cal-QL can be implemented on top of conservative Q-learning [32], a prior offline RL method, without any additional hyperparameters.###Our approach will build on the conservative Q-learning (CQL) [32] algorithm.###These approaches typically employ offline RL methods based on policy constraints or pessimism [12, 49, 16, 15, 30, 51, 36] on the offline data, then continue training with the same method on a combination of offline and online data once fine-tuning begins [43, 28, 62, 32, 4].###How can we devise a method to learn an effective policy initialization that also improves during fine-tuning? Prior work [32, 6] shows that one can learn a good offline initialization by optimizing the policy against a conservative value function obtained from an offline dataset.",impact-revealing,evaluating fine-tuning performance of offline RL methods
2376,5eabf3cd91e011664efc496f,916595fc4e701ac6125725408912a1fac3d7a60b,CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows,53e9b65bb7602d97041b86ba,SPEAR: A Hybrid Model for Speculative Pre-Execution,"A large body of prior work has focused on extracting MLP on an OoO core, such as prefetching, runahead execution [51], [52], and speculative pre-execution [53], [54], [55].",other,acknowledging existing research on MLP extraction techniques
2570,5e09a76bdf1a9c0c41677a7b,71bd6b3a2bfa54ee4ea8499be0c4ff478fd735f6,POLAR++: Active One-Shot Personalized Article Recommendation,53e99a14b7602d9702265d61,Content-Based Recommendation Systems,"Content-based filtering [4] is one of the most widely used and researched recommendation method and has been successfully applied in article recommendation [9], [19], [20], [21].###Therefore, it is difficult to directly apply the traditional recommendation methods such as Content-based Recommendation [4] or Collaborative Filtering [5] in this scenario.",other,acknowledging the challenges of applying traditional recommendation methods
998,,da442491b139ebde15ba82b5ab2c58c2f3be3bd1,Parametric manipulation of conflict and response competition using rapid mixed-trial event-related fMRI,,,"###In a follow-up study we showed that differences in performance on this task between young children with and without ADHD did not account for differential patterns of activation associated with inhibiting a response (Durston et al., 2003).",impact-revealing,reporting findings on ADHD response inhibition
2474,5e09a76bdf1a9c0c41677a7b,71bd6b3a2bfa54ee4ea8499be0c4ff478fd735f6,POLAR++: Active One-Shot Personalized Article Recommendation,53e9bd9eb7602d9704a468e8,Collaborative filtering via gaussian probabilistic latent semantic analysis.,"More advanced methods are based on uncertainty reduction [48] or error reduction [49], [50].",other,acknowledge existing advanced methods
2840,5db6c73a3a55acec0731cd68,73a5605ce482bd639078ebbb19baac7b903017e2,A Unified MRC Framework for Named Entity Recognition,58437725ac44360f1082fbea,Machine Comprehension Using Match-LSTM and Answer Pointer.,"MRC models (Seo et al., 2016; Wang et al., 2016; Wang and Jiang, 2016; Xiong et al., 2016, 2017; Wang et al., 2016; Shen et al., 2017; Chen et al., 2017) extract answer spans from passages given questions.",other,reporting prior findings on MRC models
2002,,09e642e570343cae9c8a38fc1c0157822e38dd21,"Acazicolcept (ALPN-101), a dual ICOS/CD28 antagonist, demonstrates efficacy in systemic sclerosis preclinical mouse models",,,"###decreased pulmonary and dermal fibrosis in SSc mouse models [40, 41, 43].###One limitation from the study herein might be that it does not address whether the dual-specific compound acazicolcept may offer a benefit as compared to single therapies targeting CD28 or ICOS alone; however, each single therapy has already demonstrated its relevance in SSc [28, 29, 40, 43] and in other CTDs [52–59].",impact-revealing,acknowledging limitations in the study and relevance of single therapies
2761,5d1eb9e4da562961f0b1eb04,f80be25edf309ab595fc76fddd8cefe8eb2e5a54,GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation,599c797d601a182cd2643e8a,Modeling Relational Data with Graph Convolutional Networks.,"3 indicate that R-GCNs are outperforming GGNNs substantially on the VarMisuse task, contradicting the findings of Allamanis et al. (2018).###This indicates that R-GCN’s better (average) results are not due to a more expressive architecture, but that training is just slightly more successful at finding parameters
that work well across all examples.###These include GGNN (Li et al., 2016)
(see Eq. p1q), R-GCN (Schlichtkrull et al., 2018) (see Eq. p2q), R-GAT (Veličković et al., 2018) (see Eq. p3q), and R-GIN (Hamilton et al., 2017) (see Eq. p4q)3.###This resulted in three (R-GAT), four (GGNN, GNN-FiLM, GNN-MLP1, R-GCN), or five (GNN-MLP0, R-GIN) layers (propagation steps) and a node representation size of 256 (GNN-MLP0, R-GIN) or 320 (all others).###1 indicate that GATs have no advantage over GGNNs or R-GCNs on the PPI task, which does not match the findings by Veličković et al. (2018).###Furthermore, all models used residual connections connecting every second layer and GGNN, R-GCN, GNN-FiLM and GNN-MLP0 additionally used layer normalisation (as in Eq. p8q).###3, are somewhat surprising, as they indicate a different ranking of model architectures as the results on PPI and QM9, with R-GCN performing best.###Additionally, GNN-MLP0 is a variant of R-GCN using a single linear layer to compute the edge message from both source and target state (i.e., Eq. p5q instantiated with an “MLP” without hidden layers), and GNN-MLP1 is the same with a single hidden layer.###In Relational Graph Convolutional Networks (RGCN) (Schlichtkrull et al., 2018) (an extension of Graph Convolutional Networks (GCN) (Kipf & Welling, 2017)), the gated unit is replaced by a simple non-linearity σ (e.g., the hyperbolic tangent).
hpt`1qv “ σ ˜ ÿ
uÑ̀vPE
1
cv,` ¨W`hptqu
¸
(2)
Here, cv,`…###Finally, the large variance in results on the validation set (especially for R-GCN) makes it likely that the hyperparameter grid search with only one training run per configuration did not yield the best configuration for each model.###For example, an in-depth analysis shows that 2.2% of the examples in SEENPROJTEST are predicted correctly by (at least) one of the five trained R-GCN models, whereas 2.7% of examples could be predicted correctly by one of the trained R-GIN models.###In the experiments, this will be called GNN-MLP, formally defined as follows.2
hpt`1qv “ σ ˜ ÿ
uÑ̀vPE
1
cv,` ¨MLP `
´
hptqu }h ptq v
¯
¸
(5)
Below, we will instantiate the MLP ` with a single linear layer to obtain what we call GNN-MLP0, which only differs from R-GCNs (Eq. p2q) in that the message passing function is applied to the concatenation of source and target state.###All re-implemented baselines beat the results reported by Allamanis et al. (2018), who also reported that R-GCN and GGNN show very similar performance.###Note that Eq. p4q is very similar to the definition of R-GCNs (Eq. p2q), only dropping the normalisation factor 1 cv,` and replacing linear layers by an MLP.###In contrast, the representation of the target of an edge is only updated (in the GGNN case Eq. p1q), treated as another incoming message (in the R-GCN case Eq. p2q and the R-GIN case Eq. p4q), or used to weight the relevance of an edge (in the R-GAT case Eq. p3q).###Nonetheless, experiments across all three tasks have shown that these methods outperform better-published techniques such as GGNNs, R-GCNs and GATs, without a substantial runtime penalty.###In Relational Graph Convolutional Networks (RGCN) (Schlichtkrull et al., 2018) (an extension of Graph Convolutional Networks (GCN) (Kipf & Welling, 2017)), the gated unit is replaced by a simple non-linearity σ (e.g., the hyperbolic tangent).
hpt`1qv “ σ ˜ ÿ
uÑ̀vPE
1
cv,` ¨W`hptqu
¸
(2)
Here, cv,` is a normalisation factor usually set to the number of edges of type ` ending in v.",other,discussing the performance and architecture of various graph neural network models
2180,,8d7891205fd3459b35babba7680103f3cdab0c0e,Intelligent End-To-End Resource Virtualization Using Service Oriented Architecture,,,"###Virtualization provides abstraction and isolation of lower level functionalities and underlying hardware, enabling portability of higher level functions and sharing/aggregation of physical resources [ 2 ].###According to [ 2 ] cloud computing builds upon decades of research in virtualization, distributed computing, grid computing, utility computing, and networking, as well as web services.",impact-revealing,providing context on the relationship between virtualization and cloud computing
1567,,c91cc92803726bb7bc73d250f53f80bd2e09a04a,Decision-Making From the Animal Perspective: Bridging Ecology and Subjective Cognition,,,"###Cognitive complexity is limited by using simple heuristic rules (Hutchinson and Gigerenzer, 2005; Gigerenzer, 2008; Gigerenzer and Brighton, 2009).###Instead of the best choice, the organism makes fast and frugal decisions that are good enoughmost of the time for the ecological conditions (Hutchinson and Gigerenzer, 2005; Gigerenzer, 2008).",impact-revealing,highlighting limitations of cognitive complexity in decision-making
2595,5c96086e3cb210d2716c4a08,bb76749fab841ccbe20ace74111a518f65d9870b,make the most out of last level cache in intel processors,5736956c6e3b12023e490b37,Systematic Reverse Engineering Of Cache Slice Selection In Intel Processors,"There have been many attempts to find the slice mapping and reverse-engineer Intel’s Complex Addressing [1, 27, 39, 42, 61, 84].###Others have tried to reverse engineer the Intel LLC Complex Addressing hash function [1, 27, 39, 42, 61, 84].",other,acknowledge attempts in reverse engineering Intel's Complex Addressing
3733,5efdaf7b91e01191d3d28242,6ff1eb9cdf64a464bf43b54d852456e9ddf55b28,Debiased Contrastive Learning,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"Cifar 10 and STL 10 We adopt PyTorch to implement SimCLR [ 2 ] with Resnet-50 [ 15 ] as the encoder architecture and use the Adam optimizer [ 19 ] with learning rate 0.001 and weight decay 1 e − 6.###First, for CIFAR 10 [ 23 ] and STL 10 [ 6 ], we implement SimCLR [ 2 ] with ResNet-50 [ 15 ] as the encoder architecture and use the Adam optimizer [ 19 ] with learning rate 0.001 and weight decay 1 e − 6.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2077,,4c715cfa89049ee09148e3e2fe6fcb5009d83459,Performance Analysis of Weakly-Consistent Scenario-Aware Dataflow Graphs,,,"###Such behavior is captured well by performance models that build upon the (max , +) -semi- ring [1], such as Network Calculus [5], Real-Time Calculus [27], timed Petri-nets [8], [22], max-plus automata [13], and the timed dataﬂow models.###The (max, +)-semi-ring and its linear algebra [1] are very suitable to express the behavior and semantics of such models and it forms the foundation of many popular performance analysis models, such as Network Calculus [2], Real-Time Calculus [3], timed Petri-nets [4, 5], and the family of timed dataflow models of which SADF is a member.###Such behavior is captured well by performance models that build upon the (max, +)-semi-ring [1], such as Network Calculus [2], Real-Time Calculus [3], timed Petri-nets [4, 5], max-plus automata [6], and the timed dataflow models.",impact-revealing,"acknowledge existing performance models based on the (max, +)-semi-ring"
302,5a4aef9e17c44a2190f7a4bd,1deb7f96fc92d5c9e04d2cbb277473fee878e144,Cascaded Pyramid Network for Multi-person Pose Estimation,57a4e91dac44365e35c98c40,Chained Predictions Using Convolutional Neural Networks,"Recent works [27, 14, 4, 19, 39, 42] mostly rely on the development of convolutional neural network(CNN) [22, 16], which largely improve the performance of pose estimation.",impact-revealing,acknowledge reliance on CNNs for pose estimation improvements
1427,,7be8e9e37ba5a4eb473cbc39f8cb7eba2d6362cc,Learning from Demonstration: Provably Efficient Adversarial Policy Imitation with Linear Function Approximation,,,"###The principle pessimism-in-face-of-uncertainty guides the agent to be conservative to visit the states and actions that are less covered by the additional dataset D A (Kumar et al., 2020; Jin et al., 2021; Liu et al., 2020; Yu et al., 2020; 2021; Buckman et al., 2020).###…with optimism (Auer et al., 2002; 2009; Azar et al., 2017; Jin et al., 2018; 2019; Yang & Wang, 2020), offline RL (Fujimoto et al., 2019b; Kumar et al., 2020; Fujimoto et al., 2019a; Duan et al., 2020; Levine et al., 2020; Jin et al., 2021), policy optimization (Beck & Teboulle, 2003;…###…space, the concentrability coefficients are uniformly upper bounded, or even the partial coverage assumption (Antos et al., 2007; Munos & Szepesv´ari, 2008; Yang et al., 2020b;a; Levine et al., 2020; Uehara et al., 2020; Siegel et al., 2020; Wang et al., 2020b; Zhang et al., 2020; Xu et al., 2020).###…2017; Jin et al., 2018; 2019; Yang & Wang, 2020), offline RL (Fujimoto et al., 2019b; Kumar et al., 2020; Fujimoto et al., 2019a; Duan et al., 2020; Levine et al., 2020; Jin et al., 2021), policy optimization (Beck & Teboulle, 2003; Hazan, 2019; Cai et al., 2020; Nemirovskij & Yudin, 1983),…###…the uncertainty quantification for estimated model in PGAPI is motivated by the pessimism in offline RL (Chen & Jiang, 2019; Xie et al., 2021a;b; Kumar et al., 2020; Jin et al., 2021; Liu et al., 2020; Yu et al., 2020; 2021; Buckman et al., 2020; Rashidinejad et al., 2021; Uehara & Sun, 2021).###…coverage does not assume that the additional dataset D A to be well-explored dataset (Zhang et al., 2020; Xu et al., 2020; Yang et al., 2020b;a; Levine et al., 2020), e.g. restricting that the densities of visitation measures of the behavior policy generating the dataset are uniformly lower…###Inspired by the spirit of being conservative in offline RL (Fujimoto et al., 2019b; Kumar et al., 2020; Jin et al., 2021), we propose a pessimistic variant of policy optimization in the policy update stage of PGAPI (Lines 5–11 of Algorithm 3), which ensures that PGAPI utilize the information of the…###This problem has been studied widely in offline RL (Fujimoto et al., 2019a; Kumar et al., 2020; Fujimoto et al., 2019b; Levine et al., 2020; Jin et al., 2021) and (Wang et al., 2020a) even propose that the lower bound of offline RL can grow exponentially with the horizon under linear approximation…",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
980,,d8e94f1b9ba73767487776bae055dd2cabc072d0,How the Brain Shapes Deception,,,"###Recently, Byrne and Corp (2004) further investigated the relationship between the use of tactical deception and neocortical volume across species in primates. To this end, they used a catalog of tactical deception in primates that incorporates all records known at the time of their study (Byrne and Whiten 1990). All the records met the definition of tactical deception and were contributed by researchers who were expert observers and highly experienced with their study species. Byrne and Corp (2004) then calculated an index of frequency of deception usage that was controlled for several possible confounding factors.###Although Byrne and Corp (2004) showed that the volume of the rest of the brain was not a significant predictor of deception frequency, the specific
contribution of the striatum has not been investigated fully.###Adapted from Byrne and Corp (2004) with permission from The Royal Society.###Byrne and Corp (2004) then calculated an index of frequency of deception usage that was controlled for several possible confounding factors.###Recently, Byrne and Corp (2004) further investigated the relationship between the use of tactical deception and neocortical volume across species in primates.###Consequently, Byrne and Corp (2004) found that deception frequency was significantly correlated with both the volume of the neocortex and the neocortex ratio across species (Fig.###Recently, Byrne and Corp (2004) further investigated the relationship between the use of tactical deception and neocortical volume across species in primates. To this end, they used a catalog of tactical deception in primates that incorporates all records known at the time of their study (Byrne and Whiten 1990). All the records met the definition of tactical deception and were contributed by researchers who were expert observers and highly experienced with their study species. Byrne and Corp (2004) then calculated an index of frequency of deception usage that was controlled for several possible confounding factors. They also obtained the volumes of the neocortex and the rest of the brain in species from several sources of published data on primate brains. They used two kinds of measures of neocortex size: absolute volume of the neocortex and the neocortex ratio (the volumetric fraction of the brain). Consequently, Byrne and Corp (2004) found that deception frequency was significantly correlated with both the volume of the neocortex and the neocortex ratio across species (Fig. 1). These findings strongly support the view that such complex acts as deception are supported by the function of the neocortex. In humans, the neocortex, the phylogenetically most recent part of the cerebral cortex, is involved in higher functions such as memory and language. It is therefore quite reasonable to assume that the biological evolution of the human brain enabled people to make use of deception and that deception is closely linked to the function of the neocortex. This notion is highly consistent with recent findings from neuroimaging studies of human deception. However, it is unlikely that the neocortex is evolutionarily the only structure responsible for deceptive behavior. This idea has been inspired by the findings that not only the neocortex but also the striatum have expanded relative to the rest of the brain in primate evolution (Keverne and others 1996). Although Byrne and Corp (2004) showed that the volume of the rest of the brain was not a significant predictor of deception frequency, the specific###Recently, Byrne and Corp (2004) further investigated the relationship between the use of tactical deception and neocortical volume across species in primates. To this end, they used a catalog of tactical deception in primates that incorporates all records known at the time of their study (Byrne and Whiten 1990). All the records met the definition of tactical deception and were contributed by researchers who were expert observers and highly experienced with their study species. Byrne and Corp (2004) then calculated an index of frequency of deception usage that was controlled for several possible confounding factors. They also obtained the volumes of the neocortex and the rest of the brain in species from several sources of published data on primate brains. They used two kinds of measures of neocortex size: absolute volume of the neocortex and the neocortex ratio (the volumetric fraction of the brain). Consequently, Byrne and Corp (2004) found that deception frequency was significantly correlated with both the volume of the neocortex and the neocortex ratio across species (Fig.",impact-revealing,highlighting the findings of Byrne and Corp (2004) regarding the relationship between tactical deception and neocortical volume in primates
49,59ae3c262bbe271c4c71ea21,83e7654d545fbbaaf2328df365a781fb67b841b4,Enhanced LSTM for Natural Language Inference,573696ce6e3b12023e5cecd1,Long short-term memory over recursive structures,"s continues and results in a full binary tree, where padding words are inserted to the nodes when there are no enough leaves to form a full tree. Each tree node is implemented with a tree-LSTM block [Zhu et al., 2015]. Table 2 depicts that with this replacement, the performance drops to 88.1%, showing that incorporating syntactic parse information is useful even when the parse information is added to the already ###e replace the two BiLSTM (one used in performing soft alignment and one in inference composition) with tree-LSTM. In general, tree-LSTM has recently been proposed to explicitly model tree structures [Zhu et al., 2015]. Speciﬁcally, the forward propagation of tree-LSTM is computed as follows in Equation 20– 26, and a node (a memory block) of the network is wired as in Figure 2. Figure 2 shows the memory block at e###et and we hope our work shed some light on the future work along this line. 2 On the other hand, we also employ rich information from syntactic parse and from recursive networks, i.e., the tree-LSTM [Zhu et al., 2015]. In general, exploring syntax together with semantics for NLI is very attractive to us. As pointed out in [Barker and Jacobson, 2007] “the syntax and the semantics work together in tandem&quot;, and###vity, all bias vectors are also omitted. We use binary tree-LSTM in this paper. The tree structure for each sentence (the premise or hypothesis) is producer by a constituency parser. And as noted in [Zhu et al., 2015], one can always choose to binarize a non-binary tree, and the syntactic information will largely be kept. Binarization can help avoid the need of designing different types of memory blocks for tree ###h as chain LSTM-based models had not been fully explored in the previous work. It could deserve a further exploration on the power of such models. We ensemble our EBIM model with syntactic tree-LSTM [Zhu et al., 2015] based on binary parse trees, and achieve signiﬁcant improvement over our best sequential encoding model EBIM, obtaining an accuracy of 88.3%. The syntactic tree-LSTM complement very well with EBIM i",impact-revealing,describing the implementation and significance of tree-LSTM in the model
3617,53e9a415b7602d9702d30d91,1753c2dc85cc40e0a2e8b4a405c1690eab066d8d,FENNEL: streaming graph partitioning for massive scale graphs,53e9a45cb7602d9702d783b7,"What is Twitter, a social network or a news media?","…context of online services, the Web graph amounts to at least one trillion of links [1], Facebook recently reported more than 1 billion of users and 140 billion of friend connections [2], and, in 2009, Twitter reported more than 40 million of users and about 1.5 billion of social relations [39].",other,providing context on the scale of online services and social networks
2071,,f3c639d1b7582474d128aea5960545274597a664,"Joint source coding, routing and power allocation in wireless sensor networks",,,"###This paper is also related to the work of Xiao, Cui, Luo, and Goldsmith [9], which studied an optimal power scheduling of decentralized estimation in a sensor network with energy constraints, and the work of Xiao and Luo [10], which addressed a multiterminal source-channel communication problem under orthogonal multiple access.###Further, the current work also generalizes that of [7] by applying the columngeneration method to the rate-distortion problem.###However, the current approach also differs from that of [7] in the sense that Algorithm 4 does not require the power control subproblem (19) to be solved optimally, which is computationally difficult in general.###Column generation was first used for the channel capacity problem by Johansson and Xiao [7].###The column-generation method presented here is inspired by the work of [7].###More recently, Xiao, Johansson and Boyd [3] used the dual decomposition approach to perform a joint optimization of routing and physical-layer resource allocation in a wireless network.",impact-revealing,acknowledging related work and its contributions
3841,5fd8acf991e0119b22c1f38d,5b9d8bcc46b766b47389c912a8e026f81b91b0d8,Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting,53e9af5ab7602d970399cdc3,Optimal multi-scale patterns in time series streams.,"Time-series forecasting is a critical ingredient across many domains, such as sensor network monitoring (Papadimitriou and Yu 2006), energy and smart grid management, economics and finance (Zhu and Shasha 2002), and disease propagation analysis (Matsubara et al.###The significant real-world applications include sensor network monitoring (Papadimitriou and Yu 2006), energy and smart grid management, disease propagation analysis (Matsubara et al.###Time-series forecasting is a critical ingredient across many domains, such as sensor network monitoring (Papadimitriou and Yu 2006), energy and smart grid management, economics and ﬁnance (Zhu and Shasha 2002), and disease propagation analysis (Matsubara et al. 2014).",other,highlighting the importance of time-series forecasting across various domains
3478,5c0f8548da562944ac906a71,5a3da29970d0c3c75ef4cb372b336fc8b10381d7,CNN-Based Real-Time Dense Face Reconstruction with Inverse-Rendered Photo-Realistic Face Images,53e99ca1b7602d9702553a50,Reconstructing detailed dynamic face geometry from monocular video,"[17] adapts a generic template to a static 3D scan of an actor’s face, then fits the",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1994,,15ad9cb03962b3c6d8d5f13f8f1b73e7699fed41,"THE DADIA–LEFKIMI–SOUFLI FOREST NATIONAL PARK, GREECE: BIODIVERSITY, MANAGEMENT AND CONSERVATION",,,"###The intrinsic value of biodiversity is widely recognized as is its ecological, social, economic, cultural and aesthetic value (Pimm et al. 1995, Mittermeier et al. 1999), but human-induced loss of biodiversity has currently reached alarming rates at the levels of genes, species and ecosystems…",impact-revealing,highlighting the significance of biodiversity and the alarming rates of its loss
1098,,3759a8b26c686634ac67b3e13c242f87d3617708,Object-based Modeling of Audio for Coding and Source Separation,,,"###This may be validated by the approximate W-disjoint orthogonality of speech [113] sources for linear time-frequency domain and separation algorithms utilizing this property [65].###Additionally, certain properties of mixtures of sounds can be described and formulated in frequency domain, for example, the approximate W-disjoint orthogonality of speech [113], which indicates that the time-frequency information of multiple simultaneous speakers does not overlap.",impact-revealing,providing context on properties of sound mixtures
2159,,0b25967dd4173d67c1a0acfdf4c480f557af2857,TEASER: early and accurate time series classification,,,"###Thus, we performed our experiments using three different time series classifiers: WEASEL [30], BOSS [28] and 1-NN Dynamic Time Warping (DTW).###The recent Word ExtrAction for time SEries cLassification (WEASEL) [30] also conceptually builds on the bag-of-patterns (BOP) approach and is one of the fastest and most accurate classifiers.###Based on these results we use WEASEL [30] as a slave classi-###Next, we fixed the slave classifier to WEASEL and compared the three different master classifiers (ocSVM, RBF-SVM, Regression).###As a hyper-parameter we learn the best word length between 4 and 6 for WEASEL on each dataset using 10-fold cross-validation on the train data. ocSVM parameters for the remaining experiments were determined as follows: nu-value was fixed to 0.05, i.e. 5% of the samples may be dismissed, kernel was fixed to RBF and the optimal gamma value was obtained by grid-search within {1 . . . 100} on the train dataset.###A nice aspect of WEASEL is that it is comparably fast, highly accurate, and works with variable length time series.###We first fixed the master classifier to oc-SVM and compared all three different slave classifiers (DTW+ocSVM, BOSS+ocSVM, WEASEL+ocSVM) in Figure 7.###Out of these, TEASER using WEASEL (WEASEL+ocSVM) has the best (lowest) rank.###We compared the three different slave TS classifiers: DTW, BOSS, WEASEL.###The most significant improvement over the state of the art was archived by TEASER+WEASEL+ocSVM, which justifies our design decision to model early classification as a one-class classification problem.###Based on these results we use WEASEL [30] as a slave classifier and ocSVM for all remaining experiments and refer to it as TEASER.###Furthermore, it computes the difference 4di between
Truncated Train Dataset
si…
Slave Classifier sci (WEASEL)
…
Master Classifier mci (one-class SVM)
Class probabilities, labels, and delta of positive samples
0 %
20 % 40 %
Class_1 Class_2 Class_3 Class_4
Figure 4: TEASER trains pairs of slave and master classifiers.",impact-revealing,describing the methodology and classifier comparison in time series classification
668,5ce937225ced2477cb328bd9,e4350bcfcadc5b4c0bfbc61ffd8bce05f0b8fe15,ABM-SpConv: A Novel Approach to FPGA-Based Acceleration of ConvolutionaI NeuraI Network Inference,5a9cb60d17c44a376ffb3a18,Running sparse and low-precision neural network: When algorithm meets hardware.,"The work of [2] presented an algorithm-hardware codesign scheme to improve the efficiency of sparse convolutional layers executed in hardware.###The third type of designs reduce the number of MAC operations by directly pruning the CNN model [1, 2, 8].",impact-revealing,acknowledge existing algorithm-hardware co-design schemes for efficiency improvement
3492,5cede0f2da562983788d0d44,aecddd82840323e5bd43f9c73a32fed88ee93c8c,An Effective Approach To Unsupervised Machine Translation,5b1643ba8fbcbf6e5a9bc4ec,Unsupervised Neural Machine Translation With Weight Sharing,"This method was further improved by Yang et al. (2018), who use two language-speciﬁc encoders sharing only a subset of their parameters, and incorporate a local and a global generative adversarial network.",other,reporting improvements in a specific method
2288,5e3940c73a55ace46ed436d2,845b4941d8c016aa5f8967da2f86d38ef6c18fa3,a survey on knowledge graphs representation acquisition and applications,5b67b4b417c44aac1c867387,"Relational inductive biases, deep learning, and graph networks","An investigation towards unification in a way similar to the unified framework of graph networks [172], however, will be worthy to bridge the research gap.",other,suggesting the need for a unified framework to address research gaps
1569,,43e15304b3fa9f9a92014dd1e8aa0d54f6cbd08e,Reconsidering “evidence” for fast-and-frugal heuristics,,,"###, & Gigerenzer, G. (2002). Models of ecological rationality: The recognition heuristic.###Gigerenzer, G. (2008). Why heuristics work.###Process models deserve process data: Comment on Brandstätter, Gigerenzer, and Hertwig (2006). Psychological Review, 115,###Unfortunately, Gigerenzer and Brighton (2009) and Marewski, Gaissmaier, and Gigerenzer (2010) did not consider them in their reviews or focused on those###, Gigerenzer, G., & Hertwig, R. (2006). Mak-###Recently, several theoretical contributions and reviews have been published that deal with the fast-and-frugal heuristics research program, advocated by Gigerenzer and colleagues (Gigerenzer, 2008; Gigerenzer & Brighton, 2009; Marewski, Gaissmaier, & Gigerenzer, 2010).###scriptive model of risky decision making: Comment on Brandstätter, Gigerenzer, and Hertwig (2006). Psychological Review, 115, 253-",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2747,5f7aeb7691e011983cc81e80,572c12e81319ccd47cc0c637c82efadd03fd05ab,Autoregressive entity retrieval,5550401245ce0a409eb3205c,Dropout: a simple way to prevent neural networks from overfitting,", 2011; 2014) and regularized with dropout (Srivastava et al., 2014) and label smoothing (Szegedy et al.###We train GENRE using a standard seq2seq objective, i.e., maximizing the output sequence likelihood with teacher forcing (Sutskever et al., 2011; 2014) and regularized with dropout (Srivastava et al., 2014) and label smoothing (Szegedy et al., 2016).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1075,,38cb2ae4140b1abc31f031853577e9e0f1b382d1,Improving Collocation Correction by Ranking Suggestions Using Linguistic Knowledge,,,"###…in terms of the number of sentences that contain these words) the PM Is cannot be compared in magnitude, a normalization has been suggested by Bouma (2009):
N PM ICB =
log P(a,b)P(a)P(b)
− log P(a, b) (2)
However, a mere use of PM I , N PM ICB or any similar measure does not consider two…###Since for lexemes with largely different individual probabilities (the probabilities can be measured in terms of the number of sentences that contain these words) the PM Is cannot be compared in magnitude, a normalization has been suggested by Bouma (2009):###To address this inconvenience, Bouma (2009) normalizes in Eq.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2015,,10284b6329c4f2f755cb2e92d1d6afb64d59ec10,Much of late life cognitive decline is not due to common neurodegenerative pathologies,,,"###Fourth, we did not account for hippocampal sclerosis, which is relatively common and impairs cognition, or 1 other recently described pathology that impairs cognition and may be more common than originally thought, TDP-43 (TAR DNA-binding protein-43).(31,32) We also did not measure other pathologic indices such as inflammatory markers, although we are not aware of any study that has demonstrated their relation to cognition after controlling for the common pathologic indices.",impact-revealing,acknowledging limitations in the study's scope and potential confounding factors
4025,53e9aeebb7602d970391ac0a,8681e808a9ebd7f7f155590e75fb63563a8aae6e,performance prediction based on inherent program similarity,53e9a066b7602d970294c01e,Automatically characterizing large scale program behavior,"A large body of work has also been done on the correlation between microarchitecture-independent program characteristics and processor performance, see for example [1, 9, 14].",other,acknowledge existing research on program characteristics and performance
3745,5ac1829d17c44a1fda9180a3,632d25ef7914ce962d258920460a9405b8c4553a,Sequence-Based Multi-Lingual Low Resource Speech Recognition,57d063e8ac44367354294e8f,An Empirical Exploration Of Ctc Acoustic Models,"Connectionist Temporal Classification (CTC, [19]) lends itself to low-resource multi-lingual experiments, because systems built on CTC tend to be significantly easier to train than those that have been trained using hidden Markov models [20, 21].",other,highlighting the advantages of CTC for low-resource multilingual experiments
2267,5cede10dda562983788ed645,690edf44e8739fd80bdfb76f40c9a4a222f3bba8,BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer,5a9cb60d17c44a376ffb3c6d,Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding.,"Other than recurrent neural networks, various deep learning models are also introduced for sequential recommendation [3, 22, 33, 49].###• Caser [49]: It employs CNN in both horizontal and vertical way to model high-order MCs for sequential recommendation.###, next item recommendation) task, which has been widely used in [12, 22, 49].###To ensure the quality of the dataset, following the common practice [12, 22, 40, 49], we keep users with at least five feedbacks.###For example, Tang and Wang [49] propose a Convolutional Sequence Model (Caser) to learn sequential patterns using both horizontal and vertical convolutional filters.###For dataset preprocessing, we follow the common practice in [22, 40, 49].###For easy and fair evaluation, we follow the common strategy in [12, 22, 49], pairing each ground truth item in the test set with 100 randomly sampled negative items that the user has not interacted with.",other,acknowledge existing deep learning models for sequential recommendation
1194,,58013bc27d957f7a999e48ae102cc92bae715373,Executive Guide to Preventing Information Technology Disasters,,,"###The themes are ""blackboard systems"", ""cooperative distributed problem solving"", ""expert systems"", ""natural language understanding"", ""qualitative physics"", ""knowledge-based simulation"" and ""computer vision"". Each has potential longterm implications for business. Kurzweil [1990] is perhaps best known as the developer of the world's first print-to-speech reading machine for the blind and the voice-activated word processor.###As John Monk of the Open University has pointed out [Monk 1993], the typical graduate engineer is not directly engaged in the manufacturing process: his products tend to be designs and reports rather than physical objects. However, the designs and reports only derive their meaning from their relationship with the world of physical objects, and their formulations in language and symbols that are comprehended and used by the managers who constitute their audience. This suggests that the engineer must develop expertise in a complex process of knowledge representation and mediation, maintaining dialogue with the client or commissioning manager regarding the transformation of physical objects within the terms of an agreed specification, and recording objectives and progress in a variety of media. This requires practice, rather than just theory. In modern economic and industrial conditions, it makes little sense to adhere to a traditional technocentric approach to engineering systems, where systems are regarded as complete and humans play a relatively subordinate role. Engineering efficiency and effectiveness will be enhanced if managers and workers can address technology in their own terms, recognising their role as active agents in systems that operate in a world of uncertainty and incomplete information. This was the fundamental motivation behind the work of Deming [1982] on quality.###As John Monk of the Open University has pointed out [Monk 1993], the typical graduate engineer is not directly engaged in the manufacturing process: his products tend to be designs and reports rather than physical objects. However, the designs and reports only derive their meaning from their relationship with the world of physical objects, and their formulations in language and symbols that are comprehended and used by the managers who constitute their audience. This suggests that the engineer must develop expertise in a complex process of knowledge representation and mediation, maintaining dialogue with the client or commissioning manager regarding the transformation of physical objects within the terms of an agreed specification, and recording objectives and progress in a variety of media. This requires practice, rather than just theory. In modern economic and industrial conditions, it makes little sense to adhere to a traditional technocentric approach to engineering systems, where systems are regarded as complete and humans play a relatively subordinate role. Engineering efficiency and effectiveness will be enhanced if managers and workers can address technology in their own terms, recognising their role as active agents in systems that operate in a world of uncertainty and incomplete information. This was the fundamental motivation behind the work of Deming [1982] on quality. As Brodner [1990, 1995] has demonstrated in his work on anthropocentric systems in production engineering, the adoption of a new engineering culture has had a transforming effect on quality, the time taken to develop and bring to market new models, and levels of work satisfaction. Empowering workers at cell level on the factory floor, as Kaura and Ennals [1993] have shown, enhances manufacturing efficiency and effectiveness while changing working relationships and management structures across the company.###Partridge [1986] has drawn attention to the different assumptions and technical foundations underlying artificial intelligence as opposed to conventional software, and argues that ""AI as practical software is not just about to happen"".###The development of high-power, but low-cost, computing systems has further blurred understanding of skills. Increasingly, manufacturing employees are required to mind machines, enabling unskilled staff to produce high-technology products, but leaving them impotent if the machines malfunction. Production continuity is greatly aided if workers understand the reasons for system breakdowns, and are empowered to take action to resolve problems. This implies a breadth and depth of technical knowledge that has become rarer, as skilled workers have been replaced by machines. Monk [1993] is right to note that, although computers have changed the work of the engineer, they do not make the engineer redundant:###Unfortunately, executive decisions are often made at Board level by managers who lack the necessary knowledge and understanding. Their use of the computer flows from their perception of the management task: the computer is merely the tool, a flawed management system is to blame. As Corbett [1989] has pointed out, managers need to ensure that the systems they manage are providing learning experience for those on whose judgement the organisation will depend:###and highly placed. EIS technology promises to place fmancial information at the fmgertips of those with a control and monitoring function, safely insulated from the working environment from which the information has been derived. Matthews [in Holtham 1992] introduces executive information systems in the context of corporate policy, and considers insights from EIS into the way the organisation adds value. He suggests that the question to ask in designing an EIS is: ""What information would potential predators most like to have about your organisation?"" EIS have been used in the introduction of ""Total Quality"" in London Underground, adding power to customer-supplier chains in order to effect organisational change. British Airways introduced the Airline Information Management System (AIMS), with support from the highest corporate level. The HOLOS system in British Telecom emphasises the needs of top managers. More case studies are provided in Rolph and Bartram [1992]. Companies such as Sun, Boots and British Airways justify their investments in EIS in terms of improved understanding of the business, and offer practical approaches to implementation, as well as examples of Chief Executives who are regular users of EIS.###Blanning [1987] saw managers as significant players in the information game:###Simplicity is a commendable principle, but it can be dangerously illUSOry. Pandering to current prejudices by allowing an extension to current practices may be at the expense oflonger-term objectives and developments that require a change of direction. Abelson [1973] offered an observation from his experience###It would be a brave person who could claim that their rules were unambiguous, complete, and applied to processes that are fully understood. Alternatively, we are considering only small application areas. Ambitions have become more properly modest. Training, apprenticeships and procedure manuals can now be joined by expert systems development in meeting this need. From a more academic, but transatlantic, standpoint, Partridge and Wilks [1990] bring together valuable sources.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
817,5ecb57199e795ec6f2ba59cc,512f34906ddaefe885af2e5eec9b2b3b50ffd377,deep entity matching with pre-trained language models,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"where [ SEP ] is the special token separating the two sequences and [ CLS ] is the special token necessary for BERT to encode the sequence Other serialization schemes There are diﬀerent ways to serialize “ [ COL ] ” and/or “ [ VAL ] ”, or exclude attribute names attr i during Heterogeneous schemas As shown, the serialization method of###Pre-trained LMs such as BERT [11], ALBERT [22], and GPT2 [32] have demonstrated good performance on awide range of NLP tasks.###In Ditto, we fine-tune the popular base 12-layer BERT model [11] and its distilled variant DistilBERT [36], which is smaller but more efficient.",impact-revealing,describing serialization methods and model performance
1497,,e153430f5b8853c10eac1da30af935ff929452b7,Denoising Diffusion Algorithm for Single Image Inplaine Super-Resolution in CBCT Scans of the Mandible,,,"###Much of the early work on SR is based on regression and is trained with an MSE loss [7, 34].###Recently, generative models have been increasingly used in various applications for real words [7, 38, 39].###SR3 [7] uses the LR image as an additional input to the denoising network and builds a conditional denoising network.###As suggested in [7], we will apply different sets of Gaussian filters to the LR images during training to improve the quality of the generation.###Our algorithm is inspired by recent work on denoising diffusion models with probabilistic modelling [7, 18].###This has led to the development of numerous algorithms for processing images, e.g. nature images [7], satellite images [8] or medical images [9].###For training UNet architecture to SR [7] approach will be used.",impact-revealing,reporting on the evolution and application of super-resolution methods
993,,c256d995ddbece4d7553256fd6ea5dae81c5ff1a,Justification on Knowledge Management Strategies: A New Perspective on Knowledge Creating Process,,,###We adopted the constructs that have already been used and validated by Nonaka et al. (1994) for assessing the level of knowledge creation processes.,impact-revealing,reporting prior findings on knowledge creation processes
538,573696cd6e3b12023e5ce382,e2820bffe5b42cb7d88b7f65c12171c62ab4aae2,Gradient-based Hyperparameter Optimization through Reversible Learning,53e9bccbb7602d97049523cb,Generic Methods for Optimization-Based Modeling,"Applying RMD to hyperparameter optimization was proposed by Bengio (2000) and Baydin & Pearlmutter (2014), and applied to small problems by Domke (2012).###The most closely-related work is Domke (2012), who derived algorithms to compute reverse-mode derivatives of gradient descent with momentum and L-BFGS, using them to update the hyperparameters of CRF image models.",impact-revealing,reporting prior findings on hyperparameter optimization
2127,,429b5783cd555a33c1818f44f0f88aaf9a3607ed,"Equilibrium in Capacitated Network Models with Queueing Delays, Queue-storage, Blocking Back and Control☆",,,"###As is shown by Daganzo (1998), we cannot hope to obtain the kind of existence results previously stated.###Most models incorporating blocking-back have focussed on the impact on the dynamic network loading rather than investigating the effects on route choice (Daganzo (1995a, b), Yperman (2006), Bliemer (2007)).###Most models incorporating blocking-back have focussed on the impact on the dynamic network loading rather than investigating the effects on route choice (Daganzo (1995a, b), Yperman (2006), Bliemer (2007)). Notable exceptions are in Daganzo (1998), Gentile et al (2007) and Bliemer et.###This model follows Thompson and Payne (1975), extends Smith (1987, 2012) and is motivated by many papers including especially Daganzo (1998).###Various merge models are suitable for different merge layouts, different junction geometries and different “controls”; for example: the Daganzo “fixed ratio” merge model (Daganzo, 1995), “Fair shares” merge models introduced by Jin and Zhang (2003) and Nie and Leonard (2005), and so on.###This situation is the similar to that described by Daganzo (1998).###Following the conventional representation of the network into links and nodes (with the notable exception of the cell-based representation of Daganzo, 1995a, b), dynamic link and node models have been developed and used in traffic assignment processes.###Most models incorporating blocking-back have focussed on the impact on the dynamic network loading rather than investigating the effects on route choice (Daganzo (1995a, b), Yperman (2006), Bliemer (2007)). Notable exceptions are in Daganzo (1998), Gentile et al (2007) and Bliemer et. al. (2012) who have considered various issues which arise when blocking back occurs with route choice and have shown how important blocking back is.",impact-revealing,highlighting the focus of previous models on dynamic network loading rather than route choice
1203,,ff44b6263d144c5ba834b9e3ea1f943849d39849,Metrically Regular Generalized Equations: A Case Study in Electronic Circuits,,,"###The first theorem is due to Robinson [60] and is based on the assumptions we were working with till now (see also Subsection 4.###The first theorem is due to Robinson [60] and is based on the assumptions we were working with till now (see also Subsection 4.1.1).###Though we are inspired by Robinson’s idea of strongly regular points in defining the auxiliary map (4.6), and the techniques in Theorems 4.1.1 and 4.1.2, we find it more convenient to do some modifications in the setting in order to adapt it to our problem.###This term was first introduced at 1980 by Robinson [60], and then became popular in the literature.###Thus, we can see that (1) holds if and only if x satisfies the so called variational inequality :
x ∈ Ω, and 〈 f(x) , c− x 〉 ≥ 0 for each c ∈ Ω, (3)
and this, geometrically, means that f(x) is an inward normal to Ω at x. Robinson in [59–62] has studied this particular type of generalized equations in details and found the setting of generalized equations as an appropriate way to express and analyse problems in complementarity, mathematical programming, and variational inequalities.###The strongly regular point criteria of Robinson [60] comes into play now, which guarantees a good behaviour of the problem solutions (cf.###Although the term “regularity” has been already used in the literature to describe a similar property (see, for example [42, 60]), it was firstly Borwein [15] who called this property metric regularity.###Then for each p ∈ Q the set {x ∈ Ba(x̄) |x ∈ M(ϕ(p, x))} consists of exactly one point, and the associated function
s : p 7→ {x |x = M(ϕ(p, x)) ∩ Ba(x̄)} for p ∈ Q (1.41)
satisfies
‖ s(p′)− s(p) ‖ ≤ λ 1− λν ‖ϕ(p′, s(p))− ϕ(p, s(p)) ‖ for all p′, p ∈ Q. (1.42)
Theorem 1.4.11 (Robinson Theorem Extended Beyond Differentiability).###The strongly regular point criteria of Robinson [60] comes into play now, which guarantees a good behaviour of the problem solutions (cf. Theorem 4.1.1 and the explanations therein).",impact-revealing,acknowledging foundational work and its influence on current research
2753,5c96086e3cb210d2716c4a08,bb76749fab841ccbe20ace74111a518f65d9870b,make the most out of last level cache in intel processors,5736956b6e3b12023e49037d,Mapping the Intel Last-Level Cache.,"…of our study of the non-uniform cache architecture (NUCA) [35] characteristics of LLC in Intel processors where the LLC is divided into multiple slices interconnected via a bi-directional ring bus [84], thus accessing some slices is more expensive in terms of CPU cycles than access to other slices.###There have been many attempts to find the slice mapping and reverse-engineer Intel’s Complex Addressing [1, 27, 39, 42, 61, 84].###Others have tried to reverse engineer the Intel LLC Complex Addressing hash function [1, 27, 39, 42, 61, 84].",other,highlighting the complexity and challenges in understanding Intel's cache architecture
981,,d65df6778f8492770b33ff17eb34135e7be789dd,Infants’ behaviors as antecedents and consequents of mothers’ responsive and directive utterances,,,"###…Pine (1992), as descriptions of actions, objects, and their attributes, behavioral directives, attentional directives, and other; and (b) following Akhtar et al. (1991), whether or not the utterance referred to an object or action on which the child’s attention was focused at utterance onset.###Second, Akhtar et al. (1991) emphasized the importance of distinguishing maternal directives that follow children’s focus of attention from those that lead or redirect it, reporting that following directives were positively predictive and leading directives were negatively
predictive of children’s…###Studies of mothers’ directive speech, however, have produced contradictory associations to children’s vocabulary development (Akhtar et al., 1991; Carpenter et al., 1998; Hampson & Nelson, 1993; Hoff & Naigles, 2002; Masur et al., 2005; Pine, 1992; Tomasello & Farrar, 1986).###In different studies, directive utterances have been reported to be often negatively related, but sometimes positively related or unrelated, to children’s language outcomes (Akhtar et al., 1991; Carpenter et al., 1998; Hampson & Nelson, 1993; Hoff & Naigles, 2002; Masur et al., 2005; Paavola et al., 2005; Pine, 1992; Tomasello & Farrar, 1986).###…utterances have been reported to be often negatively related, but sometimes positively related or unrelated, to children’s language outcomes (Akhtar et al., 1991; Carpenter et al., 1998; Hampson & Nelson, 1993; Hoff & Naigles, 2002; Masur et al., 2005; Paavola et al., 2005; Pine, 1992;…",impact-revealing,highlighting contradictory findings in directive speech studies
1829,,f95c2f5801d05412527a342eff2604c56f3833be,The STAR*D study: Treating depression in the real world INTERPRETING KEY TRIALS,,,"###36 The two augmentation options tested, lithium and T3 thyroid hormone (Cytomel), are commonly considered by psychiatrists but less commonly used by primary care doctors.",impact-revealing,acknowledge common augmentation options in psychiatric treatment
3230,573698426e3b12023e70bf59,1917bfe805b46fe3a45903e803b27bd41719cf3c,Symbiotic job scheduling on the IBM POWER8,55323c6d45cec66b6f9dc0cb,SMiTe: Precise QoS Prediction on Real-System SMT Processors to Improve Utilization in Warehouse Scale Computers,"Zhang et al. [23] propose a methodology to predict the interference among threads on an SMT core.###With respect to Linux, it achieves an average speedup by 8.8% for workloads comprising 12 applications, and by 4.7% on average across all evaluated workloads.",other,reporting prior findings on thread interference prediction methodology
2298,5f1022a091e01168a7d6fc4f,04e234c7e4f300b4e8a16370728d875f9f484b39,learning visual context by comparison,5c8d7c6f4895d9cbc662c17c,Comparison of Deep Learning Approaches for Multi-Label Chest X-Ray Classification,"Experimental Setting Following the previous study [2] on multi-label classification with chest X-Rays, we mainly adopt ResNet-50 as our backbone network.",other,acknowledge existing methodology in multi-label classification
1736,,1cb9e69939c891c984ce82d09126de37bcab64ac,Does mindfulness mediate the association between attachment dimensions and Borderline Personality Disorder features? A study of Italian non-clinical adolescents,,,"###Based on these considerations, the aims of this study were to use the concept of mindfulness to operationalize and measure some of the components of the mentalization construct (Bateman & Fonagy, 2004; Fonagy, 1991; Wallin, 2007), and hence to test the hypothesis that mindfulness substantially mediates the relationships between insecure attachment patterns and BPD features in a sample of Italian nonclinical adolescents.###Bearing these issues in mind, our mediation model is at least partially consistent with Fonagy and colleagues’ (Bateman & Fonagy, 2004; Fonagy, 1991) hypothesis that mentalization deficits mediate the relationship between insecure attachment and BPD, at least when an essential precursor out of which genuine mentalization arises, i.###…to BPD.
Bearing these issues in mind, our mediation model is at least partially consistent with Fonagy and colleagues’ (Bateman & Fonagy, 2004; Fonagy, 1991) hypothesis that mentalization deficits mediate the relationship between insecure attachment and BPD, at least when an essential…###In the last 15 years, Fonagy and colleagues (Bateman & Fonagy, 2004; Fonagy, 1991) have formulated Borderline Personality Disorder as a personality disorder organized around an unstable capacity for mentalization.###MBT has become the second psychotherapeutic treatment for BPD to be empirically validated by randomized, controlled trials as more effective than non specialized psychiatric treatment (Bateman & Fonagy, 1991, 2001).###…were to use the concept of mindfulness to operationalize and measure some of the components of the mentalization construct (Bateman & Fonagy, 2004; Fonagy, 1991; Wallin, 2007), and hence to test the hypothesis that mindfulness substantially mediates the relationships between insecure attachment…",impact-revealing,highlighting the significance of mindfulness in understanding BPD and its relationship with mentalization
1361,,9ce382175bf0ccef2bbb10e9b48bcf3ea64280c5,Security and Usability of Authentication by Challenge Questions in Online Examination,,,"###Similarly, a number of market-leading licence and certificate providers including Microsoft (Adelman, 2000), IBM (Reinschmidt and Francoise, 2000), Apple and Cisco (Lammle, 2011) conduct several courses online and use the Prometric (1990) service for face-to-face invigilated examinations before the final award.###The use of a relaxed algorithm (Schechter et al., 2009) compensated for these issues and increased the correct per cent to 76%.###This was compiled using a substring and distance algorithms (Schechter et al., 2009).###Previous research suggests that challenge questions can be vulnerable to guessing attacks by adversaries, acquaintances, friends and colleagues (Schechter et al., 2009, Just and Aspinall, 2009b).###Creating Dynamic Profile Questions: Dynamic profile questions were created
manually during the course for each individual student and stored in a Microsoft Word file in a secure location.###Text-based challenge questions are associated with an individual’s personal information which can be vulnerable to blind, focused and informed guessing attacks by adversaries, acquaintances, friends and colleagues (Schechter et al., 2009, Just and Aspinall, 2009b).###Capitalisation and spaces were treated programmatically and the equality algorithm (string-to-string comparison) was implemented for authentication purposes (Ullah et al., 2014a, Schechter et al., 2009).###Some websites provide a list of pre-defined fixed questions for users to select during registration (Schechter et al., 2009).###Answer recall or memorability has been an ongoing issue with challenge questions (Schechter et al., 2009).###However, this method became popular when used by leading email providers such as Yahoo, Google, Microsoft and AOL (Schechter et al., 2009).###Schechter et al. (2009) evaluated the security of challenge questions used by four mail service providers – Google, Yahoo, AOL and Microsoft.###Using the Traffic Light Access Control System: Authentication was per-
formed using the equality algorithm, i.e. a string-to-string comparison of answers (Schechter et al., 2009).###These questions are widely used and researched by AOL, Yahoo, Google and Microsoft (Schechter et al., 2009).###Email Service Providers: Challenge questions became a popular fall back
authentication method when used by leading email providers such as Yahoo, Google, Microsoft and AOL (Schechter et al., 2009).###Personal Questions: Questions regarding personal information are more memorable and therefore widely used for credential recovery (Schechter et al., 2009).###Several studies reported usability as a major issue for challenge questions (Just and Aspinall, 2012, Schechter et al., 2009).###The results of the relaxed algorithm were derived from the data collected in the online examination, disregarding capitalisation, white-spaces and minor spelling errors using a combination of substring and distance algorithms as described in an earlier study (Schechter et al., 2009).###Favourite Questions: Questions in this theme have been widely used for credential recovery (Schechter et al., 2009).###Questions in the personal and favourite themes were copied from security questions used by Google, Microsoft, AOL and Yahoo (Schechter et al., 2009).###These are traditional personal security questions, which are utilised by many email service providers, websites and online banks (Just and Aspinall, 2012, Schechter et al., 2009).###In an earlier study conducted by Schechter (2009) and sponsored by the Microsoft corporation, participants answered 76% of their challenge questions in a laboratory-based environment with 24% incorrect answers.###Schechter et al. (2009) reference Sarah Palin (the Republican vice-presidential candidate in the 2008 US election), whose Yahoo email account was compromised, as the answer to her secret question had been figured out (Bridis, 2008).###Schechter et al. (2009) indicate that personal information can be found on many social media websites.",impact-revealing,reporting on the use and evaluation of challenge questions in online security
2636,5fc61cdb91e0118947381abc,c9d736dd9f967844d2391bb13c4cb477576ab373,On Disambiguating Authors: Collaboration Network Reconstruction in a Bottom-up Manner,5b1642388fbcbf6e5a9b55a2,Mention and Entity Description Co-Attention for Entity Disambiguation.,"Our work is related to record linkage [1]–[3], entity resolution [4]–[7], object identification [8], duplicate detection [9]– [11] and entity matching [12]–[15], etc.###Author disambiguation is related to several similar tasks like record linkage [1]–[3], entity resolution [4]–[7], object identification [8], duplicate detection [9]–[11] and entity",other,acknowledge related tasks in author disambiguation
2632,5ee9f15b91e01152af022eaf,a83902f8b3aadfda633968a840ca1738bedef837,modeling graph structure via relative position for text generation from knowledge graphs,5e5e18b993d709897ce2ab34,Adaptive Structural Fingerprints for Graph Attention Networks,"abstract meaning representation incorporating a connectivity score into their graph attention network, Zhang et al. (2020) manage to increase the attention span to k-hop neighborhoods but, finally, only experiment with k = 2.###…orthogonal to our approach have been proposed in recent work: By 2 abstract meaning representation incorporating a connectivity score into their graph attention network, Zhang et al. (2020) manage to increase the attention span to k-hop neighborhoods but, ﬁnally, only experiment with k = 2 .",other,reporting on a specific method and its limitations
2249,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e9b0c7b7602d9703b3de7e,A novel ontology for computer go knowledge management,"In addition, the use of transpositions allows information to be shared between these extra nodes where the state is unchanged.",other,providing context for the use of transpositions in information sharing
3149,5f058d15dfae54570ec57ea1,fb93ca1e004cbdcb93c8ffc57357189fa4eb6770,Resnest: Split-attention networks,5c2c7a9217c44a4e7cf3188a,Improving Semantic Segmentation via Video Propagation and Label   Relaxation,"r vision research. Networks trained for image classication often serve as the backbone of the neural networks designed for other applications, such as object detection [22,46], semantic segmentation [6,43,73] and pose estimation [14,58]. Recent work has signi- cantly boosted image classication accuracy through large scale neural architecture search (NAS) [45,55]. Despite their state-of-the-art performan###te = 0:1). For evaluation, the network prediction logits are upsampled 8 times to calculate the per-pixel cross entropy loss against the ground truth labels. We use multi-scale evaluation with ipping [65,69,73]. We rst consider the Cityscapes [10] dataset, which consists of 5K highquality labeled images. We train each model on 2,975 images from the training set and report its mIoU on 500 validation images. ",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3338,5cede0e1da562983788bfe5f,988a378f640eb7fb681f977d6cb1e0c830c07b4c,Adversarial Examples Are a Natural Consequence of Test Error in Noise,5b67b4b117c44aac1c866ad9,Motivating the Rules of the Game for Adversarial Example Research.,"This suggests that defending against worst-case content-preserving perturbations (Gilmer et al., 2018a) requires removing all errors at a scale comparable to the distance between unrelated pairs of images.###There is an existing research program (Gilmer et al., 2018b; Mahloujifar et al., 2018; Dohmatob, 2018) which proves hard upper bounds on adversarial robustness in terms of the error rate of a model.###Several recent papers (Gilmer et al., 2018b; Mahloujifar et al., 2018; Dohmatob, 2018; Fawzi et al., 2018a) use con-centation of measure to prove rigorous upper bounds on adversarial robustness for certain distributions in terms of test error, suggesting non-zero test error may imply the existence…",other,highlighting the challenges in defending against adversarial perturbations
2383,573697636e3b12023e64a731,793dd71c59e0f2222d60fc5a43871cf919eeb1a2,runtime-driven shared last-level cache management for task-parallel programs,53e99998b7602d97021dfd15,Multifacet's general execution-driven multiprocessor simulator (GEMS) toolset.,We evaluate the performance of our hardware-software LLC management technique using the GEMS executiondriven full-system multiprocessor simulator [25].,other,reporting the evaluation method used
2108,,7ecd89815195563b527978a6d143b279d12b2f7f,On the Impact of Deployment Errors in Location-Based Key Predistribution Protocols for Wireless Sensor Networks,,,"###Furthermore, it offers strong resilience to node capture attacks since the compromise of keys for some nodes does not affect the security of other grids, enhancing overall network robustness [19].###Due to these advantages, various location-based pair-wise key predistribution protocols were proposed, such as SKRKP-D, GGD, and FRP [1], [19], [20].###However, previous studies on location-based pairwise key predistribution did not consider this problem and assumed idealized scenarios with Gaussian or uniform distribution on 2D flat terrains [1], [19], [20].###Examples of computation-based schemes include the structured key-pool random key predistribution using deployment knowledge (SKRKP-D) [19] and grid-group deployment scheme (GGD) [20].###Additionally, even if the keys of some nodes are compromised due to a node capture attack, the impact on other grids is limited, ensuring a high level of overall network security [19].###SKRKP-D [19] is a location-based pairwise key pre-distribution scheme that combines the SKRKP protocol, a computation-based scheme utilizing the Blom scheme, with location-based knowledge.###This optimization allows for efficient key sharing between nodes more likely to communicate, minimizing unnecessary key storage and enhancing the connectivity rate among nodes [19].",impact-revealing,highlighting the advantages of location-based key predistribution protocols in enhancing network security
1045,,52199201b8037e6cb5cdb0a129d2af945a2460fa,Sparse Bottleneck Networks for Exploratory Analysis and Visualization of Neural Patch-seq Data,,,###This approach was inspired by the usual practice for image-based regression tasks to start with a convolutional neural network with weights trained for classiﬁcation on ImageNet and to ﬁne-tune them on the task at hand [26].,impact-revealing,drawing inspiration from established practices in image-based regression tasks
2543,5f7d9bfd91e011346ad27f0c,2051548f7681c96d603de932ee23406c525276f9,a transformer-based framework for multivariate time series representation learning,5c8b7da14895d9cbc692aa45,TimeNet: Pre-trained deep recurrent neural network for time series classification,"As an example of the lat-2 ter, Malhotra et al. (2017) presented a multi-layered RNN sequence-to-sequence autoencoder, while Lyu et al. (2018) developed a multi-layered LSTM with an attention mechanism and evaluated both an input reconstruction (autoencoding) as well as a forecasting loss for…###focusing on clustering and the visualization of shifting sample topology with time [13, 17]) or RNN (most commonly, LSTM) sequence-to-sequence networks [24, 26].",other,acknowledge existing methods in sequence-to-sequence networks
3900,5e2ac357df1a9c0c41e7fa48,73bc990757d54f2161eceacd330e0aeb189393bf,Building attention and edge message passing neural networks for bioactivity and physical–chemical property prediction,53e99fe9b7602d97028cb61a,Surrogate data _x0096_ a secure way to share corporate data,"This type of method also presents an approach for the secure sharing of chemical data, i.e. it is possible to disseminate trained models for activity predictions without the risk of reverse-engineering IP-sensitive structural information [31–33].",other,highlighting a secure method for sharing chemical data
3379,5da1a6d447c8f7664606888d,91a4a5a1184a12821e7f5ddf5372b259ded96feb,Directed Statistical Warming through Time Traveling,53e9a539b7602d9702e5e0b3,Reuse-based online models for caches.,Sen and Wood [27] suggest that stack distance — which can be estimated using the reuse distance distribution — can be used to model other replacement algorithms as well.,other,reporting prior findings on stack distance and its applications
2434,5f0d85c69fced0a24be4f019,6817b5d48cf8f665b680de07d23b91814d2923b1,Bouquet of Instruction Pointers: Instruction Pointer Classifier-based Spatial Hardware Prefetching,5aed147c17c44a4438153ea5,Domino Temporal Data Prefetcher,"It is well known that spatial prefetchers fail to improve performance for server workloads like CloudSuite [14], [33], [53], [58], [59] and additional prefetchers [12], [24], [52], [58], [59] can be used on top of IPCP to improve the performance.###Temporal Prefetchers: Temporal prefetchers like temporal streaming [55], Irregular Stream Buffer (ISB) [24], and Domino [12] track the temporal order of accesses.###Apart from these spatial prefetchers, there are temporal prefetchers [54], [55], [24], [12], [59], [58] that target irregular but temporal accesses.",other,acknowledge limitations of spatial prefetchers and suggest alternatives
931,5f0d85c69fced0a24be4f028,5d9073cfec34aea00247ec625fa94f6279ff580d,tailored page sizes,556fb0872401b4b38c23789b,Increasing TLB reach by exploiting clustering in page translations,"Sub-blocked TLBs [56], CoLT [46], and Clustered TLBs [45] combine near virtual-to-physical page translations into single TLB entries.",impact-revealing,reporting prior findings on TLBs
2103,,1cbf7e474974706537402868ed17e3e2c2a40648,An Optimized LEACH Algorithm in Wireless Sensor Network,,,###It also does not consider the residual energy of nodes[10-12].,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1825,,7b6bfde671f0089ca59c396607540319e7f6b6f7,Aberrant differentiation and proliferation of hepatocytes in chronic liver injury and liver tumors,,,"###…and plated on Matrigel, a basement membrane ‐ like matrix that has been shown to be suitable for the maintenance of hepatocyte differentiation, 34,36 and this recovery was strongly reinforced by the presence of dexamethasone (Dex) plus interleukin ‐ 6 (IL ‐ 6) or oncostatin M (OSM) (Figure 2d,e).",impact-revealing,providing context on the use of Matrigel for hepatocyte differentiation
143,5d3ed25a275ded87f97deae9,2c6097792ed9e4e8a664ce2dc7492377bfd57139,LightNet: A Dual Spatiotemporal Encoder Network Model for Lightning Prediction,5736986b6e3b12023e72fa3b,Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting,"In recent years, with the success of deep neural networks (DNNs), some researches have applied DNNs to precipitation [21] and radar echo [32] nowcast.###To solve the problem of spatiotemporal dependency, Shi et al. [21] developed the conventional LSTM and propose convolutional LSTM (ConvLSTM).###Convolutional long short-term memory (ConvLSTM) network is an extension of the popular fully connected long short-term memory (FC-LSTM) network, with a goal to overcome the drawbacks of FC-LSTM in handling spatial-temporal data such as videos [21].",impact-revealing,highlighting advancements in deep neural networks for precipitation and radar echo nowcasting
1204,,9a126ee652e31d38bc5d3397fc706a5056eb2228,On Some Regularity Properties in Variational Analysis,,,"###A successful approach to the issue, subsequently developed, revisited and extended by several authors, was originally proposed in [ 19 ].",impact-revealing,acknowledging the development and extension of a successful approach
4040,5ed7796e91e011e6e91120f0,8ade0f3bbe8d8251c5de9ef39886a13055f463e5,TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding,5550446545ce0a409eb4d4d7,Political Ideology Detection Using Recursive Neural Networks,"The work conducted on ordinary citizens could also be categorized into two types according to the source of data being used: intentionally collected via strategies like survey [1, 20], and directly collected such as from news articles [2] or from social networks [13, 15, 17].",other,acknowledge different data collection methods in research
823,555048f045ce0a409eb72c02,493ee0ca34fac1fd94f82a3dfa60f5a4cbc6d8be,Distributed Power-law Graph Computing: Theoretical and Empirical Analysis,53e9a9bdb7602d970331a50b,GraphBuilder: scalable graph ETL framework,"Although vertex-cut methods can achieve better performance than edge-cut methods for power-law graphs [6], existing vertex-cut methods, such as random method in PowerGraph and grid-based method in GraphBuilder [8], cannot make effective use of the powerlaw distribution to achieve satisfactory performance.###2 Baselines and Evaluation Metric In our experiment, we adopt the Random of PowerGraph [6] and the Grid of GraphBuilder [8]1 as baselines for empirical comparison.###GraphBuilder [8] provides some heuristics, such as the grid-based constrained solution, to improve the random vertex-cut method.###For comparison, the random vertex-cut method (called Random) of PowerGraph [6] and the grid-based constrained solution (called Grid) of GraphBuilder [8] are adopted as baselines.",impact-revealing,highlighting limitations in existing vertex-cut methods for power-law graphs
482,5db929e947c8f766461fd005,11ead744b2edbdabe174258a8dfe4023be8e501d,Hierarchical Attention Prototypical Networks for Few-Shot Text Classification,5843777eac44360f108417ec,Hierarchical Attention Networks for Document Classification.,"Therefore, we apply an attention mechanism (Yang et al., 2016) to get those important words and assemble them to compose a more informative instance vector s , and the definitions are as follows###The dominant text classification models in deep learning (Kim, 2014; Zhang et al., 2015a; Yang et al., 2016; Wang et al., 2018) require a considerable amount of labeled data to learn a large number of parameters.###Therefore, we apply an attention mechanism (Yang et al., 2016) to get those important words and assemble them to compose a more informative instance vector s j , and the deﬁnitions are as follows where h is the t hidden word embedding of instance x j , it was encoded through the instance encoder,…###The dominant text classiﬁcation models in deep learning (Kim, 2014; Zhang et al., 2015a; Yang et al., 2016; Wang et al., 2018) require a considerable amount of labeled data to learn a large number of parameters.",impact-revealing,highlighting the need for labeled data in deep learning text classification models
2016,,706fc4b82075b795f79915bd9ce6ccd7f7028960,Metabolism disorder promotes isoproterenol-induced myocardial injury in mice with high temperature and high humidity and high-fat diet,,,"###Metabolomics received increasing attention in CVDs research for its help to better explain the biological mechanisms and identify novel biomarkers of CVDs [6].###Abbreviations BCAA : Branched-chain amino acids; BCKA: Branched-chain α-ketoacids; CAD: Coronary artery disease; Cer: Ceramide; CVDs: Cardiovascular diseases; eNOS: Endothelial NOS; iNOS: Inducible NOS; ISO: Isoproterenol; IVS: Inter ventricular septal thickness; LVPW: Left ventricular posterior wall thickness; LysoPC: Lysophosphatidylcholine; MI: Myocardial infarction; NO: Nitric oxide; NOS: Nitric oxide synthase; PC: Phosphatidylcholine; PCA: Principle component analysis; PLA2: Phospholipase A2; PLS-DA: Partial least squares discriminant analysis; ROS: Reactive oxygen species; SM: Sphingomyelin; TAC : Transverse aortic constriction; SphK: Sphingosine kinases; TCA : Tricarboxylic acid; TCM: Traditional Chinese Medicine; TMAO: Trimethylamine-N-oxide; VIP: Variable influence on projection.###In addition, ApoE−/− mice were used in the present study due to their advantages in simulating hyperlipidemia and atherosclerosis, which are two of the most common characteristics of “phlegm-damp” syndrome in patients with CVDs.###Phlegmdamp syndrome is characterized by hyperlipidemia combined with high temperature and high humidity in TCM. Previous researches have revealed the important role of phlegm-damp  syndrome in the development of CVDs.###It is predicted that CVDs may increase by approximately 21.3 million events and 7.7 million deaths over 2010–2030
*Correspondence: weihui.lu@gzucm.edu.cn 5 Department of Cardiology, Guangdong Provincial Hospital of Chinese Medicine, No. 111, Dade Road, Yuexiu District, Guangzhou 510020, People’s Republic of China Full list of author information is available at the end of the article
in China.###Stimulation with ISO leads to the development of oxidative stress, calcium overload, myocardial inflammation and renin–angiotensin release, which ultimately cause CVDs [1].###Isoproterenol (ISO), a synthetic on selective β-adrenergic agonist, is widely used for inducing experimental CVDs such as myocardial ischemia, hypertrophy and infarction, cardiac fibrosis, and heart failure.###Cardiovascular diseases (CVDs), the leading cause of death in humans, have emerged as a high socio-economic burden around the world with rising incidence.",impact-revealing,highlighting the significance of metabolomics in cardiovascular disease research
355,5d3ed25a275ded87f97deaab,025ea689e6ab3b544101df17233e87536a1e578a,Metapath-guided Heterogeneous Graph Neural Network for Intent Recommendation,573695fe6e3b12023e5125f8,A Survey of Heterogeneous Information Network Analysis,"In this paper, we propose to model the intent recommendation system with a HIN, through which we can flexibly exploit its rich interaction information.###In order to solve the challenges in intent recommendation, we model objects and interactions in intent recommendation system with a HIN and propose a novel metapath-guided GNN method for intent recommendation, called MEIRec.###It demonstrates that we should consider heterogeneity of objects in HIN for better performances.###As a general information modeling method, Heterogeneous Information Network (HIN) [18], consisting of multiple types of objects and links, has been widely applied to many data mining tasks [10, 17, 18].###As shown in Figure 2(a), obviously, HIN clearly demonstrates objects in intent recommendation (e.g., users, items and queries) and their interaction relations, such as “user click item”, “user search query” and “query guide item”.###More-over, the recommended query may be not previous queries Figure 2(a) shows a toy example of HIN and Figure 2(b) is the corresponding network schema.###Although, some HIN based recommendation methods have been proposed [8, 19, 23], they mainly employ metapath based features through exploiting the interaction relations between users and items, which makes them hardly interactions in intent recommendation. queries with heterogeneous Graph Neural Network (GNN).###Through modeling modeling intent recommendation system as a HIN , MEIRec utilizes metapath-guided neighbours to exploit rich interaction information in HIN.###3.1 Overview The basic idea of the proposed model MEIRec is to design a heterogeneous GNN for enriching the representations of users and queries With the help of HIN built from intent recommendation system, MEIRec leverages metapaths to guide the selection of different-step neighbors and designs a heterogeneous GNN to obtain the rich###They only utilize attribute and statistic information of users and queries, and fail to take full advantage Information Network (HIN) [18], consisting of multiple types of objects and links , has been widely applied to many data mining tasks [10, 17, 18].###MEIRec utilizes metapath-guided neighbours to exploit rich structural structural information in HIN.",impact-revealing,proposing a novel intent recommendation system using HIN and GNN
2013,,b50b9c4e85ff7d9f6a7fc2f275b6042e57697cd2,VU Research Portal Low-Back Pain Patients Learn to Adapt Motor Behavior with Adverse Secondary Consequences,,,"###Studies of individuals with and without LBP have reported differences in voluntary trunk muscle activation (45), trunk muscle reflexes (32), and trunk kinematics (20) and in cortical mapping of sensory inputs from (5), and motor outputs to (36), the trunk.###Furthermore, low-level coactivation of trunk muscles may occur in LBP patients even at rest (45), implying sustained compression of the spine, which animal models implicate as a cause of inter-vertebral disc degeneration (23).###For the second part of the hypothesis, previous publications have alluded to adverse effects of adaptations (14,45), but that this may contribute to reorganization of the sensory and motor cortex has not been discussed previously.###In addition, studies comparing participants with and without LBP have commonly reported increased activity of trunk muscles, particularly in low-intensity activities (45), as proposed by the model.###For example, according to a systematic review, there is support of both an increase and a decrease of trunk muscle activation in individuals with LBP (45).###Although an adaptive nature of motor control changes with LBP has been proposed previously ( e.g., (14,24,43,45)), resulting predictions mainly regarded levels of muscle activation.###Furthermore, these previous publications either assumed hardwired reflexive changes in motor behavior with pain (24) or did not address the processes underlying these changes (14,43,45).###Experimentally induced nociception from back muscles, or the resulting pain perception, changes control of trunk muscles in a manner resembling that observed in clinical LBP (9,45).",impact-revealing,highlighting the complexity of low back pain and its effects on trunk muscle activation and control
3948,53e9ad3bb7602d970372109a,8686368908956c506f6e3bbcaa2810adfda14914,NoC-sprinting: Interconnect for fine-grained sprinting in the dark silicon era,53e997e4b7602d9701fddcdd,Benchmarking modern multiprocessors,"We evaluate NoC-sprinting with multi-threaded workloads from PARSEC [2] by assuming the chip can sustain computational sprinting for one second in the worst case, which is consistent with [17].###1 [2] as an example.",other,providing context for evaluation methodology
1225,,dbc2db1e4fa67e299496f0d0d597cf89712dc75c,"HISTORICAL POPULATION HEALTH: SPATIOTEMPORAL MORTALITY PATTERNS OF HAMILTON, ONTARIO 1880–1882 AND 1910–1912",,,"###Research using this type of experiment can be used to fill gaps in the evidence base because of the ability to observe the outcomes and impact, or lack thereof, of an intervention, policy (Dawson & Sim, 2015; Petticrew et al., 2005), or event in time.",impact-revealing,highlighting the significance of research experiments in filling evidence gaps
3046,5b3d98cc17c44a510f80212a,cb91c2f8d3cac0b655a39be318b603334eb18987,learning to optimize tensor programs,53e99f5cb7602d970282e3fb,Polyhedral parallel code generation for CUDA,"Polyhedral models [5, 42, 41] are a popular choice for Se; they model the loop domains as integer linear constraints.###Polyhedral methods [5, 42] use integer linear programming to optimize cost.",other,acknowledge existing methods in polyhedral models
2591,5ce3a7b6ced107d4c654a979,a15cd4ac38a38fa977c912c9c360d380dfcccfee,Research on Trustiness of Software Behavior Based on Cross-References,53e9a8ffb7602d97032513e0,Intrusion Detection Via Static Analysis,"These theoretic models are based on statistics model [14], fuzzy mathematics model [15], subjective logic model [16, 17], software behavior model [2, 18], and finite state automaton model [19].",other,reporting various theoretical models used
945,,493f08812a5e1b8953af22857fd1b6c5bbe25f71,Mining the Volatilomes of Plant-Associated Microbiota for New Biocontrol Solutions,,,"###Spectral data were processed using the MetaboAnalyst 3.0 software following this procedure: detection of Gaussian-fitted peaks (4 s fwhm, binning, integrated area of original peak), followed by alignment and grouping according to their masses and retention time after retention time correction.###…from biological replicates would populate a growing database of discrete organisms, strains and experimental conditions that could serve as a basis for exploratory statistical analyses using existing metabolomics tool suites, such as MetaboAnalyst (Xia et al., 2015) or XCMS (Smith et al., 2006).###Ideally, standardized NMR/MS peak lists or LC/GC-MS spectra (converted into exchange formats, such as NetCDF or mzXML) obtained from biological replicates would populate a growing database of discrete organisms, strains and experimental conditions that could serve as a basis for exploratory statistical analyses using existing metabolomics tool suites, such as MetaboAnalyst (Xia et al., 2015) or XCMS (Smith et al., 2006).",impact-revealing,describing the procedure for processing spectral data using MetaboAnalyst software
3187,5b3d98cc17c44a510f801acc,155b7782dbd713982a4133df3aee7adfd0b6b304,Unsupervised Feature Learning via Non-parametric Instance Discrimination,599c7967601a182cd2639a25,Colorization as a Proxy Task for Visual Understanding,"Note that the results for colorization-based pretraining are from a deeper ResNet-152 network [19].###fully supervised training on the small labeled subsets, (2) Split-brain [48] for pre-training, and (3) Colorization [19] for pre-training.###In order to compare with [19], we report the top-5 accuracy here.",other,reporting results and comparisons in pre-training methods
1042,,20739bcb385c819a43d00093226b7986502489a0,An Efficient Adaptive Architecture for Multi-pattern Matching,,,###This algorithm builds upon the Rabin-Karp [15] SPM algorithm and incorporates a new feed-forward bloom filter which takes into account the memory hierarchy of modern computers.,impact-revealing,describing an algorithm improvement
1715,,9efdfd1c9694875b7cb4047aa4f79d64ccb257ee,FAM13A and POM121C are candidate genes for fasting insulin: functional follow-up analysis of a genome-wide association study,,,###Expression of three genes whose expression associated with FSI was validated by quantitative RT-PCR using RNA from subcutaneousWATobtained from a previously examined cohort of 55women (ESMTable 3) [26].,impact-revealing,reporting validation of gene expression findings
2976,5736982b6e3b12023e6fd21d,0344f9ac92006155f6fa464cab19925ac481dea4,Microarchitectural implications of event-driven server-side web applications,559163420cf2e89307ca980e,Profiling a warehouse-scale computer,"More recently, numerous research efforts have been devoted to characterizing warehouse-scale and big data workloads [32, 34, 35, 56–58].",other,acknowledge recent research efforts in big data workloads
2645,5b1643ba8fbcbf6e5a9bc5b5,8c1b00128e74f1cd92aede3959690615695d5101,QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension,5a260c2817c44a4ba8a23716,Globally Normalized Reader.,"Note that Raiman & Miller (2017) recently proposed to accelerate reading comprehension by avoiding bi-directional attention and making computation conditional on the search beams.###In addition, we can combine this method with other data augmentation methods, such as, the type swap method (Raiman & Miller, 2017), to acquire more diversity in paraphrases.###Raiman & Miller (2017) suggested using type swap to augment the SQuAD dataset, which essentially replaces the words in the original paragraph with others with the same type.",other,reporting prior findings and suggesting method improvements
3641,53e9a42bb7602d9702d44c0b,2d6f191fd9b08d2a53498f0ed9b4f1a411d83cdf,Temporal Streams In Commercial Server Applications,53e9b381b7602d9703e5fd16,Dynamic Hot Data Stream Prefetching For General-Purpose Programs,"Our results confirm prior studies [7, 8, 25]: a substantial miss fraction, 35%-90%, occurs in temporal streams.###To improve performance for these access patterns, over a decade of research has lead to the development of address-correlating prefetchers, which exploit correlation between consecutive memory accesses and are highly-effective for pointerbased structures [6, 7, 8, 9, 13, 14, 15, 19, 21, 25].###We identify temporal streams using a previouslyproposed information-theoretic analysis that locates repetitive access sequences of arbitrary length without any assumptions about specific prefetching implementation [7].###Like similar studies of repetition in L1 data accesses [7] and program paths [16], we use the SEQUITUR hierarchical data compression algorithm [10] to identify repetitive sub-sequences within the miss traces.###Furthermore, prior studies have considered only a single system organization, focusing either on uniprocessors [7, 8, 9, 21] or multi-chip distributed-shared-memory systems [25].###Building on this line of research, the latest proposals prefetch extended sequences of memory accesses that recur over the course of program execution [7, 9, 19, 21, 25].###Although the term “temporal stream” was introduced in [25], a wide variety of recent prefetchers rely on the same underlying phenomenon, including hot data stream prefetching [7], the global history buffer [19], the user-level memory thread [21], epoch-based correlation prefetching [8], and last-touch correlated data streaming [9].",other,highlighting the significance of research on memory access patterns and prefetching techniques
2619,5d9ed2d847c8f76646f797b7,c3715947bbbf648dcf29a1aa4b35cfb68044f919,GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification,5bdc315017c44a1f58a05b46,"Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference.","…to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018).",other,reporting prior findings on state-of-the-art neural models
1729,,bd8355a3e02a46a7cbbb3ac8f68650cbda2d4afb,Pier: internet scale p2p query processing with distributed hash tables,,,"###2 Relaxed Consistency While transactional consistency is a cornerstone of databa se functionality, conventional wisdom states that ACID transactions severely limit the scalabili ty and availability of distributed databases [28].",impact-revealing,highlighting the trade-offs between ACID transactions and database scalability
1861,,3f88df19b7f430c95bedabb5c8e6facb76006689,"Older Adults’ Relocation Transitions: Relation of Place, Leisure and Identity",,,"###Such a synthesis can inform practice and policy initiatives and can lead to more rigorous methods used in future research (Whittemore & Knafl, 2005).###Integrative reviews are the broadest type of literature review method that allow for the inclusion of both experimental and non-experimental empirical research and theoretical literature to better understand a problem (Whittemore & Knafl, 2005).###Whittemore and Knafl (2005) described a clear framework; however, they did not address study quality, which was deemed important in this review.",impact-revealing,highlighting the significance of integrative reviews in informing practice and policy initiatives
3856,5eb789d3da5629cf24430b41,1739466ac1411788cf1de60a3a6b59d739dc41ff,Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder,58437722ac44360f1082f4d8,Towards Evaluating The Robustness Of Neural Networks,"These so-called adversarial attacks may adopt either white-box or black-box approaches, depending on the knowledge of the target network, and they mostly use gradient-based methods [10, 18, 27] or score-based methods [4] to generate adversarial samples.",other,providing context on adversarial attacks and their methods
3017,5a73cb7417c44a0b3035a202,c1cb7a1efb1a47348e3a25c21ff0a3ff192d7058,Image Super-Resolution Using Dense Skip Connections,573695fd6e3b12023e510ff5,Deeply-Recursive Convolutional Network for Image Super-Resolution,"A skip connection was used in [12] to link the input data and the final reconstruction layer in SR.###Among them, the CNN-based approaches [11, 12] have recently set state of the art for SISR.###Recent works [11, 12] have successfully used very deep convolutional neural networks (CNN) to perform single image super-resolution (SISR), and significant improvements over shallow CNN structures [2] have been observed.###Dataset Bicubic Aplus [24] SRCNN [2] VDSR [11] DRCN [12] SRDenseNet All###The deconvolutional layers can learn the up-scaling filters, thus avoiding the use of the bicubic interpolation as adopted in previous algorithms such as VDSR and DRCN.###However, only a single skip connection was adopted in [12], which may not fully explore the advantages of skip connections.###However, the improvement of the SR performance over the DRCN method [12] that used a single skip connection is marginal.###State-of-the-art SR results were achieved in [12].###This can help the training of very deep networks and improve the reconstruction performance in SR [12].###We compared the results using the proposed method and those using other SISR methods, including bicubic, Aplus [24], SRCNN [2], VDSR [11] and DRCN [12].",other,reporting on advancements in single image super-resolution methods
3000,5c5ce4fd17c44a400fc38abb,d524f10d653ba09e36456475da0aed92d244f795,Image Super-Resolution As A Defense Against Adversarial Attacks,5aed14e217c44a4438159835,Adversarial Attacks and Defences Competition,"We also perform experiments on the NIPS 2017 Competition on Adversarial Attacks and Defenses DEV dataset [54].###Adversarial Training has been shown to enhance many recently proposed defense methods [54] under white-box attack settings.###Here, we only report results on Inception v-3, following the standard evaluation protocols as per the competition’s guidelines [54].",other,reporting experimental results and methods in adversarial training
3339,5f8cf5159e795ea21aee7f07,0ee0801ba010a441403f9ed666ef9bf006b3aa07,Adaptive Universal Generalized PageRank Graph Neural Network,573696c56e3b12023e5c58be,Image-based Recommendations on Styles and Substitutes,", 2016) and the Amazon co-purchase graphs Computers and Photo (McAuley et al., 2015; Shchur et al., 2018).###We can see that learned GPR weights are all positive for homophilic datasets (PubMed and Photo).###We use 5 homophilic benchmark datasets available from the Pytorch Geometric library, including the citation graphs Cora, CiteSeer, PubMed (Sen et al., 2008; Yang et al., 2016) and the Amazon co-purchase graphs Computers and Photo (McAuley et al., 2015; Shchur et al., 2018).",other,reporting dataset sources and characteristics
3588,5ef0816891e0112aee042a73,7e627c774997addd7423361e93d8891dd1de35ad,Attention Mesh: High-fidelity Face Mesh Prediction in Real-time,5d1dce4b3a55ac56ce82a560,On-Device Neural Net Inference with Mobile GPUs,The performance has been measured using the TFLite GPU inference engine [6].,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2458,5f33bcf791e011861cfa0fd7,21066f476b04b602b40e73f494b577f7eb660e75,Woodpecker-DL: Accelerating Deep Neural Networks via Hardware-Aware Multifaceted Optimizations,5b3d988217c44a510f7fbcfb,Diesel: DSL for linear algebra and neural net computations on GPUs.,"Typical DSL compilers include Halide [18], DLVM [19], Diesel [20], TIRAMISU [21] and Triton [22].",other,reporting existing DSL compilers
1485,,7bc56b12f55e144738d9a04b4b7968f5bb053726,How Susceptible are LLMs to Influence in Prompts?,,,"###This raises concerns about proposals to use LLMs as a substitute for human raters (Gilardi et al., 2023; Chiang & Lee, 2023).",impact-revealing,highlighting concerns regarding the use of LLMs as substitutes for human raters
1878,,5d9200ae6d217948df86d21d2a69bf500c5745d4,Parental divorce and parental death An integrative systematic review on children's double bereavement,,,"###When data were conceptualized to a higher level of abstraction, the primary source was reviewed to verify the synthesis.[1]###Data display is shown in the review matrix (see Table 2).[1, 23] The included articles cover expressions of the double loss and be-###In the discussion and conclusion of the data, the data comparison also included, as described by Whittemore and Knaf [1] and inspired by Miles and Huberman: Subsuming particulars into general, noting relations between variability, ﬁnding intervening factors and building the logical chain of evidence.###Seven studies were rated as high and four as low in terms of methodology/theoretical rigour, whereas four studies were rated as high and seven as low regarding relevance.[1]###This ensured transparency in the review process, consistency with the stated aim and replicability of the research.[1]###We were inspired by Whittemore and Knafl’s strategy for conducting an integrative review, evaluating and analyzing the data, and qualifying the synthesis of the results.[1] The review used the matrix method, which follows a structured approach and process.###In the discussion and conclusion of the data, the data comparison also included, as described by Whittemore and Knaf[1] and inspired by Miles and Huberman: Subsuming particulars into general, noting relations between variability, finding intervening factors and building the logical chain of evidence.###In the discussion and conclusion of the data, the data comparison also included, as described by Whittemore and Knaf[1] and inspired by Miles and Huberman: Subsuming particulars into general, noting relations between variability, finding intervening factors and building the logical chain of evidence.[1, 24] This was done through discussion of the themes and subcategories.###None of the articles were excluded in this process.[1, 23]###This article presents an integrative, systematic literature review[1] of research on children and adolescents who experience double bereavement.###4 The analysis process The analysis process is described by Whittemore and Knafl[1] (see Figure 1).",impact-revealing,describing the review process and methodology
1021,,e583dbca34696829ce95fd3af81ea06677d17624,"Attitudes, motivators, and barriers toward influenza vaccination for children: a study from a conflict-ridden country",,,"###The sample size was calculated using the Daniel sample size formula [33], assuming a binomial distribution, a confidence interval of 95%, a margin of error of 5%, and a distribution rate of 50%, commonly used for conservative estimations when the true proportion is unknown.",impact-revealing,describing the method for sample size calculation
84,5e2d653a3a55acc8374367fd,36ff7927f9049d37b314c2a114769b517b3f5f7a,Joint Recognition of Names and Publications in Academic Homepages,5b3d98cc17c44a510f801a7b,Working Memory Networks: Augmenting Memory Networks With A Relational Reasoning Module,"…N and P alternatingly in two consecutive hops by providing alternating boolean values for even and odd values of i : When updating the memory representation M , we use a memory updating controller based on multi-head attention [26], which is similar to that used in Working Memory Network [17].###Dynamic Memory Network (DMN) [12] uses a gated recurrent unit [2] based controller to up-date the memory, while Working Memory Network (W-MemNN) [17] uses a multi-head attention [26] based controller.",impact-revealing,describing memory updating methods in neural networks
1164,,154d762b2e5ce5386c050342e3adbbc55f6c96b2,An Anti-collision Neighbor Discovery Protocol for Multi-node Discovery,,,"###The Birthday protocol was inspired by the Birthday Paradox, that each node listens, transmits, or sleeps under probabilities for achieving fast neighbor discovery in the average case [9].",impact-revealing,describing the inspiration behind the Birthday protocol
3268,5fa909a591e011e83f7406b0,ba9f6368370ca07c1a0c9a5684b0908f6d2e0c6f,Sandslash: a two-level framework for efficient graph pattern mining,57d063a2ac44367354289748,A distributed approach for graph mining in massive networks,"Pa Patents [24] 3M 28M 10 37 Yo Youtube [13] 7M 114M 16 29 Pdb ProteinDB [52] 49M 388M 8 25 Lj LiveJournal [36] 5M 86M 18 0###We also evaluate the state-of-the-art expertoptimized GPM applications [3, 5, 16, 52] listed in Table 2 except for CECI [7] (not publicly available).###Hand C/C++ GAP [5] KClist [16] CECI [7] PGD [3] DistGraph [52] Optimized 89 394 3,000 2,538 17,459###DistGraph [52, 53] parallelizes gSpan with a customized dynamic load balancer that splits tasks on the fly.###For fair comparison, we modified DistGraph [52] and PGD [3] so that they produce the same output as Sandslash.###Sandslash performs better than expert-implemented DistGraph [52] too as it automatically enables all optimizations that are in DistGraph.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2547,53e9ba39b7602d970464970c,ad1a633f2dfa2a6bb920e978dbbcdefb1c373795,Bypass and insertion algorithms for exclusive last-level caches,53e9b9b4b7602d97045a7c79,Insertion Policy Selection Using Decision Tree Analysis,"Also, several of these studies usually identify the insertion age with LRU, MRU, or other access recency positions in a set [9, 13, 16, 20, 27].###A decision tree-based technique for selecting the insertion age relative to the access recency order has been explored in [20].",other,acknowledge existing techniques for identifying insertion age
1703,,c9fd6ad9e43c38b387132f30b799a8fc640d22f8,Charitable donations by the self-employed,,,"###Social identity theory (SIT) (Tajfel and Turner 1979; Hogg et al. 1995) aims to explain individual motivations in terms of the degree of identity found in a social setting, whereas other theories tend to argue from the perspective of personal utility maximization.###2.1 Social identity theory
Social identity theory (SIT) (Tajfel and Turner 1979; Hogg et al. 1995) aims to explain individual motivations in terms of the degree of identity found in a social setting, whereas other theories tend to argue from the perspective of personal utility maximization.###In terms of theory, we develop a conceptual framework that builds on social identity theory (SIT) (Tajfel and Turner 1979; Hogg et al. 1995).###Tajfel and Turner (1979) did not believe that the psychology of individuals is sufficient to explain behaviors favoring the group and therefore proposed a complementary set of processes situated at the collective psychology level (i.e., that of a community or society).",impact-revealing,providing context for social identity theory
3206,5fae6dced4150a363cec41f7,71c97e7d4a529a21f32d98aad72fbbf32c9ee32e,Anomaly-Based Intrusion Detection From Network Flow Features Using Variational Autoencoder,5ceddfb9da56298378796dac,Deep Learning Approach for Intelligent Intrusion Detection System.,"Vinayakumar et al. [23] explored a deep neural network (DNN) to create a useful and ﬂexible IDS, named ‘‘scale-hybrid-IDS-AlertNet’’, and to identify unforeseen and unpredictable intrusions via supervised learning approach.###…recently, according to a McAfee report [62], [23], 2) It is up-to-date, 3) It is a labelled dataset consisting of ﬂow-based features expanded by measuring some parameters statistically, 4) It possesses the characteristics of a real-time network trafﬁc [23], 5) It is non-linearly separable [23].###…due to the fact that: 1) It includes various types of attacks, which had been carried out on networks recently, according to a McAfee report [62], [23], 2) It is up-to-date, 3) It is a labelled dataset consisting of ﬂow-based features expanded by measuring some parameters statistically, 4) It…",other,reporting findings on a deep neural network for intrusion detection
4044,5eede0b791e0116a23aafe75,150f95f9c73820e0a0fa1546140e9f2bdfd25954,temporal graph networks for deep learning on dynamic graphs,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"Learning on such data is possible using graph neural networks (GNNs) [26] that typically operate by a message passing mechanism [4] aggregating information in a neighborhood of a node and create node embeddings that are then used for node-wise classiﬁcation [42, 61, 35] or edge prediction [72]…",other,providing context on the use of graph neural networks for data learning
3677,5edf5ddc91e011bc656defd7,c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87,linformer: self-attention with linear complexity,5e943b8091e011344866420c,Longformer: The Long-Document Transformer,"One popular technique is introducing sparsity into attention layers (Child et al., 2019; Qiu et al., 2019; Beltagy et al., 2020) by having each token attend to only a subset of tokens in the whole sequence.",other,acknowledge a popular technique in attention layers
821,5f8d6be69fced0a24bbab005,93e513de7bc55a6d9319b7861435f435ed85a03f,dimension relation modeling for click-through rate prediction,599c7950601a182cd262ecb7,DeepFM: a factorization-machine based neural network for CTR prediction,"Usually, the raw fields are not independent, thus it’s effective to learn information from their interactions, such as low-order interactions [4, 8] or high-order interactions [3, 5], as shown in Fig.###Our experimental results show that the proposed DRM module can catch extra useful information for CTR prediction and boost the performance of existing state-of-the-art methods such as [1, 3], especially when the size of embedding dimension is high.",impact-revealing,highlighting the effectiveness of the proposed DRM module in improving CTR prediction
1869,,2b06dd8d6368017e85bd5872fed0f33652eb65ae,Health integration across international borders: an integrative review,,,"###(8) Th e study was designed through the recommendations of the Statement for Reporting Systematic Reviews and Meta-Analyses of Studies (PRISMA) checklist and the elaboration of a protocol, validated by an expert reviewer, consisting of fi ve sequential steps: problem identifi cation, data collection, data evaluation, analysis and interpretation, and presentation of results.###(8) Th e study was designed through the recommendations of the Statement for Reporting Systematic Reviews and Meta-Analyses of Studies (PRISMA) checklist and the elaboration of a protocol, validated by an expert reviewer, consisting of fi ve sequential steps: problem identifi cation, data collection, data evaluation, analysis and interpretation, and presentation of results.(8,9) In the fi rst stage, the problem was identifi ed and the review question was defi ned: What are the types and purpose of the healthcare integration actions that take place in international borders regions? For the data collection, the fi lters included the Portuguese, English and Spanish languages, in the time frame from 2006 to 2015, which considered, as a milestone, the implementation of the Integrated Border Health System in Brazil in 2005.",impact-revealing,describing the study design and methodology
3552,5cede0f2da562983788d0d44,aecddd82840323e5bd43f9c73a32fed88ee93c8c,An Effective Approach To Unsupervised Machine Translation,53e9a439b7602d9702d53024,Large Scale Decipherment for Out-of-Domain Machine Translation,"Early attempts to build machine translation systems with monolingual corpora go back to statistical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012).###tical decipherment (Ravi and Knight, 2011; Dou and Knight, 2012).",other,acknowledge historical attempts in machine translation
652,5aed14d117c44a4438158a8d,36db98d8604df17e7ab68952c6f7a6cb70cff2e7,a deep learning approach for multimodal deception detection,5550412045ce0a409eb38b4c,Convolutional Neural Networks for Sentence Classification.,"3D-CNN has achieved state-of-the-art results in object classiﬁcation on tridimensional data [16].###During feature extraction from text, we train the TextCNN model with two diﬀerent settings: one, by keeping the word vector representation static; two, by optimizing the vector along with the training (non-static).###These vectors are concatenated and fed as input vector to the CNN.###Textual Features Extraction We use Convolutional Neural Networks (CNN) [18, 19] to extract features from the transcript of a video, v.###We use a simple CNN with one convolutional layer and a max-pooling layer, to get our sentence representation.###3D-CNN not only extracts features from each image frame, but also extracts spatiotemporal features [17] from the whole video which helps in identifying the facial expressions such as smile, fear, or stress.###The input to 3D-CNN is a video v of dimension ( c, f, h, w ), where c represents the number of channels and f, h, w are the number of frames, height, and width of each frames respectively.###We use Convolutional Neural Networks (CNN) [18, 19] to extract features from the transcript of a video, v .###Visual Feature Extraction For extracting visual features from the videos, we use 3D-CNN [16].",impact-revealing,describing the use of 3D-CNN for feature extraction in video analysis
3067,5e09aa66df1a9c0c416bebf6,4e7dd1e79f0f13650b2612325e6ba8d206dc04fb,DeepGCNs: Can GCNs Go As Deep As CNNs?,599c797b601a182cd2642e7b,Rethinking Atrous Convolution for Semantic Image Segmentation.,"It will also be interesting to study different distance measures to compute dilated k-NN, constructing graphs with different k at each layer, better dilation rate schedules [4, 41] for GCNs, and combining residual and dense connections.",other,suggesting future research directions in graph convolutional networks
3462,5e15adcb3a55ac47ab5b0b8c,aa63ac11aa9dcaa9edd4c88db18bec87e0834328,Graph Transformer Networks,5cd7fa07ced107d4c65bf2eb,Heterogeneous Graph Attention Network,"Previous works [37, 43] require manually deﬁned meta-paths and perform Graph Neural Networks on the meta-path graphs.###We used the GCN [19], GAT [33], and HAN [37] as GNN based methods.###Meta-Path [37] denoted by p is a path on the heterogeneous graph G that is connected with heterogeneous edges, i.e., v , where t l ∈ T e denotes an l -th edge type of meta-path.###Then conventional GNNs can operate on the transformed homogeneous graphs [37, 43].###Here, we test HAN on the selected sub-graphs whose nodes are linked with meta-paths as described in [37].###The metapath2vec [10] learns graph representations by using meta-path based random walk and HAN [37] learns graph representation learning by transforming a heterogeneous graph into a homogeneous graph constructed by meta-paths.",other,describing the use of GNN methods and meta-paths in graph representation
1805,,7414efb630057cf5a84701d50d749adb9a32729e,"Can being scared cause tummy aches? Naive theories, ambiguous evidence, and preschoolers' causal inferences.",,,"###Although some research on the development of scientific reasoning has emphasized the importance of integrating domain-specific knowledge with domain-general strategies (Barrett, Abdi, Murphy, & Gallagher, 1993; Klahr & Dunbar, 1988; Koslowski, 1996; Koslowski, Okagaki, Lorenz, & Umbach, 1989; Pazzani, 1991; Penner & Klahr, 1996; Schauble, 1990), those studies have focused primarily on adolescents and adults.###…mechanisms is fundamental to an understanding of causal relationships (Ahn, Kalish, Medin, & Gelman, 1995; Bullock, Gelman, & Baillargeon, 1982; Koslowski, 1996; Shultz, 1982) and it would be interesting to know how offering children explicit information about causal mechanisms might affect…###Many researchers have proposed that an understanding of causal mechanisms is fundamental to an understanding of causal relationships (Ahn, Kalish, Medin, & Gelman, 1995; Bullock, Gelman, & Baillargeon, 1982; Koslowski, 1996; Shultz, 1982) and it would be interesting to know how offering children explicit information about causal mechanisms might affect their learning.",impact-revealing,highlighting the importance of integrating domain-specific knowledge with domain-general strategies in scientific reasoning
1100,,8f72717578b96e7d8d3481fdfb4ee4555fd80d66,Focused Microarray Analysis of Peripheral Mononuclear Blood Cells from Churg–Strauss Syndrome Patients,,,"###The advent of array technology and the subsequent development of high-density oligonucleotide arrays(1) have been enormously helpful in improving our understanding of the genome-wide transcriptional profiles of many biological systems in both basic and applied research.(2) Array technology has also been extremely useful for discovering and developing diagnostic gene markers for disease subcategories, disease prognosis, and treatment outcome; this has paved the way for effective pharmaceutical drug discovery, the development of novel strategies for molecular (DNA, RNA) diagnostics, and the design of personalized drug regimens.###Array technology has also been extremely useful for discovering and developing diagnostic gene markers for disease subcategories, disease prognosis, and treatment outcome; this has paved the way for effective pharmaceutical drug discovery, the development of novel strategies for molecular (DNA, RNA) diagnostics, and the design of personalized drug regimens.(2) Oligonucleotide microarrays, which were initially designed to analyze genome-wide gene expression levels, have turned out to be particularly useful in DNA diagnostics as they can be used formany different Edited by Mitsuo Oshimura * To whom correspondence should be addressed.",impact-revealing,highlighting the significance and applications of array technology in biological research
552,5cede0eeda562983788cd285,e4d99f390901df5caac0b587ff685f9cde100342,end-to-end speech translation with knowledge distillation,5550411c45ce0a409eb3897f,Neural Machine Translation by Jointly Learning to Align and Translate.,"End-to-end model has already become a dominant paradigm in machine translation task, which adopts an encoder-decoder architecture and generates target words from left to right at each step [1, 3, 5].###Conventional speech translation system is a pipeline of two main components: an automatic speech recognition (ASR) model which provides transcripts of source language utterances, and a text machine translation (MT) model which translates the transcripts to target language [1, 2, 3, 4, 5].",impact-revealing,describing the architecture of end-to-end models in machine translation
1403,,9e3079df985f3464bd3a49cce10550ed49fcbf78,Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples,,,"###Experiment Setting In Equation (11), our arg-max operator includes two adjustable hyperparameters: the number of uniformly sampled actions and the threshold.###Consider the belief variable b t = b ( o t , a t , h t ) and ˆ b the optimizer of Equation (6), the estimated value residual between l ( b t ) and l (ˆ b ) is controlled by the corpus residual: is the operator norm for the linear mapping.###Such as distributional matching [7, 29], regularization techniques to prevent overfitting [30], conservation [11, 31] or adding noise [14] to the policy or using adversarial training [32].###3 with a sample size of 100 , only the top 30% of actions with the smallest residuals are considered in the arg-max operator in Equation (11).###As suggested by Equation (3 Note that an accurate belief corpus subset decomposition for a specific control time belief value b t may be unattainable when b t / ∈ CB ( C ) .###Moreover, defining the nearest neighbor with a heuristically determined Euclidean metric also suffers the problem of aggressive extrapolation and thus is not suitable for the offline setting [7, 11].###Here, the weights can be understood as representing the similarity and importance in reconstructing the control time belief variable through the belief corpus subset; otherwise, the optimizer of Equation (6) can be employed as a proxy for b t .",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3832,5ed7796e91e011e6e91120f0,8ade0f3bbe8d8251c5de9ef39886a13055f463e5,TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"Some models such as GraphSAGE [14] will be super slow sampling our graph.###GraphSAGE [14] is not very suitable on our dataset.###For example, for GraphSAGE, neighborhood-sampling can not be easily done both effectively and efficiently.###To improve GraphSAGE’s expressiveness, GIN [40] is developed, enabling more complex forms of aggregation.###In practice, due to the sampling time cost brought by our links’ high density, GIN, GraphSAGE and its extension onto heterogeneous information network such as HetGNN [43] and GATNE [3] are not very suitable on our datasets.###GCN convolutional operation could also be viewed as sampling and aggregating of the neighborhood information, such as GraphSAGE [14] and FastGCN [4], enabling training in batches.",other,highlighting the limitations of GraphSAGE and its suitability for the dataset
282,5edcbb9a91e0110f1d6db1c2,c99e2877d4b1f171746a3aa73d5fd5fd680c982d,Hierarchical Bipartite Graph Neural Networks: Towards Large-Scale E-commerce Applications,5d4d46fb3a55acff992fdc6f,Hierarchical Representation Learning for Bipartite Graphs.,"However, learning hierarchical representations of graph enjoys its outstanding features in graph classiﬁcation and clustering, and becomes prevailing in several scenarios such as link prediction, e-commerce recommendation, etc, [19], [22].###[19] learns a hierarchical representation of graphs by decomposing user information into two orthogonal spaces, each of which represents information captured by community level and individualized user preference respectively, which improves the prediction accuracy with promising scalability.###The parameter of CGNN refers to [19].###However, in this regard, most state-of-the-art methods including neural graph collaborative ﬁltering do not consider underlying user-community interactions or user hierarchy which have shown an advantageous performance over paradigms using user-item interactions alone [19]–[21].",impact-revealing,highlighting the advantages of hierarchical representations in graph classification and clustering
3469,599c7ea4601a182cd28b8342,103baca878b17a15d148a684c0b0152e78591be1,A Split Cache Hierarchy for Enabling Data-Oriented Optimizations,53e9af33b7602d970396c8ac,Victim Replication: Maximizing Capacity while Hiding Wire Delay in Tiled Chip Multiprocessors,", cooperative caching [6], [22], victim replication [23], RNUCA [5] and commercial NUMA/COMA designs [24]).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1111,,9c2e0c01c4cb04fa9577843a9179f0731f295d0e,Diffusion-Inspired Quantum Noise Mitigation in Parameterized Quantum Circuits,,,"###Diffusion models are inspired by non-equilibrium thermodynamics [34, 13, 35].",impact-revealing,providing context on the inspiration behind diffusion models
3638,5c87a964da56296d04a90b97,a20bc2ecd8eb9bd735e6b3ba3d1a88f1c872d8eb,Shinjuku: Preemptive Scheduling for μsecond-scale Tail Latency,5a260c3b17c44a4ba8a25ffb,Exploiting heterogeneity for tail latency and energy efficiency.,"We will also explore microsecond-scale scheduling policies that are localityand heterogeneity-aware [30, 27].",other,suggesting future research directions on scheduling policies
1818,,72246a0826e43b8653f29fbd7abe030fa76b3c4a,How the Powerful Decide: Access to Research Participation by those at the Margins,,,###This study builds on the history of using similar measures to ascertain likely behaviors in community and organizational power and empowerment research (cf. Bartunek and Keys 1982; Zimmerman and Rappaport 1988).,impact-revealing,acknowledging the historical context of measures in community and organizational research
2939,5ce937225ced2477cb328bd9,e4350bcfcadc5b4c0bfbc61ffd8bce05f0b8fe15,ABM-SpConv: A Novel Approach to FPGA-Based Acceleration of ConvolutionaI NeuraI Network Inference,5a9cb61f17c44a376ffb4456,A Framework for Generating High Throughput CNN Implementations on FPGAs.,"Designs of [4, 12, 13] are based on spatial convolution, while the works of [3, 10] use frequency domain convolution.###55× speedup in throughput compared to [3] as a result of being able to utilize 1.###Layer C R N K×K M Ratio SDConv FDConv[3] SpConv[7] Acc.###The throughput achieved by [3] on a Intel HARP platform is 669.###Note that, although our scheme quantizes the CNNmodel in 8-bit, the precision of the datapath is of the same (16-bit) as [3].###2% of the MAC operation is saved in [3], resulting in a theoretical speedup of 3.###By reducing the number ofMACoperations required for convolution, FDConv-based accelerator raises the computational roof over SDConv-based design by a factor of Rmac , where Rmac is the reduction rate in MAC operation.###6% of the total operations (accumulate and multiply) is saved compared to SDConv, while the reduction over FDConv [3] and SpConv [7] are 47.###The second category of designs [3, 10] perform convolution in the Frequency Domain (referred to as FDConv).###83.6% of the total operations (accumulate and multiply) is saved compared to SDConv, while the reduction over FDConv [3] and SpConv [7] are 47.1% and 50%, respectively.###The design space of SpConv-based accelerators share a similar computational roof of 2×Rmac ×Nmac × Freq with FDConv-based ones.###The performance of the reported SpConv-based FPGA accelerators have not exceed that of [3].###The latest work of [3] uses a frequency domain convolution scheme which gains 3.",other,comparing designs based on different convolution methods
895,5f0d85c69fced0a24be4f019,6817b5d48cf8f665b680de07d23b91814d2923b1,Bouquet of Instruction Pointers: Instruction Pointer Classifier-based Spatial Hardware Prefetching,58d83045d649053542fe853e,Path confidence based lookahead prefetching.,"It is well known that spatial prefetchers fail to improve performance for server workloads like CloudSuite [14], [33], [53], [58], [59] and additional prefetchers [12], [24], [52], [58], [59] can be used on top of IPCP to improve the performance.###Recent prefetching proposals [11], [14], [33] have also been coded and evaluated with ChampSim, helping the community for a fair comparison of techniques.###Prefetchers like variable length delta prefetching (VLDP) [45] and signature path prefetching (SPP) [33] are well known delta prefetchers.###The problem: State-of-the-art spatial prefetchers [45] [33], [11], [14], [13] are designed speciﬁcally for L2’s access patterns.###Well-established and recent spatial L2 prefetchers (prefetchers that prefetch within a spatial region) [33], [13], [14], [38], [11], [45] have pushed the limits of data prefetching.",impact-revealing,highlighting the limitations of spatial prefetchers and the need for additional techniques
3626,5e2d653a3a55acc8374367fd,36ff7927f9049d37b314c2a114769b517b3f5f7a,Joint Recognition of Names and Publications in Academic Homepages,5b3d98cc17c44a510f801a7b,Working Memory Networks: Augmenting Memory Networks With A Relational Reasoning Module,"…N and P alternatingly in two consecutive hops by providing alternating boolean values for even and odd values of i : When updating the memory representation M , we use a memory updating controller based on multi-head attention [26], which is similar to that used in Working Memory Network [17].###Dynamic Memory Network (DMN) [12] uses a gated recurrent unit [2] based controller to up-date the memory, while Working Memory Network (W-MemNN) [17] uses a multi-head attention [26] based controller.",other,providing context for memory representation updating methods
1236,,52c167a95347a3b0fd9c8596264c3daec3f149d6,MULTIPRED: a computational system for prediction of promiscuous HLA binding peptides,,,"###Several web-based systems have been developed and widely used for the prediction of MHC binders, such as SYFPEITHI (17), BIMAS (18), SMM (19), MHCPred (20), RANKPEP (21), TEPITOPE (22), NetMHC (23) and SVMHC (24).###BIMAS, MHCpred, RANKPEP and TEPITOPE use quantitative matrices, and SMM is based on an improved matrix-based algorithm called stabilized matrix method.",impact-revealing,reporting existing web-based systems for MHC binder prediction
342,5da2f8aa3a55ac3402d8c092,fd2a0a326db4f034fe22340c20b7bacd9a14c3d6,second-order attention network for single image super-resolution.,5c8f22cb4895d9cbc62c6c05,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,"As in [38], we also add long and short skip connections in Base model.###Zhang et al. [38] proposed a residual in residual structure to form a very deep network.###Most existing CNN-based methods mainly focus on designing a deeper or wider network structure [2, 12, 13, 6, 39, 38].###In addition to focusing on increasing the depth of the network, some other networks, such as NLRN [22] and RCAN [38], improve the performance by considering feature correlations in spatial or channel dimension.###During the past decade, a plenty of image SISR meth-ods have been proposed in the computer vision community, including interpolation-based [37], model-based [34], and CNN-based methods [2, 29, 14, 13, 29, 17, 30, 39, 38].###Channel attention [9, 38] has been shown to be effective for better discriminative representations.###Following [20, 6, 39, 38], we use 800 high-resolution images from DIV2K dataset [31] as training set.###Recently, SENet was introduced to deep CNNs to further improve SR performance [38].###By fully exploiting the image statics inherent in training datasets, CNNs have achieved state-of-the-art re-sults in SISR [2, 12, 14, 36, 39, 38].###To test the effectiveness of our SAN, we compare our SAN with 11 state-of-the-art CNN-based SR methods: SR-CNN [1], FSRCNN [3], VDSR [12], LapSRN [14], Mem-Net [30], EDSR [20], SRMD [36], NLRN [22], DBPN [6], RDN [39] and RCAN [38].",impact-revealing,acknowledge existing methods and their contributions in image super-resolution
1274,,6f0b458259b4d494b2546f9a589b65f560943e6f,Environmental drivers of increased ecosystem respiration in a warming tundra,,,"###Models were validated by evaluating funnel and profile plots and testing the residuals for the assumption of normality and variance homogeneity 73,82 .",impact-revealing,reporting validation methods used for models
2685,5da2f8aa3a55ac3402d8c2e1,17f2f3f7e58b916175d495109bc74b2757ef952a,Barrage of Random Transforms for Adversarially Robust Defense,53e9a7e4b7602d9703124f55,Fast Median And Bilateral Filtering,"This includes a Gaussian blur, median, mean, and mean-bilateral [31] filtering, Chambolle and wavelet [32] denoising, and nonlocal mean denoisng.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2830,5d0b00ed8607575390fedd4e,493d5f344eea1468260946b29a80dc81b2be409c,HopSkipJumpAttack: A Query-Efficient Decision-Based Attack,5a9cb66717c44a376ffb89eb,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,"For region-based classiﬁcation, we include backward pass differentiable approximation (BPDA) [34], which calculates the gradient of the model at a randomized input to replace the gradient at the original input in C&W Attack and BIM.###The second defense, region-based classiﬁcation, belongs to a wide family of mechanisms which add test-time randomness to the inputs or the model, causing the gradients to be randomized [34].###Adversarial training [2, 3, 7, 17] is known to be one of the most effective defense mechanisms against adversarial perturbation [34, 40].",other,describing defense mechanisms against adversarial attacks
682,5e982cc591e0119e8a952209,b5ef0f91663f0cbd6910dec9a890c138f7ec10e0,Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Since the alignments between q and w, both in text, are relatively easy to identified by using pretrained BERT models [6], which are used as initialization for VLP in Oscar, the image regions from which the object tags are detected are likely to have higher attention weights than other regions, when queried by the semantically related words in the text.###Similarly, the SoTA model on NLVR2 is UNITER [6] large.###The SoTA result for VQA is from UNITER [6] large model.###04%) than UNITER [6] large.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1697,,2f2565151184974dbf74a94627c27230dde12933,The role of moral emotions and individual differences in consumer responses to corporate green and non-green actions,,,"###At the collective level of the self-concept, individuals are motivated by the welfare of the groups to which they belong (Brewer and Gardner 1996; Tajfel and Turner 1979).",impact-revealing,providing context for self-concept motivation
3,5b67b45517c44aac1c860884,9fa3e53b5937a0ec92499ed415e339ede6c92010,DeepInf: Social Influence Prediction with Deep Learning,58d82fcbd649053542fd5fc7,DeepCas: An End-to-end Predictor of Information Cascades,"[26] proposed an end-toend predictor for inferring cascade size by incorporating recurrent neural network (RNN) and representation learning.###, the DeepCas model [26] which formulate cascade prediction as a sequence problem and solve it with Recurrent Neural Network.###Indeed, extensive work has been done on social influence prediction in the literature [26, 32, 42, 43].",impact-revealing,reporting existing findings on cascade prediction methods
1287,,4bf86181621b13605ad84808c314707e5d121ce7,Differences in apparent diffusion coefficients of brain metabolites between grey and white matter in the human brain measured at 7 T,,,"###Only metabolites with Cramér Rao lower bounds (CRLBs) (22,25) of less than 20% were included for analysis, as is recommended by the authors of the software package and commonly adopted by several groups as one of the criteria for accurate quantification (e.###For Gln, the data in some subjects for the higher b-values did not have CRLBs above 20% and so had to be omitted.###Only metabolites with Cramér Rao lower bounds (CRLBs) (22,25) of less than 20% were included for analysis, as is recommended by the authors of the software package and commonly adopted by several groups as one of the criteria for accurate quantification (e.g., Ref.###For NAA, tCr, tCho, Glu, and NAAG CRLBs remained below 20% for all b-values.###As expected, CRLB values were lowest for NAA, tCr, tCho, and Glu, with higher values for Gln.###By using a linear combination of complete metabolite spectra, instead of fitting individual resonances, partially overlapping signals such as Glu and Gln and NAA and NAAG can be accurately quantified, even at the long echo times that are needed to accommodate the diffusion weighting gradients (21,22).###Also, the low CRLB values for Glu and the higher ones for Gln are commensurate with the concentration differences between Glu and Gln and thus indicate a linear independence in the determination of the two compounds.###Even at high b-values, the quality of the MR spectra was sufficient to quantify not only the major signals of tNAA, tCr, and tCho but also Glu, Gln and, in white matter, NAAG, as indicated by CRLB of less than 20%.###Metabolites with a lower SNR, Glu and NAAG, had CRLBs below 20% for all b-values.###As a measure of the quality of the fit, we used the CRLB (22,25) for accepting or rejecting spectra for quantification.###ADCs were only calculated when metabolites concentrations passed the CRLB test for at least five b-values.###In some subjects, CRLB values higher than 20% for Gln for too many b-values prevented the calculation of trace/3 ADCs.###The resulting spectra were analyzed in the frequency domain using the LCModel (22) with a simulated set of basis spectra and a simulated base line for the macromolecules.###The difference between the CRLBs of Gln and Glu indicates that Gln contamination of the Glu signal was minimal (16) and the signals could be well separated.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
847,58d83045d649053542fe853e,51898a5e35b2646a4e5121ca74f612fab831ad6a,path confidence based lookahead prefetching,5736982b6e3b12023e6fd15e,Efficiently prefetching complex address patterns,"Lookahead prefetchers [9], [12] efficiently encode the relationship between accesses to yield future predictions, enabling further speculative lookahead accesses.###Moreover, most hardware prefetchers work in the physical address space [4], [9], [10], [11], where the mapping between virtual and physical memory is not known.###These studies, however, suffer from high hardware complexity [12], [13], or do not implement adaptive throttling [9].###To address both prefetching coverage and accuracy, prior work has adopted lookahead mechanisms [9], [12], [13].###Moreover, SPP outperforms recent, best of class, lookahead and non-lookahead prefetchers [8], [9], [10], including the winner of the most recent data prefetching competition, by 6.###Since lbm has a variety of memory access patterns [9], deeper prefetching with a simple delta predictor wastes bandwidth, and pollutes 978-1-5090-3508-3/16/$31.###Previously proposed history-based techniques [4], [9] address this by making predictions using the first offset in a page, or with very short delta histories.###To avoid over-prefetching, existing lookahead prefetchers [12], [9] globally and statically limit the depth to which lookahead is pursued ahead of the current demand access stream.###This recursion allows lookahead prefetchers [9], [12] to prefetch far ahead of the current program execution, and generate timely prefetches for as long as their predictions remain accurate.###attempt to predict complex, irregular access patterns [6], [7], [8], [9], [10].",impact-revealing,highlighting the challenges and advancements in lookahead prefetching techniques
1246,,ff41f8e2892f55ccd13ebf0cea85f5e929b12629,Winnowing Ontologies Based on Application Use,,,"###So even though the approach suggested in [20] is good for a general structure-based scaling down of ontologies, it is not suitable for usage-driven ontology winnowing.###Stuckenschmidt and Klein [20] proposed the use of classical clustering algorithms to partition ontologies based on how their class hierarchies are structured.",impact-revealing,Critiquing the suitability of a proposed approach for ontology winnowing
3657,5f58a1b491e011e46ee73247,435bc42450259a22cfba92b40217b8d26f4a7ed5,Adversarial Attack on Large Scale Graph,5bdc316717c44a1f58a06f5b,Heterogeneous Neural Attentive Factorization Machine for Rating Prediction.,"Graph structures are ubiquitous in nature and society, there is a great deal of research interest in studying graph data [5], [6], [7], [8], [9].",other,highlighting the research interest in graph data
1963,,73c49d03465f1f532008ddb2d83c0488d9568411,Monitoring and Evaluation of Substance Abuse Services in South Africa: Implications for Policy and Practice,,,"###This raises concerns about the reliability of data, especially as combining data from urinalysis and self-reports is a substantially more accurate measure of drug use than selfreport data alone (Chermack et al. 2000; Dennis et al. 2003).",impact-revealing,highlighting the importance of combining data sources for accurate drug use measurement
573,5ecbc6199fced0a24b4eefa9,3c31c95870824736ecaba2ba01f2ccab145fd91d,Leveraging Unpaired Text Data for Training End-To-End Speech-to-Intent Systems,5cede0e7da562983788c6b40,Speech Model Pre-training for End-to-End Spoken Language Understanding,"There are many advantages of end-to-end SLU systems [5], the most significant of which is that E2E systems can directly optimize the end goal of intent recognition, without having to perform intermediate tasks like ASR.###Similarly, [5, 12] advocate pre-training an ASR model on a large amount of transcribed speech data to initialize a speech-to-intent model that is then trained on a much smaller training set with both transcripts and intent labels.",impact-revealing,highlighting advantages of end-to-end SLU systems and pre-training methods
442,573697f96e3b12023e6d2f31,97acdfb3d247f8250d865ef8a9169f06e40f138b,EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding,5550414d45ce0a409eb3a0ad,End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results.,"Meanwhile, another line of work [6, 7, 8, 9, 10, 11, 12] has focused on end-to-end ASR, i.",impact-revealing,acknowledge existing research directions in end-to-end ASR
1987,,d3454fcbb380fb4fcc3a5f325b8ca52ec6b79d09,Weakening Forensic Science in Spain: From Expert Evidence to Documentary Evidence *,,,"###Even though in the last two decades forensic statisticians and scientists have emphasized the importance of avoiding fallacies in reporting conclusions and in following the likelihood paradigm (7), and some international forensic institutions such as ENFSI have made efforts in both directions, nowadays there are still offi-###often published prosecution fallacy, continue to be either unknown or misunderstood by some forensic experts, prosecutors, and judges all over the world (7).###falls when forensic experts assessed analytical results, nowadays these ‘‘fallacies’’ are ignored, more or less consciously, in quite a few laboratories of many countries (7,13).###experiments or observations be interpreted as evidence (5)?’’; ‘‘What is the role of forensic experts evaluating evidence (7)?’’, as well as many others, do not have simple answers.",impact-revealing,highlighting ongoing issues in forensic reporting and the need for awareness
3860,5ce3cd34e1cd8e3f7932b9ee,4f9ba5e89a7d23675ca65473ae85e352a6d2c379,Aging-aware Lifetime Enhancement for Memristor-based Neuromorphic Computing,5b3d988917c44a510f7fcc01,Long Live TIME: Improving Lifetime for Training-In-Memory Engines by Structured Gradient Sparsification,"Furthermore, the swapping method in [12] uses rows of memristors that are slightly aged to replace the rows that are heavily aged to extend the lifetime of the whole crossbar.",other,providing context for a method
440,5eccb534e06a4c1b26a8358b,8a8a5f327ead63fa56d72e8e75a647e3e6154bc8,Residual Feature Aggregation Network for Image Super-Resolution,59ae3be32bbe271c4c71ba2e,Enhanced Deep Residual Networks for Single Image Super-Resolution,"Here we introduce one of the basic architecture used by some state-of-the-art methods [18, 40, 38, 3].###Following these pioneering works, many CNN-based methods have been proposed and achieved state-of-the-art results in SISR [15, 18, 40, 38, 3, 17, 7, 9, 39].###3(Left) depicts a basic residual module used in EDSR [18] and ESRGAN [31].###residual scaling in EDSR [18]).###In this section, we investigate the combination of our RFA framework with the basic residual block used in EDSR [18].###9740 EDSR [18] ×2 38.###ness of our RFANet, we compare RFANet with 12 stateof-the-art image SR methods: SRCNN [4], FSRCNN [5], VDSR [13], LapSRN [15], MemNet [25], EDSR [18], SRMD [36], NLRN [19], DBPN [6], RDN [40], RCAN [38] and SAN [3].###proposed a very deep and wide network EDSR [18] by stacking modified residual blocks in which the batch normalization (BN) layers are removed.",impact-revealing,acknowledge foundational architecture in state-of-the-art methods
3349,5fae6dced4150a363cec41f7,71c97e7d4a529a21f32d98aad72fbbf32c9ee32e,Anomaly-Based Intrusion Detection From Network Flow Features Using Variational Autoencoder,548901c445ce471f909f7f22,Fast Training Support Vector Machines Using Parallel Sequential Minimal Optimization,The best machine is discovered by utilizing a mathematical optimization method [45].,other,reporting a method for discovering the best machine
607,53e997ddb7602d9701fd5474,7aca628a75775530a5b946900af827890a4208de,"A PPM-like, Tag-based Predictor",53e998cdb7602d97021066c4,Data compression using adaptive coding and partial string matching,"5 is a global-history based predictor derived from PPM. PPM was originally introduced for text compression [1], and it was used in [2] for branch prediction.",impact-revealing,providing context on the application of PPM in text compression and branch prediction
3579,599c7959601a182cd2633b3e,c0c0990b84a350d5efde8d3b2cb2636b6b57c21c,On Sampling Strategies for Neural Network-based Collaborative Filtering,5550416845ce0a409eb3b00b,Collaborative Deep Learning for Recommender Systems,"As pointed out in [32], the precision is not a suitable performance measure since non interactions may be due to (1) the user is not interested in the item, or (2) the user does not pay attention to its existence.###In this work, we use the text recommendation task [1, 4, 31, 32] as an illustrative application for the proposed framework.###Hence, it is natural to combine deep learning with traditional collaborative filtering for recommendation tasks, as seen in recent studies [1, 4, 32, 37].###In [1, 30, 32], mean square loss (MSE) has been applied where “negative terms” are weighted less.###To address the issue and improve performance, hybrid methods are proposed to incorporate side information [5, 7, 25, 28, 38], as well as content information [4, 11, 31, 32].###[32] adopts autoencoder for extracting item-side text information for article recommendation, [1] adopts RNN/GRU to better understand the text content.###To leverage the extraordinary feature extraction or content understanding abilities of DNNs for recommender systems, recent efforts are made in combining collaborative filtering and neural networks [1, 4, 30, 32].###For recommendation performance, we follow [1, 32] and use recall@M.",other,acknowledge existing methods and performance measures in recommendation tasks
2336,53e9aa73b7602d97033eb1bd,e958a6795a040a2df061a773ac844a9a85367cd5,an analytical approach for network-on-chip performance analysis,53e9b304b7602d9703dc713d,Computation and communication refinement for multiprocessor SoC design: A system-level perspective,", bus, point-to-point (P2P)], buffer space, and so on [19].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1889,,ba09d2e493ef31232d5be7c4488eea648f101af3,“Shouting into the Void”: Experiences of Virtual Sex Educators during the COVID-19 Pandemic,,,"###…(2015) examined five approaches to translating in-person sexuality education activities to virtual platforms, focused on the COI (community of inquiry) pedagogical theoretical framework introduced by Garrison et al. (1999) and elaborated by Garrison (2011) that builds on a constructivist approach.###(2015) examined five approaches to translating in-person sexuality education activities to virtual platforms, focused on the COI (community of inquiry) pedagogical theoretical framework introduced by Garrison et al. (1999) and elaborated by Garrison (2011) that builds on a constructivist approach.",impact-revealing,providing context on pedagogical frameworks in sexuality education
3985,5a73cb4d17c44a0b3035672d,7cd5d7f8295b219b029a4231ae5cffb261e00ebe,Early Active Learning with Pairwise Constraint for Person Re-identification,53e99a67b7602d97022d36ab,Margin Based Active Learning,"They include uncertainty sampling methods [11,6,1,22] query by committee methods [21,3].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3248,5f1ff7ea91e011d50a621ab3,39b45b0d60807296db97d9eb37b1fb99d244e2ed,MACU-Net for Semantic Segmentation of,5ce2d003ced107d4c63320a8,Joint Deep Learning for land cover and land use classification,"emantic segmentation using remotely sensed images plays a critical role in a wide range of applications, such as land resource management, yield estimation, and economic assessment [1-3].",other,highlighting the importance of semantic segmentation in various applications
2791,5d9ed2d847c8f76646f797b7,c3715947bbbf648dcf29a1aa4b35cfb68044f919,GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification,5c04966a17c44a2c74708670,Combining Fact Extraction and Verification with Neural Semantic Matching   Networks,"…Malon (2018) ﬁne-tunes the generative pre-training transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modiﬁcation of ESIM and achieves the best results in the…###Nie et al. (2019) propose a two-hop evidence enhancement method which improves 0.08% on their ﬁnal FEVER score.###Nie et al. (2019) propose a two-hop evidence enhancement method which improves 0.###The UNC NLP team (Nie et al., 2019) proposes the neural semantic matching network and uses the model jointly to solve all three subtasks.###Nie et al. (2019); Yoneda et al. (2018) and Hanselowski et al. (2018) have achieved the top three results among 23 teams.",other,reporting prior findings and methods in evidence enhancement
162,5a9cb60d17c44a376ffb3c6d,75a927501749c2cbc0e19a58f798f04de59df64a,Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding,5550412045ce0a409eb38b4c,Convolutional Neural Networks for Sentence Classification.,"Similar to [12], we regard this embedding matrix as the “image” of the L items in the latent space and search for sequential patterns as local features of this “image” using various convolutional filters.###Borrows the idea of using CNN in text classification [12], our approach regards the L × d matrix E as the “image” of the previous L items in the latent space and regard sequential patterns as local features of this “image”.###This model leverages the recent success of convolution filters of Convolutional Neural Network (CNN) to capture local features for image recognition [11, 16] and natural language processing [12].###Our approach leverages the recent success of convolution filters of CNN in capturing local features for image recognition [11, 16] and natural language processing [12].",impact-revealing,drawing parallels between CNN applications in image recognition and NLP
2628,5bdc315017c44a1f58a05e7e,7a71941e60894ae7e1f5af8e79c37cec6cd6c6ad,Multi-scale Residual Network for Image Super-Resolution,5550441b45ce0a409eb4b4d1,A Plus : Adjusted Anchored Neighborhood Regression For Fast Super-Resolution,"3 Comparisons with State-of-the-art Methods We compare our model with 10 state-of-the-art SR methods, including Bicubic, A+ [23], SelfExSR [20], SRCNN [1], ESPCN [2], FSRCNN [3], VDSR [4], DRCN [5], LapSRN [6] and EDSR [9].",other,reporting comparisons with state-of-the-art methods
565,58437722ac44360f1082f160,8aa3358a34a17abd0a65622aad8c85317b851af4,very deep convolutional networks for end-to-end speech recognition,5550417d45ce0a409eb3bc08,Going Deeper With Convolutions,"Recently, very deep CNNs architectures [15] have also been shown to be successful in ASR [16, 17], using more non-linearities, but fewer parameters.###NiN has seen great success in computer vision, building very deep models[18].###We are driven by same motivation that led to the success of very deep networks in vision [14, 18, 21, 23] – add depth of processing using more non-linearities and expressive power, while keeping the number of parameters manageable, in effect increasing the amount of computation per parameter.###CNNs have shown improvement over traditional fully-connected deep neural networks on many ASR tasks [14, 12], we investigate the effect of convolutional layers in seq2seq models.###We explored very deep CNNs for end-to-end speech recognition.###Moreover, strided convolutions are an essential element of CNNs.###While very deep CNNs have been successfully applied to ASR, recently there have been several advancements in the computer vision community on very deep CNNs [15, 18] that have not been 1.###Unlike fully connected layers, Convolutional Neural Networks (CNNs) take into account the input topology, and are designed to reduce translational variance by using weight sharing with convolutional ﬁlters.###The standard formulation of BN for CNNs can be readily applied to DNN acoustic models and cross-entropy training.###Our strategy to alleviate this problem is to apply striding in the ﬁrst and second layer of the CNNs to reduce the time dimensionality and memory footprint.###In our work, we replace Listen with a network of very deep CNNs and BLSTMs.###Unlike Deep Neural Networks (DNNs) [13], CNNs explicitly exploit structural locality in the spectral feature space.###Convolutional Neural Networks (CNNs) [9] have been successfully applied to many ASR tasks [10, 11, 12].###CNNs use shared weight ﬁlters and pooling to give the model better spectral and temporal invariance properties, thus typically yield better generalized and more robust models compared to DNNs [14].###While very deep CNNs have been successfully applied to ASR, recently there have been several advancements in the computer vision community on very deep CNNs [14, 18] that have not been",impact-revealing,highlighting advancements in very deep CNNs for ASR and their potential benefits
415,5a4aef9e17c44a2190f7a8b8,ff772950f66ac6a57f4201ce1f02f0013ccdc1bb,Receptive Field Block Net for Accurate and Fast Object Detection,599c797b601a182cd2642e7b,Rethinking Atrous Convolution for Semantic Image Segmentation.,"Actually, there exist several studies that discuss RFs in CNN, and the most related ones are GoogleNet and its variants [34, 35, 33], Dilated Convolution [3], and Deformable CNN [4].###This design has rapidly proved competent at semantic segmentation [3], and has also been adopted in some reputable object detectors, such as SSD [24] and R-FCN [19], to elevate speed or/and accuracy.###Block architecture: We also compare our RFB to Deformable CNN [4], Inception [34], Dilated Convolution [3], ResNet [13], ResNext [40], and several RFB-like modules with special settings.###A similar idea appears in [3], where an Atrous Spatial Pyramid Pooling (ASPP) is exploited to capture multi-scale information and four parallel atrous convolutions with different atrous rates are applied on the top feature map.",impact-revealing,acknowledge related studies and their contributions to semantic segmentation
1514,,cd94694bb8e3a4a4afa1fb6aabf01c886a0b582d,A virtual screening procedure combining pharmacophore filtering and molecular docking with the LIE method,,,"###Docking, which involves a simulation of binding of all molecules in a database to the actual or potential binding site of a target protein, may be the most prominently used tool in computational drug discovery studies.(37,38) Since docking calculations simulate the interactions between a ligand and a protein’s binding site, and assign a qualitative score to these interactions, the results may be compared to those of biochemical assays.###As for the conformational space search algorithms, a variety of methods are commonly used for scoring in molecular docking and they can be roughly categorized as empirical, force-field based and knowledge based approaches.(37,38) While some scoring functions employ a single approach, some scoring functions use combinations of different approaches.",impact-revealing,providing context on molecular docking in drug discovery
1186,,cb3a9ab834a979379259e8406c57551a6b6919eb,A Data Mining Framework for Glaucoma Decision Support Based on Optic Nerve Image Analysis Using Machine Learning Methods,,,"###…agglomerative clustering, were deemed unsuitable for our purpose since it does not provide a topological visualization of the clusters (rather it offers a clustering tree — i.e., dendro-gram), is not suitable for large datasets given higher computation time, and is not robust toward outliers [80].###Therefore, we used a distribution-based clustering (Gaussian mixture models using the Expectation-Maximization algorithm), where each new observation is assigned to the cluster (mixture component) corresponding to the highest posterior probability [80].",impact-revealing,providing context for the choice of clustering method
3210,5f75aa6a9fced0a24b64599d,69fb130409d48478fab0c4545dc2ff102a2630ef,The forward slice core microarchitecture,53e9bc97b7602d9704918095,"""Flea-Flicker""* Multipass Pipelining: An Alternative To The High-Power Out-Of-Order Offense","Proposals such as speculative-slice execution [23], flea-flicker multi-pass pipelining [3], braid processing [22] and OUTRIDER [6] also exploit critical instruction slices [24] for improving performance.",other,reporting prior findings and applications
66,5ee9f15b91e01152af022ce0,bf2174c69f84f4e57813e0bed4571c6dbff123ed,Automatic Validation of Textual Attribute Values in E-commerce Catalog by Learning with Limited Labeled Data,53e9a301b7602d9702c0cf92,Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation.,"To enable distribution qφc (z |x q c ) differentiable, we follow previous work [2, 3, 19] to use reparameterization trick to parameterize z.",impact-revealing,reporting a method used for parameterization
3051,58d82fced649053542fd729f,fddc32f3880688238847077fd927ab3025db7a6a,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,58437777ac44360f10840253,Learning High-Order Filters For Efficient Blind Deconvolution Of Document Photographs,", optical character recognition performance has been utilized to compare the quality of text deblurring algorithms [21, 55] and face-detection performance has been used for the evaluation of super-resolution algorithms [30].",other,acknowledge applications of performance comparisons in different domains
2958,5d9ed30647c8f76646f7f04c,f160c69c428122e8fa7ba96f220b4ded5f8761f4,ARNOR: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification,5c2c7a9217c44a4e7cf313af,A Tutorial on Deep Latent Variable Models of Natural Language,"In the future, we hope to improve our work by the utilization of better model-based pattern extractor, and resorting to latent variable model (Kim et al., 2018) for jointly modeling instance selector.",other,suggesting future improvements in model-based pattern extraction
3030,5ede0553e06a4c1b26a8419c,1f3c381eedfe8914b81e93070bfdb00cf86ac943,Contrastive Multi-View Representation Learning on Graphs,53e99acab7602d970234a328,Weisfeiler-Lehman Graph Kernels,"Graph kernels (Borgwardt & Kriegel, 2005; Shervashidze et al., 2009; 2011; Yanardag & Vishwana, 2015; Kondor & Pan, 2016; Kriege et al., 2016) decompose graphs into sub-structures and use kernel functions to measure graph similarity between them.###, 2009), Weisfeiler-Lehman sub-tree kernel (WL) (Shervashidze et al., 2011), deep graph kernels (DGK) (Yanardag & Vishwana, 2015), and multi-scale Laplacian kernel (MLG) (Kondor & Pan, 2016) reported in (Sun et al.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3509,555048d145ce0a409eb71b05,df787a974fff59f557ed1ec620fc345568aec491,learning deep representations for graph clustering,53e9ac95b7602d970366ced6,Sparse coding with an overcomplete basis set: A strategy employed by V1?,"Furthermore, there are works on sparse autoencoder, which aims to penalize the large hidden layer outputs (Olshausen and Field 1997)(Boureau, Cun, and others 2007).",other,acknowledge existing research on sparse autoencoders
2308,5c2c7a9217c44a4e7cf3189c,9ae43e25b04f5c35173b0bf490612015bd86c08f,Face-Focused Cross-Stream Network for Deception Detection in Videos,599c7bb2601a182cd2758316,Large-scale Affective Content Analysis: Combining Media Content Features and Facial Reactions,"With the recent rapid development of human-centric AI, human-centric video analysis [48, 49, 54, 30, 32, 61, 27] has also begun to draw much attention from the computer vision community.",other,highlighting the growing interest in human-centric video analysis
1648,,cacf7f1e9db3f3e056eaa57abc33c4d71419f924,A Tormenting Dilemma: American Identity and Attitudes Towards Torture,,,"###…intergroup categorization and affective processes lead to in-group favoritism and hostility towards out-groups (Brewer 2001; Stephan & Stephan 2000; Tajfel & Turner 1979; 1986; Tajfel 1981) - with clear attitudinal consequences, including the development of political tribalism and affective…###As such, in line with Social Identity Theory (Tajfel 1981; Tajfel & Turner 1979, 1986; Tajfel et al., 1971), attachment to the American identity should lead to increasingly hostile attitudes towards the threatening out-group; thus, explaining to a certain extent the favorable attitudes on torture.###In line with Social Identity Theory, these ‘threatening individuals’ can be conceived as an out-group (Tajfel and Turner 1979; 1986).###Social identification has been recognized as being a key element in intergroup behavior (Tajfel et al., 1971; Tajfel & Turner 1979).###According to Social Identity Theory, at the heart of social identity is the process of social categorization, which distinguishes individuals into groups with differing meanings and values (Tajfel 1981; Tajfel & Turner 1979; Tajfel et al., 1971).###Social Identity Theory (Tajfel and Turner 1979, 1986; Tajfel et al., 1971) is theoretically extended by applying Brewer’s (2001) assumption that out-group hostility is expressed in an aggressive fashion only in the presence of a perceived threat to the values of the in-group.",impact-revealing,highlighting the theoretical framework of Social Identity Theory and its implications for intergroup behavior
2880,5ecb57199e795ec6f2ba59cc,512f34906ddaefe885af2e5eec9b2b3b50ffd377,deep entity matching with pre-trained language models,53e9b6d0b7602d970425997c,Evaluation of entity resolution approaches on real-world match problems,"We present the experiment results on benchmark datasets for EM: the ER Benchmark datasets [21], the Magellan datasets [20] and the WDC product data corpus [31].###These datasets are from the ER Benchmark datasets [21] and theMagellan data repository [10].###• Weevaluated the effectiveness ofDitto on three benchmark datasets: the Entity Resolution benchmark [21], the Magellan dataset [20], and the WDC product matching dataset [31] of various sizes and domains.",other,reporting experiment results on benchmark datasets
659,573698486e3b12023e711478,c2fd72cb2a77941e655b5d949d0d59b01e173c3b,grarep: learning graph representations with global structural information,5736977f6e3b12023e66632b,LINE: Large-scale Information Network Embedding,"[25] later proposed a large-scale information network embedding, which optimizes a loss function where both 1-step and 2-step relational information can be captured in the learning process.###LINE [25].###According to [25], LINE yielded better results when the learned graph representations are L2 normalized, while DeepWalk and E-SGNS can achieve optimal performance without normalization.###As suggested in [25], for LINE, we set the mini-batch size of stochastic gradient descent (SGD) as 1, learning rate of starting value as 0.025, the number of negative samples as 5, and the total number of samples as 10 billion.###For LINE, the reconstruction strategy does help, since it can capture additional structural information of the graph beyond 1-step and 2-step local information.###LINE is a recently proposed method for learning graph representations on large-scale information networks.###For LINE and GraRep, the boundaries of each group become much clearer, with vertices of different colors appearing in clearly distinguishable regions.###As suggested in [25], for LINE, we set the mini-batch size of stochastic gradient descent (SGD) as 1, learning rate of starting value as 0.###It is worth mentioning that GraRep and LINE can achieve good performance with a small graph.###LINE defines a loss function based on 1-step and 2-step relational information between vertices.###The results for GraRep appear to be better, with clearer boundaries for each regions as compared to LINE.###For LINE, we employ the reconstruction strategy proposed by their work by adding neighbors of neighbors as additional neighbors to improve performance.###Besides, for tasks with more labels, such as 9NG, GraRep and LINE can provide better performances than other methods, with GraRep providing the best.###LINE will get the best performance, if concatenating the representation of 1- step and 2-step relational information and tuning the threshold of maximum number of vertices.###As suggested in [25], we set k-max as 0, 200, 500 and 1000, respectively, and we report the best performance with kmax=500.###Another recently proposed work is LINE [25], which has a loss function to capture both 1-step and 2-step local relational information.###For a fair comparison, the dimension d of representations is set as 128 for Blogcatalog network and DBLP network as used in [25] and is set as 64 for 20-NewsGroup network as used in [29].###Due to limited space, we do not report the detailed results with d=128 and d=256 for all baseline methods, as well as k-max=500, 1000 and without reconstruction strategy for LINE, since these results are no better than the results presented in Table 3.",impact-revealing,reporting findings and details on the LINE method for learning graph representations
2508,5dbebb7447c8f766462c22a6,e1fd81af050dbdc4232ff8b1ab71cf0973d530b6,graph convolutional networks with motif-based attention,5736986c6e3b12023e7308a0,Convolutional Networks on Graphs for Learning Molecular Fingerprints,"Some work introduced architectures tailored for more specific problem domains [9, 23] – like NeuralFPS [9] which is an end-to-end differentiable deep architecture which generalizes the well-known Weisfeiler-Lehman algorithm for molecular graphs – while others defined graph convolutions based on…",other,acknowledge existing architectures tailored for specific problem domains
3021,5d79a6ff3a55ac5b650357fb,6fb4facc2d16c76047f7cd96af5a691ab16c08c5,Combining Prefetch Control and Cache Partitioning to Improve Multicore Performance,56d8ee11dabfae2eee63378f,To hardware prefetch or not to prefetch?: a virtualized environment study and core binding approach,[31] studies the effect of hardware prefetching in virtualized environments.,other,reporting prior findings
2090,,6f8e9aa5ba67d6a8907af1943de3573123f22091,Bandwidth allocation in hexagonal wireless sensor networks for real-time communications,,,"###This MAC protocol is inspired by the timed token protocol [3, 9, 5, 4, 1].",impact-revealing,highlighting inspiration from existing protocols
369,599c7987601a182cd2648373,204e3073870fae3d05bcbc2f6a8e263d9b72e776,Attention Is All You Need,58d82fced649053542fd6acb,Structured Attention Networks.,"Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16].",impact-revealing,highlighting the significance of attention mechanisms in sequence modeling
2690,5e9ef9b69fced0a24b1b65a2,69f1ab7fd22c3df3c9900430566be890e1529b4e,NetTaxo: Automated Topic Taxonomy Construction from Text-Rich Network,53e9bb29b7602d97047684e8,Clustering on the Unit Hypersphere using von Mises-Fisher Distributions,"Consider that cosine similarity between term embedding has demonstrated its effectiveness in term similarity search [17], we apply vMF mixture clustering [3] in NetTaxo.###The number of mixtures k for vMF mixture clustering is manually selected by incrementally increasing k by 1 in the range of [3, 6] until coherent clusters are observed.",other,describing the application of cosine similarity in term similarity search
1688,,1476fc3d21f714d0d9509180cc93913e5a475f6f,Identity matters: exploring supply chain sustainability with a social identity perspective,,,"###As a result, they see themselves as categorically interchangeable with other in-group members; they influence, and are influenced by, in-group members; and they enhance self-esteem by working collaboratively in shared norms towards shared in-group goals (Haslam, 2004, Tajfel & Turner, 1979).###…these questions is conducted through the lens of social identity theory which has been widely used to look at inter-group relations (Tajfel, 2010; Tajfel & Turner, 1979; Turner, 1987); organisational behaviours (Ashforth & Mael, 1989; Hogg & Terry, 2000; Haslam, 2004) and inter-organisational…###Social identity activities and intergroup behaviours are motivated by needs for self-enhancement and uncertainty reduction (Ashforth & Mael, 1989; Grant & Hogg, 2011; Hogg, 2009; Hogg & Mullin, 1999; Tajfel & Turner, 1979).###Identification with a social group/category is linked to one’s value and emotional salience with this social group/category (Hogg, 2001; Tajfel & Turner, 1979).###There are several determinants of social identity salience: the prestige of
the group, the distinctness of the group, the continuity of the prototype, and the group formation (Tajfel & Turner, 1979; Turner, 1987).###…and social identity salience normally associated with the following factors: 1) the prestige of the group; 2) the distinctness of the outer group; 3) the continuity of the prototype; 4) prototypicality fit; and 5) group categorization /affiliation (Tajfel & Turner, 1979; Turner, 1987).###An identity is a component of the self-concept/definition (Tajfel & Turner, 1979), which is comprised of a personal identity and a social identity.###The broad social identity theory consists of two related parts: social identity theory (Tajfel & Turner, 1979) and
self-categorization theory (Turner, 1987).###2.2.1 Introduction to the social identity approach
Social identity theory (Tajfel & Turner, 1979) is one of the most influential theories in contemporary social psychology and is widely used in organisations (Haslam, 2004).",impact-revealing,highlighting the significance and influence of social identity theory in various fields
3512,5a73cbcc17c44a0b3035f34a,04957e40d47ca89d38653e97f728883c0ad26e5d,Cascade R-CNN: Delving Into High Quality Object Detection,558c7303e4b00c3c48e2594d,Cascaded pose regression,"The difficult regression task can be decomposed into a sequence of simpler steps, inspired by the works of cascade pose regression [6] and face alignment [2, 35].",other,drawing inspiration from existing methods for task decomposition
651,5aed14d117c44a4438158a8d,36db98d8604df17e7ab68952c6f7a6cb70cff2e7,a deep learning approach for multimodal deception detection,53e9a8f9b7602d970324a2f9,"Recent developments in openSMILE, the munich open-source multimedia feature extractor",Audio Feature Extraction openSMILE [22] is an open-source toolkit used to extract high dimensional features from an audio ﬁle.,impact-revealing,reporting a tool used for audio feature extraction
2534,57a4e91dac44365e35c9886f,52b2bca872dac6cd708d36c009daa6f90db371be,A Soft Processor Overlay with Tightly-coupled FPGA Accelerator,57a4e91dac44365e35c98531,GRVI Phalanx: A Massively Parallel RISC-V FPGA Accelerator Accelerator,"Moreover, there exists some lightweight RV32I designs such as zscale [28], GRVI [29] or ORCA [6] which are similar to the proposed processor.###Compared with GRVI, the proposed processor puts emphasis on design portability and compatibility as well as architectural extension to the tightly-coupled accelerator.###GRVI [29] core, on the other hand, is an efficient, FPGAoptimized 3-stage pipeline implementation that is specifically designed for Phalanx framework.",other,highlighting similarities and differences between processor designs
2954,5d3ed25a275ded87f97deb36,37f7eaf57b88ef03efd4196f2f57ae0b4c657a79,Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommendation,53e99991b7602d97021d5b2b,Video suggestion and discovery for youtube: taking random walks through the view graph.,"Based on different settings of edge weights in the input graph, these methods are classified as: (1) Edge weights are assumed to be given as input and therefore fixed [1, 37, 38]; (2) Edge weights are parameterized and therefore learnable [10, 21, 35].",other,providing context on classification of methods based on edge weights
2356,5f7af09591e011983cc81efc,87b008a6289fa22c72e1726a8929e815dfbbc65f,Hard Negative Mixing for Contrastive Learning,5aed14d617c44a44381591f1,Mining on Manifolds: Metric Learning without Labels,"[23] mine hard negatives from a large set by focusing on the features that are neighbors with respect to the Euclidean distance, but not when using a manifold distance defined over the nearest neighbor graph.###A few recent works discuss issues around the selection of negatives in contrastive self-supervised learning [4, 11, 23, 45, 47, 22].###Hard negatives are critical for contrastive learning [1, 19, 23, 30, 37, 44, 48].",other,highlighting the importance of hard negatives in contrastive learning
3010,5e524da993d709897cb69fde,09e48adf1a3f012ec56fbb4f92acae975ee43d87,a deep learning approach to antibiotic discovery,55a3e6b865ce5cd7b3bc3929,Targeting bacterial membrane function: an underexploited mechanism for treating persistent infections.,"Moreover, given that cells must maintain an electrochemical transmembrane gradient for viability (Hurdle et al., 2011; Coates and Hu, 2008), dissipation of the proton motive force would result in the death of tolerant cells.",other,highlighting the importance of maintaining electrochemical gradients for cell viability
2634,5d9ed2d847c8f76646f797b7,c3715947bbbf648dcf29a1aa4b35cfb68044f919,GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification,57a4e91aac44365e35c97eb6,Neural Tree Indexers for Text Understanding.,"…to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018).",other,reporting prior findings on state-of-the-art neural models
1933,,af8bf420314e266a185439ef8c90b999f9937bf5,"Kucukkilic, Ezgi and Brookes, Keeley and Barber, Imelda and Guetta-Baranes, Tamar and Morgan, Kevin and Hollox, Edward (2018) Complement receptor 1 gene (CR1) intragenic duplication and risk of Alzheimer’s disease. Human Genetics",,,"###This has highlighted the importance of the immune response to amyloid plaque formation in Alzheimer’s disease, possibly mediated by microglial cells (Efthymiou and Goate 2017; Naj and Schellenberg 2017; Villegas-Llerena et al. 2016).",impact-revealing,highlighting the significance of immune response in Alzheimer's disease
269,5d04e905da56295d08dd7af3,8112e6c7a5284e012485dc534dedc4047e434f3a,Binarized Collaborative Filtering with Distilling Graph Convolutional Networks,58437767ac44360f1083db82,Binary Optimized Hashing,"To this end, inspired by [Dai et al. , 2016], we transform the binary optimization problem to an equivalent continuous optimization problem by imposing a stochastic penalty term.",impact-revealing,describing a methodological transformation inspired by prior work
3083,5c8d57d74895d9cbc653a9e9,3afa826a594d90ee0f4fe062c988289bb213a114,Deep Back-Projection Networks For Super-Resolution,58d82fced649053542fd7254,Learning from Simulated and Unsupervised Images through Adversarial   Training,"Significant progress in deep learning for vision [15, 13, 5, 39, 26, 33, 17] has recently been propagating to the field of super-resolution (SR) [19, 29, 6, 12, 20, 21, 24, 42].",other,highlighting the influence of deep learning advancements on super-resolution
1152,,efc5cffe9aa94f843415378c00a0d136ad3ea2ec,Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing,,,"###DDIMs (Song et al., 2020) accelerate the reverse process of pretrained DDPMs, which are also faced with the inconsistency problem that exists in DDPMs.###4 (Rombach et al., 2022) as the base text-to-image diffusion model, and fuse the attention maps in DDIM inversion (Song et al., 2020) and sampling processes for retaining both structural and motion information.###For example, Tune-A-Video (Wu et al., 2022) employs DDIM (Song et al., 2020) inversion to provide structural guidance for sampling, and proposes efficient attention tuning for improving temporal consistency.###Text-to-video diffusion models mainly build upon pretrained DDPMs, and extend them with designed temporal modules ( e.g. , spatio-temporal attention) and DDIM Song et al. (2020) inversion for both temporal and structural consistency (Wu et al., 2022; Qi et al., 2023).",impact-revealing,reporting on the use of DDIMs in text-to-video diffusion models and their improvements
1173,,e3c721a524313e39a63303103eb18ad9d1e58d8a,Developing a Powerful and Resilient Smart Body Sensor Network through Hypercube Interconnection,,,"###The communication standard connected with the outside smarter planet relies on the particular accomplishment of the gateway services within the smart device through the wireless local area network or additional wireless technologies [8, 9].###(1) Tier 1: the intelligent sensor nodes (ISNs) or biosensors [8] are presented in white circles.###Throughout this process, the smart device can integrate two different types of network structures within the Smart BodyNet and the outside smarter environment [7, 8].###Additionally, smart phone interoperability, portability, mobility, and intelligent sensors nodes capabilities can be easily extended by using the Smart BodyNet [3, 8, 11].",impact-revealing,providing context on smart device communication standards and network structures
267,5f0277e911dc830562231dab,6deae79dec438eaaa524bca3b82c6b8d93553b20,A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data,58d82fcbd649053542fd658e,Related Pins at Pinterest: The Evolution of a Real-World Recommender System,"In a recent work [16], it is shown that a uniform (i.###Moreover, a uniform data is useful for counterfactual learning, such as imputation model learning [33], propensity computation [24] and modeling with uniform data directly [5, 11, 16, 23].###Recommender Systems as a feedback loop system may suffer from the bias problems such as popularity bias [1, 6], previous model bias [9, 16, 17] and position bias [3, 28].###A recent work has shown that a uniform data can alleviate the previous model bias problem [16].###Previous studies have shown that models and evaluation metrics that ignore the biases do not reflect the true performance of a recommender system, and that explicitly handling of the biases may help improve the performance [16, 28, 31].###Along the line of [16], we conduct empirical studies on a real advertising system and a Session 5B: Learning for Recommendation SIGIR ’20, July 25–30, 2020, Virtual Event, China",impact-revealing,highlighting the importance of uniform data in addressing bias problems in recommender systems
1024,,f8ebb1c77e0847c672c27fc387f115e2ea6a616c,Relational Stressors and Depressive Symptoms in Late Adolescence: Rejection Sensitivity as a Vulnerability,,,"###Consistent with diathesisstress models of depression (e.g., Caspi et al. 2003; Metalsky and Joiner 1992), neither relational stressors nor rejection sensitivity may trigger depression in all individuals; however, the combination of the two substantially increased risk for developing depressive symptoms.###Such models propose that individuals at risk for depression have attributional biases and negative self and/or interpersonal schemas that make them vulnerable to interpreting life events negatively (Lewinsohn et al. 2001; Metalsky and Joiner 1992; Safran 1990).###Consistent with diathesisstress models of depression (e.g., Caspi et al. 2003; Metalsky and Joiner 1992), neither relational stressors nor rejection sensitivity may trigger depression in all individuals ; however, the combination of the two substantially increased risk for developing depressive…###Overall, findings build on previous diathesis stress models of depression (Abramson et al. 1989; Metalsky and Joiner 1992), by suggesting that rejection sensitivity represents a specific cognitive-affective diathesis that, when combined with relational stressors that inhibit the development of…###Overall, findings build on previous diathesis stress models of depression (Abramson et al. 1989; Metalsky and Joiner 1992), by suggesting that rejection sensitivity represents a specific cognitive-affective diathesis that, when combined with relational stressors that inhibit the development of autonomy and relatedness within close relationships, creates a high level of risk for late adolescent depressive symptoms.",impact-revealing,highlighting the contribution of diathesis-stress models to understanding depression
850,5a4aef9e17c44a2190f7a6fb,af03709f0893a7ff1c2656b73249d60030bab996,NISP: Pruning Networks Using Neuron Importance Score Propagation,53e9aed7b7602d97039011d1,Predicting Parameters in Deep Learning.,"In [6] the redundancy in the parameterization of deep learning models has been studied and demonstrated.###Recent studies have investigated the significant redundancy in deep networks [6] and reduced the number of neurons and filters [3, 13, 22, 26] by pruning the unimportant ones.",impact-revealing,highlighting research on redundancy in deep learning models
2576,555044e045ce0a409eb518a7,0a07a56ed17c6541e490df16f6381073494d0058,An online auction framework for dynamic resource provisioning in cloud computing,53e99b9bb7602d9702446fbf,Network Aware Resource Allocation In Distributed Clouds,[23] consider network load when allocating VMs in a distributed cloud system.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3257,5e85c28491e0114016e821dc,2db6b9621862b4293c1433c91ee8fb45e394f376,DeepGS: Deep Representation Learning of Graphs and Sequences for Drug-Target Binding Affinity Prediction,5b67b46417c44aac1c861228,Interpretable Drug Target Prediction Using Deep Neural Representation.,[10] proposed a neural model for drug-target prediction and used a two-way attention mechanism to provide biological interpretation of the prediction.,other,reporting prior findings on drug-target prediction
2900,5bdc31b817c44a1f58a0c039,abfa95058fa50c55a0b923a6c35830f470c125ad,Adaptive sampling towards fast graph representation learning,5a9cb66717c44a376ffb8667,FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling,"For FastGCN, we directly make use of the provided results by [20].###More recently, two kinds of sampling-based methods including GraphSAGE [3] and FastGCN [20] were developed for fast representation learning on graphs.###The codes of GraphSAGE [3] and FastGCNN [20] provided by the authors are implemented inconsistently; here we re-implement them based on our framework to make the comparisons more fair.###We contrast our approach with GraphSAGE [3] and FastGCN [20] regarding the following aspects:###Following the supervised learning scenario in FastGCN [20], we use all labels of the training examples for training.###For FastGCN, we adopt the Independent-Identical-Distribution (IID) sampler proposed by [20] in Eq.",other,acknowledge existing methods and their implementations
2156,,aaa7cec92a9374a35174eaaea0a92386a2388fb8,Composants pour la grille,,,"###This layer contains high-level programming abstractions facilitating interaction between application and middleware, such as the GGF’s initiative called SAGA [GOO , GGF04] which is inspired by the Grid Application Toolkit (GAT) [ALL 05], and Globus-COG [LAS 01] which enhances the capabilities of the Globus Toolkit by providing workflows, control flows and task management at a high level of abstraction.",impact-revealing,providing context on high-level programming abstractions in middleware
3020,5eb789d3da5629cf24430b41,1739466ac1411788cf1de60a3a6b59d739dc41ff,Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder,599c796f601a182cd263cd58,Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection   Methods,"Some approaches focus on optimizing attack objective function like Carlini & Wagner attack (C&W) and DeepFool [5, 20], while others utilize the decision boundary to attack the network [2, 7].###As for white-box attacks, we adopt following attacks: L∞-FGSM, L∞-PGD, L∞C&W, L2-DeepFool and L2-C&W.",other,describing different approaches to adversarial attacks
1782,,aefdfa15dc08ef7c6e03008b3dec7f5b24c7aa28,PHARMACODYNAMIC AND PHARMACOKINETIC INTERACTIONS OF HERBS WITH PRESCRIBED DRUGS: A REVIEW,,,"###It is widely used as anti-oxidant and enhances absorption of various drugs such as tetracycline and phenytoin (Srinivasan, 2007).",impact-revealing,highlighting the significance of a substance's properties in drug absorption
1058,,c9eba8da86c21df1779f2b1458b9f8e5228b090c,PromSec: Prompt Optimization for Secure Generation of Functional Source Code with Large Language Models (LLMs),,,"###ASTs provide a hierarchical and structured representation of the syntactic elements within a program, enabling in-depth examination of code syntax and organization [67].###Software engineering has significantly benefited from ML and deep learning advancements, with ASTs commonly used as input data [67].",impact-revealing,highlighting the significance of ASTs in software engineering and their role in code analysis
3498,5e5e190b93d709897ce4997e,cb4571fa905abb70868d0bb9d4681f0a612c2d0f,Differentiable Reasoning On Large Knowledge Bases And Natural Language,5736986c6e3b12023e73097a,Learning to Transduce with Unbounded Memory,"A line of research consists of enriching neural network architectures with a differentiable external memory (Sukhbaatar et al. 2015; Graves, Wayne, and Danihelka 2014; Joulin and Mikolov 2015; Grefenstette et al. 2015; Kaiser and Sutskever 2016).",other,acknowledge existing research on neural network architectures
3790,5e09a801df1a9c0c41680233,1438d8c68b4495947fd5de001b87e6ef5a263a3e,Learning to Jointly Generate and Separate Reflections,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"…and separator exhibit similar structures: a downsampling unit with two convolutional layers to increase the receptive ﬁeld size, a feature extraction unit with 9 residual blocks [8] to extract robust features and an upsampling unit at the last stage with two transposed convolutional layers.",other,describing the architecture of a model
106,5ede0553e06a4c1b26a83f63,1d81e7f428fea2b2e15ee3a96fe843ca603acc4c,Simple and Deep Graph Convolutional Networks,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"• Similar to the motivation of ResNet (He et al., 2016), identity mapping ensures that a deep GCNII model achieves at least the same performance as its shallow version does.###To simulate the skip connection in ResNet (He et al., 2016), (Kipf & Welling, 2017) proposes residual connection that combines the smoothed representation P̃H with H.###To simulate the skip connection in ResNet (He et al., 2016), (Kipf & Welling, 2017) proposes residual connection that combines the smoothed representation ˜ PH ( (cid:96) ) with H ( (cid:96) ) .###ResNet (He et al., 2016) solves a similar problem in computer vision with residual connections , which is effective for training very deep neural networks.###ResNet (He et al., 2016) solves a similar problem in computer vision with residual connections, which is effective for training very deep neural networks.",impact-revealing,drawing parallels between GCNII and ResNet for performance improvement
50,59ae3c262bbe271c4c71ea21,83e7654d545fbbaaf2328df365a781fb67b841b4,Enhanced LSTM for Natural Language Inference,573696d96e3b12023e5d8d36,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,"trees of a premise and hypothesis through treeLSTM (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al.###To this end, we will also encode syntactic parse trees of a premise and hypothesis through tree-LSTM (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al., 2011).",impact-revealing,describing the method of encoding syntactic parse trees
35,5e09a83ddf1a9c0c41685fc3,90820988957e74db0fbb1df79175610e08016ba4,Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling,58d82fc8d649053542fd5be4,Bidirectional Attention Flow for Machine Comprehension.,"Similar to Seo et al. (2017), we perform co-attention from two directions: query-to-context and context-to-query, as follows: where max col is the column-wise max-pooling operator.###Context-aware representation learning, such as co-attention methods (Seo et al., 2017), has been proved effective in many benchmarks.###, 2016) and attention (Seo et al., 2017; Tay et al., 2019b), have been proposed to model semantic similarity using diverse techniques.###Existing state-of-the-art techniques for SM usually comprise three major components: (1) sequential sentence encoders that incorporate word context and sentence order for better sentence representations; (2) interaction and attention mechanisms (Tay et al., 2019b; Seo et al., 2017; Parikh et al., 2016; Conneau et al., 2017; Gong et al., 2018) to emphasize salient word pair interactions; (3) structure modeling (Chen et al.###Various neural network architectures, e.g., Siamese networks (He et al., 2016) and attention (Seo et al., 2017; Tay et al., 2019b), have been proposed to model semantic similarity using diverse techniques.###…that incorporate word context and sentence order for better sentence representations; (2) interaction and attention mechanisms (Tay et al., 2019b; Seo et al., 2017; Parikh et al., 2016; Conneau et al., 2017; Gong et al., 2018) to emphasize salient word pair interactions; (3) structure modeling…",impact-revealing,acknowledge existing co-attention methods and their effectiveness
3148,57a4e92bac44365e35c9ab55,d98063b0eb446c99e98684b34cb53914ca6b7206,a survey of techniques for architecting dram caches,53e9aae6b7602d97034688f7,Exploring Dram Cache Architectures For Cmp Server Platforms,"[39] evaluate latency/area/bandwidth tradeoffs of multiple DRAM cache designs, viz.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
139,5cede10dda562983788ed645,690edf44e8739fd80bdfb76f40c9a4a222f3bba8,BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer,5bbacbad17c44aecc4eb00ee,Self-Attentive Sequential Recommendation,"For sequential recommendation, Kang and McAuley [22] introduce a two-layer Transformer decoder (i.###To model such sequential dynamics in user behaviors, various methods have been proposed to make sequential recommendations based on users’ historical interactions [15, 22, 40].###Other than recurrent neural networks, various deep learning models are also introduced for sequential recommendation [3, 22, 33, 49].###, next item recommendation) task, which has been widely used in [12, 22, 49].###To ensure the quality of the dataset, following the common practice [12, 22, 40, 49], we keep users with at least five feedbacks.###For sequential recommendation, Kang and McAuley [22] introduce a two-layer Transformer decoder (i.e., Transformer language model) called SASRec to capture user’s sequential behaviors and achieve state-of-the-art results on several public datasets.###• Steam4: This is a dataset collected from Steam, a large online video game distribution platform, by Kang andMcAuley [22].###• SASRec [22]: It uses a left-to-right Transformer language model to capture users’ sequential behaviors, and achieves state-of-the-art performance on sequential recommendation.###2Here, following [22, 40], we use the relative time index instead of absolute time index for numbering interaction records.###For dataset preprocessing, we follow the common practice in [22, 40, 49].###For easy and fair evaluation, we follow the common strategy in [12, 22, 49], pairing each ground truth item in the test set with 100 randomly sampled negative items that the user has not interacted with.###• Amazon Beauty3: This is a series of product review datasets crawled from Amazon.com by McAuley et al. [34].",impact-revealing,acknowledge existing methods and datasets in sequential recommendation
1323,,84d8e3fda1f50caabc7be182cfb9130e63c0c8e8,Inequality in Black and White High School Students’ Perceptions of School Support: An Examination of Race in Context,,,"###Mplus software adjusts for missingness using full-information maximum-likelihood (FIML) estimation, which is widely recognized as an appropriate means of handling missing data assumed to be MAR (Schafer and Graham 2002).###Our analyses assumed that data were missing at random (MAR; Arbuckle and Wothke 1999).",impact-revealing,reporting the use of Mplus software for handling missing data
1622,,6695ac488fc328c46fb51e4db190d438fdbd3fce,Spectrum-Based Log Diagnosis,,,"###Our method takes inspiration from Spectrum-Based Fault Localization (SBFL) in which the suspiciousness or fault-proneness of a program statement is treated as a function of the number of times the statement was activated in a failing test case, combined with the number of times it is skipped in a passing test case [6, 7, 8].###Fault Localization: As mentioned, our approach was inspired by Spectrum-Based Fault Localization (SBFL), where the faultproneness of a statement is computed as a function of the number of times that the statement was executed in a failing test case, combined with the number of times that the statement was skipped in a passing test case [6, 7, 8, 14, 17].",impact-revealing,drawing inspiration from Spectrum-Based Fault Localization for fault-proneness assessment
121,5cede0e1da562983788bfe5f,988a378f640eb7fb681f977d6cb1e0c830c07b4c,Adversarial Examples Are a Natural Consequence of Test Error in Noise,53e9a93eb7602d97032928b5,Intriguing properties of neural networks.,"At the same time, they are also sensitive to small, worst-case perturbations of the input, so-called “adversarial examples” (Szegedy et al., 2014).###Since the work of Szegedy et al. (2014), a subﬁeld has focused speciﬁcally on the phenomenon of small adversarial perturbations of the input, or “adversarial examples.”###At the same time, they are also sensitive to small, worst-case perturbations of the input, socalled “adversarial examples” (Szegedy et al., 2014).###Since Szegedy et al. (2014), hundreds of adversarial defense papers have been published.",impact-revealing,highlighting the significance of adversarial examples in the field
2388,5c6a37d03a69b1c9e12a9fc4,81b6d24e8f313fd88b0fe5ff6c21dd154fbe32d2,SimGNN: A Neural Network Approach to Fast Graph Similarity Computation,53e9b991b7602d9704584df0,Graph Based Shapes Representation And Recognition,"to [39]: nGED(G1,G2) = GED(G1,G2) ( |G1 |+ |G2 |)/2 , where |Gi | denotes the number of nodes of Gi .",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
726,5d1eb9d5da562961f0b0fa03,037aeb767ab431eeebc74a0b85df0d2f5641c652,Pre-Training With Whole Word Masking for Chinese BERT,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"We use ofﬁcial create_pretraining_data.py provided by [2] to convert the raw input text to the pre-training examples.###We use Wikipedia dump 3 (as of March 25, 2019), and pre-process with WikiExtractor.py as suggested by [2], resulting in 1,307 extracted ﬁles.###We carried out experiments under TensorFlow framework [39] with slight modiﬁcations to the ﬁne-tuning scripts 5 provided by [2] to better adapt to Chinese tasks.###RoBERTa (Robustly Optimized BERT Pretraining Approach) [13] aims to adopt original BERT architecture but make much more precise modiﬁcations to fully release the power of BERT, which is underestimated in [2].###B ERT [2] has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations.###Phrase-level masking is to mask consecutive words, which is similar to the N-gram masking strategy [2], [14], [19].###BERT (Bidirectional Encoder Representations from Trans-formers) [2] has demonstrated its effectiveness in a wide range of natural language processing tasks.###…ERNIE 2.0 [37], NEZHA [38], we only compare BERT [2], BERT-wwm, BERT-wwm-ext, RoBERTa-wwm-ext, RoBERTa-wwm-ext-large, ELECTRA, along with our MacBERT to ensure relatively fair comparisons among different models, where all models are trained by ourselves except for the original Chinese BERT [2].###…quite different among various existing Chinese pre-trained language models, such as ERNIE [11], ERNIE 2.0 [37], NEZHA [38], we only compare BERT [2], BERT-wwm, BERT-wwm-ext, RoBERTa-wwm-ext, RoBERTa-wwm-ext-large, ELECTRA, along with our MacBERT to ensure relatively fair comparisons among…###For training BERT series, we adopt the scheme of training on a maximum sequence length of 128 tokens then on 512, suggested by [2].",impact-revealing,reporting the methodology and tools used in the study
2861,5b67b46b17c44aac1c861edd,3bcb5a3e296d4fb427400e112d865abc52435b56,Disconnected Recurrent Neural Networks for Text Categorization,5550411645ce0a409eb38730,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.,A variant of RNN has been introduced by Cho et al. (2014) with the name of gated recurrent unit (GRU).,other,reporting prior findings on gated recurrent units
3666,5b67b47917c44aac1c8639a8,957e4c03e7846a1f8670b250f5f9661c91dfcb75,Widar2.0: Passive Human Tracking with a Single Wi-Fi Link,5550488345ce0a409eb6f020,Multi-Person Localization via RF Body Reflections.,"Various hardware [2–4 , 11] are manufactured manufactured to Since dedicated hardware are difficult to be generalized, re-searchesareshiftedtoubiquitousCOTSRFdevices,suchasRFID [26, 27, 33, 42], millimetre wave [35, 45] and Wi-Fi [21, 32].###To achieve localization in complicated multipath environments indoors, previous works either rely on specialized hardware or software-defined radios [2, 3, 13], rendering them not ubiquitously applicable, or",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1955,,1139d18c23c5d438488330e4f582ec67757dea15,Coflow: A Networking Abstraction for Distributed Data-Parallel Applications,,,"###By approximating FIFO (as in Baraat [82]) for lighttailed coflows and smallest-coflow-first (as in Varys [68]) for heavy-tailed coflows, the proposed scheduler works well in practice.###0 and Varys [68] – described in more details in the next chapter – uses MADD as a building block.###95th percentile results are similar. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 6.9 [Simulation] CCT distributions for Aalo, Varys, per-flow fairness, and uncoordinated non-clairvoyant coflow scheduling.###This dissertation is the culmination of many successful collaborations: Chapters 2 and 3 build upon joint work with Ion Stoica [65]; Chapter 4 includes materials from joint work with Matei Zaharia, Justin Ma, Michael Jordan, and Ion Stoica [67] and with Yuan Zhong and Ion Stoica [68]; Chapter 5 was joint work with Yuan Zhong and Ion Stoica [68]; Chapter 6 was joint work with Ion Stoica [66]; and Chapter 7 was joint work with Zhenhua Liu, Ali Ghodsi, and Ion Stoica [64].###Assuming each port can transfer one unit of data in one time unit, (c)–(f) depict the allocations of ingress port capacities (vertical axis) for different mechanisms: The average CCT for (c) per-flow fairness is 5.33 time units; (d) decentralized LAS is 5 time units; (e) CLAS with instant coordination is 4 time units; and (f) the optimal schedule is 3.67 time units. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
x 6.2 Aalo architecture.###53 5.5.3 Inter-Coflow Scheduling to Minimize CCT . . . . . . . . . . . . . . . .###Varys [68] improves performance using heuristics such as smallest-bottleneck-first and smallest-total-size-first, but it assumes complete prior knowledge of coflow characteristics such as the number of flows, their sizes, and endpoints.###Time is in GMT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
xii
B.1 Allocation of ingress port capacities (vertical axis) for the coflows in (a) on a 2×2 datacenter fabric for (b) a work-conserving and (c) a CCT-optimized schedule.###Increased deadlines improve performance. . . . . . . . . . . . . . . . . . . . 65 5.9 [Simulation] More coflows meet deadline using inter-coflow scheduling than using per-flow fairness and prioritization schemes. . . . . . . . . . . . . . . . . . . . . . . . 66 5.10 [EC2] Improvements in the average CCT using coflows for different coflow mixes from Table 5.4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 5.11 [Simulation] Improvements in the average CCT for varying numbers of concurrent coflows. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 5.12 [Simulation] Changes in the percentage of coflows that meet deadlines for varying numbers of concurrent coflows. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .###A large body of solutions [37,67,68,82,118] work under this assumption.###In contrast, traditional point-to-point flows from different applications are indistinguishable. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.4 Centralized coordination architecture used in this dissertation. . . . . . . . . . . . . . 7 1.5 [EC2] Performance comparison between coflow-based solutions and the state-of-the-
art: (a) communication time in each iteration of a logistic regression application; (b) average CCT of a SQL (Hive) workload from Facebook trace; and (c) minimum network share across coflows in MapReduce applications.###59 5.7 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 5.7.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 5.7.2 Varys’s Performance in Minimizing CCT . . . . . . . . . . . . . . . . . . 62 5.7.3 Varys’s Performance for Deadline-Sensitive Coflows . . . . . . . . . . .###For light-tailed distributions of coflow sizes, however, a straightforward implementation of CLAS can lead to fine-grained sharing,3 which is known to be suboptimal for minimizing the average CCT [67, 68, 82].###55 5.4 [EC2] Average and 95th percentile improvements in job and communication completion times over per-flow fairness using Varys. . . . . . . . . . . . . . . . . . . . . . . 62 5.5 [EC2] Improvements in the average and 95th percentile CCTs using coflows w.r.t. the default per-flow fairness mechanism. . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 5.6 CCT distributions for Varys, per-flow fairness, and per-flow prioritization schemes (a) in EC2 deployment and (b) in simulation.###1 Background Inter-coflow scheduling to minimize the average coflow completion time (CCT) is NP-hard [68].###This is a viable objective for both clairvoyant [68] and non-clairvoyant [66, 67, 82] coflows.###(c) CoflowIds assigned by Aalo. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 6.5 [EC2] Average and 95th percentile improvements in job and communication completion times using Aalo over per-flow fairness and Varys. . . . . . . . . . . . . . . . . . 89 6.6 [EC2] Improvements in the average and 95th percentile CCTs using Aalo over per-flow fairness and Varys. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .###92 6.10 [EC2] Average improvements in CCTs w.r.t. Varys for multi-wave coflows. . . . . .###or switch [34,37,46,68,86,129] and consider machine uplinks and downlinks, i.###118 7.3 [Simulation] Slowdowns using different mechanisms w.r.t. the minimum CCTs. . . . . 119
A.1 Summary of Facebook traces. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .###119 7.14 [Simulation] Average and 95th percentile improvements in CCT using HUG. . . . . .###The X-axis is in log scale. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 6.8 [Simulation] Average improvements in CCTs using Aalo.###120 7.15 [Simulation] CCTs using different mechanisms w.r.t. the minimum completion times.###The average FCT and CCT for (b) per-flow fairness are 3.4 and 4 time units; (c) per-flow prioritization are 2.8 and 3.5 time units; (d) WSS are 3.6 and 4 time units; and (e) the optimal schedule are 3 and 3 time units. . . . . . . . . 50 5.2 Varys architecture.###(b) HUG isolates coflows CA and CC from coflow CB. . . . . . . . . . . 116 7.12 [Simulation] HUG ensures higher isolation guarantee than high-utilization schemes such as per-flow fairness and PS-P, and provides higher utilization than multi-resource fairness schemes such as DRF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 7.13 [Simulation] Slowdowns of coflows using different mechanisms w.r.t. the minimum CCTs.###For example, we show that improving a coflow’s completion time (CCT) improves the completion time
CHAPTER 1.###B.1 Problem Formulation and Complexity in the Offline Case . . . . . . . . . . . . . . 139 B.2 Tradeoffs in Optimizing CCT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 B.3 Ordering Properties of Coflow Schedules . . . . . . . . . . . . . . . . . . . . . . . 141###Although the former schedule is work-conserving and achieves higher utilization, the latter has a lower average CCT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141 B.2 Flow-interleaved allocation of egress port capacities (vertical axis) for the coflows in (a) for CCT-optimality (b). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
D.1 Hard tradeoff between work conservation and strategy-proofness.###93 6.11 [EC2] Improvements in job-level communication times using Aalo for coflow DAGS in the Cloudera benchmark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 6.12 [Simulation] Aalo’s sensitivity (measured as improvements over per-flow fairness) to
(a) the number of queues, (b) the size of the highest-priority queue, and (c) exponential and (d) linear queue thresholds. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
6.13 [Simulation] Improvements in average CCTs using Aalo (a) when coflow sizes are uniformly distributed up to different maximum values and (b) when all coflows have the same size. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 6.14 [EC2] Aalo scalability: (a) more daemons require longer coordination periods (Y-axis is in log scale), and (b) delayed coordination can hurt overall performance (measured as improvements over per-flow fairness). . . . . . . . . . . . . . . . . . . . . . . . . 96
7.1 Bandwidth allocations in two independent links (a) for CA (orange) with correlation vector
−→ dA = 〈12 , 1〉 andCB (dark blue) with −→ dB = 〈1, 16〉.###89 6.7 [EC2] CCT distributions for Aalo, Varys, and per-flow fairness mechanism.###64 5.7 [Simulation] Improvements in the average and 95th percentile CCTs using inter-coflow scheduling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
576,5b1643998fbcbf6e5a9bc25e,080e1bb6bbebeb78f822b3998b7ed898ab6457aa,End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction,5c8b52594895d9cbc68108b0,A two-stage algorithm for noisy and reverberant speech enhancement,"There are some previous attempts at naively applying such iterative algorithms as a post-processing step on the magnitudes produced by deep learning based speech enhancement and separation [18, 19, 20, 3].###A follow-up work [19] of [23] supplies clean phase during training.",impact-revealing,acknowledging prior attempts at applying iterative algorithms in speech enhancement
2026,,2681afa545a51f04971c8583c8e5d75e293e806c,Video streaming distribution over mobile Internet: a survey,,,"###To support the movement of system members, AMCV designs a construction method of static and dynamic connections between communities.###AMCV, described in our previous work [35], is a minicommunity-based solution for VoD services in wireless mobile networks that has a two-layer overlay network structure: a community member layer and a mini-community network layer.###Moreover, AMCV does not consider the mobility of mobile nodes, so that the high transmission delay and loss of video data reduce the user QoE.###In addition, to improve community scalability, AMCV also designs the maintenance strategy for the community structure, which selects the appropriate candidate broker members in terms of the service time and capacity of ordinary members.###AMCV designs a video lookup based on the mini-community network, which enables the broker nodes in communities to implement fast message forwarding by formulating the shortest lookup path to the objective community.###AMCV [35] O ( N Ks ) Ks • Community-based overlay • Shortest video lookup path • Overload of brokers • Unguaranteed delivery performance",impact-revealing,describing the design and functionality of the AMCV system
91,5ec49a639fced0a24b4de8d2,5c5751d45e298cea054f32b392c12c61027d2fe7,S2ORC: The Semantic Scholar Open Research Corpus,5b1642a68fbcbf6e5a9b7e65,Construction of the Literature Graph in Semantic Scholar.,"…graphs like AMiner’s Open Academic Graph (Tang et al., 2008), the Microsoft Academic Graph (MAG) (Shen et al., 2018), and the Semantic Scholar literature graph (Ammar et al., 2018), have had widespread application in bibliomet-rics, science-of-science, information retrieval, and network analysis.###2 Constructing the corpus S2ORC is constructed using data from the Semantic Scholar literature corpus (Ammar et al., 2018).",impact-revealing,highlighting the widespread application of academic graphs in various fields
2684,5eede0b091e0116a23aafbd3,91fb815361fdbf80ff15ce4d783a41846bd99232,GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training,5b67b4b417c44aac1c867919,Representation Learning with Contrastive Predictive Coding.,"[35] and instance discrimination task from Wu et al.###Inspired by the recent success of contrastive learning in CV [17, 59] and NLP [9, 30], we propose to use instance discrimination [59] as our pre-training task, and InfoNCE [35] as our learning objective.###In this work, we adopt InfoNCE [35] such that:###In this work, we adopt the InfoNCE loss from Oord et al. [35] and instance discrimination task from Wu et al. [59], as discussed in Section 3.###In this work, we adopt InfoNCE [35] such that:
L = − log exp
( q⊤k+/τ )∑K i=0 exp (q⊤ki /τ )
(1)
where τ is the temperature hyper-parameter. fq and fk are two graph neural networks that encode the query instance xq and each key instance xk to d-dimensional representations, denoted by q = fq (xq ) and k = fk (xk ).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1353,,c64d5197424a01f4c1765028902c12769e1f7ad8,"How the love of muscle can break a heart: Impact of anabolic androgenic steroids on skeletal muscle hypertrophy, metabolic and cardiovascular health",,,"###In addition to the genomic and non-genomic effects of AAS decreasing atrophy related gene expression and activity of catabolic pathways (FOXO pathway), AAS may cause direct inhibition on glucocorticoid receptor (GR) signalling and/or its expression [5].###Due to advancements in technology and pharmacology, a range of anabolic androgenic steroids (AAS) (Table 1, [5, 6]) began to be commonly used by the recreational gym-user in the 1980s, primarily by young men to improve body image [1, 7].",impact-revealing,highlighting the effects and historical context of anabolic androgenic steroids
1324,,bd71da628a1858c6b2f598a101074a3524b4a225,"Attachment Style, Vagal Tone, and Empathy During Mother–Adolescent Interactions",,,"###For continuous variables, these techniques are identical to those described and validated by Schafer and Graham (Graham & Hofer, 2000; Schafer, 2001; Schafer & Graham, 2002).###This technique has been shown to perform well when data are missing at random and even acceptably under some cases of nonrandom missingness (Schafer & Graham, 2002).",impact-revealing,reporting prior findings on techniques for handling missing data
201,5b1643ba8fbcbf6e5a9bc79b,b3f83e8416010e9c3a705a0b6390d268e5ddf5c0,Black-box adversarial attacks with limited queries and information,58d82fced649053542fd6eb1,Delving into Transferable Adversarial Examples and Black-box Attacks.,"As Liu et al. (2017) demonstrated, it is more difﬁcult to transfer targeted adversarial examples with or without their target labels, particularly when attacking models trained on large datasets like ImageNet.###Using ensemble-based methods, Liu et al. (2017) overcame these limitations to attack the Clarifai API.###In this paper, we use the definition of black-box access as query access (Chen et al., 2017; Liu et al., 2017; Hayes & Danezis, 2017).###In this paper, we use the deﬁnition of black-box access as query access (Chen et al., 2017; Liu et al., 2017; Hayes & Danezis, 2017).",impact-revealing,reporting findings on adversarial examples and their transferability
795,5f0423a69e795e06bbe12b1e,21082bd98071f6948097df05cc9e4770fcd87de6,Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems,5b67b4b417c44aac1c867919,Representation Learning with Contrastive Predictive Coding.,"(3) has recently achieved remarkable success in various fields [11, 18, 37].###, speech processing [37], computer vision [11, 18, 21], graph data [48], and compositional environments [28].###The contrastive loss we investigate in this paper is a generalization of the InfoNCE loss [37].###We study the following type of contrastive loss [37, 41] under a negative sampler pn (y | x):###InfoNCE is previously understood as a bound of the mutual information between two variables [37].###Our theory complements the previous studies of contrastive learning [37].###InfoNCE [37] demonstrates that this loss maximizes a lower bound of the mutual information between x and y if we set the proposal distribution pn (y | x) to be the actual data distribution pdata (y), i.",impact-revealing,highlighting the success of contrastive loss in various fields
1894,,22b598f05943e6ecb4193b90d5947d55f3254dc9,"Pandemic pedagogies, practices and future possibilities: emerging professional adjustments to the working practices of university teacher educators",,,"###Collectively these cover the ability for the learner to be a “real” person (Garrison et al., 2000), projecting their personality through their engagement and communication via the technology (Kilis & Yildirim, 2019, p.###Cognitive presence is identified by Garrison et al. (2000) as the element that is “most basic to success in Higher Education” (Garrison et al., 2000, p. 89).###Teaching presence is seen as the “binding element in creating a community of inquiry” (Garrison et al., 2000, p. 96).###Collectively these cover the ability for the learner to be a “real” person (Garrison et al., 2000), projecting their personality through their engagement and communication via the technology (Kilis & Yildirim, 2019, p. 180).###Developed by Garrison et al. (2000), and discussed more fully below, the Community of Inquiry (CoI) is widely used as a theoretical framework when considering how meaningful learning can be generated online (Fiock, 2020).",impact-revealing,highlighting the significance of the Community of Inquiry framework in online learning
367,5fdc8e9d91e01104c91811a8,849b88ddc8f8cabc6d4246479b275a1ee65d0647,A Generalization of Transformer Networks to Graphs,5e15adcb3a55ac47ab5b0b8c,Graph Transformer Networks,"Yun et al. (2019) developed Graph Transformer Networks (GTN) to learn on heterogeneous graphs with a target to transform a given heterogeneous graph into a meta-path based graph and then perform convolution.###…to develop graph transformers (Li et al. 2019; Nguyen, Nguyen, and Phung 2019; Zhang et al. 2020) with few focused on specialized cases such as on heterogeneous graphs, temporal networks, generative modeling, etc. (Yun et al. 2019; Xu, Joshi, and Bresson 2019; Hu et al. 2020; Zhou et al. 2020).",impact-revealing,reporting on the development of Graph Transformer Networks and their applications
1064,,43c67c75a862280f73ce5f6ae5427ec225771de0,Internationalisation of firms and their innovation and productivity†,,,"###Helpman, Melitz, and Yeaple (2004) demonstrate that, when foreign direct investment is motivated by market size, the most productive firms become multinationals, the next most productive export, the less productive serve only the domestic market, and the least productive, exit.###A number of analyses have followed using data from the Community Innovation Survey (CIS),1 to estimate the CDM model in different countries:2 Janz, Lööf, and Peters (2004) for Germany and Sweden; Benavente (2006) for Chile; Chudnovsky, López, and Pupato (2006) for Argentina; Griffith et al. (2006)…",impact-revealing,highlighting the findings of foreign direct investment motivations and subsequent analyses in various countries
193,5b67b45517c44aac1c860876,6c96c2d4a3fbd572fef2d59cb856521ee1746789,Graph Convolutional Neural Networks for Web-Scale Recommender Systems,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"Hamilton et al. (2017b) [19] and Bronstein et al. (2017) [6] provide comprehensive surveys of recent advancements.###(2017a)’s GraphSAGE algorithm [18] and the closely###In terms of algorithm design, our work is most closely related to Hamilton et al. (2017a)’s GraphSAGE algorithm [18] and the closely related follow-up work of Chen et al. (2018) [8].###First, selecting a fixed number of nodes to aggregate from allows us to control the memory footprint of the algorithm during training [18].###, γ = mean); • mean-pooling-xent is the same as mean-pooling but uses the cross-entropy loss introduced in [18].###Following on this work, a number of authors proposed improvements, extensions, and approximations of these spectral convolutions [6, 10, 11, 13, 18, 21, 24, 29, 31], leading to new state-of-the-art results on benchmarks such as node classification, link prediction, as well as recommender system tasks (e.",impact-revealing,acknowledge existing surveys and advancements in the field
1397,,01ca34d41b2d3392be0efaaac59bbd0767166da3,"When Intercepting Moving Targets, Mid-Movement Error When Intercepting Moving Targets, Mid-Movement Error Corrections Reflect Distinct Responses to Visual and Haptic Corrections Reflect Distinct Responses to Visual and Haptic Perturbations Perturbations",,,"###…builds on a long history of studies examining corrections to ongoing pointing movements, especially those employing iterative-corrections models (Crossman and Goodeve 1983; Keele 1968), which use a series of overlapping submovements (discrete strokes or other basis functions) to compose a broad…",impact-revealing,acknowledging the historical context of iterative-corrections models
628,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",558ba88be4b00c3c48ddc0f3,Discrete Signal Processing on Graphs,"When data labels are presented as signals on a (nearest neighbor) graph, graph signal regularization techniques can be used in the process of estimating labels [5], optimizing the prediction of unknown labels in classification [51] or semisupervised learning problems [195].###For the total variation criterion it has been shown experimentally and justified theoretically that the frequency bases obtained from the shift operator tend to be ordered [51].###Other norms could be used to define the total variation, see [51][3].###ASP led to the introduction of, possibly weighted, graph adjacency matrices as shifts that generate the graph signal model for signals indexed by nodes of an arbitrary directed or undirected graph [2], [51].###Such a smooth graph signal model makes it possible to detect outliers or abnormal values by highpass filtering and thresholding [51], [155], or to build effective signal reconstruction methods from sparse set of sensor readings, as in [156], [157], [158], which can potentially lead to",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3710,5ecbc6199fced0a24b4eefa9,3c31c95870824736ecaba2ba01f2ccab145fd91d,Leveraging Unpaired Text Data for Training End-To-End Speech-to-Intent Systems,5ac1829d17c44a1fda917ed4,Towards end-to-end spoken language understanding,"In contrast, an end-to-end (E2E) SLU system [1–7] processes speech input directly into intent without going through an intermediate text transcript.",other,providing context for end-to-end speech input processing
2536,5d04e8d7da56295d08daef06,448b64df68335d3695a37c54770e7d5cd5f6fe68,Context Attentive Document Ranking and Query Suggestion,53e9ab3db7602d97034cb70f,Towards context-aware search by learning a very large variable length hidden markov model from search logs.,"[6] model the development of users’ search intent in search sessions with a variable length Hidden Markov Model, and utilize the inferred search intent for document ranking and query suggestion.",other,reporting prior findings on user search intent modeling
3053,5ede0553e06a4c1b26a8419c,1f3c381eedfe8914b81e93070bfdb00cf86ac943,Contrastive Multi-View Representation Learning on Graphs,5bdc31b417c44a1f58a0ba6c,How Powerful are Graph Neural Networks?,"Graph neural networks (GNN) (Li et al., 2015; Gilmer et al., 2017; Kipf & Welling, 2017; Velikovi et al., 2018; Xu et al., 2019b) reconcile the expressive power of graphs in modeling interactions with unparalleled capacity of deep models in learning representations.###Early-fusion models (Xu et al., 2019a; Jiang et al., 2019) use graph diffusion to decide the neighbors, e.g., graph diffusion convolution (GDC) replaces adjacency matrix in graph convolution with a spar-siﬁed diffusion matrix (Klicpera et al., 2019b), whereas, late-fusion models (Tsitsulin et al.,…###When compared to supervised baselines, our approach performs on par with or better than strong supervised baselines, e.g., graph isomorphism network (GIN) (Xu et al., 2019b) and graph attention network (GAT) (Velikovi et al., 2018), on 4 out of 8 benchmarks.###We also compare with ﬁve supervised GNNs reported in (Xu et al., 2019b) including GraphSAGE (Hamilton et al., 2017), GCN, GAT, and two variants of GIN: GIN-0 and GIN-(cid:15) .",other,highlighting the performance of the proposed approach compared to supervised baselines in graph neural networks
2569,5c04967517c44a2c74709354,04c131293bf64c67972baa0053e85a510c4aa725,Intervention Harvesting for Context-Dependent Examination-Bias Estimation,5b3d98cc17c44a510f8019ba,A Click Sequence Model for Web Search.,"Built upon the PBM and the Cascade model, more complex models like UBM [13], DBN [8], CCM [14] and CSM [6] were proposed to infer relevance judgments from click logs.",other,acknowledge development of complex models for relevance judgments
2513,5bdc315017c44a1f58a05c5e,5201efab94c9376ef894f6f33cab06a5c5e00073,Learning Named Entity Tagger using Domain-Specific Dictionary,5c8a2c404895d9cbc61f4a5c,SwellShark: A Generative Model for Biomedical Named Entity Recognition without Labeled Data.,"Existing distantly supervised NER models usually tackle the entity span detection problem by heuristic matching rules, such as POS tag-based regular expressions (Ren et al., 2015; Fries et al., 2017) and exact string matching (Giannakopou-los et al., 2017; He, 2017).###Even though SwellShark is designed for the biomedical domain and utilizes much more expert effort, AutoNER outperforms it in almost all cases.###The only outlier happens on the NCBI-disease dataset when the entity span matcher in SwellShark is carefully tuned by experts for many special cases.###…labeled data, and has gained successes in various natural language processing tasks, including phrase mining (Shang et al., 2018), entity recognition (Ren et al., 2015; Fries et al., 2017; He, 2017), aspect term extraction (Giannakopoulos et al., 2017), and relation extraction (Mintz et al., 2009).###SwellShark, in the biomedical domain, is arguably the best distantly supervised model, especially on the BC5CDR and NCBI-Disease datasets (Fries et al., 2017).###, 2018), entity recognition (Ren et al., 2015; Fries et al., 2017; He, 2017), aspect term extraction (Giannakopoulos et al.###Existing distantly supervised NER models usually tackle the entity span detection problem by heuristic matching rules, such as POS tag-based regular expressions (Ren et al., 2015; Fries et al., 2017) and exact string matching (Giannakopoulos et al.###SwellShark , in the biomedical domain, is arguably the best distantly supervised model, especially on the BC5CDR and NCBI-Disease datasets (Fries et al., 2017).###For example, SwellShark (Fries et al., 2017), specifically designed for biomedical NER, leverages a generative model to unify and model noise across different supervision sources for named entity typing.###There are attempts on the distantly supervised NER task recently (Ren et al., 2015; Fries et al., 2017; He, 2017; Giannakopoulos et al., 2017).",other,acknowledging existing approaches and their limitations in NER
2742,5c234870da562935fc1d4db3,0b6a531754e67379518a946f3a3cf685f59358cc,CritICs Critiquing Criticality in Mobile Apps,573697786e3b12023e660116,The Load Slice Core Microarchitecture,"…latencies, slack, and execution graph representations, as well as (ii) optimizing for those identiﬁed using techniques such as critical load optimizations [9], [11], [12], [18], [79] or even backend optimizations for critical instructions such as [5], [6], [14], [16], [24], [25], [31], [75]–[77].###However, such techniques require fairly extensive hardware to identify these chains, and optimizing for them, e.g. techniques such as [18], [76], [77] require 16KB SRAM, and [79] incurs 22% additional power, making them less suitable for resource-constrained mobile SoCs.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
648,5d3ed2653a55ac61d998598b,077f8329a7b6fa3b7c877a57b81eb6c18b5f87de,roberta: a robustly optimized bert pretraining approach,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"We ﬁnd that this setting outperforms the originally published BERT BASE results and that removing the NSP loss matches or slightly improves downstream task performance , in contrast to Devlin et al. (2019).###Devlin et al. (2019) observe that removing NSP hurts performance, with signiﬁcant performance degradation on QNLI, MNLI, and SQuAD 1.1.###1 we follow the same ﬁnetun-ing procedure as Devlin et al. (2019).###Our ﬁnetuning procedure follows the original BERT paper (Devlin et al., 2019).###In particular, while both BERT (Devlin et al., 2019) and XL-Net (Yang et al., 2019) XLNet, while we use the same learning rate for all layers.###For example, the recently proposed XLNet architecture (Yang et al., 2019) is pretrained us-ing nearly 10 times more data than the original BERT (Devlin et al., 2019).###Unlike Devlin et al. (2019), we do not randomly inject short sequences, and we do not train with a reduced sequence length for the ﬁrst 90% of updates.###ity to measure the effects of the modeling advances. ∗Equal contribution. 1Our models and code are available at: https://github.com/pytorch/fairseq We present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We ﬁnd that BERT was signiﬁcantly undertrained and propose an improved recipe for training BERT mode###Most of the top systems build upon either BERT (Devlin et al., 2019) or XLNet (Yang et al., 2019), both of which rely on additional external training data.###rad NLI (WNLI) (Levesque et al., 2011). V2.0 some questions are not answered in the provided context, making the task more challenging. For SQuAD V1.1 we adopt the same span prediction method as BERT(Devlin et al., 2019). For SQuAD V2.0, we add an additional binary classiﬁer to predict whether the question is answerable, which we train jointly by summing the classiﬁcation and span loss terms. During evaluation, we on###In this section, we give a brief overview of the BERT (Devlin et al., 2019) pretraining approach and some of the training choices that we will examine experimentally in the following section.###opment sets after ﬁnetuning thepretrained modelsonthecorresponding singletask training data (i.e., without multi-task training or ensembling). Our ﬁnetuning procedure follows the original BERT paper (Devlin et al., 2019). In Section 5 we additionally report test set results obtained from the public leaderboard. These results depend on a several task-speciﬁc modiﬁcations, which we describe in Section 5.1. SQuAD The St###We present a replication study of BERT pre-training (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size.###Devlin et al. (2019).###The original BERT implementation (Devlin et al., 2019) uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tok-enization rules.###her recently published methods. We release our model, pretraining and ﬁne-tuning code implemented in PyTorch (Paszke et al., 2017). 2 Background In this section, we give a brief overview of the BERT (Devlin et al., 2019) pretraining approach and some of the training choices that we will examine experimentally in the following section. 2.1 Setup BERT takes as input a concatenation of two segments (sequences of tokens)###Self-training methods such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019), and XLNet (Yang et al., 2019) have brought signiﬁcant performance gains, but it can be challenging to determine which aspects of the methods contribute…###We ﬁrst compare the original SEGMENT - PAIR input format from Devlin et al. (2019) to the SENTENCE - PAIR format; both formats retain the NSP loss, but the latter uses single sentences.###To better understand this discrepancy, we compare several alternative training formats: • SEGMENT - PAIR + NSP : This follows the original input format used in BERT (Devlin et al., 2019), with the NSP loss.###1 we adopt the same span prediction method as BERT (Devlin et al., 2019).###onally expensive,oftendoneonprivatedatasetsofdifferent sizes, and, as we will show, hyperparameter choiceshave signiﬁcantimpacton the ﬁnal results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measurestheimpactofmanykeyhyperparametersandtrainingdatasize. WeﬁndthatBERT was signiﬁcantly undertrained, and can match or exceed the performance of every model published after it. Ou###This formulation signiﬁcantly simpliﬁes the task, but is not directly comparable to BERT (Devlin et al., 2019).###Pretraining methods have been designed with different training objectives, including language modeling (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018), machine translation (McCann et al., 2017), and masked language modeling (Devlin et al., 2019; Lample and Conneau, 2019).###Performance is also typically improved by training bigger models on more data (Devlin et al., 2019; Baevski et al., 2019; Yang et al., 2019; Radford et al., 2019).###Devlin et al. (2019) originally trained BERT BASE for 1M steps with a batch size of 256 sequences.",impact-revealing,reporting findings on BERT pretraining and its performance
1543,,a60f029bb6bb884caafe2dead939953daa1ac21c,The effect of nucleus reuniens inactivation on neocortical K-complex and hippocampal sharp wave-ripple correlations,,,"###It is well established that the hippocampus is critically involved in declarative memory processes (Eichenbaum, 2000), and damage to the hippocampus and associated temporal lobe structures can cause complete anterograde amnesia and temporally graded retrograde amnesia (Zola-Morgan et al.###It is well established that the hippocampus is critically involved in declarative memory processes (Eichenbaum, 2000), and damage to the hippocampus and associated temporal lobe structures can cause complete anterograde amnesia and temporally graded retrograde amnesia (Zola-Morgan et al., 1986).###Previous studies (Wilson and McNaughton, 1994; Kudrimoti et al., 1999; Eichenbaum, 2000; Lee and Wilson, 2002; Buzsaki, 1998; Benchenane et al., 2010; Born and Wilhelm, 2012) have demonstrated the importance of the hippocampus in learning and memory but an important question remains:
How are…",impact-revealing,highlighting the critical role of the hippocampus in memory processes
3069,5bdc317017c44a1f58a08086,6062d8f5cc50014e5ff0f9aa467e1b044d33c051,Learning Cloud Dynamics to Optimize Spot,57d063d8ac4436735429325f,SpotLight: An Information Service for the Cloud,Some works have attempted to bid across different geographical regions and instance types [20].,other,acknowledging attempts in geographical and instance diversity
632,5cede104da562983788e3653,05dc5fb3a3bdefdf181aafcc42cd80ff6b7704e7,TuckER: Tensor Factorization for Knowledge Graph Completion,53e9b802b7602d97043b5798,The extension of factor analysis to three-dimensional matrices,"Tucker (Tucker, 1964), decomposes a tensor into a set of matrices and a smaller core tensor.",impact-revealing,providing context for tensor decomposition method
1026,,607fd60f64544515ad30adac3126cab124f1d176,Ensuring cross-cultural data comparability by means of anchoring vignettes in heterogeneous refugee samples,,,"###…become increasingly popular in empirical cross-cultural research [6–8], but the results often point to data that is not suitable for the comparisons under investigations, which in turn is associated with data not supporting measurement invariance between different countries and languages [9–13].###…fail to support strong or even weak invariance in their data [14], as shown for the Trends in International Mathematics and Science Study (TIMMS) [13], for different concepts of the European Social Survey (ESS) [9, 11], or for some of the concepts in the International Social Survey Program…###Measurement invariance analysis provides information on whether between-group comparisons of latent variables or summarized scores deliver valid results, as certain levels of measurement invariance point to bias free statistical comparisons [4, 13, 27].",impact-revealing,highlighting issues with data suitability in cross-cultural research
3213,5a73cbc317c44a0b3035eccf,7a0feeee49ac7692257fb21041e25511bc45192a,“Zero-Shot” Super-Resolution using Deep Internal Learning,558abfe684ae84d265bf6a52,A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics,"purpose we created a new dataset from BSD100 [14] by downscaling the HR images using random (but reasonably sized) Gaussian kernels.###To test the robustness of ZSSR in coping with unknown damage , we chose for each image from BSD100 [13] a random type of degradation out of 3 degradations: (i) Gaussian noise [ σ = 0 .###To test the robustness of ZSSR in coping with unknown damage, we chose for each image from BSD100 [14] a random type of degradation out of 3 degradations: (i) Gaussian noise [σ = 0.###For this purpose VDSR [8] EDSR+ [12] Blind-SR [ we created a new dataset from BSD100 [13] by downscaling the HR images using random (but reasonably sized) Gaussian kernels.",other,describing the dataset creation process for testing robustness
3825,5cede0f6da562983788d5a5f,31c343d741b31daeab7cce6ddb768767523d185e,Relational Graph Attention Networks.,5550418045ce0a409eb3be00,Massively Multitask Networks for Drug Discovery.,"…classiﬁcation mean Receiver Operating Characteristic ( ROC ) AUC across all 12 tasks (mean and standard deviation over 3 splits) for Multitask (Ramsundar et al., 2015), Bypass (Wu et al., 2018), Weave (Kearnes et al., 2016), RGCN (Altae-Tran et al., 2016), and (mean and standard deviation…###…graph classiﬁcation mean Receiver Operating Characteristic ( ROC ) AUC across all 12 tasks (mean and standard deviation over 3 splits) for Multitask (Ramsundar et al., 2015), Bypass (Wu et al., 2018), Weave (Kearnes et al., 2016), RGCN (Altae-Tran et al., 2016), and (mean and standard deviation…###(b) Graph classiﬁcation mean Receiver Operating Characteristic ( ROC ) Area Under the Curve ( AUC ) across all 12 tasks (mean and standard deviation over 3 splits) for Multitask (Ramsundar et al., 2015), Bypass (Wu et al., 2018)###Table 3: Graph classiﬁcation mean Area Under the Curve ( AUC ) across all 12 tasks (mean and standard deviation over 3 splits) for Multitask (Ramsundar et al., 2015), Bypass (Wu et al., 2018)###(c): Blue Baseline graph classiﬁcation mean Receiver Operating Characteristic ( ROC ) AUC across all 12 tasks (mean and standard deviation over 3 splits) for Multitask (Ramsundar et al., 2015), Bypass (Wu et al., 2018), Weave (Kearnes et al., 2016), RGCN (Altae-Tran et al., 2016), and (mean and standard deviation over 3 splits, 2 seeds per split) our implementation of RGCN .",other,reporting classification performance metrics across multiple tasks
1107,,aa59b834711645f768e58b904a3585c2ba935973,Understanding Reinforcement Learning-Based Fine-Tuning of Diffusion Models: A Tutorial and Review,,,"###Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) are widely recognized as powerful tools for generative modeling.",impact-revealing,highlighting the recognition of diffusion models in generative modeling
686,53e9aeebb7602d970391ac0a,8681e808a9ebd7f7f155590e75fb63563a8aae6e,performance prediction based on inherent program similarity,53e9b95bb7602d970454b9b5,Register traffic analysis for streamlining inter-operation communication in fine-grain parallel processors,We collect a number of characteristics concerning registers [6].,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
311,5f0277e911dc830562231dac,72a5feb4835b3e6cab3754437bb033371c5df154,DVGAN: A Minimax Game for Search Result Diversification Combining Explicit and Implicit Features,59a02ff0b161e8ad1a7b6eb9,Learning to Diversify Search Results via Subtopic Attention,"method we adapt the DSSA [12] score function for the generator.###In our experiments, we conduct the list-pairwise loss [12] to train DSSA method.###DSSA [12] introduces the machine learning method into explicit approaches.###We use ListMLE [22], R-LTR [26], PAMM [6], R-LTR-NTN, PAMM-NTN [24], and DSSA [12] as supervised baseline methods.###Similar to implicit approaches, explicit diversification approaches can also be categorized into heuristic approaches such as xQuAD [17, 18] and PM2 [5, 6, 9] and supervised approaches such as DSSA [12].###Different from implicit approaches which mainly model document novelty based on similarities between documents, explicit approaches regard the query as several subtopics, and explicitly leverage subtopics to determine the diversity of results [6, 12, 17, 18].###In the training process, we first train R-LTR [26] and DSSA [12] respectively using MLE loss in both ways.###Studies have shown that supervised approaches [12, 23, 24, 26] are able to outperform the heuristic approaches [1, 6, 18] by learning an optimized ranking function.###The explicit approaches [6, 12, 17, 18, 25] stress the relevance between the documents and the subtopics of the query, which infers that the selected document should cover the subtopics which the previously selected documents do not cover.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1243,,0889bef9047670888c465dc542e10396b132abad,Phantom shocks unmasked: clinical data and proposed mechanism of memory reactivation of past traumatic shocks in patients with implantable cardioverter defibrillators,,,"###Various neuroimaging studies have previously highlighted the importance of amygdalar, hippocampal, and prefrontal cortical dysfunction in abnormal memory processing in PTSD patients [19, 20].###Decreased or dysfunctional medial prefrontal cortical activity [19] further makes the fear conditioning resistant to extinction.",impact-revealing,highlighting prior findings on neuroimaging studies related to PTSD
393,5a260bfb17c44a4ba8a1c61e,a55970013b984f344dfbbbba677d89dce0ba5f81,Image Super-Resolution via Deep Recursive Residual Network,573695fd6e3b12023e510ff5,Deeply-Recursive Convolutional Network for Image Super-Resolution,"With the same depth but far fewer parameters, DRRN B1U9 achieves better performance than the state-of-theart methods [13,14].###1 overviews DL-based SISR, this section focuses on three most related work to ours: ResNet [8], VDSR [13] and DRCN [14].###Our DRRN has two major differences compared to DRCN: (a) Unlike DRCN that shares weights among convolutional layers, DRRN has a recursive block consisting of several residual units, and the weight set is shared among these residual units.###Strategies used in ResNet [8], VDSR [13], DRCN [14] and DRRN.###To mitigate the difficulty of training DRCN, the authors use recursive-supervision and skip-connection, and adopt an ensemble strategy to further improve the performance.###After increasing the depth without adding any parameters, the 52-layer DRRN B1U25 further improves the performance and significantly outperforms VDSR [13], DRCN [14] and RED30 [17] by 0.###[13, 14] propose two very deep convolutional networks for SR, both stacking 20 convolutional layers, from the viewpoints of training efficiency and storage, respectively.###1 shows the Peak Signal-to-Noise Ratio (PSNR) performance of several recent CNN models for SR [2, 13, 14, 17, 25, 32] versus the number of parameters, denoted as k.###In DRCN [14], a deep recursive layer (up to 16 convolutional recursions) is learned and the weights are shared in the 16 convolutional recursions.###For fair comparison, we also construct a DRRN B1U9 (d = 20, k = 297K) structure, which has the same depth as VDSR and DRCN, but fewer parameters.###To address this issue, we propose a novel Deep Recursive Residual Network (DRRN) to effectively build a very deep network structure, which achieves better performance, but with 2×, 6×, and 14× fewer parameters than VDSR, DRCN, and RED30, respectively.###1 lists the mathematical formulations of ResNet, VDSR, DRCN and DRRN.###The inference net f2(H0) stacks T recursions (T = 16 in [14]) in a recursive layer, with shared weights among these recursions.###On the other hand, to control the model parameters, the Deeply-Recursive Convolutional Network (DRCN) [14] introduces a very deep recursive layer via a chain structure with up to 16 recursions.###Very deep models (d ≥ 20) include VDSR [13], DRCN [14], RED [17] and DRRN with d = 20 and 52.###In VDSR and DRCN, the residual image is estimated from the input and output of the networks, termed as Global Residual Learning (GRL).###(b) To address the vanishing/exploding gradients problem of very deep models, DRCN supervises every recursion so that the supervision on early recursions help backpropagation.###DRCN [14] is motivated by the observation that adding more weight layers introduces more parameters, where the model is likely to overfit and also becomes disk hungry.###DRCN consists of three parts: embedding net, inference net and reconstruction net, which are illustrated as the first, middle 4, and last convolutional layer(s) in Fig.",impact-revealing,highlighting the performance improvements of the proposed DRRN model compared to existing methods
987,,e4fbc6e786fca3ba92b3683d28eac5a99510ecb0,The Classroom Language of Primary School English Teachers in Ethiopia: A Study in Communication Strategies,,,"###While the interactional framework is based on the definition given by the earlier researchers that CSs are mutual attempts of interlocutors to agree on meaning in situations where the requisite meaning structures do not seem to be shared (Tarone, 1980), the shift towards the cognitive approach is a more recent one ( Bialystok, 1990).###Researchers generally agree that there are basically two closely connected purposes that communication strategies are intended to serve – to fill gaps in a communication process created by lack of expression on the part of the discourse participants, and to facilitate comprehension (Tarone 1980; Faerch and Kasper 1983; Bialystok 1990; Poulisse 1990).###Tarone (1980) further modified this definition saying that CSs refer to ‘the mutual attempt of two interlocutors to agree on meaning structures that are not shared’.###The former stresses the mutual attempt of interlocutors to achieve communication goal (Tarone, 1980).###49 the interactional and the cognitive approaches (Tarone, 1980; Ellis, 1994; Cohen, 1988).###Researches on CSs have been done from the 1970s – the 1990s (for example, Tarone, 1977, 1980, 1983; Corder 1978; Faerch and Kasper 1983; Poulisse 1989; Bialystock 1990; Kasper and Kellerman, 1997).",impact-revealing,providing historical context and evolution of communication strategies research
1899,,9be3764e3340e1bc9f411c3cc3e76c8b4d161a08,The Curious Case of Blended Learning: An Evaluation of a Curriculum Innovation in the Global Mental Health Master’s Programme,,,###This perceived area for improvement demonstrated the importance of increasing the teaching presence (26) within the VLE to provide instructional management regarding optimal use of online resources.,impact-revealing,highlighting the significance of teaching presence in virtual learning environments
3043,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,573698436e3b12023e70c513,Autofdo: Automatic Feedback-Directed Optimization For Warehouse-Scale Applications,"As we will show, these microservices could benefit from larger Icache and ITLB and other techniques that address instruction misses [64, 65].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2304,5f86cae991e011dbc7eba2fa,5e9bb0f3e74be4d8f75cca6ceb3ec87b3e04d7cc,piuma: programmable integrated unified memory architecture,5c0495e417c44a2c74704e5d,Scalability of Hybrid Sparse Matrix Dense Vector (SpMV) Multiplication,"For Xeon, graph applications do not scale well beyond a single node, with even worse performance for small node counts, due to the overhead of the fine-grained communication [13].",other,highlighting performance limitations of graph applications on Xeon
3220,573698426e3b12023e70bf13,4e9dbca4218d32a9f92d58c340f3f8f3c5020a44,best-offset hardware prefetching,5550428045ce0a409eb42c99,Mitigating Prefetcher-Caused Pollution Using Informed Caching Policies for Prefetched Blocks,"Still, we found that important performance gains can be obtained by making the L3 replacement policy prefetch-aware (confirming previous studies [19, 38, 37, 39, 29]) and core-aware.",other,highlighting performance improvements through specific policy adjustments
2246,53e9b49bb7602d9703fa7aed,65cd7e21193ced281fecb0894a1245a8c3988286,The evicted-address filter: A unified mechanism to address both cache pollution and thrashing,53e9b7c7b7602d9704373a4b,Balancing Thoughput And Fairness In Smt Processors,"Table 4 shows the percentage improvement of the EAF-cache over the baseline LRU policy and the best previous mechanism, SHIP, on four metrics: weighted speedup [11, 50], instruction throughput, harmonic speedup [28] and maximum slowdown [25, 26].###We also evaluate three other metrics, namely, instruction throughput, harmonic speedup [28], and maximum slowdown [25, 26], in Section 7.",other,reporting evaluation metrics and results
2763,5c8f2a8b4895d9cbc62ecf7c,10ab21b120e305b6d3cbf81c5a906d36521152f1,Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors,53e9ab07b7602d970348c918,Query Strategies For Evading Convex-Inducing Classifiers,Nelson et. al [NRH + 12] presented the ﬁrst such iterative attack on a special class of binary classiﬁers.,other,reporting prior findings on iterative attacks in binary classifiers
3913,5f8fffb591e01125c27ddec9,67f473caaa52a97e65bb1bcb9029a580c4f8d10f,FLAG: Adversarial Data Augmentation for Graph Neural Networks,5c757e0df56def9798b4cc72,Robustness May Be at Odds with Accuracy,"Despite the wide belief that adversarial training harms standard generalization and leads to worse accuracy (Tsipras et al., 2018; Balaji et al., 2019), recently a growing amount of attention has been paid to using adversarial perturbations to augment datasets and ultimately alleviate overﬁtting.###By hunting for and stamping out small perturbations that cause the classiﬁer to fail, one may hope that adversarial training could beneﬁt standard accuracy (Goodfellow et al., 2014; Tsipras et al., 2018; Miyato et al., 2018).###Observation 2: In general, adversarial training hurts the clean accuracy in image classiﬁcation, but Tsipras et al. (2018) showed that CNNs could beneﬁt from adversarial augmentations on MNIST, where the pixel values are closer to discrete distribution than other more natural image datasets.###It is widely observed that when the data distribution is sparse and discrete, the beneﬁcial eﬀect of adversarial perturbations on generalization takes over (Tsipras et al., 2018; Gan et al., 2020).",other,highlighting the potential benefits of adversarial training for standard accuracy
744,5b67b46b17c44aac1c861fc4,f6d06993e003fa6fec5bf630efded9e4fd90a030,fluency boost learning and inference for neural grammatical error correction,573696136e3b12023e525abe,Improving Neural Machine Translation Models with Monolingual Data,"Back-boost learning borrows the idea from back translation (Sennrich et al., 2016) in NMT, referring to training a backward model (we call it error generation model, as opposed to error correction model) that is used to convert a ﬂuent sentence to a less ﬂuent sentence with errors.",impact-revealing,describing the concept of back-boost learning in relation to back translation
1470,,5c3487c5e5ab5a63dd7c15cc3d95aba904b6dc29,THEORY AND RESEARCH,,,"###A considerable amount of psychological research supports Wittgenstein's views (e.g ., Posner & Keele, 1968 ; Reed, 1972 ; Franks & Bransford, 1971 ; Rosch, 1973, 1975 ; Rosch & Mervis, 1975 ; Rosch et al., 1976; and Hampton, 1979, 1981) .###Given that subjects can readily and consistently rate exemplars of a concept on how typical they are of the concept (e.g ., Rosch, 1975), Rosch and Mervis (1975) investigated the psychological baisis for these judgements .###Consequently, such research has been criticized for its apparent lack of concern for the complex structure of natural concepts (Rosch, 1973, 1975, 1976) .",impact-revealing,acknowledge psychological research supporting Wittgenstein's views
2293,5eabf3cd91e011664efc496f,916595fc4e701ac6125725408912a1fac3d7a60b,CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows,55323dad45cec66b6f9df511,Quantifying sources of error in McPAT and potential impacts on architectural studies,"To estimate energy consumption and chip area overhead, we use modiﬁed versions of McPAT [42], [43] and CACTI 6.5 [44], considering only core components excluding L2 cache, main memory, and interconnection networks.",other,reporting methods for estimating energy consumption and chip area
2606,5dbebb7447c8f766462c22a6,e1fd81af050dbdc4232ff8b1ab71cf0973d530b6,graph convolutional networks with motif-based attention,53e99c8bb7602d9702539072,Network Motifs: Simple Building Blocks Of Complex Networks,"Higher-order Structures with Network Motifs Network motifs [25] are fundamental building blocks of complex networks; investigation of such patterns usually lead to the discovery of crucial information about the structure and the function of many complex systems that are represented as graphs.###However, in many cases, it has been shown that it may be beneficial to consider the higher-order structure in graphs [6, 25, 33, 35, 46].",other,highlighting the importance of higher-order structures in complex networks
893,53e9ba8ab7602d97046ac03f,67b3d45164531806e14697a3b4d268d5f294bb82,Object Storage on CRAQ: High-Throughput Chain Replication for Read-Mostly Workloads,53e9a2ecb7602d9702bf713c,Chain replication for supporting high throughput and availability,"The simulation results of previous work [ 47 ] showed competitive or superior throughput for CR compared to primary/backup replication, while arguing a principle advantage from quicker and easier recovery.###CRAQ and Chain Replication [ 47 ] are both examples of object-based storage systems that expose wholeobject writes (updates) and expose a flat object namespace.###This paper presents the design, implementation, and evaluation of CRAQ (Chain Replication with Apportioned Queries), an object storage system that, while maintaining the strong consistency properties of chain replication [ 47 ], provides lower latency and higher throughput for read operations by supporting apportioned queries: that is, dividing read operations over all nodes in a chain, as opposed to requiring that they all be handled by a ...###An alternative approach, taken by GFS [22] and promoted in CR [ 47 ], is to use the membership management service as a directory service in assigning and storing randomized chain membership, i.e., each chain can include some random set of server nodes.###Recently, van Renesse and Schneider presented a chain replication method for object storage [ 47 ] over fail-stop servers, designed to provide strong consistency yet improve throughput.###In particular, CRAQ’s choice of allowing a node to join anywhere in a chain (as opposed only to at its tail [ 47 ]), as well as properly handling failures during recovery, requires some careful consideration.",impact-revealing,reporting on previous findings related to object storage systems
4056,5f7fdd328de39f0828397e7f,9fa283d4f9c2ed991383c0434ef6043bee0dc8e2,Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification,59ae3bf12bbe271c4c71bc64,Proximal Policy Optimization Algorithms.,We implement the proximal policy optimization (PPO) algorithm proposed by [44] to train the patch proposal network π.,other,reporting the implementation of a specific algorithm
2651,5db929e947c8f766461fd005,11ead744b2edbdabe174258a8dfe4023be8e501d,Hierarchical Attention Prototypical Networks for Few-Shot Text Classification,599c7980601a182cd2644cc9,Meta Networks.,"…such as Finetune and kNN, Then we compare our model with ﬁve state-of-the-art few-shot learning models based on neural networks, they are MetaN (Munkhdalai and Yu, 2017), GNN (Garcia and Bruna, 2018), SNAIL (Mishra et al., 2018), Proto (Snell et al., 2017) and PHATT (Gao et al., 2019)…###Firstly, we compare our model with several traditional models such as Finetune and kNN, Then we compare our model with ﬁve state-of-the-art few-shot learning models based on neural networks, they are MetaN (Munkhdalai and Yu, 2017), GNN (Garcia and Bruna, 2018), SNAIL (Mishra et al., 2018), Proto (Snell et al., 2017) and PHATT (Gao et al., 2019) respectively.###For FewRel dataset, we cite the results reported by Snell et al. (2017) which includes Finetune, kNN, MetaN, GNN, and SNAIL, then we cite the results reported by Gao et al. (2019) which includes Proto and PHATT.",other,comparing model performance with state-of-the-art few-shot learning models
605,53e9a90db7602d970325e354,90189b948a37477938a8d83d81b6737f930d7997,illumination for computer generated pictures,53e9ac33b7602d97035f8a8f,Computer display of curved surfaces.,"of Volume 18 the ACM Number 6 or ""Bezier patches,"" Gouraud [ 11 ] developed an algorithm to shade curved surfaces.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2970,5eede0b791e0116a23aafe75,150f95f9c73820e0a0fa1546140e9f2bdfd25954,temporal graph networks for deep learning on dynamic graphs,5736986c6e3b12023e7308a0,Convolutional Networks on Graphs for Learning Molecular Fingerprints,"Graphs are ubiquitously used as models for systems of relations and interactions in many ﬁelds [5, 52, 42, 10, 16, 20, 49, 53], in particular, social sciences [68, 43] and biology [76, 62, 18].",other,highlighting the widespread use of graphs in various fields
634,5736960c6e3b12023e51ee06,492f57ee9ceb61fb5a47ad7aebfec1121887a175,Gated Graph Sequence Neural Networks,5550411645ce0a409eb38730,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.,"The biggest modification of GNNs is that we use Gated Recurrent Units (Cho et al., 2014) and unroll the recurrence for a fixed number of steps T and use backpropagation through time in order to compute gradients.###The biggest mod-iﬁcation of GNNs is that we use Gated Recurrent Units (Cho et al., 2014) and unroll the recurrence for a ﬁxed number of steps T and use backpropagation through time in order to compute gradients.",impact-revealing,describing modifications in GNNs using Gated Recurrent Units
3226,5aed14d617c44a4438158e20,9e788f1530af08a1f2140e6016fd4aeaa8b29033,Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform,58d82fced649053542fd7289,Image-to-Image Translation with Conditional Adversarial Networks,"In image generation [20, 5], semantic segments are used as input conditions to generate natural images.",other,providing context for the use of semantic segments in image generation
426,5e5e18ca93d709897ce315f0,68f86237dadcf2f570f0cd5b5e56161693619a74,Residual Energy-Based Models for Text Generation,5d06e484da562926acc48677,Real or Fake? Learning to Discriminate Machine from Human Generated Text.,"The book corpus is more challenging because the range of style and topics is more diverse than CC-News.###Bakhtin et al. (2019) found that the learned discriminator is not robust to random perturbations, and argued that the discriminator operates in the “residual” space of the language model.###In our work, the pretrained locally normalized language model can be seen as a ﬁxed generator, like in Bakhtin et al. (2019).###For training the joint models, for efﬁciency we generated 16/128 samples per preﬁx for CC-News/Book Corpus ofﬂine, and sample uniformly from those samples at training time.###Datasets We consider two datasets: the Toronto Book Corpus (Zhu et al., 2015; Kiros et al., 2015) and CC-News (Bakhtin et al., 2019).###First, we formalize the residual interpretation by Bakhtin et al. (2019) and use a generative model of the form: where P LM ( x ) is a locally normalized language model which is ﬁxed during training, and E θ is the energy function parameterized by θ .###Second, the language model provides a natural proposal distribution for training (Bakhtin et al., 2019), and training can be made efﬁcient by using the conditional noise contrastive estimation objective (Gut-mann & Hyv ¨ arinen, 2010) as we shall see in § 3.###Also, the book corpus is 30 times smaller than CC-News and may pose generalization challenges because of its smaller size.###We subsampled 333 sentences from the test set of CC-News, and asked 3 Amazon Mechanical turkers to vote.###Recently, Bakhtin et al. (2019) carefully studied the problem of training a discriminator to distinguish human written text from language model generations.",impact-revealing,highlighting the challenges and differences in datasets for language model training
423,5e5e18b693d709897ce29a22,53a77e8f73f2ca422d6e38fa9ecc490231ac044c,Neural Text Generation with Unlikelihood Training,5cede0e6da562983788c5226,The Curious Case of Neural Text Degeneration.,"…include the problem being (i) a by-product of the model architecture choices, e.g. the Transformer attention architecture preferring repeats (Holtzman et al., 2019; Vig, 2018), (ii) an intrinsic property of human language (Holtzman et al., 2019) rather than a modeling deﬁciency, or that…###Unlike previous work which only focused on degenerate sequence-level repeats (Holtzman et al., 2019), we additionally observe that neural text generators exhibit substantially more repetition in next-token prediction compared to human text: For instance, the Transformer language model (6.1)…###U is the smallest subset with x ∈ U p θ ( x | x <t ) > = p (Holtzman et al., 2019).###As demonstrated by Holtzman et al. (2019), such models with greedy or beam search tend to produce high frequency tokens too often and low frequency tokens too rarely, where frequency is deﬁned by the human token distribution.###9 } ) (Holtzman et al., 2019), respectively.###For language modeling, the work of Holtzman et al. (2019), from which this paper takes its name, highlighted both problems with the frequency distribution and the level of repetition of the model during generations compared to human utterances.###The generated text in open-ended applications such as language modeling or dialogue has been observed to be dull, using high frequency tokens too often and interesting content words too rarely (Holtzman et al., 2019; Dinan et al., 2019).###…e.g. the Transformer attention architecture preferring repeats (Holtzman et al., 2019; Vig, 2018), (ii) an intrinsic property of human language (Holtzman et al., 2019) rather than a modeling deﬁciency, or that (iii) a training objective relying on ﬁxed corpora cannot take into account the real…###Top k -sampling (Fan et al., 2018) and nucleus sampling (Holtzman et al., 2019) are two methods that sample sequences based on a function of the predicted next token probability distribution given by the model.###However, as the underlying model is unchanged, the model often prefers semantically similar phrasing, depending on the temperature parameter of the sampling (Holtzman et al., 2019).",impact-revealing,highlighting issues with model architecture and its impact on text generation
383,58d82fcbd649053542fd5d36,81db3f78f346eecf2f378070712feade6d45d6b1,MOLIERE: Automatic Biomedical Hypothesis Generation System,53e9a396b7602d9702ca3602,PLDA+: Parallel latent dirichlet allocation with data placement and pipeline processing,"PLDA+, a scalable implementation of LDA [28], allows us to quickly _x0080_nd topic models in these clouds.###LatentDirichletAllocation [8] is the most common topic modeling process and PLDA+ is a scalable implementation of this algorithm [20, 28].###We perform topic modeling on these documents using PLDA+ [28].",impact-revealing,describing the topic modeling method used
2565,5e09a7e4df1a9c0c4167dacc,3f82c0b2ca9d3ce40ad867bb5b0a3e93c7517b82,Attention-Based Dual-Source Spatiotemporal Neural Network for Lightning Forecast,53e9b137b7602d9703bba055,Prediction of lightning flash density with the WRF model,"Lynn and Yair [7] proposed Lightning Potential Index (LPI) that calculated from the max mixing ratio of snow, cloud ice, and graupel.###[14] Y. Yair, B. Lynn, C. Price, V. Kotroni, K. Lagouvardos, A. Mugnai, and M. del C. L. E. Morin, ‘‘Predicting the potential for lightning activity in Mediterranean storms based on the Weather Research and Forecasting (WRF) model dynamic and microphysical fields,’’ J. Geophys.###[7] B. Lynn and Y. Yair, ‘‘Prediction of lightning flash density with the WRF model,’’ Adv.###Second, although lightning is the result of multiple factors, it tends to be more closely related to some of them [5], [7], [12], [13].###We choose parameters that are closely related to lightning [5], [7], [8], [15], [16]: the mixing ratio of ice, snow and graupel, the maximum vertical wind speed and precipitation.",other,reporting prior findings on lightning prediction methods
1824,,b34b815e9b8c826d2fb160d10aced41a2d2a502c,Rapid determination of aliphatic amines in water samples by pressure‐assisted monolithic octadecylsilica capillary electrochromatography‐mass spectrometry,,,###Lowmolecular-weight aliphatic amines are widely used as industrial chemicals [7].,impact-revealing,acknowledging the common use of low-molecular-weight aliphatic amines in industry
3014,555045d745ce0a409eb59fe4,460d131e081486491af532604351a0691b52b11f,User-level psychological stress detection from social media using deep neural network,53e9ae84b7602d97038a5d90,Predicting Depression via Social Media.,"Actually, the most related user-level prediction work is [11], with the best result of 74% for a binary choice.###[11] leverage behavioral cues indicated from Twitter postings to predict depression before it is reported.",other,reporting prior findings in user-level prediction work
487,573698456e3b12023e70ee1b,524664475292ad6cdbdda3992fe5dc8f036b6ce5,Deep learning for emotion recognition on small datasets using transfer learning,53e9b40eb7602d9703f04187,Visualizing And Understanding Convolutional Networks,"The CNN-M-2048 model from [2] (VGG-CNN-M-2048), which is a variant of the model introduced in [28].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
716,5f0ed12691e011ead96652e9,3611f04c1861b6e956597d56afafdefc71c6af6a,TCGM: An Information-Theoretic Framework for Semi-Supervised Multi-Modality Learning,53e9b115b7602d9703b9264f,The multiinformation function as a tool for measuring stochastic dependence,"Total Correlation/Mutual information maximization Total Correlation [29], as an extension of Mutual Information, measures the amount of information shared by M (M ≥ 2) variables.###Inspired by such an assumption and the fact that the Total Correlation [29] can measure the amount of information shared by M (M ≥ 2) variables, in this paper, we propose Total Correlation Gain (TCG), which is a function of classifiers of all the modalities, as a surrogate goal for maximization of mutual information, in order to infer the ground-truth labels (i.",impact-revealing,introducing a new concept related to information measurement
1929,,693f27c640696799f489bbb2b629d0000be970d0,Chitosan Protects Cooked Ground Beef and Turkey Against Clostridium perfringens Spores During Chilling,,,"###perfringens in meat and poultry and because it is widely used as a food additive and in pharmaceutical formulations (Hirano 1997; Shepherd and others 1997; Shahidi and others 1999; Singla and Chawla 2001; Roller 2003), the major aim of this study was to explore its potential to control or inhibit outgrowth of C.###…and because it is widely used as a food additive and in pharmaceutical formulations (Hirano 1997; Shepherd and others 1997; Shahidi and others 1999; Singla and Chawla 2001; Roller 2003), the major aim of this study was to explore its potential to control or inhibit outgrowth of C. perfringens from…",impact-revealing,highlighting the significance of the study's aim related to C. perfringens
3912,5e2d653a3a55acc8374367fd,36ff7927f9049d37b314c2a114769b517b3f5f7a,Joint Recognition of Names and Publications in Academic Homepages,53e9a906b7602d97032573bd,FireCite: lightweight real-time reference string extraction from webpages,"There have been extensive research interests in the extraction and mining of such information from academic homepages [3, 8, 16, 20, 28, 31].###Previous studies on academic homepages usually use rule-based [8, 30] or a hybrid of machine learning and rule-based methods [3] on the HTML DOM trees of webpages.",other,acknowledge existing research on academic homepage information extraction
883,599c7ea4601a182cd28b81a7,f323036340ad0ec30cafacd6b09a3e28379e24f1,maximizing cache performance under uncertainty,57d063e0ac443673542947ae,Back to the Future: Leveraging Belady's Algorithm for Improved Cache Replacement.,"Likewise, SDBP [21], PRP [11], and Hawkeye [16] learn the behavior of different PCs.###And Hawkeye [16] emulates MIN’s past decisions.###Instead, replacement policies for processor caches are designed empirically, using heuristics based on observations of common-case access patterns [11,14,16,17,19,20,21,30,35,39].###Several recent policies break accesses into many classes, often using the requesting PC, and then adapt their policy to each class [11, 16, 21, 39].",impact-revealing,acknowledge existing methods and their empirical design
2471,5c6a37d03a69b1c9e12a9fc4,81b6d24e8f313fd88b0fe5ff6c21dd154fbe32d2,SimGNN: A Neural Network Approach to Fast Graph Similarity Computation,57aa28de0a3ac518da9896d6,Structural Deep Network Embedding,"ofmethodsthathavebeenproposedforlearningnoderepresentations,includingmatrixfactorizationbasedmethods(NetMF[38]), skip-grambasedmethods(DeepWalk[37],Node2Vec[15],LINE[47]), autoencoderbasedmethods(SDNE[50]),neighboraggregationbased methods (GCN [9, 26, 27], GraphSAGE [16]), etc. Graph-levelembedding. Themostintuitivewaytogenerateone embedding per graph is to aggregate the node-level embeddings, either ",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1952,,aca7b0314a02c80f62b1797fcdd1c1c1f7074d0d,CODA: Toward Automatically Identifying and Scheduling Coflows in the Dark,,,"###Clairvoyant Schedulers Consider Minimum-Allocation-forDesired-Duration (MADD), the optimal algorithm used in Varys [23] for intra-coflow scheduling when flow sizes are known a priori.###In prototype implementation, we build upon our coﬂow API integration, and reuse the same technique (bytecode instrumentation).###Our analysis reveals that stragglers significantly affect CCT, and recent coflow schedulers [23, 24] suffer performance degradation in the presence of errors.###In this section, we discuss the difﬁculties we have faced implementing an existing coﬂow API in Hadoop 2.7 & Spark 1.6, and describe the implementation of CODA prototype.###We adopted the same compute engine used in both Varys [23] and Aalo [24].###Clairvoyant Schedulers Consider Minimum-Allocation-for-Desired-Duration (MADD), the optimal algorithm used in Varys [23] for intra-coﬂow scheduling when ﬂow sizes are known a priori.###In order to validate CODA, we implemented Aalo’s coﬂow API in Hadoop 2.7 and Spark 1.6 to collect ground truth coﬂow information.###Hence, unlike existing coflow schedulers [21, 23, 24, 30], CODA combines per-flow (intra-coflow) prioritization with inter-coflow scheduling.###We faced several challenges, including intrusive refactoring of framework code, interactions with third-party libraries to collect coﬂow information, and Java byte-code instrumentation to support non-blocking I/O APIs.###To make things worse, most coﬂow-based solutions propose their own API [23, 24, 30].###A growing body of recent work [21, 23, 24, 30, 38, 68] has demonstrated that leveraging application-level information us-###1 Unlike the traditional flow abstraction, a coflow captures a collection of flows between two groups of machines in successive computation stages, where the communication stage finishes only after all the flows have completed [23, 26].###Existing coflow schedulers assume prior coflow knowledge [21, 23, 24, 30] for efficient scheduling.###As a ﬁrst-hand exercise, we have attempted to update Apache Hadoop 2.7 [58] and Apache Spark 1.6 [65] to use Aalo’s coﬂow API [24] and faced multiple roadblocks in three broad categories ( 5): the need for intrusive refactoring, mismatch between blocking and non-blocking I/O APIs, and involvement of third-party communication libraries.###Because many coflows are tiny [23] and can effectively be scheduled through local decisions [24], they do not face coordination overheads.###Because only 17% coflows create 99% traffic [23], flows from the 83% small coflows can easily be misidentified into the larger ones and suffer performance loss.###Non-Clairvoyant Schedulers Unlike Varys, Aalo [24] uses Discretized Coﬂow-Aware Least-Attained Service (D-CLAS) – that divides coﬂows into multiple priority queues and schedules in the FIFO order within each queue – to minimize average CCT without any prior knowledge of ﬂow sizes.###However, extracting these beneﬁts in practice hinges on one major assumption: all distributed data-parallel applications in a shared cluster – be it a platform-as-a-service (PaaS) environment or a shared private cluster – have been modiﬁed to correctly use the same coﬂow API.###To make things worse, most coflow-based solutions propose their own API [23, 24, 30].###Coﬂow Information Collection Modern applications are built on top of high-level abstractions such as Remote Pro-cedure Call (RPC) or message passing, rather than directly using low-level BSD socket APIs or equivalent coﬂow primitives.###By taking a holistic, application-level view, coflows avoid stragglers and yield benefits in terms of scheduling [21, 23, 24, 30], routing [68], and placement [38].",impact-revealing,discussing challenges and limitations in implementing coflow scheduling
932,5b67b4b917c44aac1c867dbc,d18b48f77eb5c517a6d2c1fa434d2952a1b0a825,hierarchical graph representation learning with differentiable pooling,5f0e4ee09fced0a24b2697ee,Spectral Networks and Deep Locally Connected Networks on Graphs,"network (GNN) models have been proposed in recent years, including methods inspired by convolutional neural networks [5, 8, 11, 16, 21, 24, 29, 36], recurrent neural networks [25], recursive neural networks [1, 30] and loopy belief propagation [7].",impact-revealing,acknowledge variations in existing GNN models
2252,5db9295f47c8f766461f5135,2c1006c856fefdbd6cd710e840e57153f2d6cd04,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,5d0b00388607575390fb4571,Exploring the Landscape of Spatial Robustness,"Many encouraging progresses been made towards improving model robustness against adversarial examples under different scenarios [58, 36, 33, 67, 72, 16, 71].",other,highlighting advancements in model robustness against adversarial examples
729,53e9aa73b7602d97033eb1bd,e958a6795a040a2df061a773ac844a9a85367cd5,an analytical approach for network-on-chip performance analysis,53e9a832b7602d97031788fd,An analytical model for wormhole routing in multicomputer interconnection networks,"The study presented in [9] is not restricted to a particular topology, but it assumes an exponential message length distribution and it has a very high complexity for high dimensional networks.###While this metric ignores the queuing delays and network contention, approaches that do consider queuing delays often make other idealistic assumptions such as exponential service times, infinite buffers, and so on [7], [9], [13].",impact-revealing,highlighting limitations in existing network metrics and assumptions
247,5db6c73a3a55acec0731cd68,73a5605ce482bd639078ebbb19baac7b903017e2,A Unified MRC Framework for Named Entity Recognition,5b67b4b417c44aac1c86756a,The Natural Language Decathlon: Multitask Learning as Question Answering.,"…time and the intensiveness in developing hand-crafted features, etc. Inspired by the current trend of formalizing NLP problems as question answering tasks (Levy et al., 2017; McCann et al., 2018; Li et al., 2019), we propose a new framework that is capable of handling both ﬂat and nested NER.###McCann et al. (2018) transformed NLP tasks such as summarization or sentiment analysis into question answering.###Inspired by the current trend of formalizing NLP problems as question answering tasks (Levy et al., 2017; McCann et al., 2018; Li et al., 2019), we propose a new framework for NER that is caar X iv :1 91 0.",impact-revealing,highlighting the inspiration drawn from recent trends in NLP
2732,5d04e8d7da56295d08daef06,448b64df68335d3695a37c54770e7d5cd5f6fe68,Context Attentive Document Ranking and Query Suggestion,53e9a789b7602d97030c0bf9,Evaluating the effectiveness of search task trails.,"This further introduces the compounding concepts of in-session task [26] and across-session task [49].###Context information embedded in a search task has shown to be useful for modeling user search intent [4, 24, 26].###, query reformulation and result clicks, often exhibit strong inter-dependency, which provides rich context information for systems to improve their retrieval performance [13, 26, 30, 51].",other,highlighting the importance of context information in user search tasks
271,5feb068b91e011f5d3420813,e339c5d31ffc7029c1f72d567ac07b4606701c72,ALP-KD: Attention-Based Layer Projection for Knowledge Distillation,5e8da0c991e011f2de5839a7,Mobilebert: A Compact Task-Agnostic Bert For Resource-Limited Devices,"Different alternatives have been proposed to this end, which compare networks’ internal layers in addition to final predictions (Jiao et al. 2019; Sun et al. 2020, 2019), but they suffer from other types of problems.###Other models such as TinyBERT (Jiao et al. 2019) and MobileBERT (Sun et al. 2020) also found it crucial for training competitive student models.###2019) and MobileBERT (Sun et al. 2020) also found it crucial for training competitive student models.",impact-revealing,acknowledge existing alternatives and their limitations
4028,5c7572b7f56def97988385ce,c342c71cb23199f112d0bc644fcce56a7306bf94,active learning for convolutional neural networks: a core-set approach,5736960c6e3b12023e51fb74,TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems.,"We optimized all models using RMSProp with a learning rate of 1e−3 using Tensorflow (Abadi et al., 2016).",other,describing the optimization method used for models
541,5fe4094e9e795e14f30e634a,94497472eecb7530a2b75c564548c540ebd61e9b,Learning to Pre-train Graph Neural Networks,599c7974601a182cd263f01c,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.,"Furthermore, our strategy is a form of meta-learning, in particular, model agnostic meta-learning (MAML) (Finn, Abbeel, and Levine 2017).###Speciﬁcally, MAML learns a prior that can be quickly adapted to new tasks by one or a few gradient updates, so that the prior, after being adapted to the so-called support set of each task, can achieve optimal performance on the so-called query set of the task.###We ﬁrst present a self-supervised base GNN model for learning graph structures in the MAML setting, followed by our dual node-and graph-level adaptations designed to simulate ﬁne-tuning during the pre-training process.###The proposed learning to pre-train can be deemed a form of meta-learning (Finn, Abbeel, and Levine 2017), also known as learning to learn.###Finally, some optimization-based methods directly adjust the optimization algorithm to enable quick adaptation with just a few examples (Finn, Abbeel, and Levine 2017; Yao et al. 2019; Lee et al. 2019; Lu, Fang, and Shi 2020).###In our case, the output of our pre-training θ 0 is the prior knowledge that can quickly adapt to new downstream tasks, while D tr T G and D te T G correspond to the support and query sets in MAML, respectively.###Speciﬁcally, our approach can be formulated as a form of MAML.",impact-revealing,describing a meta-learning approach using MAML
2457,5ec49a639fced0a24b4de8d2,5c5751d45e298cea054f32b392c12c61027d2fe7,S2ORC: The Semantic Scholar Open Research Corpus,53e9ab13b7602d9703498aff,Automatic classification of citation function,"…sentiment (Athar and Teufel, 2012), identifying meaningful citations (Valenzuela et al., 2015), extracting key phrases (Caragea et al., 2014), and citation context-based paper summarization (Teufel et al., 2006; Qazvinian and Radev, 2008; Cohan and Goharian, 2015; Mitrović and Müller, 2015).###, 2014), and citation context-based paper summarization (Teufel et al., 2006; Qazvinian and Radev, 2008; Cohan and Goharian, 2015; Mitrović and Müller, 2015).###Other tasks that leverage citation contexts in-
clude classifying citation intent (Teufel et al., 2006; Jurgens et al., 2018; Cohan et al., 2019), identifying citation sentiment (Athar and Teufel, 2012), identifying meaningful citations (Valenzuela et al., 2015), extracting key phrases (Caragea et…",other,acknowledge various tasks related to citation contexts
3797,5db9298647c8f766461f8ed6,784b018c87c7dcbbe772374e45d5191bae9938ee,Hyperbolic Graph Neural Networks,53e9bd59b7602d97049f3661,A New Model For Learning In Graph Domains,"Originally proposed by [19, 41] as a method for learning node representations on graphs using neural networks, this idea was extended to convolutional neural networks using spectral methods [9, 13] and the iterative aggregation of neighbor representations [26, 34, 45].",other,describing the evolution of a method for learning node representations
3530,5f058d15dfae54570ec57ea1,fb93ca1e004cbdcb93c8ffc57357189fa4eb6770,Resnest: Split-attention networks,599c796d601a182cd263c833,"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour","ing the learning rate linearly from 0 to the initial value for the cosine schedule [18, 36].###The batch normalization (BN) parameter γ is initialized to zero in the final BN operation of each block, as has been suggested for large batch training [18].###2 For effectively training deep CNN models, we follow the prior work [18, 34, 36] to train our models using 8 servers (64 GPUs in total) in parallel.",other,providing context for training deep CNN models
885,5f0d85c69fced0a24be4f03a,120ff576a1e722e9cb0a28c848f84d988df62292,The IBM z15 High Frequency Mainframe Branch Predictor Industrial Product,53e9b56cb7602d97040a8546,The L-TAGE Branch Predictor.,"Direction accuracy and target accuracy for indirect branches have rightfully had significant focus from academia over the decades [7] [8] [19].###The tagged PHT has been used on z branch predictions since the z196 [15] as a single table, but starting with z15 it exploits a variation of the TAGE algorithm based off of [8].###However, a weak TAGE PHT prediction can sometimes be detrimental, particularly after a context switch [8].",impact-revealing,highlighting the focus on direction accuracy and target accuracy in academia
1701,,7e48362bb32901c664de809ceb4aea89b6d3d636,Volunteer leadership: The role of pride and respect in organizational identification and leadership satisfaction,,,"###Social identity theory (Tajfel and Turner, 1979) provides such an approach (Ellemers et al., 2004) and we build upon the concepts of pride and respect (Blader and Tyler, 2009; Tyler and Blader, 2000).###…interpretation of our data not only reflects the causal relationships proposed in the theoretical framework that we used (Blader and Tyler, 2009; Tajfel and Turner, 1979; Tyler and Blader, 2000), but also is consistent with results from prior correlational and experimental research (e.g. Blader…",impact-revealing,building upon social identity theory to interpret data and establish causal relationships
28,5b3d98cc17c44a510f802054,8a564ee07fa930ebc1176019deacdc9951063a99,Collaborative Learning for Deep Neural Networks.,53e9a603b7602d9702f348f6,Multitask learning: a knowledge-based source of inductive bias,"Multi-task learning is an approach to learn multiple related tasks simultaneously so that knowledge obtained from each task can be reused by the others [4, 3, 21].###To keep the exact same computational complexity for inference, several training techniques have been developed by adding additional networks in the training graph to boost accuracy without affecting the inference graph, including auxiliary training [19], multi-task learning [4, 3], and knowledge distillation [10].###This structure is very similar to multi-task learning [4, 3], in which different supervised tasks share the same input, as well as some ILR.",impact-revealing,describing multi-task learning and its benefits
779,599c797a601a182cd2642797,668db48c6a79826456341680ee1175dfc4cced71,Get To The Point: Summarization with Pointer-Generator Networks,5736986b6e3b12023e72f5c0,Pointer Networks.,"Our pointer-generator network is a hybrid between our baseline and a pointer network (Vinyals et al., 2015), as it allows both copying words via pointing, and generating words from a ﬁxed vocabulary.###The pointer network (Vinyals et al., 2015) is a sequence-tosequence model that uses the soft attention distribution of Bahdanau et al.###Our hybrid pointer-generator network facilitates copying words from the source text via pointing (Vinyals et al., 2015), which improves accuracy and handling of OOV words, while retaining the ability to generate new words.###The pointer network (Vinyals et al., 2015) is a sequence-to-sequence model that uses the soft attention distribution of Bahdanau et al. (2015) to produce an output sequence consisting of elements from the input sequence.###Our pointer-generator network is a hybrid between our baseline and a pointer network (Vinyals et al., 2015), as it allows both copying words via pointing, and generating words from a fixed vocabulary.",impact-revealing,describing the hybrid pointer-generator network and its advantages
257,5c8fd41a4895d9cbc66534e9,950aae7979a2faa874b7481179c064d3ad151ea8,Objects Segmentation From High-Resolution Aerial,5aed14d617c44a4438158d63,Land cover mapping at very high resolution with rotation equivariant CNNs: towards small yet accurate models.,There are several algorithms that are applied to object segmentation based on CNNs [22]–[24].,impact-revealing,acknowledge existing algorithms for object segmentation
3796,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,56d88a4cdabfae2eeeb0b267,On campus network P2P and its link control,"There is interest from ISPs and network administrators to identify and control the P2P network traffic [49, 154].",other,highlighting interest in controlling P2P network traffic
867,53e9a232b7602d9702b3a1a9,327722247ffc70a0d51f5c2246bc9a53c0e7daa3,Accurate branch prediction for short threads,56d85d58dabfae2eee60a7ff,Simultaneous Subordinate Microthreading (SSMT),"Helper threads [4, 8, 52, 31, 50, 51] execute short threads in support of the original thread of execution, often for cache prefetching.",impact-revealing,providing context for helper threads in execution
1355,,685c0b1a99e6694499c75c88a67eb1c9a20547b6,Visual Acuity in Mammals: Effects of Eye Size and Ecology,,,"###…Alouatta caraya, Chlorocebus aethiops, Saguinus midas and Choloepus didactylus) , we modified established protocols for calculating anatomical acuity based on published eye lengths and either peak retinal ganglion cell density or peak cone density [Pettigrew et al., 1988; Arrese et al., 1999].###Consequently, diurnal mammals are expected to have higher acuity than cathemeral or nocturnal species [Walls, 1942; Hughes, 1977; Pettigrew et al., 1988; Heesy and Hall, 2010].###As a result, studies that calculate visual acuity based on PND and peak ganglion cell density typically use measurements of eye length to estimate PND [Hughes, 1977; Pettigrew et al., 1988; Arrese et al., 1999; Pettigrew et al., 2010].###Accordingly, we calculated visual acuity for diurnal haplorhines using the formula (RMF     ·     √ peak cone cell density)/2, and for all other taxa using the formula (RMF    ·    √ peak ganglion cell density)/2 [Pettigrew et al., 1988].###Although eye length is one of the variables that is often used to calculate anatomical estimates of visual acuity [e.g. Hughes, 1977; Pettigrew et al., 1988], we also found that the relationship between acuity and eye length does not substantially change de-
pending on whether acuity is measured…###This method for estimating PND accounts for the fact that eye morphology in mammals varies predictably with diel activity pattern [Kirk, 2004; Hall et al., 2012].###As a result, the density of ganglion cells represents a limiting factor for acuity [Pettigrew et al., 1988; Kay and Kirk, 2000].###These larger eyes of cathemeral species probably offset increased retinal summation and relatively shorter PND compared to the diurnal sample, leading to comparable raw acuity in the two groups.###…acuity (i.e. the ability to resolve spatial details), ranging from the low acuity vision of microchiropteran bats and small rodents (0.4–1.0 cycles per degree, or cpd) to the highly acute vision (30–64 cpd) of diurnal anthropoid primates [Walls, 1942; Pettigrew et al., 1988; Kirk and Kay, 2004].###We therefore reclassified these two species, yielding a total comparative sample of 6 nocturnal, 6 cathemeral and 11 diurnal vertebrate species for which both PND and eye length can be used to calculate k [cf. table  1 in Schmitz, 2009].###…Behav Evol 2014;83:43–53 DOI: 10.1159/000357830
51
active mammals differ from nocturnal mammals in having eye morphology and retinal anatomy that supports higher acuity [Walls, 1942; Hughes, 1977; Pettigrew et al., 1988; Kirk, 2006b; Heesy and Hall, 2010; Hall et al., 2012; Land and Nilsson, 2012].###Following Pettigrew et al. [1988], we first estimated PND from eye length using a multiplicand k , which differs according to diel activity pattern.###If eye shape is held constant, an increase in eye length produces a longer posterior nodal distance (PND) and in-
© 2014 S. Karger AG, Basel
Carrie C. Veilleux, PhD Department of Anthropology SAC 4.102, University of Texas at Austin 2201 Speedway Stop C3200, Austin, TX 78712 (USA) E-Mail carrie.veilleux   @   utexas.edu © 2014 S. Karger AG, Basel 0006–8977/14/0831–0043$39.50/0 www.karger.com/bbe
Veilleux/Kirk Brain Behav Evol 2014;83:43–53 DOI: 10.1159/000357830 44
creases the size of the retinal image ( fig.###0 cycles per degree, or cpd) to the highly acute vision (30–64 cpd) of diurnal anthropoid primates [Walls, 1942; Pettigrew et al., 1988; Kirk and Kay, 2004].###By contrast, diurnal haplorhines exhibit no retinal summation in the fovea [Kirk and Kay, 2004], so acuity is limited by the density of cones [Pettigrew et al., 1988].###This result is consistent with expectations based on optical considerations [Walls, 1942; Kirschfeld, 1976; Hughes, 1977; Pettigrew et al., 1988] and previous findings based on smaller comparative samples [e.g. Kiltie, 2000; Heesy and Hall, 2010].###Accordingly, we calculated visual acuity for diurnal haplorhines using the formula (RMF     · √ peak cone cell density)/2, and for all other taxa using the formula (RMF · √ peak ganglion cell density)/2 [Pettigrew et al., 1988].###After estimating PND based on eye length, we calculated the retinal magnification factor (RMF) as (2π     ·     PND)/360.###…aspects of visual morphology have linked increased visual acuity to a variety of ecological factors, including diel activity pattern, diet and speed of locomotion [Walls, 1942; Hughes, 1977; Pettigrew et al., 1988; Arrese et al., 1999; Kirk and Kay, 2004; Peichl, 2005; Heard-Booth and Kirk, 2012].### 1 ) [Hughes, 1977; Pettigrew et al., 1988; Ross, 2000].###Anatomical estimates of acuity represent a theoretical maximum and tend to slightly overestimate acuity measured behaviorally [Pettigrew et al., 1988; Arrese et al., 1999].###Because eye length is involved in calculations of anatomical acuity [albeit transformed by the factor k according to diel activity pattern; Pettigrew et al., 1988], there is some concern of circularity in using anatomically derived estimates to explore the relationships between acuity and eye length.###For 14 species (Bos taurus, Capra hircus, Dama dama, Sus scrofa, Acinonyx jubatus, Canis lupus, Sarcophilus harrisi, Didelphis virginiana, Macropus fuliginosus, Setonix brachyurus, Alouatta caraya, Chlorocebus aethiops, Saguinus midas and Choloepus didactylus) , we modified established protocols for calculating anatomical acuity based on published eye lengths and either peak retinal ganglion cell density or peak cone density [Pettigrew et al., 1988; Arrese et al., 1999].###Comparative studies of eye length and other aspects of visual morphology have linked increased visual acuity to a variety of ecological factors, including diel activity pattern, diet and speed of locomotion [Walls, 1942; Hughes, 1977; Pettigrew et al., 1988; Arrese et al., 1999; Kirk and Kay, 2004; Peichl, 2005; Heard-Booth and Kirk, 2012].###Because eye length is involved in calculations of anatomical acuity [albeit transformed by the factor k according to diel activity pattern; Pettigrew et al., 1988], there is some concern of circularity in using anatomically derived estimates to explore the relationships between acuity and eye…",impact-revealing,highlighting the relationship between visual acuity and ecological factors
1898,,4f48b17eea18112ee07bef164001b346870a40d0,Students' Learning Experiences towards the Use of Assessments in a Virtual Learning Environment (VLE),,,"###Garrison et al. (2000) described that the CoI framework as a communication and interaction framework to support the optimal learning process and builds on social-constructivist approaches to instruction and learning.###In this study, Community of Inquiry (COI) framework which contains teaching presence, cognitive presence and social presence have been used to describe the online and face-to-face learning environments (Garrison, Anderson, & Archer, 2000; Garrison & Arbaugh, 2007).",impact-revealing,describing the Community of Inquiry framework and its relevance to learning environments
2611,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,5e5e191a93d709897ce50e0c,Low Resource Sequence Tagging With Weak Labels,Nguyen et al. (2017); Rodrigues and Pereira (2018); Simpson et al. (2020) regards crowd annotations as noisy gold labels and constructs crowd components to model annotator-speciﬁc bias which were discarded during the inference process.,other,reporting prior findings on crowd annotations
1856,,b588f725afa88498e2a09b5b9e56f51f7007cca6,Thymoquinone and diallyl sulfide protect against fipronil-induced oxidative injury in rats,,,"###Fipronil (FPN) is a phenylpyrazole insecticide, widely used for agricultural and veterinary activities (Tingle et al. 2003).",impact-revealing,providing context about Fipronil as an insecticide
2157,,775769ae9822ba541bd55e24bd45fba2744eb16c,Branch-and-Bound with Peer-to-Peer for Large-Scale Grids,,,"###This initiative is inspired by the Grid Application Toolkit (GAT) [ALL 05] and Globus-COG [LAS 01] which enhances the capabilities of the Globus Toolkit by providing workflows, control flows and task management at a high level of abstraction.",impact-revealing,highlighting the inspiration from existing frameworks for enhancing capabilities
545,5e5e189993d709897ce1e202,2bf7c350a8280e7c593d46a60127f99b21517121,on the variance of the adaptive learning rate and beyond,5cede0f4da562983788d2b3d,Adaptive Gradient Methods with Dynamic Bound of Learning Rate.,"…al., 2013), smoothing the adaptive learning rate ( i.e. , increasing (cid:15) , applying geometric mean ﬁlter (Chen et al., 2018), or adding range constraints (Luo et al., 2019)), initialization (Balduzzi et al., 2017; Zhang et al., 2019) and normalization (Ba et al., 2016; Ioffe & Szegedy, 2015).###, increasing , applying geometric mean filter (Chen & Gu, 2018), or adding range constraints (Luo et al., 2019)), initialization (Balduzzi et al.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1789,,89a7cac3ccad4f23f469fccede4bc44e139e9dee,FollowAKOInvestor: Using Machine Learning to Hear Voices from All Kinds of Investors,,,"###This method is commonly used by conventional methodologies [2, 3, 20, 21, 22] with the formulation: where #Bul exp(s) and #Bea exp(s) are weighted count of bullish and bearish opinions from all investors to stock s while the opinion weights are expertise values of the opinion authors.",impact-revealing,describing a commonly used method in conventional methodologies
2703,5ec49a639fced0a24b4de8d2,5c5751d45e298cea054f32b392c12c61027d2fe7,S2ORC: The Semantic Scholar Open Research Corpus,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"To demonstrate the suitability of S2ORC for language model pretraining, we train BERT-Base (Devlin et al., 2019) on the parsed full text",other,demonstrating the application of S2ORC for language model pretraining
3712,5ede0553e06a4c1b26a83f63,1d81e7f428fea2b2e15ee3a96fe843ca603acc4c,Simple and Deep Graph Convolutional Networks,5d9ed28247c8f76646f6e046,Encoding Social Information With Graph Convolutional Networks For Political Perspective Detection In News Media,"…al., 2016; Veliˇckovi´c et al., 2018) have been successfully applied to a wide range of applications, including social analysis (Qiu et al., 2018; Li & Goldwasser, 2019), trafﬁc prediction (Guo et al., 2019; Li et al., 2019), biology (Fout et al., 2017; Shang et al., 2019), recommender systems…###…et al., 2016; Veliˇckovi´c et al., 2018) have been successfully applied to a wide range of applications, including social analysis (Qiu et al., 2018; Li & Goldwasser, 2019), trafﬁc prediction (Guo et al., 2019; Li et al., 2019), biology (Fout et al., 2017; Shang et al., 2019), recommender systems…",other,acknowledge the broad applicability of existing methods
3505,5a260c8117c44a4ba8a30f54,33998aff64ce51df8dee45989cdca4b6b1329ec4,Graph Attention Networks,573695fd6e3b12023e511373,Deep Convolutional Networks on Graph-Structured Data,Henaff et al. (2015) introduced a parameterization of the spectral ﬁlters with smooth coefﬁcients in order to make them spatially localized.,other,reporting prior findings on spectral filters
2349,5c8b99db4895d9cbc69c7956,add350d0c5605c98d285b87493fc77c1d68281df,architectural support for server-side php processing,5736982b6e3b12023e6fd21d,Microarchitectural Implications Of Event-Driven Server-Side Web Applications,"Prior work [73] concentrated on server-side Javascript applications.###CPU-like workloads contrary to the instruction cache behavior observed in prior works with other server-side applications (serversside Javascript applications [73] or memcached workloads [55]).###Prior works propose instruction prefetching [28], preexecution techniques [29] and modifying cache insertion policy [73] to mitigate this.###Around 12% of all dynamic instructions are branches in the SPEC CPU2006 workloads [73], whereas in the PHP applications about 22% of all instructions are branches, thus adding more pressure on BTB.",other,highlighting differences in instruction behavior across applications
2744,5b1643ba8fbcbf6e5a9bc79b,b3f83e8416010e9c3a705a0b6390d268e5ddf5c0,Black-box adversarial attacks with limited queries and information,57a4e91aac44365e35c975d0,Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples.,"Papernot et al. (2016a; 2017) have successfully used this method to attack commercial classiﬁers like the Google Cloud Prediction API, the Amazon Web Services Oracle, and the MetaMind API, even evading various defenses against adversarial attacks.###Prior work considers various threat models (Papernot et al., 2016b; Carlini & Wagner, 2017).###Papernot et al. (2016a; 2017) trained the Cloud Prediction API with small datasets like MNIST and successfully demonstrated an untargeted attack.###Copyright 2018 by the author(s). a substitute network to emulate the original network and then attacks the substitute with ﬁrst-order white-box meth-ods (Papernot et al., 2016a; 2017).###To attack this setting, we can use “standard” ﬁrst-order techniques for generating adversarial examples Goodfellow et al. (2015); Papernot et al. (2016b); Madry et al. (2017); Carlini & Wagner (2017), substituting the gradient of the loss function with an estimate of the gradient, which is…",other,highlighting the effectiveness of adversarial attacks on commercial classifiers
3743,5f4f6ec291e0111f07b30a2b,3259c9ab1714a4cfdf6439cca6bdc5f78d78fda3,NATS-Bench: Benchmarking NAS Algorithms for Architecture Topology and Size,5a9cb65d17c44a376ffb820b,Regularized Evolution for Image Classifier Architecture Search,"These NAS-generated architectures have shown promising results in many domains, such as image recognition [4], [5], [6] and sequence modeling [5], [7], [8].###Other methods mutate an architecture to become another one [6], [39].###THE deep learning community is undergoing a transition from hand-designed neural architectures [1], [2], [3] to automatically designed neural architectures [4], [5], [6], [7], [8].###NAS has been dominated by multi-fidelity based methods [6], [27], [36], [38], which learn to search based on an approximation of the performance of each candidate in order to accelerate searching.",other,highlighting the transition in deep learning from hand-designed to automatically designed neural architectures
3685,53e9b9fbb7602d97045fabff,f89facea7ae5a3f51af96b549f04f3dbe8c884b3,SHIFT: Shared history instruction fetch for lean-core server processors,53e9a2b2b7602d9702bb9cc3,Code Layout Optimizations For Transaction Processing Workloads,"Prior software-base approaches proposed optimizing the code layout to avoid conflict misses [27, 38, 47], inserting instruction prefetch instructions at compile time [23], and exploiting the recurring call-graph history [3].",other,reporting prior software-based approaches
1928,,993e219c32d23fcb06278d9324afc3ad3484a915,Nanotechnology as Emerging Tool for Enhancing Solubility of Poorly Water-Soluble Drugs,,,"###Therefore, at present, polyeletrolyte complexs are widely used as carrier of drugs [99, 102-105], nonviral vectors of transferred genes [92, 106-109], biospecific sorbents [110, 111], films [112-114], and gels [115-119].",impact-revealing,reporting various applications of polyelectrolyte complexes
2032,,713b7944217ac4971b521c940fc0cb4a0e1821c2,Computational infrared and Raman spectra by hybrid QM/MM techniques: a study on molecular and catalytic material systems,,,"###The system was optimized at the QM/MM level using the DLFIND [53] module before calculating the vibrational spectra.###The calculation of classical vibrational modes at a harmonic level is implemented in Py-ChemShell via the integrated DL-FIND geometry optimization library [53] using a finite-difference approximation of the dynamical matrix.###Implementing the IR and Raman calculations with the DL-FIND module results in a flexible, modular framework for vibrational spectroscopy, which can be used with any of the QM packages interfaced to ChemShell, including NWChem [28], GAMESS-UK [27], FHI-aims [61], LSDalton [32] and others, controlled by a common Python interface.###Kästner J, Carr JM, Keal TW, Thiel W, Wander A, Sherwood P. 2009 DL-FIND: an opensource geometry optimizer for atomistic simulations.###Geometries were optimized using the built-in DL-FIND [53] module of Py-ChemShell.###The work builds on existing functionality in DL-FIND, which can calculate vibrational frequencies in the harmonic approximation, with elements of the Hessian matrix evaluated numerically as the first differences of analytical gradients in mass-weighted coordinates, using two-point central differences.###The previously developed DL-FIND design has been exploited here to support numerical evaluation of the first derivatives of the dipole moment and polarizability tensor along vibrational normal modes that appear in equations (2.3) and (2.8).",impact-revealing,describing the optimization process and tools used for vibrational spectroscopy calculations
2166,,2215579116cbca9e3a02e7490545f8f1045e6433,Tentative steps toward a development method for interfering programs,,,"###Modal logic is now being widely used for such problems [1, 9, 23, 27, 35].",impact-revealing,highlighting the growing use of modal logic in various problems
960,,13c6f1c902e487a4aa65810b62e4e1b22a9feafc,The Wisconsin Card Sorting Test and the cognitive assessment of prefrontal executive functions: A critical update,,,"###Some ERP components elicited to the target cards show an analogous mid-parietal scalp topography as the ERP signature of the shift in set at cue onset, and these components need to be segregated through experimental or statistical design to avoid potential confounds (Barceló et al., 2006; Periañez & Barceló, 2009).###Short-term phasic ERPs (P1 and N1) are overlapped with longer latency slow brain potentials, indicating both serial and parallel processing of information (Barceló et al., 2006; Periañez & Barceló, 2009).###The activation of a category representation in working memory together with the concurrent inhibition of the previous category are some of the component operations thought to occur during set-shifting (Barceló et al., 2006; Robbins, 2007; Rogers et al., 2000).###, 2008) and as indexed by overlapping slow ERP negativities (Barceló et al., 2006; Nicholson et al., 2005).###, 1998), temporo-parietal association cortex (Barceló & Rubia, 1998; Barceló, Sanz, Molina, & Rubia, 1997; Barceló et al., 2006; Konishi et al., 2002; Ragland et al., 1998), as well as in primary and secondary association visual cortices (Berman et al.###Finally, the ERPs to the negative feedback (or task-switch) cues is reminiscent of a brain novelty P3 response to contextually novel and surprising distractors (Friedman, Cycowicz, & Gaeta, 2001) and they both seem to recruit activity from a common fronto-posteriorly distributedneural network responsible for processing contextual novelty (Barceló et al., 2006; Ranganath & Rainer, 2003).###The finding that ‘task-irrelevant’ novel distractors and ‘task-relevant’ task-switch cues elicit similar brain responses was quite surprising (Barceló et al., 2006), but has ultimately provided important insights on clinical studies showing that prefrontal lesions cause both perseverative and non-perseverative (‘set loss’) errors.",impact-revealing,highlighting the significance of ERP components in cognitive processing and their implications for clinical studies
774,5dea04309e795e693620e97c,b0c35bf9ddffefb0dab4f76c20b30e00a22b1e0a,unsupervised author disambiguation using heterogeneous graph convolutional network embedding,5b67b45517c44aac1c86078b,"Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop.","[1] use a global metric learning and local linkage graph auto-encoder algorithm to learn the representation of publications, but it requires lots of human labeled data to train the model.###Zhang et al. [1] also use a graph convolutional network based encoder-decoder model but on homogeneous graph that can not extract multi-layer relationship that contains various relation types.###High quality representations play a critical role to quantify distinctions and similarities between publications [1].###For example, [5] need to specify the number of distinct author, [1] need labeled data to estimate the number.###Hierarchical Agglomerative Clustering (HAC) method works well for skewed data and is widely used in many name disambiguation methods [1], [5], [9], [20], [21].###Zhang et al. [1]: This method uses a global metric learning and local linkage learning based on a graph auto-encoder method to learn the publications embeddings, then it propose an end-to-end model to estimate the number of clusters using a recurrent neural network and use HAC to determine the…###Supervised methods [1], The research is supported by the National Key Research and Development Plan (2017YFC1601504), the Natural Science Foundation of China (61836013), the CNTC (China National Tobacco Corporation ) Science and Technology Major Project (110201901027(SJ-06)), and the Guangdong…",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3144,5b67b4b917c44aac1c867dbc,d18b48f77eb5c517a6d2c1fa434d2952a1b0a825,hierarchical graph representation learning with differentiable pooling,599c797b601a182cd2642a1a,Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs,"• Edge-conditioned filters in CNN for graphs (ECC) [35] incorporates edge information into the GCN model and performs pooling using a graph coarsening algorithm.###Lastly, there are some recent works that learn hierarchical graph representations by combining GNNs with deterministic graph clustering algorithms [8, 13, 35], following a two-stage approach.###1We do not consider edge features, although one can easily extend the algorithm to support edge features using techniques introduced in [35].###• Edge-conditioned filters in CNN for graphs (ECC) [36] incorporates edge information into the
GCN model and performs pooling using a graph coarsening algorithm.",other,describing recent advancements in graph neural networks
3115,573697c96e3b12023e6a9ac3,c8d4d601e1677ab93fa8c0a3392b152c48b94d80,How to bid the cloud,555043f945ce0a409eb4a77a,Pricing in Infrastructure Clouds - An Analytical and Empirical Examination.,"The majority of today’s pricing plans for cloud resources are variants on usage-based pricing, in which users pay a static per-unit price (per workload or per hour) to access cloud resources [21].",other,providing context on cloud pricing models
3027,5c6a37d03a69b1c9e12a9fc4,81b6d24e8f313fd88b0fe5ff6c21dd154fbe32d2,SimGNN: A Neural Network Approach to Fast Graph Similarity Computation,5b67b45517c44aac1c8608a9,Graph Classification using Structural Attention.,[24] uses an attention-guided walk to find the most relevant parts for graph classification.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
711,53e99e61b7602d97027252c8,e3369553f399fb63b9f24261247afe64fb6c3f65,Virtual Program Counter (VPC) Prediction: Very Low Cost Indirect Branch Prediction Using Conditional Branch Prediction Hardware,53e9bc10b7602d9704873443,Reducing indirect function call overhead in C++ programs,"However, even though there could be many different shapes in the program, if the types of shapes are mostly either an instance of the Rectangle class or the Circle class at runtime, the compiler can convert the indirect call to multiple guarded direct calls [21], [18], [6], as shown in Fig.###usually has higher accuracy than an indirect branch predictor [6].###The VPC prediction algorithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) [11], [24], [21], [6] or devirtualization [28].###Devirtualization is the substitution of an indirect method call with direct method calls in object-oriented languages [11], [24], [21], [6], [28].###In order to perform RCPO, the following conditions need to be fulfilled [18], [6]:",impact-revealing,discussing compiler optimization techniques and their implications
2731,5ec49a639fced0a24b4de922,0d965ed237a3b4592ecefdb618c29f63adedff76,Towards Debiasing Sentence Representations,5b8c9f5317c44af36f8b75ab,Reducing Gender Bias in Abusive Language Detection,"In particular, Zhao et al. (2019), Park et al. (2018), and Garg et al. (2019) are not able to perform post-hoc debiasing and require changing the data or underlying word embeddings and retraining which is costly.",other,highlighting limitations in existing debiasing methods
1760,,fb682aafe7eb097eb7af9bbd2e660b20df723ed0,SMAUG,,,"###Broadly speaking, the architecture community has focused on designing efficient dataflows to maximize local reuse of data and functional unit utilization [1], [2], [4]–[7], [52], [53], exploit model sparsity and data quantization [3], [21], [54]– [58], map DNN accelerators to FPGAs [59]–[61], explore alternative computation and memory technologies [62], [63], or use multi-chip-module package integration to achieve highperformance DNN inference [64].###But such a schedule is not suitable for an accelerator that computes 1D or 2D convolutions, like the row-stationary dataflow [2].",impact-revealing,acknowledge existing research directions in architecture for efficient dataflows
920,5c757416f56def979890a549,0ded7a6f60e160f6d6baed1dddc39371797f8fef,network-based prediction of protein interactions,53e99b36b7602d97023d34e7,Protein Networks In Disease,"[28] Ideker T & Sharan R, Protein networks in disease.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
755,5aed146117c44a44381527f9,0aef7a464840621c384380ac8877ae53b73d23a8,Blasting through the Front-End Bottleneck with Shotgun,599c7ea4601a182cd28b81ed,Boomerang: A Metadata-Free Architecture for Control Flow Delivery,"The rest of the predecoded branches are stored in the BTB Prefetch Buffer [13].###Furthermore, conditional branches that guide local control flow tend to have very short displacements, typically within a few cache blocks [13], as shown by dashed arrows in Figure 2.###For filling the BTBs, Shotgun takes a hybrid approach by incorporating the features from both Boomerang [13] and Confluence [10].###Furthermore, it adds significant complexity to the processor as it requires LLC tag extensions, reduction in effective LLC capacity, pinning of metadata cache lines in the LLC and the associated system software support, making it an expensive proposition as shown in prior work [13].###To overcome the overwhelming metadata storage costs of temporal streaming, the latest work in relieving the frontend bottleneck leverages fetch-directed instruction prefetching (FDIP) [15] and extends it with unified prefetching into the BTB [13].###Recent work has addressed this limitation by adding a BTB prefetch capability in a technique called Boomerang [13].###[13], owing to frequent LLC accesses for loading history metadata.",impact-revealing,acknowledge existing methods and their complexities in processor design
3194,5efb0d5691e011063336d27d,2b364917b0c51e91fcf2ab9c1d66a14ed4b44c03,BERTology Meets Biology: Interpreting Attention in Protein Language Models,5bdc318017c44a1f58a089e3,NGL viewer: web-based molecular graphics for large complexes.,"Visualizations based on the NGL Viewer [54, 67, 68]. seeks to shed light on biological processes that are not yet fully understood.",other,highlighting the role of visualizations in understanding biological processes
788,53e99d12b7602d97025c4239,806c3a77f5128e7bd927e07784de655971058a07,does cache sharing on modern cmp matter to the performance of contemporary multithreaded programs,53e9b693b7602d970420604e,The PARSEC benchmark suite: characterization and architectural implications,"…to commonly perceived importance of cache sharing, 
neither positive nor negative effects from the cache sharing are signi.cant for most of the program executions, 
regardless of the types of par­allelism, input datasets, architectures, numbers of threads, and as­signments 
of threads to cores.",impact-revealing,Highlighting the insignificance of cache sharing effects across various conditions
1184,,3ccf41665519d307700eb907b69138d8626ca927,Optimizing OpenCL Code for Performance on FPGA: k-Means Case Study With Integer Data Sets,,,"###The k-means algorithm [15] is a process of data quantization that is widely used for data clustering and classiﬁcation [37], [38], and is summarized in Algorithm 1.",impact-revealing,providing context for the k-means algorithm and its applications
4053,573698456e3b12023e70ee1b,524664475292ad6cdbdda3992fe5dc8f036b6ce5,Deep learning for emotion recognition on small datasets using transfer learning,53e9b316b7602d9703ddc5f6,Partial Least Squares Regression On Grassmannian Manifold For Emotion Recognition,In [13] convolutional and audio features were fused us-ing Partial Least Squares and multiple classiﬁers.,other,reporting prior findings on feature fusion methods
1915,,7beb1e8eb8e72a37d60fee61bc4fcaa4a504f9fc,Cognitive presence among mathematics teachers: An analysis of tasks and discussions in an asynchronous online graduate course,,,"###The social presence element of the COI model is viewed as the ability of students to project themselves socially and affectively into a community of inquiry (Rourke, Anderson, Garrison, & Archer, 1999), which is important due to the lack of physical presence that occurs in a face-to-face classroom.###Online conferencing is more than a means to access information, it is believed to have the potential to facilitate deep and meaningful learning (Garrison et al., 2000; Garrison, 2003; Littleton & Whitelock, 2005; McLoughlin & Luca, 2000).###A research technique for the objective, systematic, quantitative description of the manifest content of communication ((Berelson, 1952, p. 519) in Rourke, Anderson, Garrison, & Archer (2000)).###The Community of Inquiry (COI) model developed by Garrison et al. (2001) “was designed to guide the use of computer conferencing to support critical thinking in higher education” (Rourke, Anderson, Garrison, & Archer, 1999, ¶ 1), thus providing a framework for taking into account a sociocultural perspective within the new educational medium of online learning.###…learning within a computer mediated conferencing environment occurs when three core elements interact: social, teaching, and cognitive presence (Garrison et al., 2000).
to be useful in analyzing and understanding interactions that occur within an asynchronous learning environment (Garrison &…###Anderson, T., Rourke, L., Garrison, D.R., & Archer, W. (2001).###Retrieved electronically April 19, 2005 from http://www.jime.open.ac.uk/2002/1/
Rourke, L., Anderson, T., Garrison, D.R., & Archer, W. (1999).###Garrison describes teaching presence as “a means to an end” where social and cognitive presence is enhanced in order to reach the educational goals set forth (Garrison et al., 2000).###(management of task)
Community of Inquiry Model
“The Community of Inquiry model was designed to guide the use of computer
conferencing to support critical thinking in higher education” (Rourke, Garrison, Anderson, & Archer, 1999).###Cognitive presence is defined as the extent to which participants are able to construct meaning through sustained communication (Garrison et al., 2000).###Higher order learning, in terms of both process and outcome, is frequently cited as the goal of higher education (Garrison et al., 2000), yet the adoption of computer###Garrison et al. (2000) believe that text-based communication is preferable to oral communication when the end objective is to promote understanding in the form of higher order learning.###Statement of the Problem
Higher order learning, in terms of both process and outcome, is frequently cited as
the goal of higher education (Garrison et al., 2000), yet the adoption of computer mediated communication in higher education has far outpaced our understanding of how this medium should…###Rourke, L., Anderson, T., Garrison, D. R., & Archer, W. (2000).###Appropriate teaching presence is also needed to produce discussions with high levels of cognitive presence (Garrison et al., 2000).###In one study, Garrison et al. (2000) used their original coding scheme to code
three transcripts.###The validity of the descriptions depends largely on four criteria discussed by Rourke, Garrison, Anderson, and Archer (2000): objectivity, reliability, replicability, and systematic coherence.###“Socio-emotional interaction and support are important and sometimes essential in realizing meaningful and worthwhile educational outcomes” such as cognitive presence (Garrison et al., 2000, p. 95).###Cognitive Presence
Cognitive presence is defined as the extent to which participants are able to
construct meaning through sustained communication (Garrison et al., 2000).###Garrison et al. (2000) note that, “Critical inquiry and the quality of discourse is facilitated and optimized when students see themselves as part of a group rather than as individuals” (p. 101) so finding strategies to enhance the student’s sense of group cohesion would likely benefit the…###Teaching presence relates to the design, facilitation, and direction of cognitive and social processes for the purpose of realizing personally meaningful and educationally worthwhile learning outcomes (Anderson, Rourke, Garrison, & Archer, 2001).###learning networks have considerable potential for creating an educational community of inquiry (Garrison et al., 2000).###The model of a Community of Inquiry (Figure 1) assumes that learning within a computer mediated conferencing environment occurs when three core elements interact: social, teaching, and cognitive presence (Garrison et al., 2000).###Teaching presence has been cited in numerous studies as an important aspect in moving students from Exploration to higher levels of thinking (Bullen, 1998; Garrison et al., 2000; Garrison et. al, 2005; Kanuka & Anderson, 1998; McLoughlin & Luca, 2000).",impact-revealing,highlighting the importance of the Community of Inquiry model in online learning
3842,5e5e18ba93d709897ce2b48e,04f3203f1214063436d81ce0c2ad7623204da488,Geom-GCN: Geometric Graph Convolutional Networks,58d82fc8d649053542fd59aa,Geometric deep learning on graphs and manifolds using mixture model CNNs,"One can design a more sophisticated operator τ , such as borrowing the structure of descriptors in manifold geometry (Kokkinos et al., 2012; Monti et al., 2017), thereby preserving more and richer structural information in neighborhood. lower left lower right Finally, to implement the bi-level…",other,providing context for designing sophisticated operators
1613,,002b01a4a73b7ec224813ae4c0de75fa3d464be1,C-Reactive-Protein-Associated Increase in Myocardial Infarct Size After Ischemia/Reperfusion,,,"###The pathophysiology of ischemic heart disease has an important inflammatory component (Yeh et al., 2001) in which an increased serum C-reactive protein concentration (CRP), commonly used as a marker for an acute inflammatory response, is associated with increased mortality due to cardiovascular…###The pathophysiology of ischemic heart disease has an important inflammatory component (Yeh et al., 2001) in which an increased serum C-reactive protein concentration (CRP), commonly used as a marker for an acute inflammatory response, is associated with increased mortality due to cardiovascular events (Lagrand et al.",impact-revealing,highlighting the significance of inflammatory components in ischemic heart disease
1840,,0404ad567284976b7cf5019aa850b572fa762e80,Verifying security protocols as planning in logic programming,,,"###As in Bella and Paulson’s approach [Bella and Paulson 1998], modeling message reception is simple:
{gets(B, M, T )}← says(A, B, M, T ) We use the choice rule (see Section 3) to specify that if A attempts to send a message M to B at time T , then B may receive it.###This is a widely used underlying semantic model adopted by researchers in formal verification [Abadi and Tuttle 1991; Meadows 1994; Lowe 1996; Paulson 1998; Schneider 1998].###…1994], model checking and verification in process algebras [Lowe 1996; Schneider 1998], theorem proving with induction [Paulson 1998; Bella and Paulson 1998], and state exploration methods [Mitchell et al. 1997; Meadows 1994] have been successfully used to verify and debug security protocols.###As in Paulson’s approach for verifying security protocols [Paulson 1998; Bella and Paulson 1998], we describe each protocol run as a trace of send and receive actions.###As for the approaches based on theorem proving, we have already pointed out connections between our proposal and Paulson’s inductive method [Paulson 1998; Bella and Paulson 1998].###Even if it is conceptually identical to the corresponding rules in other formal verification papers [Burrows et al. 1990; Lowe 1996; Paulson 1998; Schneider 1998] it is not satisfactory from an epistemological point of view: in fact, it is not enough for A to know K I and for K I to be the inverse…###…Syverson and van Oorschot 1994], model checking and verification in process algebras [Lowe 1996; Schneider 1998], theorem proving with induction [Paulson 1998; Bella and Paulson 1998], and state exploration methods [Mitchell et al. 1997; Meadows 1994] have been successfully used to verify and…###These actions are basically present in the inductive theory of traces by Paulson and Bella [Paulson 1998; Bella and Paulson 1998].###The definition is borrowed from Schneider [1998] and is substantially equal to Paulson’s properties [Bella and Paulson 1998; Paulson 1998].###…Bis , mis , T ), got(A, m j1 , T ), . . . , got(A, m jr , T ) knows(spy, m, T ), protocol-dependent literals
In some cases, to correctly model authentication and confidentiality attacks, it is necessary to add an oops-rule, where the intruder gets hold of past confidential data [Paulson 1998].###This description of actions is fairly close to Paulson’s inductive presentation of security protocols which uses traces of atomic send and receive actions [Paulson 1998; Bella and Paulson 1998].###As for operators on messages, we only need Paulson’s inductively defined operators on set of messages: analz, synth, and parts [Paulson 1998].###Thus, our method, based on a logical specification for model-checking security protocols, offers an alternative to process algebras [Lowe 1996; 1998] and state exploration methods [Meadows 1994; Mitchell et al. 1997], and complements Paulson’s inductive approach [Paulson 1998].",impact-revealing,describing a widely used semantic model in formal verification
3463,599c7945601a182cd2629f72,000178cd12c8a6e5da8215b6365fae03c20fd18d,End-to-End Representation Learning for Correlation Filter Based Tracking,573696126e3b12023e5247d6,Differentiation of the Cholesky decomposition.,"[14] and Murray [24] have presented back-propagation forms for the SVD and Cholesky decomposition respectively, enabling gradient descent to be applied to a network that computes the solution to either a system of linear equations or an eigenvalue problem.###Ionescu et al. [15] and Murray [25] have presented back-propagation forms for the SVD and Cholesky decomposition respectively, enabling gradient descent to be applied to a network that computes the solution to either a system of linear equations or an eigenvalue problem.",other,reporting prior findings on back-propagation forms for SVD and Cholesky decomposition
1684,,cbb8fbe24886cad8c7aadc95a3c4f4fab8fd8b7b,Recommending teams promotes prosocial lending in online microfinance,,,"###In comparison, an under-explored class of mechanisms uses group membership and inter-group competition (2, 3) to increase both participation and giving amounts.###Last, our study builds upon social identity theory (2, 3) and recent experimental research that uncovers the positive effects of group identity on voluntary contribution and coordination in the laboratory (11–16 ,",impact-revealing,highlighting the significance of group identity in voluntary contributions
2528,5efb0d5691e011063336d39c,0cee58946a13a5c2845647b4af8b9d2bf52a8b6b,BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision,5c8a2c404895d9cbc61f4a5c,SwellShark: A Generative Model for Biomedical Named Entity Recognition without Labeled Data.,"The matching can be achieved by string matching [9], regular expressions [8] or heuristic rules (e.###Fries et al. Fries et al. (2017) and Giannakopoulos et al. Giannakopoulos et al. (2017) have proposed distantly-supervised NER methods for speciﬁc domains ( e.g. , biomedical domain), where the adopted domain-speciﬁc gazetteers or KBs are often of high matching quality and yield high precision and…###Fries et al. Fries et al. (2017) and Giannakopoulos et al. Giannakopoulos et al. (2017) have proposed distantly-supervised NER methods for speciﬁc domains ( e.g. , biomedical domain), where the adopted domain-speciﬁc gazetteers or KBs are often of high matching quality and yield high precision and high recall distant labels.###The matching can be achieved by string matching (Giannakopoulos et al., 2017), regular expressions (Fries et al., 2017) or heuristic rules (e.g., POS tag constraints).",other,acknowledge existing methods for matching in NER
229,5bdc31b417c44a1f58a0b894,510d98681e5e85fb1265513728f16e2543ae1b4b,Hypergraph Neural Networks,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,"Moreover, GCN (Kipf and Welling 2017) can be regarded as a special case of HGNN, for which the edges in simple graph can be regarded as 2-order hyperedges which connect just two vertices.###Graph-based convolutional neural networks (Kipf and Welling 2017), (Defferrard, Bresson, and Vandergheynst 2016) have attracted much attention in recent years.###Moreover, GCN (Kipf and Welling 2017) can be regarded as a special case of HGNN, for which the edges in simple graph can be regarded as 2-order hyper-edges which connect just two vertices.###Then, in (Kipf and Welling 2017), the chebyshev polynomials are simplified into 1-order polynomials to form an efficient layer-wise propagation model.###It is also suggested in (Kipf and Welling 2017) that λmax ≈ 2 because of the scale adaptability of neural networks.###It is also suggested in (Kipf and Welling 2017) that λ max ≈ 2 because of the scale adaptability of neural networks.###Then, in (Kipf and Welling 2017), the chebyshev polynomials are simpliﬁed into 1-order polynomials to form an efﬁ-cient layer-wise propagation model.",impact-revealing,providing context on graph-based convolutional neural networks
1210,,c81ff642a7955ffdcb2d4172c545093f12dbf4ea,A synthetic differentiation circuit in Escherichia coli for suppressing mutant takeover,,,"###2,45 We thus deﬁne a ‘‘pure stem cell’’ as one with no cut plasmids, a ‘‘progenitor cell’’ as a cell with an intermediate number of cut plasmids, and a ‘‘fully differentiated cell’’ as a cell in which all plasmids are cut.###2,3,15,16 In this work, we are inspired by the simple bacterial form of differentiation.###Thismayofferapotentialrolefortransit-amplifyingcells in the context of animal stem-based tissues, in addition to their role of amplifying cell numbers 2,3 and reducing noise in cell proportions.",impact-revealing,highlighting potential roles of transit-amplifying cells in stem-based tissues
846,53e99fa2b7602d9702875e25,1cfd3d03abdbdca8fbfadf68001199e92bb9c3b4,SMA: A Self-Monitored Adaptive Cache Warm-Up Scheme for Microprocessor Simulation,53e9ace1b7602d97036bdc96,Memory Reference Reuse Latency: Accelerated Sampled Microarchitecture Simulation,"( 2 ) However, careful analysis of the experiment reveals the functional###5 No warm-up length number is given in the MRRL paper. ( 2 ) This number is based on###Based on the above premise, Haskins and Skadron employ the concept of Memory Reference Reuse Latency (MRRL), ( 2 ) which refers to the number of dynamic instructions between I( a, n − 1) and I( a, n). Instructions in a sampling unit and its pre-sample are profiled to get the empirical distribution of MRRL.###This sampling unit size was used in the MRRL paper, ( 2 ) and in Variance SimPoint.###For example, Haskins et al. reported that ignoring warm-up in their experiment could result in an error as high as 15% in simulated CPI. ( 2 ) Thus adequate###4 Fifty 1 million-instruction sampling units are used in the MRRL paper. ( 2 ) Suppose that a bench-###For MRRL, we choose the p-value to be 99.9%, which is the default value suggested by its inventors. ( 2 ) For BLRL, we use the p-value of 90%.###The two most recently proposed state-of-the-art cache warm-up methods are MRRL, ( 2 ) also by Haskins and Skadron, and BLRL (15) by",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3623,5c96086e3cb210d2716c4a08,bb76749fab841ccbe20ace74111a518f65d9870b,make the most out of last level cache in intel processors,5b8c9f2b17c44af36f8b4d68,"Software-Defined ""Hardware"" Infrastructures: A Survey on Enabling Technologies and Open Research Directions.","Considering the need for hardware changes in the future data centers [57], we hope this paper will encourage hardware vendors to adopt one or more of these alternatives.",other,encouraging hardware vendors to consider alternatives for future data centers
3982,5b67b46b17c44aac1c861fc4,f6d06993e003fa6fec5bf630efded9e4fd90a030,fluency boost learning and inference for neural grammatical error correction,59ae3bf12bbe271c4c71be3a,Artificial Error Generation with Machine Translation and Syntactic Patterns.,"…data augmentation along with training for GEC, despite some previous studies that explore artiﬁcial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017).###Unlike the models trained only with original error-corrected data, we propose a novel fluency boost learning mechanism for dynamic data augmentation along with training for GEC, despite some previous studies that explore artificial error generation for GEC (Brockett et al., 2006; Foster and Andersen, 2009; Rozovskaya and Roth, 2010, 2011; Rozovskaya et al., 2012; Felice and Yuan, 2014; Xie et al., 2016; Rei et al., 2017).",other,highlighting the novelty of a proposed mechanism for data augmentation in GEC
212,5ac1829d17c44a1fda917e29,ef2ec69e7c94b4194ba01719ac76d4595e6b4bdf,L2-Nonexpansive Neural Networks,5ca600ae6558b90bfa4d76e9,Towards Deep Learning Models Resistant to Adversarial Attacks.,"So far the mainstream and most successful remedy is that of adversarial training (Madry et al., 2017).###This results in poor robustness and vulnerability under adversarial attacks which has been reported on a variety of networks including image classiﬁcation (Carlini & Wagner, 2017a; Goodfellow et al., 2014), speech recognition (Kreuk et al., 2018; Alzantot et al., 2018; Carlini & Wagner, 2018), image captioning (Chen et al., 2017) and natural language processing (Gao et al., 2018; Ebrahimi et al., 2017).###For example, Athalye et al. (2018) reported that out of eight recent defense works, only Madry et al. (2017) survived strong attacks.###In contrast, we are able to build MNIST and CIFAR-10 classiﬁers, without needing any adversarial training, that exceed the state of the art (Madry et al., 2017) in robustness against white-box L 2 - bounded adversarial attacks.###The second image is an attack on Model 2 (Madry et al., 2017) found after 1K iterations, with noise L 2 -norm of 4.4.###Adversarial defense is a well-known difﬁcult problem (Szegedy et al., 2014; Goodfellow et al., 2014; Carlini & Wagner, 2017a; Athalye et al., 2018; Gilmer et al., 2018).###This section evaluates robustness of L2NNN classiﬁers for MNIST and CIFAR-10 and compares against the state of the art Madry et al. (2017).###However, as will be shown in Tables 1 and 2, the robustness by adversarial training diminishes when a white-box attacker (Carlini & Wagner, 2017a) is allowed to use more iterations.###We repeat this experiment for Model 2 (Madry et al., 2017) and our Model 3 of Tables 1 and 2.###The reported robustness results of Cisse et al. (2017), however, are much weaker than those by adversarial training in Madry et al. (2017).###There are many avenues to defense (Carlini & Wagner, 2017b; Meng & Chen, 2017), and here we will focus on defense works that fortify a neural network itself instead of introducing additional components.###The attack code of Carlini & Wagner (2017a) is used.###It’s worth noting that the slow degradation of Model 2’s accuracy is an artifact of the attacker (Carlini & Wagner, 2017a): when gradients are near zero in some parts of the input space, which is true for MNIST Model 2 due to adversarial training, it takes more iterations to make progress.###Our implementation applies the technique of Madry et al. (2017) on the ﬁrst loss term (4): we use distorted inputs in calculating L a .###The work of Madry et al. (2017) has the best results to date and effectively ﬂattens gradients around training data points, and, prior to our work, it is the only work that achieves sizable white-box defense.###The ﬁrst has 4 layers and is the architecture used in Madry et al. (2017).",impact-revealing,highlighting the challenges and limitations of adversarial training in various domains
4008,5dbebb7447c8f766462c22a6,e1fd81af050dbdc4232ff8b1ab71cf0973d530b6,graph convolutional networks with motif-based attention,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,This may be for reasons similar as to why skip-connections are needed in deep architectures since the signal starts to degrade as the model gets deeper [16].,other,providing context for the need of skip-connections in deep architectures
969,,aee501847901eb85f6419af4df20a7fef1fa3673,A STRAWSON–LEWIS DEFENCE OF SOCIAL PREFERENCES,,,"###Players with the mutual belief that they are motivated by kindness can attain the cooperative outcome in a one-shot PD.
Ingenious experimental designs have demonstrated that both concerns play a role in cooperation, e.g. Cox (2004), Bacharach et al. (2007), Cox et al. (2008), Falk et al. (2008).",impact-revealing,acknowledging the role of experimental designs in understanding cooperation
1694,,8e6fc98d24674fe5c133aee57bd061904dfc619f,Social identity theory and the family business,,,"###Looking back on the impact of SIT, Brown (2000) argues that the theory has made several important contributions.###One of the most compelling theories about identity as a collective construct was derived by Henri Tajfel and John Turner (Tajfel, 1972; Tajfel & Turner, 1979; Turner, 1984).###Relying on SIT, the authors find that teams linked by family bonds and teams linked by love react differently when the “new identity of entrepreneur is imposed on the existing family identity” (2013, p. 125); couples are more likely to reach first sales as they are better able to cope with family and entrepreneurship identities by developing a meta-identity (Shepherd & Haynie, 2009).###Tajfel and Turner (1979) made the distinction early on between personal identity, concerned with person-specific characteristics, such as interests or bodily attributes, and social identity.###As Tajfel and Turner (1979) expressed, members of low-status groups would try to change their group affiliation to gain higher selfesteem.###Also Block builds on SIT, arguing that owners identify more strongly with their businesses and want to avoid actions that damage the reputation and thus harm their positive social identity.###SIT in family firm research has grown in importance as it is used both as the main theoretical framework (Miller & Le Breton-Miller, 2011; Shepherd & Haynie, 2009) as well as to support single arguments or hypotheses (Björnberg & Nicholson, 2012; Sharma & Irving, 2005; Vallejo, 2008). Looking at the conflicting identities in family firms, Shepherd and Haynie (2009) introduce the ‘family business meta-identity’, which exists at the intersection of family and business identities.###Seeing their paper through the lens of SIT, in-group bias could possibly determine the manufacturers’ perception of their environment.###At the fundament of SIT, Tajfel and Turner (1979) express three general assumptions.",impact-revealing,highlighting the contributions and significance of Social Identity Theory (SIT) in understanding identity dynamics in family firms
2764,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",58d82fd2d649053542fd77ae,When Slepian Meets Fiedler: Putting a Focus on the Graph Spectrum.,"principle to graph signals, while other recent work considers alternative frequency representations that can take into consideration the specific localization properties encountered in irregular graphs [137], [138], [139]",other,acknowledge variations in recent research on graph signals
3955,5d9ed4a047c8f76646fb6da2,0fd26ed185aaf860f2db491c194884914fc29311,A Neural Multi-digraph Model for Chinese NER with Gazetteers,573696106e3b12023e5227c8,Neural Architectures for Named Entity Recognition.,"We use BiLSTMCRF (Lample et al., 2016) with character+bigram embedding without using any gazetteer as the comparison baseline6.###We use BiLSTM-CRF (Lample et al., 2016) with character+bigram embedding without using any gazetteer as the comparison baseline 6 .###Combined with an adapted Gated Graph Sequence Neural Networks (GGNN) (Li et al., 2016) and a standard bidirectional LSTM-CRF (Lample et al., 2016) (BiLSTM-CRF), our model learns a weighted combination of the information from different gazetteers and resolves matching conﬂicts based on contextual…###, 2016) and a standard bidirectional LSTM-CRF (Lample et al., 2016) (BiLSTM-CRF), our model learns a weighted combination of the information from different gazetteers and resolves matching conflicts based on contextual information.",other,describing the model architecture and methods used
2982,5ee9f15b91e01152af022eb9,6360aaece0d6bf153183b9ecd075f42f7b127cc9,Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting,53e99c8bb7602d9702539072,Network Motifs: Simple Building Blocks Of Complex Networks,"For example, small substructures (graphlets) have been extensively analysed in protein-protein interaction networks [44], triangles and cliques characterise the structure of ego-nets and social networks in general [20], simple cycles (rings) are central in molecular distributions, directed and temporal motifs have been shown to explain the working mechanisms of gene regulatory networks, biological neural networks, transportation networks and foodwebs [45], [46], [47].###The seminal paper of [45] coined the term network motifs as over-represented subgraph patterns that were shown to characterise certain functional properties of complex networks in systems biology.",other,highlighting the significance of network motifs in understanding complex networks
3777,5f75aa6a9fced0a24b64599d,69fb130409d48478fab0c4545dc2ff102a2630ef,The forward slice core microarchitecture,53e9a6d0b7602d97030065a6,Achieving Out-Of-Order Performance With Almost In-Order Complexity,"Proposals such as speculative-slice execution [23], flea-flicker multipass pipelining [3], braid processing [22] and OUTRIDER [6] also exploit critical instruction slices [24] for improving performance.",other,acknowledge existing proposals for performance improvement
2305,5fdb2e1691e0118a02c4f566,16913a534b1630d33770b392767bb316f4fdb11e,Clinical Temporal Relation Extraction with Probabilistic Soft Logic Regularization and Global Inference,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"2019; Leeuwenberg and Moens 2017) leveraging the ILP optimization; (4) Basic version of our model, CTRL, which only fine-tunes a BERT-BASE (Devlin et al. 2018) language model with one layer of FFN, similar to the implementations in Lin et al.###We follow the experimental settings in (Devlin et al. 2018) to set the dropout rate, and batch size as 10−1 and 8.###BERT (Devlin et al. 2018), to derive the sentence representation vi of ds-dimension to encode the input sequence si including two marked named entities xi,1, xi,2 from the instance I, where i ∈ {1, 2, 3}.###We follow the experimental settings in (Devlin et al. 2018) to use 12 Transformer layers and attention heads and set the embedding size ds as 768.###In the framework of CTRL-PG, any contextualized word embedding method, such as BERT (Devlin et al. 2018), ELMo (Peters et al. 2018), and RoBERTa (Liu et al. 2019b), can be utilized.###In the framework of CTRL-PG, any contextualized word embedding method, such as BERT (Devlin et al. 2018), ELMo (Peters et al.###…method, SP-ILP (Han et al. 2019; Leeuwenberg and Moens 2017) leveraging the ILP optimization; (4) Basic version of our model, CTRL, which only fine-tunes a BERT-BASE (Devlin et al. 2018) language model with one layer of FFN, similar to the implementations in Lin et al. (2019); Guan et al. (2020).###We leverage the pretrained BERT-BASE model (Devlin et al. 2018) to generate the sentence embeddings, which contains 110M parameters to fine-tune.###…ȳ3 with l3; dt ← max{P(y = ŷ1|s1) + P(y = ŷ2|s2)− 1, 0}; dt ← max{dt − P(y = ȳ3|s3), 0}; dr ← min{dr, dt}; IsGround← true;
end if IsGround == false then
dr ← 0;
BERT (Devlin et al. 2018), to derive the sentence representation vi of ds-dimension to encode the input sequence si including two…###We choose BERT (Devlin et al. 2018) to derive contextualized sentence embeddings without loss of generality.",other,reporting the use of BERT for sentence representation
1142,,32a3c2fbd3e733bd0eea938517fec2ff8dc7c701,Pix2Video: Video Editing using Image Diffusion,,,"###Stable Diffusion [1, 14], like many other large-scale image diffusion models, is a denoising diffusion implicit model (DDIM) where at each diffusion step, given a noisy sample x t , a prediction of the noise-free sample ˆ x 0 , along with a direction that points to x t , is computed.###Hence, we decided to use DDIM inversion due to efficiency (2.5 seconds vs 40-60 seconds per frame).###We use 50 DDIM steps both for inversion and editing.###Diffusion-based algorithms [8, 19, 47] have emerged as the generative model of choice for image creation.###Denoising Diffusion Probabilistic Model (DDPM) [18] and its variant Denoising Diffusion Implicit Model (DDIM) [48] have been widely used for unconditional text-to-image generation.###Additionally, one can invert [30, 47] a given image into a pretrained diffusion model and subsequently edit using only textual guidance [16].###We use an inversion mechanism, DDIM inversion [48], while other inversion methods aiming to preserve the editability of an image can be used [31] as well.###Denoising Diffusion Probabilistic Model (DDPM) [19] and its variant Denoising Diffusion Implicit Model (DDIM) [47] have been widely used for unconditional text-to-image generation.###We use an inversion mechanism, DDIM inversion [47], while other inversion methods aiming to preserve the editability of an image can be used [30] as well.",impact-revealing,describing the characteristics and efficiency of diffusion models in image generation
3975,5dce788a3a55ac9580a162f8,56cafbac34f2bb3f6a9828cd228ff281b810d6bb,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,5b67b46b17c44aac1c861f6c,Adaptive Knowledge Sharing In Multi-Task Learning: Improving Low-Resource Neural Machine Translation,"…not only many works integrating knowledge embed-dings into NLP models to improve the performance of NLP applications such as machine translation (Zaremoodi et al., 2018), reading comprehension (Mihaylov and Frank, 2018; Zhong et al., 2019) and dialogue system (Madotto et al., 2018), but also…###These embeddings can not only help with the KG completion but also benefit various NLP applications (Yang and Mitchell, 2017; Zaremoodi et al., 2018; Han et al., 2018a).",other,highlighting the benefits of knowledge embeddings in various NLP applications
489,58437785ac44360f108432a7,92527ace7f75188b5ec209ff7d59f431343075e4,Video-based emotion recognition using CNN-RNN and C3D hybrid networks,5550488e45ce0a409eb6f5e4,Combining Multiple Kernel Methods on Riemannian Manifold for Emotion Recognition in the Wild,"For example, in the EmotiW 2014 winner’s work [4], all video frames are extracted from videos and regarded as the static images for further process.###A particular type of recurrent neural networks, the Long Short-Term Memory (LSTM) recurrent neural network is widely adopted [4, 5, 8].",impact-revealing,acknowledge prior work in video frame processing and LSTM usage
562,5550443b45ce0a409eb4c3b9,0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f,Towards End-To-End Speech Recognition with Recurrent Neural Networks,53e9a238b7602d9702b3fc8d,Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,"(15) can be efficiently evaluated and differentiated using a dynamic programming algorithm (Graves et al., 2006).###The spectrograms are processed by a deep bidirectional LSTM network (Graves et al., 2013) with a Connectionist Temporal Classiﬁcation (CTC) output layer (Graves et al., 2006; Graves, 2012, Chapter 7).###(15) can be ef-ﬁciently evaluated and differentiated using a dynamic programming algorithm (Graves et al., 2006).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3193,5db9297247c8f766461f6d13,9ec95c1130a6ac4238ac2e5c7b2b66047511ea92,long and diverse text generation with planning-based hierarchical variational model,573696d96e3b12023e5d8d8a,A Hierarchical Neural Autoencoder For Paragraphs And Documents,"To model dependencies among sentences, Li et al. (2015) utilized a hierarchical recurrent neural network (RNN) decoder.",other,reporting prior findings on hierarchical RNN decoder for sentence dependencies
1724,,802a857a47452ed01101e75708009e5024a8be68,Quantifying Isolation Anomalies,,,"###The seminal example is the prediction of waiting and deadlock probabilities in [12]; later examples include [11, 7].###We were inspired by Gray et al’s predictions of deadlock rates [12, 11], and by the Elnikety et al model predicting performance of replication algorithms [7].",impact-revealing,drawing inspiration from prior models in predicting deadlock rates
2489,5736982b6e3b12023e6fd21d,0344f9ac92006155f6fa464cab19925ac481dea4,Microarchitectural implications of event-driven server-side web applications,53e9bb4bb7602d970478b9b9,Improving Javascript Performance By Deconstructing The Type System,"Prior work on improving the performance of JavaScript, especially its dynamic typing system [3,67], complements our event-level optimizations.",other,acknowledging complementary prior work on JavaScript performance
589,5c04966a17c44a2c74708959,6b98bef930182a848c027dece1bfb58ca706449d,Improving End-to-end Speech Recognition with Pronunciation-assisted Sub-word Modeling,5b3d98cc17c44a510f801eb2,Improved Training Of End-To-End Attention Models For Speech Recognition,"Sub-word representations have recently seen their success in ASR [6].###In Table 3, we report the WER results on the LibriSpeech dataset, using the parameters described in [6].",impact-revealing,reporting results on sub-word representations in ASR
2510,5c2348ceda562935fc1d57a4,007d08aee3b88489fe0377849f70688de66adae8,Bandit Learning with Implicit Feedback,57fdf424654a3f2774ecce07,Learning Hidden Features for Contextual Bandits,"Unfortunately, 23 a common practice in contextual bandit applications simply treats no click as a form of negative 24 feedback [18, 23, 5].###studied the problem of latent feature 82 learning for contextual bandits [23].",other,highlighting a common practice in contextual bandit applications
237,5a73cbcc17c44a0b3035f1d3,14058c2ebe9905fe5c3acf8b1bfcd5390fe32a28,Deception Detection in Videos,53e99d80b7602d970263c601,Exploiting generative models in discriminative classifiers,"Fisher Vectors were ﬁrst introduced in (Jaakkola and Haussler 1999) to combine the advantages of generative and discriminative models, and are widely used in other computer vision tasks, such as image classiﬁcation (Perronnin and Dance 2007), action recognition (Wang et al. 2016) and video…",impact-revealing,reporting prior findings and applications of Fisher Vectors
3227,53e9ac54b7602d9703622ebc,0d8524a1eca5e41ee755acd30a0c28a782d05331,The sharing architecture: sub-core configurability for IaaS clouds,53e9b295b7602d9703d3bdf4,Power Management Of Online Data-Intensive Services,"For example, one Cloud customer may run Online Data-Intensive (OLDI) workloads driven by queries that interact with massive data sets and require responsiveness at a sub-second time scale[41].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2323,5cede0f2da562983788d0d44,aecddd82840323e5bd43f9c73a32fed88ee93c8c,An Effective Approach To Unsupervised Machine Translation,53e999bbb7602d9702202274,Z-MERT: A Fully Configurable Open Source Tool for Minimum Error Rate Training of Machine Translation Systems,"Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al.###Our unsupervised tuning implementation is based on Z-MERT (Zaidan, 2009), and we use FastAlign (Dyer et al., 2013) for word alignment within the joint reﬁnement procedure.",other,reporting method used for unsupervised tuning and word alignment
2162,,9d1a51cbe4abab5d97d99378421c56c42303b325,Business Policy Modeling and Enforcement in Relational Database Systems,,,"###Using terminology that is consistent with prior work [122, 97, 19, 123, 24, 79, 147], we provide a formal interpretation of our policy model that employs temporal integrity constraints over uniquely identifiable tuples in database views.###A large body of work in the area of policy specification using temporal logic, including this thesis, is motivated by the classical work of Lamport (Temporal Logic of Actions [97]) and by the seminal work of Pnueli [123, 122].",impact-revealing,providing context and motivation for policy model interpretation
2025,,3f52ea6ddd193436808d0a48d7db0e6e92c00172,BEBRF Database Survey,,,"###This study
was limited by the fact that it was not prospective or blinded; nevertheless it clearly supports that the location of the injection site can impact BoNT efficacy (Cakmur et al, 2002).###The position of the injection sites around the orbicularis oculi muscle is thought to influence the efficacy and side effects of BoNT type A (Cakmur et al, 2002).",impact-revealing,highlighting the influence of injection site location on BoNT efficacy
3305,5edf5dd891e011bc656ded70,374f36c9081ab5dc686ab833c42a7297235cd13f,Denoising Implicit Feedback for Recommendation,5ce2d098ced107d4c63970ce,Evaluating Recommender System Stability with Influence-Guided Fuzzing,"Prior work [17, 29] has tried to evaluate the robustness of recommender systems under various attack methods, such as shilling attacks [17] and fuzzing attacks [29].",other,acknowledging prior evaluations of recommender systems
548,5d84a3433a55acc20782ce9e,554d300f00fc14c2e4f48a740019496137d060c1,self-training for end-to-end speech recognition,5550411c45ce0a409eb3897f,Neural Machine Translation by Jointly Learning to Align and Translate.,"Our sequence-to-sequence model is an encoder-decoder architecture with attention [5, 6, 7].",impact-revealing,describing the model architecture used
425,5e5e18ca93d709897ce315f0,68f86237dadcf2f570f0cd5b5e56161693619a74,Residual Energy-Based Models for Text Generation,56d8f9dcdabfae2eeeabe6a6,A Generalization Of Sampling Without Replacement From A Finite Universe,"Lastly, this formulation enables efﬁcient evaluation and generation via importance sampling (Horvitz & Thompson, 1952; Grover et al., 2019).###5 are an instance of importance sampling (Horvitz & Thompson, 1952).",impact-revealing,highlighting the efficiency of evaluation and generation methods
3434,5f0423a69e795e06bbe12b1e,21082bd98071f6948097df05cc9e4770fcd87de6,Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems,5b67b45517c44aac1c860876,Graph Convolutional Neural Networks for Web-Scale Recommender Systems.,"We observe that negative sampling [36], including its variant [54] that makes the instances in a batch share the same large set of negative samples, does not perform well in our settings.###Deep candidate generation methods are widely deployed in industrial systems, e.g., YouTube [10, 14, 25], Taobao [30, 33, 57, 59], and Pinterest [54].###, YouTube [10, 14, 25], Taobao [30, 33, 57, 59], and Pinterest [54].",other,highlighting the limitations of negative sampling in specific settings
1458,,6243c8f4d426e368b8ccbc8a7fb396438488dc99,Tracing Semantic Variation in Slang,,,"###We model g generatively using semantic chaining models from historical word sense extension which are motivated by mechanisms of human categorization (Rosch, 1975; Nosofsky, 1986).",impact-revealing,providing context for generative modeling approach
3281,5ebe685391e0117693a52241,99314a532a3358cb86064fc8917ed2c283227539,NAT: Noise-Aware Training for Robust Neural Sequence Labeling,5a260c8417c44a4ba8a316b6,Mitigating the impact of speech recognition errors on chatbot using sequence-to-sequence model,"Moreover, we plan to evaluate NAT on other real noise distributions (e.g., from ASR) and other sequence labeling tasks to support our claims further.###Sequence labeling systems are generally trained on clean text, although in real-world scenarios, they often follow an error-prone upstream component, such as Optical Character Recognition (OCR; Neudecker, 2016) or Automatic Speech Recognition (ASR; Parada et al., 2011).###The impact of noisy input data In the context of ASR, Parada et al. (2011) observed that named entities are often OOV tokens, and therefore they cause more recognition errors.###Inspired by recent research in computer vision (Zheng et al., 2016), Neural Machine Translation (NMT; Cheng et al., 2018), and ASR (Sperber et al., 2017), we propose two Noise-Aware Training (NAT) objectives that improve the accuracy of sequence labeling performed on noisy input without reducing efﬁciency on the original data.###Noise can also be introduced in an upstream task, like OCR (Alex and Burns, 2014) or ASR (Chen et al., 2017), causing the errors to be propagated downstream.###Consequently, they exhibit degraded performance in real-world scenarios where the transcriptions are produced by the previous up-stream component, such as OCR or ASR ( § 2.2), which results in a detrimental mismatch between the training and the test conditions.",other,highlighting the challenges of noisy input data in sequence labeling tasks and proposing solutions
3527,5e3d353b3a55ac4de4104f40,3024f58826a5bce3378af94f677e8fb90cbb49e0,LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation,5ce2d032ced107d4c635260c,Predict then Propagate: Graph Neural Networks meet Personalized PageRank,"Then we discuss the relationwith the Approximate Personalized Propagation of Neural Predictions (APPNP) [24], which is recent GCN variant that addresses oversmoothing by inspiring from Personalized PageRank [15]; this analysis shows the underlying equivalence between LightGCN and APPNP, thus our LightGCN enjoys the sames benefits in propagating long-range with controllable oversmoothing.###It is worth mentioning that several recent efforts provide deep insights into GNNs [24, 27, 40], which inspire us developing LightGCN.###In a recent work [24], the authors connect GCN with Personalized PageRank [15], inspiring from which they propose a GCN variant named APPNP that can propagate long rangewithout the risk of oversmoothing.",other,highlighting the relationship and benefits of LightGCN in relation to APPNP
3890,5ce3ad3fced107d4c65b6bd9,eeecea3097cf5629eb72a06e5caaf24d774adce7,Unsupervised label noise modeling and loss correction,57a4e91dac44365e35c9853d,Deep Image Homography Estimation.,"Convolutional Neural Networks (CNNs) have recently become the par excellence base approach to deal with many computer vision tasks (DeTone et al., 2016; Ono et al., 2018; Beluch et al., 2018; Redmon et al., 2016; Zhao et al., 2017; Krishna et al., 2017).",other,highlighting the prominence of CNNs in computer vision tasks
3357,5ec49a639fced0a24b4de8d2,5c5751d45e298cea054f32b392c12c61027d2fe7,S2ORC: The Semantic Scholar Open Research Corpus,53e9a751b7602d9703087fbe,The ACL Anthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics,"It is built from the ACL Anthology (Bird et al., 2008) and consists of 24.###It is built from the ACL Anthology (Bird et al., 2008) and consists of 24.6k papers manually augmented with citation information.###Digital archives like arXiv, 2 PubMed Central, 3 CiteSeerX (Giles et al., 1998), 4 and the ACL Anthology (Bird et al., 2008), 5 are popular resources for deriving large text corpora for summarization and language modeling or, with further annotation, development of datasets for tasks like entity…###, 1998),4 and the ACL Anthology (Bird et al., 2008),5 are popular resources for deriving large text corpora for summarization and language modeling or,",other,reporting sources of text corpora for summarization and language modeling
418,599c7945601a182cd2629f72,000178cd12c8a6e5da8215b6365fae03c20fd18d,End-to-End Representation Learning for Correlation Filter Based Tracking,558bc45be4b00c3c48de5a18,Visual object tracking using adaptive correlation filters,"To reduce the effect of circular boundaries, the feature map x is pre-multiplied by a cosine window [4] and the final template is cropped [29].###The CF is an efficient algorithm that learns to discriminate an image patch from the surrounding patches by solving a large ridge regression problem extremely efficiently [4, 13].###[4], the Correlation Filter has enjoyed great popularity within the tracking community.###However, in practice we did not find learning this parameter to improve the tracking accuracy compared to the conventional choice of a fixed Gaussian response [4, 13].",impact-revealing,reporting findings on the effectiveness of the Correlation Filter in tracking
96,5f3f917891e011d38f9242d9,14156438bafed28a626738630b5181b83ed5d79c,Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters,53e9b0e6b7602d9703b60e78,Multi-armed bandit algorithms and empirical evaluation,"A is the action space, f is the reward function, and T is the terminal condition [36].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1230,,d2ee48174077076ad3b7ccb59bcc68bb24934f77,"A Longitudinal Evaluation of Strain, Work Engagement, and Intervention Strategies to Address the Health of High-Risk Employees",,,"###In contrast, if the measured time-lag is too long, it is likely that additional, extraneous processes have influenced the causal process (De Lange et al., 2004; Zapf et al., 1996).###reverse, and reciprocal causal relationships (Pentz & Chou, 1994; Zapf et al., 1996).###constellation of factors that influence well-being, it is unlikely that the small amount of variables measured in this research would completely explain the variance in wellbeing and work characteristics over time (Zapf et al., 1996).###This perspective fails to account for the influence of current states of well-being on future perceptions of job characteristics (reverse causation) or how they mutually intensify or neutralise one another over time (reciprocal causation; de Lange et al., 2003; Houkes et al., 2003b; Zapf et al., 1996).###Third, it is generally assumed that strain impacts the work environment and perceptions of future stressors in a uniformly negative and passive manner, as highly strained individuals gravitate towards jobs or situations with fewer resources (e.g., drift hypothesis; Zapf et al., 1996).###(Arnold et al., 2007; Zapf et al., 1996), which was not the case in this research as not all correlation results were significant.###duration of time-lags, they are typically derived due to arbitrary factors, such as practical convenience (Dormann & Zapf, 2002; Zapf et al., 1996).###recommended that to overcome this issue, multiple wave studies should be conducted that include even and uneven time-lags (de Lange et al., 2003; Zapf et al., 1996).###This aspect of the study was important as research comprising two or more waves is rare (Dormann & Zapf, 2002; Zapf et al., 1996) and at least three waves of data are required to truly examine hypotheses relating to reciprocal and mediated relationships (Frazier et al.###amongst work characteristics and well-being (e.g., Bakker & Demerouti, 2007; de Lange et al., 2003; Zapf et al., 1996).",impact-revealing,highlighting the complexity of causal relationships and the need for multiple wave studies
2497,5c0f8548da562944ac906a71,5a3da29970d0c3c75ef4cb372b336fc8b10381d7,CNN-Based Real-Time Dense Face Reconstruction with Inverse-Rendered Photo-Realistic Face Images,5843774eac44360f10839d48,Demo of Face2Face: real-time face capture and reenactment of RGB videos.,"Most of them use a 3D Morphable Model [18], [22], [53] or a multi-linear face model [7], [8], [9], [45], [48] as a prior.###The model has been widely used in various computer vision tasks, such as face recognition [5], [54], face alignment [27], [34], [60], and face reenactment [53].###Recently, several approaches have been proposed for RGB video based facial performance captureing [7], [8], [18], [22], [45], [53].###The fitting process is based on the analysis-by-synthesis strategy [4], [53], and we seek a solution that by minimizes the difference between the input face image and the rendered image with###[53] achieves real-time face reconstruction and facial reenactment through data-parallel optimization strategy, but their method cannot recover fine-scale details such as wrinkles and also requires facial landmark inputs.###such as facial expression transfer [52], [53] and face replacement [12], [16], [30].###[51] proposes to use an analysis-by-synthesis energy function as the loss function during network training [4], [53].###Actually image-based 3D face reconstruction itself is a fundamental problem in computer vision and graphics, and has many applications such as face recognition [5], [54] and face animation [23], [53].###Recently, [53] presented an approach for real-time face track-",other,acknowledge existing methods and their applications in computer vision
579,5d04e8dbda56295d08db13cf,56e3ce0ff4cbd05e404214d19ae264fe6c457a16,cif: continuous integrate-and-fire for end-to-end speech recognition,5d9ed28c47c8f76646f6fc1b,End-To-End Speech Recognition With Adaptive Computation Steps,"Especially, it achieves a huge absolute WER improvements on the result of Adaptive Computation Steps [15] which is reproduced by utilizing the same model setting, thus further demonstrating the superiority of the CIF which locates and integrates at a finer time granularity.###96 ‘Soft’ and ‘monotonic’: ACS [15] 67M 16.###Besides, the processing of CIF ensures the full utilization of acoustic information and lays a foundation for the effective application of its supporting strategies, which are what the ACS (that has a huge performance gap from a HMMDNN model) lacks of.###In [14], Li al. present the important Adaptive Computation Steps (ACS) algorithm whose motivation is to dynamically decide a block of frames to predict a linguistic output.",impact-revealing,highlighting the performance improvements of the CIF method over ACS
1326,,5d74da07e76a2ae9aac98646ac3a23e66630e3ff,Simulating PDGF-Driven Glioma Growth and Invasion in an Anatomically Accurate Brain Domain,,,"###PDGF dependence in our rates is motivated by experimental data, which demonstrate that the motility and cell division rates of OPCs are dependent on local PDGF concentration (Armstrong et al. 1990; Frost et al. 2009; Pringle et al. 1989).###…are experimentally derived, or came from a combination of literature sources and testing in simulations a Massey et al. (2012) b Thorne et al. (2004) c Pringle et al. (1989) d Note that the value given here for Rwg comes from varying the parameter in simulations, as described in the results (Sect.",impact-revealing,Highlighting the dependence of rates on experimental data
655,573698426e3b12023e70bf13,4e9dbca4218d32a9f92d58c340f3f8f3c5020a44,best-offset hardware prefetching,5550488245ce0a409eb6ef7f,Sandbox Prefetching: Safe run-time evaluation of aggressive prefetchers,"We implemented the SBP prefetcher as described in the original paper [26], but with a few modifications to make the comparison with BO prefetching meaningful.###is the first published full-fledged offset prefetcher[26].###introduced a new sort of prefetcher, offset prefetchers, and the sandbox method for selecting the prefetch offset dynamically [26].",impact-revealing,describing the implementation and modifications of the SBP prefetcher
3420,5a260bfb17c44a4ba8a1c61e,a55970013b984f344dfbbbba677d89dce0ba5f81,Image Super-Resolution via Deep Recursive Residual Network,5550415945ce0a409eb3a820,ImageNet Large Scale Visual Recognition Challenge,"Inspired by the success of very deep networks [8, 27, 28] on ImageNet [21], Kim et al.###Their deeper structures with 4 or 5 layers do not achieve better performance, which was attributed to the difficulty of training deeper networks and led to the observation that “the deeper the better” might not be the case in SR. Inspired by the success of very deep networks [8, 27, 28] on ImageNet [21], Kim et al. [13, 14] propose two very deep convolutional networks for SR, both stacking 20 convolutional layers, from the viewpoints of training efficiency and storage, respectively.",other,highlighting the challenges and observations regarding deep networks in super-resolution
2607,5d971ab13a55ac1c613f54e6,6e47b9e4ea00300450984088c8366d28782b17f1,A Multi-Attentive Pyramidal Model for Visual Sentiment Analysis,5a73cb8717c44a0b3035c091,Image sentiment prediction based on textual descriptions with adjective noun pairs,"Besides, some works have tried to utilize middle attribute level features like the visual sentiment taxonomy of “adjective-noun pairs (ANP)” to improve classification results [11], [12].###cn level features like the visual sentiment taxonomy of “adjectivenoun pairs (ANP)” [11], [12].",other,acknowledge attempts to improve classification results using visual sentiment features
1325,,e79365af9466872b5dc4f7c811f71d7f8937c2fc,Treating adolescent drug abuse: a randomized trial comparing multidimensional family therapy and cognitive behavior therapy.,,,"###FIML estimation is widely recognized as an optimal method for dealing with missing data in randomized controlled trial research designs [25,26] Although technically FIML estimation only produces unbiased estimates under the assumption the data are Missing at Random (MAR, meaning that missingness is not randomly distributed across all observations but is random within each independent variable), the MAR assumption is not directly testable.",impact-revealing,highlighting the significance and limitations of FIML estimation in research designs
717,5f1022a091e01168a7d6fc4f,04e234c7e4f300b4e8a16370728d875f9f484b39,learning visual context by comparison,5c8dd8774895d9cbc6a78838,Mask-Guided Contrastive Attention Model for Person Re-identification,"MGCAM [31] uses the contrastive feature between persons and backgrounds, but it requires extra mask supervision for persons.###Instead of finding an affinity map based attention as in NL [36], ACM explicitly uses direct comparison procedure for context modeling; instead of using extra supervision to localize regions to compare as in MGCAM [31], ACM automatically learns to focus on meaningful regions to compare.###Sharing a similar philosophy, there have been works on contrastive attention [31,42].",impact-revealing,acknowledge variations in existing methods
4059,5dbebb7447c8f766462c21c0,3d9baf7e87ec43f0ad486e2077824a346a58118e,Emotion-aware Chat Machine: Automatic Emotional Response Generation for Human-like Emotional Interaction,573696106e3b12023e5239eb,How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation.,"2 Evaluation Metric As reported in [19], BLEU might be improper to evaluate the conversation generation problem, as it correlates weakly with human judgements of the response quality, similar situations for METEOR [3] and ROUGE [17].",other,highlighting the limitations of evaluation metrics in conversation generation
623,5e8ef2ae91e011679da0f1a3,07d85a6c8a31f43ee6e128f7ef0a1bf1494567cd,Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence,5db9297d47c8f766461f7bb9,Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,"We compare the performance of CombinedTM using two different models for embedding the contextualized representations found in the SBERT repository: 8 stsb-roberta-large (Ours-R), as employed in the previous experimental setting, and using bert-base-nli-means (Ours-B).###We extend this model with contextualized document embeddings from SBERT (Reimers and Gurevych, 2019), 2 a recent extension of BERT that allows the quick generation of sentence embeddings.###We derive SBERT document representations from unpreprocessed text for Wiki20k External word embeddings topic coherence ( α ) provides an additional measure of how similar the words in a topic are.###Our model is built around two main components: (i) the neural topic model ProdLDA (Srivastava and Sutton, 2017) and (ii) the SBERT embedded representations (Reimers and Gurevych, 2019).###These descriptors illustrate the increased coherence of topics obtained with SBERT embeddings.###If a document is longer than SBERT’s sentence-length limit, the rest of the document will be lost.",impact-revealing,comparing performance of different models for contextualized representations
296,5eff040a91e011ea6db8de11,21a4cd35f19cfe8df1065b066b16edd048d2535d,DAPPLE: a pipelined data parallel approach for training large models,5daaea4247c8f766460f928f,PipeDream: generalized pipeline parallelism for DNN training,"Some researchers have been seeking for the optimal placement strategy to assign operations in a DNN to different devices[21, 35, 37, 46] to further improve system efficiency.",impact-revealing,highlighting ongoing research efforts in optimizing DNN operations
161,5c2c7a9217c44a4e7cf3189c,9ae43e25b04f5c35173b0bf490612015bd86c08f,Face-Focused Cross-Stream Network for Deception Detection in Videos,5550411545ce0a409eb386d9,Two-Stream Convolutional Networks for Action Recognition in Videos.,"Such a two-stream architecture was originally proposed for action recognition in videos and has been popular for many human-centric video analysis tasks [40, 6].###Different from the popular two-stream networks [40, 6, 47, 35], our FFCSN model has two novel components: (a) Face detection is added into the spatial stream subnet to capture the facial expressions explicitly.###This topic covers a wide range of research problems such as deception detection [36, 37], emotion recognition in videos [54, 18], personality computing [49, 56], and action recognition [6, 26, 28, 35, 40, 47, 59].",impact-revealing,acknowledge the origin and application of a two-stream architecture in video analysis
420,5f03f3b611dc830562231f99,6b65f02a0b4826b5b26ea0ed5fdefebeda76c597,A Framework for Recommending Accurate and Diverse Items Using Bayesian Graph Convolutional Neural Networks,5dcbd5da3a55ac789b0dbdc8,Bayesian Graph Convolutional Neural Networks using Node Copying,"These limitations were addressed in the follow-up works [23, 24], where [23] uses a non-parametric model for the graph generative model and [24] proposes a node copying model to achieve flexibility in the generative model and improve computational efficiency.###The proposed BGNN incorporates a random graph generative model based on node-copying [24].###In [24], Pal et al. introduce the node copying model for 𝑝 (G) .###Bayesian GNNs have not previously been used for the task of recommendation, but it has been shown that they can produce significant performance improvements in semi-supervised node classification when there are very few training labels [23, 24, 30, 39].###As an alternative, we use a more general generative model for graphs based on copying nodes, as introduced in [24].",impact-revealing,highlighting advancements and addressing limitations in graph generative models
594,5e3940c73a55ace46ed436d2,845b4941d8c016aa5f8967da2f86d38ef6c18fa3,a survey on knowledge graphs representation acquisition and applications,5687e3190cf2afdcd165fd54,A Review of Relational Machine Learning for Knowledge Graphs,"Previous survey papers on knowledge graphs mainly focus on statistical relational learning [9], knowledge graph refinement [6], Chinese knowledge graph construction [10], KGE [8] or KRL [11].",impact-revealing,acknowledge focus areas in previous survey papers on knowledge graphs
3326,5b67b45517c44aac1c860876,6c96c2d4a3fbd572fef2d59cb856521ee1746789,Graph Convolutional Neural Networks for Web-Scale Recommender Systems,58d82fd2d649053542fd75d8,Geometric deep learning: going beyond Euclidean data.,", to exploit user-to-item interaction graphs as well as social graphs) [6, 19, 21, 24, 29, 30].###(2017) [6] provide comprehensive surveys of recent advancements.###Following on this work, a number of authors proposed improvements, extensions, and approximations of these spectral convolutions [6, 10, 11, 13, 18, 21, 24, 29, 31], leading to new state-of-the-art results on benchmarks such as node classification, link prediction, as well as recommender system tasks (e.###, [13, 20]) we use the term “convolutional” to refer to a module that aggregates information from a local graph region and to denote the fact that parameters are shared between spatially distinct applications of this module; however, the architecture we employ does not directly approximate a spectral graph convolution (though they are intimately related) [6].",other,acknowledge advancements and state-of-the-art results in spectral convolutions
2366,5c04966a17c44a2c74708959,6b98bef930182a848c027dece1bfb58ca706449d,Improving End-to-end Speech Recognition with Pronunciation-assisted Sub-word Modeling,58437725ac44360f1082ffd7,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,"Concur-rently, [15] borrow the practice in voice search [16] to segment words into wordpiece which maximizes the language model probability.",other,reporting prior findings on word segmentation in voice search
2810,5edf5dd891e011bc656ded70,374f36c9081ab5dc686ab833c42a7297235cd13f,Denoising Implicit Feedback for Recommendation,59a02b94b161e8ad1a7b6b77,The Adressa dataset for news recommendation,"e, which may suffer more severely from the sparsity issue. For instance, many 1Each false-positive interaction is identified by auxiliary information of postinteraction behaviors, e.g., rating score ([1, 5]) &lt; 3, indicating that the interacted item dissatisfies the user. Refer to Section 2 for more details. arXiv:2006.04153v1 [cs.IR] 7 Jun 2020 Conference’17, July 2017, Washington, DC, USA Trovato an###g an opportunity for us to study the influence of falsepositive interactions. Dataset. We conduct experiments on two datasets (see more details in Section 5): •Adressa: This is a news reading dataset [5] that contains the dwell time of user click on news articles. Based on the experience of former researchers [13, 36], we identify the clicks with dwell time shorter than 10 seconds as false-positive i###to explore the effectiveness of denoising implicit feedback. More statistics about the three datasets are summarized in Table 2. •Adressa: This is a real-world news reading dataset from Adressavisen3 [5]. It includes anonymous users and their 3https://www.adressa.no/ Denoising Implicit Feedback for Recommendation Conference’17, July 2017, Washington, DC, USA Table 2: Statistics of the datasets. In pa",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3601,5ecbc7639fced0a24b502b60,326974bd97dea3278515bcb3551a59e304d6d755,Improving Low-Resource Speech Recognition Based on Improved NN-HMM Structures,5c04967517c44a2c74708d0f,Improving speech emotion recognition via Transformer-based Predictive Coding through transfer learning,"It has shown promising results in several areas, including Computer Vision (CV) [12], [41]–[44], Natural Language Processing (NLP) [13], [14] and so on [15]–[17].",other,highlighting the broad applicability and promising results of the method in various fields
1627,,1fe9b1a6c2943fe7163ce2986e20d603f01ab055,Evaluation and Analysis of Spectrum-Based Fault Localization with Modified Similarity Coefficients for Software Debugging,,,"###These programs are widely used for software testing and comparison of fault localization techniques [13, 19, 23, 33].###This criterion is widely used to measure the numbers of statements in the program that would be examined by the developer until finding the real faulty statements [13, 19, 29-30].###Over recent years, spectrum-based fault localization is very popular for both researchers and practitioners since it is a lightweight automated diagnosis technique and can easily be integrated with existing testing procedures [13].",impact-revealing,highlighting the popularity and utility of spectrum-based fault localization in software testing
2373,5db9298647c8f766461f8ed6,784b018c87c7dcbbe772374e45d5191bae9938ee,Hyperbolic Graph Neural Networks,5550411645ce0a409eb38730,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.,GGNN adds a GRU-like update [11] that incorporates information from neighbors and previous timesteps in order to update node representations.,other,describing the functionality of GGNN in updating node representations
805,5c04967517c44a2c7470927f,b9015d4f1e591eba7ea21c3566c919f80f7c2afe,attentive long short-term preference modeling for personalized product search,53e9ba59b7602d9704676d47,Applying collaborative filtering techniques to movie search for better ranking and browsing,"It is worth mentioning that our model is applicable to many other scenarios, such as personalized movie search [45] and academic article search [57].",impact-revealing,highlighting the applicability of the model to various scenarios
2924,5f06e5e591e0117f54657c19,f6d32ed0eee5fb3f6ac518f3aebc8ceff2aae397,NVAE: A Deep Hierarchical Variational Autoencoder,58d82fcbd649053542fd6178,The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables.,"…as reducing the gap between approximate and true posterior distributions [3, 4, 5, 6, 7, 8, 9, 10], formulating tighter bounds [11, 12, 13, 14], reducing the gradient noise [15, 16], extending VAEs to discrete variables [17, 18, 19, 20, 21, 22, 23], or tackling posterior collapse [24, 25, 26, 27].",other,acknowledge various approaches to improve posterior distributions
2558,53e9b79fb7602d970434793d,45ce4be870f0a5be7b45b064726696dacd83c786,Improving memory Bank-Level Parallelism in the presence of prefetching,53e99b2cb7602d97023c4503,Prefetch-Aware Dram Controllers,"Prefetch bits are already used in many proposals [7, 29, 23, 12] to indicate whether or not a cache line (or request) was brought in (or made) by the prefetcher.###800MHz DRAM bus cycle, DDR3 1600MHz [14], 8 to 1 core to DRAM bus frequency ratio; DRAM and bus 8B-wide data bus per channel, BL = 8; 1 rank, 8 banks per channel, 8KB row buffer per bank; On-chip, open-row, demand-first [12] FR-FCFS [20] DRAM controllers 1, 2, 4 channels for 1, 4, 8-core CMPs; 64-entry (8 × 8 banks) for single-core processor DRAM request 256 and 512-entry (16 × 8 banks per channel) buffers for 4 and 8-core CMPs###To implement this, we need to measure the run-time accuracy of the prefetcher [23, 12, 5].###Figure 13 shows the performance of PADC alone and PADC combined with our mechanisms for the 4- core workloads.###PADC also delays and drops useless prefetches to reduce waste in on-chip buffer resources and DRAM bandwidth.###A number of DRAM scheduling policies [20, 28, 17, 15, 12] have been proposed.###PADC aims to minimize DRAM latency of useful requests by prioritizing useful row-hit requests over others to the same bank.###We conclude that our mech-
anisms complement PADC and thus significantly improve system performance.###PADC significantly improves WS and HS by 14.1% and 16.3% respectively compared to the baseline.###Prefetch-Aware DRAM Controllers (PADC) [12] was proposed to maximize DRAM row buffer hits for useful requests (demands and useful prefetches).###When combined with PADC, BAPI and BPMRI improve WS and HS by 20.6% and 22.5%.###5, our mechanisms are complementary to prefetch-aware DRAM controllers [12] which employ an adaptive prefetch handling technique that is reported to outperform feedback-directed prefetching [23].###We use a stream prefetcher [23, 12] similar to the one in IBM’s POWER 4 [25] for most of our experiments.###In other words, the main goal of PADC is to exploit row buffer locality in each bank in a useful manner.###Adaptive prefetch handling techniques [10, 23, 12, 5] aim to reduce the interference between prefetch and demand requests in the memory system.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1835,,900fa32d4ef0c56cc4f2781c6d7bc1e49ec78a79,Formal Verification of Effectiveness of Control Activities in Business Processes,,,"###Our work is motivated by ver-iﬁcation of cryptographic protocol such as [10] and [23], which assume existence of attackers, while conventional business process modeling and veriﬁcation are focusing on correctness of sequences of actions which are performed for achieving business goals without…",impact-revealing,highlighting the motivation behind the research on cryptographic protocol verification
2302,5ebbc75d91e0119bc4e43623,407f1d16ba4eb3cb4851429cae46c97d723a35a5,invertible image rescaling,573696cd6e3b12023e5ce21a,Variational Inference with Normalizing Flows,"Due to such flexibility, INN architectures are also used for many variational inference tasks [44,30,10].",other,acknowledge applications of INN architectures in variational inference tasks
1121,,4470a2cc17ae41ee62e862a1cd5404ee9b1ef0e0,StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based Semantic Control,,,"###…the high-level algorithm represented in equation (3) and (4) embraces almost every generation methods for conditional diffusion models [12,21, 33], it does not consider cases of high demand in practice: the case where the desired shape of the image ˆ x ′ 0 ∈ R H ′ × W ′ × D is different from…###On the former line of works dealing with time, DDIM [33] and latent consistency models (LCM) [24, 25, 34] have reduced the number of required inference steps from several thousand to a few tens and then down to 4.###For example, DPM-Solver [21], the default sampler for LCM [24], is an instance of the former class, while DDIM [33], the default sampler for MultiDiffusion [5], is an example of the latter.###Unfortunately, simply replacing the Stable Diffusion (SD) model [31] with a Latent Consistency Model (LCM) [24] and the default DDIM sampler [33] with an LCM sampler [24] does not lead to faster MultiDiffusion.###Early methods such as [33], extend the forward diffusion process from DDPM [12] to a non-Markovian, significantly improving sampling efficiency and exhibiting superior consistency compared to DDPM [12].###Diffusion models, such as those operating in latent space as demonstrated by [31, 33], have shown impressive performance in high-resolution image synthesis tasks.###…additive white Gaussian noise (AWGN) denoising problem [39], this direct one-step inference typically produces highly blurry estimate of x 0 if t is close to T , enforcing practitioners to rely on various multi-step inference algorithms [12,21,33]: where t = { t 1 , . . . , t n } ⊂ { 0 , 1 , . . .",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1430,,900a5e6fac28c8f3180a5c80499634da92832bda,Oﬄine Reinforcement Learning without Regularization and Pessimism,,,"###…SOTA model-free ofﬂine RL methods, including value regularization methods: Fisher-BRC [40] that penalizes the gradient for the Q function, CQL [25] that penalizes Q-values on OOD actions; policy regularization methods: BCQ [21] uses a state-conditioned generative model to produce actions…###For example, CQL [25] augments the standard Bellman error objective with a Q-value regularizer to obtain conservative value estimates for OOD actions.###This distributional bias often ampliﬁes signiﬁcant extrapolation errors in deep RL algorithms, as the estimated Q-function tends to overestimate the OOD behavior [17], [25].###To tackle the aforementioned problems, most model-free ofﬂine RL methods [14]–[16], [18], [19], [21], [25]–[29], [34], [41], [42] build on existing deep RL algorithms by imposing constraints or penalties to encourage more conservative policy learning.###Value regularization methods [17]–[19], [25]–[28], for instance, penalize the value of OOD state-action pairs to make estimated Q-values more pessimistic.###Figure hopper-medium-v2 { 2 , 5 example, CQL [25] augments error objective with a Q-389 value regularizer to obtain value for actions.###Importantly, RFORL differs signiﬁcantly from the approach proposed in [12], [14]–[19], [21], [25]–[28], [34], [41]–[43] as it does not employ regularization techniques or pessimism.###A popular value estimation method for ofﬂine RL is conservative value estimation [17], [25], which counteracts incorrect value estimates by penalizing the values of OOD actions.",impact-revealing,describing various model-free offline reinforcement learning methods and their challenges
1708,,1759b0635746dbbb7a05952103c62575bc03cf61,"Mitigating inter- and intra-group ethnocentrism: Comparing the effects of culture knowledge, exposure, and uncertainty intolerance",,,"###…the link, additional consideration of Uncertainty-Identity Theory145 is needed.146 Uncertainty-Identity Theory (Hogg, 2009) is a recent social psychological account that extends the work of both Social147 Identity Theory (Tajfel & Turner, 1979) and Self-Categorization Theory (Turner & Oakes, 1989).###Thus,144 in order to better appreciate the potential causal nature of the link, additional consideration of Uncertainty-Identity Theory145 is needed.146 Uncertainty-Identity Theory (Hogg, 2009) is a recent social psychological account that extends the work of both Social147 Identity Theory (Tajfel & Turner, 1979) and Self-Categorization Theory (Turner & Oakes, 1989).",impact-revealing,providing context for Uncertainty-Identity Theory and its relation to existing theories
75,5f7ee07491e011a5faf0feb2,c54fb59319288d0d17ad09e6b7dfee6d042f83a8,Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer,58d82fced649053542fd6ed1,Multi-lingual Knowledge Graph Embeddings for Cross-lingual Knowledge Alignment.,"On the one hand, knowledge transfer across different embeddings is hindered by the lack of reliable alignment information that bridges different KGs. Recent works on multilingual KG embeddings provide support for automated entity matching (Chen et al., 2017, 2018b; Sun et al., 2018, 2020a).###The embedding learning process jointly trains the knowledge model and the alignment model following Chen et al. (2017), while self-learning is added to improve the alignment learning.###MTransE (Chen et al., 2017) jointly learns a transformation across two separate translational embedding spaces along with the KG structures.###Following Chen et al. (2017), instead of directly optimizing J in Eq.",impact-revealing,highlighting challenges in knowledge transfer across different embeddings
1565,,1bc115c1ad72e42abb55462da93bbc4cc7c0fd03,Lossless data hiding for halftone images,,,"###Reversible watermarking [11]–[14] is one example of lossless data hiding technique, which is widely used for content protection and authentication of digital images.",impact-revealing,providing context on reversible watermarking as a data hiding technique
3223,5fd8964891e0119b22c1f219,80bcfee1766e56a01e6feeaa3cb47d3291acdcdf,Pre-Training Graph Neural Networks for Cold-Start Users and Items Representation,5cfa5b985ced2477cb3c5175,Exploiting Centrality Information with Graph Convolutions for Network Representation Learning,"On another line, inspired by the recent development of graph neural networks (GNNs) [2, 11, 19], NGCF [38] and LightGCN [13] encode the high-order collaborative signal in the user-item interaction graph by a GNN model, based on which they perform the recommendation task.",other,highlighting the application of graph neural networks in recommendation tasks
1890,,504afed7b00fbee2bbc3b5c805e35f1442e2dbe8,An effective online learning for complex theoretical content: experience of Community of Inquiry,,,"###Social presence’s primary importance is, hence, “its function as a support for cognitive presence, indirectly facilitating the process of critical thinking carried on by the community of learners” [2].###CoI builds on three core elements, cognitive presence, social presence, and teaching presence [2].###In short, cognitive presence assumes that the goal of any educational experience is critical thinking and refers to the extent to which learners can construct knowledge through discourse and reflection [2].###content and supporting discourse are elements of cognitive presence [2].###The goal is to build a solid foundation of social presence and teaching presence to stimulate cognitive presence in a course [1, 2].###Teaching presence consists of the design and facilitation of the educational experience [2].###The framework was developed to address the problems of lack of connectedness and limited collaboration in online learning [2].###For it to happen, teaching presence is also needed [2].###The intertwining of these presences can be illustrated (Figure I) by their three shared sub-elements: setting climate, supporting discourse, and selecting content [2].",impact-revealing,providing context for the Community of Inquiry framework in online learning
783,5e5e190893d709897ce48240,76b8d5f2ef97d71167aa78309918bf3f7d633c96,Author Name Disambiguation on Heterogeneous Information Network with Adversarial Representation Learning,5b67b45517c44aac1c86078b,"Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop.","…similarity between each pair of papers using
the carefully designed pairwise features, including author names, titles, institute names etc.
AMiner (Zhang et al. 2018): This model designs a supervised global stage to fine-tune the word2vec result, and designs an unsupervised local stage based on…###The dataset is released by (Zhang et al.
2018), which contains 500 author names for training and 100 author names for testing.###And Zhang et al. (2018) actually transform the academic network into a homogeneous paper network after a complicated feature engineering.###, paper network (Zhang et al. 2018), paper-author network (Zhang and Al Hasan 2017).###Thus constructing the network becomes the critical part of these methods, e.g., paper network (Zhang et al. 2018), paper-author network (Zhang and Al Hasan 2017).###However, either complicated feature engineering or the supervision (Zhang et al. 2018) is required.###AMiner (Zhang et al. 2018): This model designs a supervised global stage to fine-tune the word2vec result, and designs an unsupervised local stage based on the first stage.###In the experiment on AMiner-AND, we use 100 names for testing and compare the result with the results of other models reported in (Zhang et al. 2018).###Zhang et al. (2018) construct paper networks, where the weights of edges are decided by a supervised model based on the sharing information between two papers.###The dataset is released by (Zhang et al. 2018), which contains 500 author names for training and 100 author names for testing.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
404,5bdc315017c44a1f58a05e7e,7a71941e60894ae7e1f5af8e79c37cec6cd6c6ad,Multi-scale Residual Network for Image Super-Resolution,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"[12] proposed a residual learning framework (Fig.###To validate the effectiveness of our module, we design a set of comparative experiments to compare the performance with residual block [12], dense block [24] and MSRB in SISR tasks.###Quantitative comparison of three different feature extraction blocks (residual block [12], dense block [24], and MSRB(our)) on SISR.###Represent the output of the residual block [12], the dense block [24], and our MSRB, respectively.###[12] first introduced the residual architecture for training much deeper network (20 layers) and achieved",impact-revealing,reporting prior findings on residual learning framework
3572,5ecbc7639fced0a24b502b60,326974bd97dea3278515bcb3551a59e304d6d755,Improving Low-Resource Speech Recognition Based on Improved NN-HMM Structures,5b8c9f4a17c44af36f8b6f47,Data Augmentation for Robust Keyword Spotting under Playback Interference,"Data augmentation has been proved to be a simple and effective technique, not only in speech recognition but also in other fields such as image recognition [25] and keyword search [26], [27].",other,acknowledge the effectiveness of data augmentation across various fields
357,5e5e18f793d709897ce40800,b21025dedb3dfcd80be001c7eff091d83579fddf,CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning,573696076e3b12023e51a3b4,Incorporating Copying Mechanism In Sequence-To-Sequence Learning,"Inspired by CopyNet (Gu et al. 2016), the ﬁrst step uses Generate-Mode to predict a relation.###Seq2Seq : CopyRE (Zeng et al. 2018) is another method for solving the overlapping relation problem, which extracts triplets by a Seq2Seq framework (Sutskever, Vinyals, and Le 2014) with copy mechanism (Gu et al. 2016).",impact-revealing,acknowledge existing methods for relation extraction
1692,,a846bfe666dc93e428e2e125337c938ec12ec38a,The influence of military identity on work engagement and burnout in the norwegian army rapid reaction force,,,"###Thus, professionalism could be regarded as the preferred identity in the Norwegian armed forces, and should in terms of SIT represent a set of preferred values, goals and tasks of the organization.###Military identity and its relationship to performance may also be explored by Social Identity Theory (SIT) (Tajfel & Turner, 1979).###Military identity in relation to work engagement and burnout
Grounded in SIT, Haslam (2004) argues that social identification plays a key role in establishing important organizational behaviors such as compliance, extra-role proorganizational behavior, loyalty, improved performance, reduced absenteeism and higher levels of physical and emotional well-being.###All these elements appear fundamental to the functioning of the Armed Forces, thus linking military identity to both SIT and actual coping and performance.###SIT is an empirically based context dependant theory and represents the relationship between the self and the context.###Further he argues that the main prediction of SIT for organizational contexts is that the more an individual define him- or herself in terms of membership in an organizational group (as for instance the Armed Forces) the more his or her attitudes and behaviours are governed by this group membership.###SIT, along with Social Categorisation Theory, also suggests that people categorize themselves as members of certain social groups at different abstraction levels, or as unique individuals.###In light of SIT, an identity which builds on a variation of individualistic occupational attitudes and motives thus appears to be in conflict with the structure, tasks, and demands of military service.",impact-revealing,discussing the relationship between military identity and organizational behavior through Social Identity Theory
3672,53e99f3bb7602d970280aaa9,5b324db597c57cbed6b15baa757e3bf7611dd06c,Practical off-chip meta-data for temporal memory streaming,53e999cbb7602d9702212df6,Efficient Representations And Abstractions For Quantifying And Exploiting Data Reference Locality,"More than a decade of research has repeatedly shown that address-correlating prefetchers can eliminate about half of all off-chip misses in pointerintensive commercial server workloads, whereas stride prefetchers provide only minimal benefit [ 4 ,6,26,27].###Temporal streaming. The observation that memory access sequences recur was first quantified in memory trace analysis by Chilimbi and Hirzel [ 4 ].###However, offline analyses of miss repetition [ 4 ,9,26] have shown that temporal streams vary drastically in length, from two to hundreds of misses.",other,highlighting the effectiveness of address-correlating prefetchers in reducing off-chip misses
3157,5b3d98cc17c44a510f801c3a,f722b0a7a9b7709d693b9d39195c779832a943fe,end-to-end speech-driven facial animation with temporal gans,573698016e3b12023e6da477,U-Net: Convolutional Networks for Biomedical Image Segmentation,A U-Net [19] architecture is used with skip connections between the Identity Encoder and the Frame Decoder to help preserve the identity of the subject.,other,describing the architecture and its purpose
781,599c797a601a182cd2642797,668db48c6a79826456341680ee1175dfc4cced71,Get To The Point: Summarization with Pointer-Generator Networks,5843775dac44360f1083c2b4,Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.,"However, the recent success of sequence-to-sequence models (Sutskever et al., 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016).###The pointer network has been used to create hybrid approaches for NMT (Gul-cehre et al., 2016), language modeling (Merity et al., 2016), and summarization (Gu et al., 2016; Gulcehre et al., 2016; Miao and Blunsom, 2016; Nallapati et al., 2016; Zeng et al., 2016).###We use the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016), which contains online news articles (781 tokens on average) paired with multi-sentence summaries (3.75 sentences or 56 tokens on average).###This theory is supported by the large boost that coverage gives our ROUGE scores (see Table 1), compared to the smaller boost given by temporal attention for the same task (Nallapati et al., 2016).###(...) Summary: more questions than answers emerge in controversial s.c. police shooting. of Nallapati et al. (2016) by several ROUGE points.###…has been augmented with recurrent decoders (Chopra et al., 2016), Abstract Meaning Representations (Takase et al., 2016), hierarchical networks (Nallapati et al., 2016), variational autoencoders (Miao and Blunsom, 2016), and direct optimization of the performance metric (Ranzato et al., 2016),…###Nevertheless, given that the disparity in the lead-3 scores is (+1.1 ROUGE-1, +2.0 ROUGE-2, +1.1 ROUGE-L) points respectively, and our best model scores exceed Nallapati et al. (2016)###We used scripts supplied by Nallapati et al. (2016) to obtain the same version of the the data, which has 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs.###5 In addition to our own models, we also report the lead-3 baseline (which uses the ﬁrst three sentences of the article as a summary), and compare to the only existing abstractive (Nallapati et al., 2016) and extractive (Nallapati et al., 2017) models on the full dataset.###Both the dataset’s published results (Nallapati et al., 2016, 2017) use the anonymized version of the data, which has been pre-processed to replace each named entity, e.g., The United Nations , with its own unique identiﬁer for the example pair, e.g., @entity5 .###Our baseline model is similar to that of Nallapati et al. (2016), and is depicted in Figure 2.###Nallapati et al. (2016) adapted the DeepMind question-answering dataset (Hermann et al., 2015) for summarization, resulting in the CNN/Daily Mail dataset, and provided the ﬁrst abstractive baselines.###Our approach is considerably different from that of Gulcehre et al. (2016) and Nallapati et al. (2016).###Unlike Nallapati et al. (2016), we do not pre-train the word embeddings – they are learned from scratch during training.###6 Given that we generate plain-text summaries but Nallapati et al. (2016; 2017) generate anonymized summaries (see Section 4), our ROUGE scores are not strictly comparable.###Temporal attention is a related technique that has been applied to NMT (Sankaran et al., 2016) and summarization (Nallapati et al., 2016).",impact-revealing,highlighting the advancements and methodologies in abstractive summarization
3558,5c7a561ff56def9798e6a297,abe4e92d6e4868115aef57a4eba97ec4d48ac124,language-adversarial transfer learning for low-resource speech recognition,558be262e4b0cfb70a1a629e,Multilingual acoustic models using distributed deep neural networks,"The transfer learning methods can be roughly classified into two categories: transferring bottleneck features [17], [23]–[26] and transferring model parameters [10], [11], [27].###A lot of variants of SHL-Models are proposed to train multilingual models [10], [23], [27], [32].",other,describing categories of transfer learning methods
839,5a260c0917c44a4ba8a1e00e,93f9607034c9b7b7693c60e9d2631adc15a2a524,learning to model the tail,53e9ab6fb7602d970350dc5d,A Survey on Transfer Learning,"Our approach is broadly related to different meta-learning concepts such as learning-to-learn, transfer learning, and multi-task learning [29, 30, 18, 31].###While transfer learning from a source to target task is a well studied problem [18, 19], by far the most common approach is fine-tuning a model pre-trained on the source task [20].",impact-revealing,acknowledge existing meta-learning concepts and their relation to the approach
3887,5b1642d68fbcbf6e5a9b7e77,0be19fd9896e5d40222c690cc3ff553adc7c0e27,Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods,599c794e601a182cd262eb67,Counterfactual Fairness,"Kusner et al. (2017) propose the method based on causal inference to achieve the model fairness where they do the data augmentation under speciﬁc cases, however, to the best of our knowledge, we are the ﬁrst to propose data augmentation based on gender swapping in order to reduce gender bias.",other,highlighting a novel approach to data augmentation for reducing gender bias
525,59a03016b161e8ad1a7b6ed2,3d1f9a530e710fdd4e2313bda4c8a1f574e60ab6,neural factorization machines for sparse predictive analytics,53e99796b7602d9701f5d980,Factorization Machines,"lued feature vector for supervised learning. Clearly, it enhances linear/logistic regression (LR) using the second-order factorized interactions between features. By specifying input features, Rendle [27] showed that FM can mimic many speci•c factorization models such as the standard MF, parallel factor analysis, and SVD++ [22], Owing to such genericity, FM has been recognized as one of the most e‡ect###on-linear models. In what follows, we shortly recapitulate the two representative techniques. 2.1 Factorization Machines Factorization machines are originally proposed for collaborative recommendation[27,30]. Givenarealvaluedfeaturevectorx 2Rn, FM estimates the target by modelling all interactions between each pair of features via factorized interaction parameters: yˆFM„x”= w0 + Õn i=1 wixi + Õn i=1 Õn j###-looking-atmachine-learning arXiv:1708.05027v1 [cs.LG] 16 Aug 2017 that have complex and non-linear underlying structure, FM may not be expressive enough. Although higher-order FMs have been proposed [27], they still belong to the family of linear models and are claimed to be di†cult to estimate [28]. Moreover, they are known to have only marginal improvements over FM, which we suspect the reason migh###, i.e., Vx = fxivigwhere xi , 0. Note that we have rescaled an embedding vector by its input feature value, rather than simply an embedding table lookup, so as to account for the real valued features [27]. Bi-InteractionLayer. We then feed the embedding set Vx into the Bi-Interaction layer, which is a pooling operation that converts a set of embedding vectors to one vector: fBI„Vx”= Õn i=1 Õn j=i+1 xi###omains. Insteadofaugmentingfeaturevectorsmanually, anothersolution is to design a ML model to learn feature interactions from raw data automatically. A popular approach is factorization machines (FMs)[27], whichembedsfeaturesintoalatentspaceandmodelsthe interactions between features via inner product of their embedding vectors. While FM has yielded great promise1 in many prediction tasks [2, 8, 21, 24",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2048,,a85d31d3f3571cb683a20767404a1f8ccec38e1d,An Efficient Functionality Learning Contigent Stroller Algorithm to Demonstrate the 3d Image Segmentation,,,"###Subsequently, each individual nucleus was segmented by a graph cut-based algorithm which was inspired by previous works in open pit mining [6] [7] and medical image segmentation [8] [9].",impact-revealing,acknowledging inspiration from previous works in related fields
662,5aed14d617c44a4438159341,c8efcc854d97dfc2a42b83316a2109f9d166e43f,self-attention with relative position representations,599c7987601a182cd2648373,Attention Is All You Need.,"We compared our model using only relative position representations to the baseline Transformer (Vaswani et al., 2017) with sinusoidal position encodings.###, 2016), attention (Vaswani et al., 2017), or a combination of recurrence and attention (Bahdanau et al.###The Transformer computes self-attention efficiently for all sequences, heads, and positions in a batch using parallel matrix multiplication operations (Vaswani et al., 2017).###The Transformer computes self-attention efﬁ-ciently for all sequences, heads, and positions in a batch using parallel matrix multiplication operations (Vaswani et al., 2017).###The Transformer (Vaswani et al., 2017) employs an encoder-decoder structure, consisting of stacked encoder and decoder layers.###These position encodings can be a deterministic function of position (Sukhbaatar et al., 2015; Vaswani et al., 2017) or learned representations.###…sequence learning typically leverage recurrence (Sutskever et al., 2014), convolution (Gehring et al., 2017; Kalch-brenner et al., 2016), attention (Vaswani et al., 2017), or a combination of recurrence and attention (Bahdanau et al., 2014; Cho et al., 2014; Lu-ong et al., 2015; Wu et al., 2016)…###We used the same warmup and decay strategy for learning rate as Vaswani et al. (2017), with 4,000 warmup steps.###For our machine translation experiments, the result was a modest 7% decrease in steps per second, but we were able to maintain the same model and batch sizes on P100 GPUs as Vaswani et al. (2017).",impact-revealing,providing context on the Transformer model and its components
31,5ce2d0feced107d4c63dd498,d07284a6811f1b2745d91bdb06b040b57f226882,Decoupled Weight Decay Regularization,53e9a8bdb7602d970320bb24,Comparing biases for minimal network construction with back-propagation,"In the weight decay described by Hanson & Pratt (1988), the weights θ decay exponentially as
θt+1 = (1− λ)θt − α∇ft(θt), (1)
where λ defines the rate of the weight decay per step and ∇ft(θt) is the t-th batch gradient to be multiplied by a learning rate α.###In order to decouple the effects of these two hyperparameters, we advocate to decouple the weight decay step as proposed by Hanson & Pratt (1988) (Equation 1).",impact-revealing,describing the weight decay method and its hyperparameters
1441,,3611a93e67dc7efb587f8afdd8663e13c735ec7f,Cognitive Aids in Medicine Assessment Tool (CMAT): preliminary validation of a novel tool for the assessment of emergency cognitive aids,,,"###The probability of demonstrating that cognitive aids reduce morbidity or mortality in emergency care is limited by the rarity of these events; however, in the field of anaesthesia and surgery, checklists have been shown to improve the management of simulated intra-operative crises [5, 6].",impact-revealing,highlighting the effectiveness of checklists in improving crisis management in anaesthesia and surgery
3703,53e99b9bb7602d9702446a39,64441c8396211b5e799b9ad5138dade15ff5cd0a,grassmann discriminant analysis: a unifying view on subspace-based learning,53e997e8b7602d9701fdff81,Binet-Cauchy Kernels,"The Binet-Cauchy metric is defined with the product of canonical correlations (Wolf & Shashua, 2003;  Vishwanathan & Smola, 2004 ).",other,reporting a definition of a metric
2245,5b67b45517c44aac1c860823,7f01c6fe27f57ee6191b51efa18b9199baf7b82a,Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System,599c7968601a182cd263a485,End-to-End Neural Ad-hoc Ranking with Kernel Pooling,"As in [10, 27, 28, 37], three different evaluation metrics used are Precision@n (Prec@n), nDCG@n, and Mean Average Precision (MAP).###Several successful ranking models with neural networks have been investigated [14, 33, 35, 37].",other,acknowledge evaluation metrics and successful ranking models
3630,5fdb2e1691e0118a02c4f566,16913a534b1630d33770b392767bb316f4fdb11e,Clinical Temporal Relation Extraction with Probabilistic Soft Logic Regularization and Global Inference,5f63297c91e011242e3f2ac1,Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction,"Some recent studies (Leeuwenberg and Moens 2017; Ning, Feng, and Roth 2017; Han, Zhou, and Peng 2020) convert the task to a structured prediction problem and solve it with Maximum a posteriori Inference.",other,acknowledging recent studies on structured prediction problems
2832,5bdc31b417c44a1f58a0b240,6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da,Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks,53e9a455b7602d9702d7474f,Computational capabilities of graph neural networks.,"…GraphSAGE (Hamilton, Ying, and Leskovec 2017a), SplineCNN (Fey et al. 2018), and the spectral approaches proposed in (Bruna et al. 2014; Defferrard, X., and Vandergheynst 2016; Kipf and Welling 2017)—all of which descend from early work in (Merkwirth and Lengauer 2005) and (Scarselli et al. 2009b).###Moreover, Scarselli et al. (Scarselli et al. 2009a) investigates the approximation capabilities of GNNs.###(Scarselli et al. 2009a) investigates the approximation capabilities of GNNs.",other,acknowledge foundational work in graph neural networks
1,5c2348ceda562935fc1d57a4,007d08aee3b88489fe0377849f70688de66adae8,Bandit Learning with Implicit Feedback,53e9a3f4b7602d9702d0a17e,An experimental comparison of click position-bias models,"As having been extensively studied in click modeling of user search results [6], various factors affect 62 users’ click decisions, and among them result examination plays a central role [12, 7].###Examination 28 hypothesis [7], which is a fundamental assumption in click modeling, postulates that a user clicks on 29 a system’s returned result if and only if that result has been examined by the user and it is relevant to 30 the user’s information need at the moment.",impact-revealing,highlighting the importance of result examination in click modeling
2226,5da1a6d447c8f7664606888c,404d82a8da658f16c46714442155490d79a413f1,temporal prefetching without the off-chip metadata,57d063e0ac443673542947ae,Back to the Future: Leveraging Belady's Algorithm for Improved Cache Replacement.,"To identify important metadata, Triage uses the Hawkeye replacement policy [25], which provides significant performance benefits for small metadata stores, as it identifies frequently accessed metadata over a long history of time.###To accomplish these goals, we modify Hawkeye [25], a state-of-the-art cache replacement policy, which learns from the optimal solution for past memory references.###Our metadata replacement is based on the Hawkeye policy [25], and like the Hawkeye policy, our metadata replacement policy is trained on the behavior of a few sampled sets.###For more details on how the Hawkeye policy works, we refer the reader to the original paper [25].",other,describing the modification and application of the Hawkeye replacement policy for metadata management
1936,,0e957fc782caedbb4ceb5639b548fb96fe852656,Resting-state EEG reveals global network deficiency in dyslexic children,,,"###Morphological awareness is another reliable predictor of dyslexia in Chinese (Lei et al., 2011; Li et al., 2009; Shu et al., 2006).###Following the protocols of previous studies (Ding et al., 2016; Lei et al., 2011; Shu et al., 2006), to be included in the dyslexic group a child needs to a###This test was developed by Shu et al. (2003) and it has been widely used for screening dyslexia in Mandarin-speaking Chinese children (Ding et al., 2016; Lei et al., 2011; Shu et al., 2006; Zhang et al., 2012).",impact-revealing,highlighting the role of morphological awareness in predicting dyslexia
1260,,12dc68621deda095a40793b042b548d5b2ca6bf4,"A mixed methods approach to examining the ""Getting Ready"" intervention for supporting young children with challenging behaviors",,,"###These include First Step to Success (Walker et al., 1998), Living with a Purpose Self-Determination Program (Forness et al., 2000), Incredible Years (Webster-Stratton, Reid, & Hammond, 2001), and Prevent-Teach-Reinforce for Young Children (Dunlap, Wilson, Strain, & Lee, 2013).###40
Incredible Years (Webster-Stratton et al., 2001).###…able to address gaps in support and maximize teaching of positive behavior, allowing children many opportunities to practice and generalize skills throughout the day across their natural environments (Sheridan, Eagle, Cowan, & Mickelson, 2001; Sheridan et al., 2008; Webster-Stratton et al., 2001).###Early Childhood Research Quarterly, 26, 442- 452. doi: 10.1016/j.ecresq.2011.03.003
166
Table 1 Intervention Programs for Improving Young Children’s Social, Emotional, and/or Behavioral Outcomes
Name of Program
Key Features Summary of Evidence Base
First Step to Success (Walker et al., 1998) Target: at-risk kindergartners Universal screening Coordinated interventions--
School: target child, teacher,
peers
Home: Parent training
Service delivered by consultant
Reduced aggression and
maladaptive behaviors
Increased academic engaged
time
Increased adaptive behavior Durable results into Grade 2
Living with a Purpose Self-
Determination
Program
(Forness et al., 2000)
Target: 3- to 5-year-olds Universal screening Classroom-wide
intervention— self-determination curriculum builds skills such as direction following, decision-making, sharing
Parent training Follow-up with pre-referral
intervention or special education referral, if needed
Improved adaptive behavior Increased social interaction Reduced inattention Reduced problem behavior No significant effects for
aggression or non-compliance
Incredible Years
(Dinosaur School) (Webster-Stratton,
Reid, & Hammond,
2001)
Target: preschoolers through
1 st grade
Classroom-wide management Teacher training Parent training
Increased positive parenting
and parent-school bonding
Reduced harsh parental
discipline
Reduced conduct problems and
aggression at school
Improved child compliance
and social contact
Increased school readiness
skills
Greater positive classroom
atmosphere
Positive satisfaction rating:
Parents- 89% Teachers- 97%
(continued)
167
Name of Program
Key Features
Summary of Evidence Base
Prevent-Teach-
Reinforce for Young
Children
Dunlap, Lee, &
Strain, (2013)
Target: toddlers or preschoolers
in classroom settings
Tier 3- individualized
intervention
5-step process: team formation
and goal setting, data collection, functional behavioral assessment, behavior intervention planning, progress monitoring
Controlled trial in progress Research support for
components of PTR-YC including PBS, functional behavioral assessment, efficacy of PTR in school-aged children
168
Table 2 Characteristics of Participants
Characteristics Children
(n = 19)
Parents (n = 19)
Preschool Teachers
(n = 11)
Age (years)
3.9 (SD = .3) 31.1 (SD = 6.6)
36.3 (SD = 9.9)
Gender
Female Male
32% 68%
84% 16%
100%
0%
Race
White 53% 84% 91% Black 5% 5% 0% American Indian 5% 5% 0% Biracial/multi-racial Asian 37% 0% 5% 0% 0% 9%
Language Preference
English 95% Spanish 5%
Marital Status
Married 26% Living with partner 16% Divorced, single, or separated 58%
Highest Level Education
Below 12 th grade 16% 0% High school diploma/ GED 31% 0% Training beyond high school 21% 0% Two- or four-year college degree 32% 73% Graduate degree 0% 27%
Early Childhood Teaching Endorsement or Certificate
73%
Experience (years)
Teaching children ages birth
to 5 in early childhood setting
9.4 (SD = 8.8)
Being employed in current
position
2.9 (SD = 2.7)
169
Table 3 Phase 1 Quantitative Measures Collected Time 1 and Time 2
Instrument
Author
Description
Reliability
Child Measures
Social Skills
Improvement
System:
Parent Form
Gresham &
Elliott, (2008)
Parent checklist
assessing child problem behaviors and social skills via two subscales
Yields standard
scores (X = 100; SD = 15)
Internal consistency, alpha
coefficients for domains/ overall scores:
Social Skills = .76 - .88; Overall
= .96
Problem Behaviors = .80-.90;
Overall = .94
Test-retest reliability ranges for
scales/ subscales: .70-.92
Social Skills
Improvement
System:
Teacher Form
Gresham &
Elliott, (2008)
Teacher checklist
assessing child problem behaviors and social skills via two subscales
Yields standard
scores (X = 100; SD = 15)
Internal consistency, alpha
coefficients for domains/ overall scores:
Social Skills = .85-.90; Overall =
.97
Problem Behaviors = .75-.93;
Overall = .94
Test-retest reliability ranges for
scales/ subscales: .74-.86
Brief Rating Inventory of
Executive
Functioning-
Pre-School
Gioia,
Isquith, Guy, &
Kenworthy,
(2000)
Teacher checklist
assessing inhibitory selfcontrol, flexibility, and emergent metacognition
Yields t- scores
(X = 50; SD = 10)
Internal consistency, alpha
coefficients:
Inhibit = .94, Shift = .90, Emotional Control = .91, Working Memory = .94, Plan/Organize = .97, Global Executive Composite = .97 Test-retest reliability ranges for
overall scale/ subscales: .65-.88
(continued)
170
Instrument
Author
Description
Reliability
Child Measures
Peabody
Picture
Vocabulary
Test-4
Dunn &
Dunn, (2007)
Direct test of child
receptive vocabulary
Yields standard
scores (X = 100; SD = 15)
Internal consistency, alpha
coefficient for overall score = .97
Expressive Vocabulary
Test-2
Williams
(2007)
Direct test of
expressive vocabulary and word retrieval
Yields standard
scores (X = 100; SD = 15)
Internal consistency, alpha
coefficient for overall score = .96
Bracken
Basic Concept
Scale-3: Revised
Bracken
(2006)
Direct test of
concept development.###Thirty-six Head Start classrooms were randomly assigned to treatment (Incredible Years Parent and Teacher training) and control (Head Start business as usual) conditions.###A number of scholars have drawn attention to promising process-oriented professional development to improve in-service teachers’ relationships with children (Pianta, Mashburn, Downer, Hamre, & Justice, 2008; Webster-Stratton et al., 2001).###generalize skills throughout the day across their natural environments (Sheridan, Eagle, Cowan, & Mickelson, 2001; Sheridan et al., 2008; Webster-Stratton et al., 2001).",impact-revealing,acknowledge existing intervention programs for improving children's outcomes
3508,5eede0b091e0116a23aafb82,1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af,Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,5e58e54091e01189527e2b29,Learning Representations By Predicting Bags Of Visual Words,"Our work is also related to clustering-based meth-ods [2, 4, 7, 8, 19, 29, 58, 61, 62, 67].###Note that this is line with previous works that also show that self-supervision can outperform supervised pretraining on object detection [19, 24, 43].",other,acknowledge related clustering-based methods and their performance
1313,,d4add17d2674760ef66ebb18d92a71b218481e86,Abrupt increase in phosphorus and potassium fluxes during a masting event in a Bornean tropical forest,,,"###Many temperate and tropical tree species exhibit conspicuous inter-annual variation in resource allocation to reproduction, which is a phenomenon known as masting (Kelly 1994; Curran and Webb 2000).###…and mineral nutrients; for example, the production of fruits in a mast year attains 20–60% of the dry mass of leaf litterfall in temperate and tropical forests (reviewed by Green and Newbery 2002), and it is predicted that masting is limited by stored resources (Kelly 1994; Koenig and Knops 2000).",impact-revealing,highlighting the phenomenon of masting in tree species and its resource allocation implications
683,5d04eeba8607575390f83f4d,9cceaadb580c24d0d5c381fa8e3d4afb32fd88b9,perceptron-based prefetch filtering,53e9aca8b7602d97036868cf,Dynamic Branch Prediction with Perceptrons,"Perceptron learning for microarchitectural prediction was introduced for branch prediction [20].###Branch prediction is done using the perceptron branch predictor [20].###In addition to branch prediction [20], perceptron-based learning has been applied to the area of cache management.",impact-revealing,reporting prior findings on perceptron learning applications
1846,,af0449cb57130861fb254c28b1cf364c1d27d0c9,A Proposed Framework for Analysing Security Ceremonies,,,"###We decided to implement our description and verification model for security ceremonies based on Paulson’s Inductive Method (Paulson, 1998).###, 1989) for their belief logic, Abadi for spi-calculus (Abadi and Gordon, 1997), Ryan (Ryan and Schneider, 2000), Lowe (Lowe, 1996) and Meadows (Meadows, 1996) for works on state enumeration and model checking, and Paulson and Bella (Paulson, 1998; Bella, 2007) for their inductive method as the principal initiatives.###…logic, Abadi for spi-calculus (Abadi and Gordon, 1997), Ryan (Ryan and Schneider, 2000), Lowe (Lowe, 1996) and Meadows (Meadows, 1996) for works on state enumeration and model checking, and Paulson and Bella (Paulson, 1998; Bella, 2007) for their inductive method as the principal initiatives.###We must cite Burrows et al. (Burrows et al., 1989) for their belief logic, Abadi for spi-calculus (Abadi and Gordon, 1997), Ryan (Ryan and Schneider, 2000), Lowe (Lowe, 1996) and Meadows (Meadows, 1996) for works on state enumeration and model checking, and Paulson and Bella (Paulson, 1998; Bella, 2007) for their inductive method as the principal initiatives.###Based on the current model for protocols set by Paulson (Paulson, 1998) and extended by Bella (Bella, 2007), we include a new agent type called Human.###This approach has already been used to prove a series of classical protocols (Paulson, 1998) as well as some wellknown industry grade protocols, such as the SET online payment protocol, Kerberos and SSL/TLS (Bella et al.###This approach has already been used to prove a series of classical protocols (Paulson, 1998) as well as some wellknown industry grade protocols, such as the SET online payment protocol, Kerberos and SSL/TLS (Bella et al., 2002; Bella, 2007).",impact-revealing,acknowledge foundational works in security protocols
502,5f9be24691e011dcf482d8d6,842bd2d15d53e3083c110e0af55ffd7447ad03e4,Prediction-Based Power Oversubscription in Cloud Platforms,5ff68911d4150a363cc921ef,Protean: Vm Allocation Service At Scale,Azure’s scheduler [17] implements its heuristics as two sets of rules.,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2274,5b3d98cc17c44a510f802054,8a564ee07fa930ebc1176019deacdc9951063a99,Collaborative Learning for Deep Neural Networks.,599c797d601a182cd2643eb4,The Numerics of GANs,"In terms of convergence, recent work [15, 16] reveals that simultaneous SGD has faster convergence and achieves better performance than the alternative one.",other,highlighting the significance of recent findings on convergence in SGD
30,58437722ac44360f1082f15c,cc16e43cce64b649da00892d1493425620c2d61c,Learning to Match Using Local and Distributed Representations of Text for Web Search.,573698636e3b12023e729d7c,Text Matching as Image Recognition,"[31] propose the use of matching matrices to represent the similarity of short texts, then apply a convolutional neural network inspired by those in computer vision.###[12] classify recent DNN models for short-text matching as either interaction-focused [15, 22, 31] or representation-focused [15, 16, 35–37].",impact-revealing,reporting prior findings on short text similarity methods
701,53e99845b7602d9702072224,00ae1c36f7eb925873322e548073a64d6795787e,multiple stream prediction,53e99804b7602d970201668d,Fetching instruction streams,"The stream fetch engine [11,16] model is shown in Figure 2.###Our approach to achieve high fetch bandwidth, while maintaining the complexity under control, is the stream fetch engine [11,16].###In addition, data are shown for the original single-stream predictor, described in [11,16], and a 2-stream multiple predictor.###The next stream predictor [11,16], which is shown in Figure 4.###To avoid this increase in the fetch engine complexity, we propose to use long instruction streams [11,16] as basic prediction unit, which makes it possible to hide the prediction table access delay.###This makes it possible for the stream fetch engine to provide high fetch bandwidth while requiring low implementation cost [11,16].###Our instruction cache setup uses wide cache lines, that is, 4 times the processor fetch width [11], and 64KB total hardware budget.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3561,5e6cacbe91e011436e692287,317cf17506eab84cab8e36f81f6b1bcd7e427871,a graph neural network approach for scalable wireless power control,5ac1829d17c44a1fda918033,Fractional Programming for Communication Systems-Part I: Power Control and Beamforming.,"Although several optimization-based methods have been proposed in [12], [14], they are computationally demanding, and thus cannot be applied for real-time implementation [1].",other,highlighting limitations of optimization-based methods for real-time application
2279,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,5550447d45ce0a409eb4e3c3,Understanding User Beliefs About Algorithmic Curation In The Facebook News Feed,"” These stories are then characterized into dense feature vectors by feature extractors and learned models [17,26–28].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
102,599c797d601a182cd2643e8a,cd8a9914d50b0ac63315872530274d158d6aff09,Modeling Relational Data with Graph Convolutional Networks,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,"Relations in these datasets need not necessarily encode directed subject-object relations, but are also used to encode the presence, or absence, of a specific feature for a given entity.###†Canadian Institute for Advanced Research
Academy is marked as a university).###Predicting missing information in knowledge bases is the main focus of statistical relational learning (SRL).###We introduce the following notation: we denote directed and labeled multi-graphs as G = (V, E ,R) with nodes (entities) vi ∈ V and labeled edges (relations) (vi, r, vj) ∈ E , where r ∈ R is a relation type.1###In practice this can easily lead to overfitting on rare relations and to models of very large size.",impact-revealing,providing context on the nature of relations in datasets and their implications for statistical relational learning
2648,5b1642388fbcbf6e5a9b54be,b3dae9529f3caeeec9cc6872e94aa690418acb22,Reinforcement Learning for Relation Classification from Noisy Data,58437762ac44360f1083cdb0,A Position Encoding Convolutional Neural Network Based on Dependency Tree for Relation Classification.,"Recently, neural models have been widely applied to relation classification (Zeng et al. 2014; dos Santos, Xiang, and Zhou 2015; Mooney and Bunescu 2005; Yang et al. 2016) including convolutional neural networks, recursive neural network (Ebrahimi and Dou 2015; Liu et al.###…neural models have been widely applied to relation classiﬁcation (Zeng et al. 2014; dos Santos, Xiang, and Zhou 2015; Mooney and Bunescu 2005; Yang et al. 2016) including convolutional neural networks, recursive neural network (Ebrahimi and Dou 2015; Liu et al. 2015), and long short-term…",other,acknowledge the application of neural models in relation classification
1249,,7568d1964e126b34a8b03058d169c788acd6e755,"Neuro-ophthalmologic evaluation, quality of life, and functional disability in patients with MS",,,"###Although we found better predictive value with structural parameters, VEP latency and MD of the visual field correlated with the EDSS score and EDSS visual system and other functional parameters (such as visual acuity, contrast sensitivity, and Ishihara score) also correlated with the EDSS visual system, so a decrease in functional parameters might be a poor prognostic marker of disability in patients with MS.###We previously reported an association between RNFL measurements and overall ability in patients with MS using the EDSS scale.13,25,26 We and others demonstrated that a reduction in theMSQOL-54 score is associated with an increase in the EDSS score, suggesting that subjects with better QOL are those with better functional ability.17–20,27,28 In the present study, QOL scores correlated with neuro-ophthalmologic results, suggesting an association between RNFL structure and functional ability in MS.
Analysis of MSQOL-54 subscales showed that patients with higher decrease of RNFL thickness have
Continued
Neurology 81 July 2, 2013 81
worse energy/fatigue, physical health, and EDSS of the visual system.###Another color test such as the Color Vision Recorder (Optical Diagnostics, Culemborg, Netherlands), Farnsworth D15 test, or Lanthony Desaturated 15- Hue test may be more useful for patients with MS.
OCT provides a noninvasive, objective, and reproducible method for measurement of RNFL and easy follow-up of axonal damage during MS disease progression.###The aim of the present study was to assess progressive damage according to RNFL results over a 3-year period and evaluate the association between RNFL changes and health-related QOL scores and disability in patients with MS.###Multiple sclerosis (MS) reduces quality of life (QOL).1 The Kurtzke Expanded Disability Status Scale (EDSS) has been criticized for its overemphasis on mobility and inadequate capture of all of the elements involved in the global impact of MS.2–4
QOL measures are alternative indicators of disease and focus more attention on the patient with MS as a whole and his or her physical problems.5 In relapsing-remitting MS, fatigue and depression levels are higher than in healthy individuals and are associated with QOL decrease.1,6,7
Long-term preservation of health-related QOL is a critical marker of therapeutic success in chronic debilitating diseases.###Some authors suggested that VEP magnitude reflects the degree of synaptic activity in striate cortex of the brain.24
We previously reported that OCT measurements are useful for predicting patients who will experience greater health repercussions from the MS in their lifetime, for determination if MS progression is under control, and for monitoring the treatment effectiveness in MS.23 We demonstrated that OCT results correlate with functional ability reduction and QOL in patients with MS, but we included all RNFL thicknesses (mean and quadrants).###DISCUSSION Information about the effect of axonal damage on functional disability and the resulting changes in QOL in these patients is crucial to optimal care for patients with MS. Retinal ganglion cell loss as evaluated using OCT and visual function as evaluated by VEP have demonstrated their utility as biomarkers of MS progression,9,13,21–23 but their utility as predictors of QOL in MS has not been evaluated.###All patients presented with relapsing-remitting MS. Antecedents of diplopia were present in 6 patients (11.1%), unilateral neuritis in 13 (23.2%), and bilateral neuritis in 2 patients (3.6###We previously reported that OCT measurements are useful for predicting patients who will experience greater health repercussions from the MS in their lifetime, for determination if MS progression is under control, and for monitoring the treatment effectiveness in MS.(23) We demonstrated that OCT results correlate with functional ability reduction and QOL in patients with MS, but we included all RNFL thicknesses (mean and quadrants).###Longer studies with a larger sample size might be useful for examining the association between RNFL measurements and QOL changes in MS.",impact-revealing,highlighting the association between RNFL measurements and quality of life in MS patients
1126,,abca18159e0836406a648414cd4275715f3cc12e,Fast Sampling via Discrete Non-Markov Diffusion Models,,,"###In this context, DNDM can be considered as the discrete counterpart of DDIM (See Table 1).###…state spaces, continuous-time processes have been proposed to accommodate algorithms that offer faster sampling speeds and enhanced sample quality (Jolicoeur-Martineau et al., 2021; Zhang and Chen, 2022; Salimans and Ho, 2022; Chung et al., 2022; Song et al., 2020b; Dockhorn et al., 2021).###…probabilistic model (Sohl-Dickstein et al., 2015; Ho et al., 2020) and score matching with Langevin dynamics (Song and Ermon, 2019) are unified by Song et al. (2020b) through introducing the SDE framework for SGM. Based on it, subsequent works (Dockhorn et al., 2021; Nachmani et al., 2021;…###…decoding strategies, including implicit dynamics (Song et al., 2020a), analytical processes (Bao et al., 2022), or differential equation solvers (Song et al., 2020b; Liu et al • We propose discrete non-Markov diffusion models (DNDM), which preserve two important properties of the original…###The derivatives are inspired by the reasoning in DDIM (Song et al., 2020a).###Once trained, the model can generate samples using various decoding strategies, including implicit dynamics (Song et al., 2020a), analytical processes (Bao et al., 2022), or differential equation solvers (Song et al., 2020b; Liu et al • We propose discrete non-Markov diffusion models (DNDM), which…###DDIM is non-Markov and admits a de-randomized, faster sampling algorithm compared to DDPM.",impact-revealing,discussing advancements in sampling algorithms and model properties
2501,5ea2b8c391e01167f5a89e2d,38643c2926b10f6f74f122a7037e2cd20d77c0f1,Supervised Contrastive Learning,573696136e3b12023e5258e2,Colorful Image Colorization,"In the image domain, pixelpredictive approaches have also been used to learn embeddings [9, 61, 62, 37].",other,acknowledge existing methods in image domain
3186,5b1642d68fbcbf6e5a9b7e77,0be19fd9896e5d40222c690cc3ff553adc7c0e27,Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods,5736973c6e3b12023e62b9e5,Certifying and removing disparate impact,"Various other approaches have been proposed to produce “fair” classifiers (Calders et al., 2009; Feldman et al., 2015; Misra et al., 2016).###Various other approaches have been proposed to produce “fair” classiﬁers (Calders et al., 2009; Feldman et al., 2015; Misra et al., 2016).",other,acknowledge existing approaches to fair classifiers
3916,5bdc315017c44a1f58a05a1d,f9fb0ad85f903e3ecf5d4bafdf7c74f647e75f58,LEMNA: Explaining Deep Learning based Security Applications,5550467945ce0a409eb5eb5f,DREBIN: Effective and Explainable Detection of Android Malware in Your Pocket.,"For example, researchers have trained MLP to detect malware at the binary code level [48] and classify Android malware [2, 21].###So far, researchers have successfully applied deep neural networks to train classifiers for malware classification [2, 16, 21, 48, 68], binary reverse-engineering [15, 52, 71] and network intrusion detection [24, 62], which all achieved an exceptionally high accuracy.",other,highlighting successful applications of deep neural networks in malware classification and related tasks
2301,5c8ddce94895d9cbc6a97820,8fb5a6fe93a3d19ffee50677c0ae563e3377d2d4,Chinese NER Using Lattice LSTM,53e9aab0b7602d9703428f00,The Third International Chinese Language Processing Bakeoff: Word Segmentation and Named Entity Recognition,", 2011), MSRA (Levow, 2006) Weibo NER (Peng and",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1124,,da3867b60a4245fb3c67d5c5b41905dfdd9cc53e,Contextualized Diffusion Models for Text-Guided Image and Video Generation,,,"###DDIMs (Song et al., 2020) accelerate the reverse process of pretrained DDPMs, which are also faced with the inconsistency problem that exists in DDPMs.###4 (Rombach et al., 2022) as the base text-to-image diffusion model, and fuse the attention maps in DDIM inversion (Song et al., 2020) and sampling processes for retaining both structural and motion information.###For example, Tune-A-Video (Wu et al., 2022) employs DDIM (Song et al., 2020) inversion to provide structural guidance for sampling, and pro-2 A red rose. poses efficient attention tuning for improving temporal consistency.###Text-to-video diffusion models mainly build upon pretrained DDPMs, and extend them with designed temporal modules ( e.g. , spatio-temporal attention) and DDIM Song et al. (2020) inversion for both temporal and structural consistency (Wu et al., 2022; Qi et al., 2023).",impact-revealing,reporting on advancements in text-to-video diffusion models and their methodologies
761,5d79a6ff3a55ac5b650357fb,6fb4facc2d16c76047f7cd96af5a691ab16c08c5,Combining Prefetch Control and Cache Partitioning to Improve Multicore Performance,53e9a806b7602d9703145b0f,Making data prefetch smarter: adaptive prefetching on POWER7,[3] shows a 50:1 of “execution epoch vs sampling period” is a reasonable choice.###[3] proposed an adaptive prefetch control to independently adjust each core’s prefetch aggressiveness on,impact-revealing,reporting prior findings on execution epoch and sampling period
722,5de4e0b73a55ac2224ba53a6,a75649771901a4881b44c0ceafa469fcc6e6f968,how can we know what language models know?,573696136e3b12023e525abe,Improving Neural Machine Translation Models with Monolingual Data,"method of using back-translation (Sennrich et al., 2016; Mallinson et al., 2017) to first translate the initial prompt into B candidates in another language, each of which is then back-translated into B candidates in the original language.###…methods could be used for paraphrasing (Romano et al., 2006; Bhagat and Ravichandran, 2008), we follow the simple method of using back-translation (Sennrich et al., 2016; Mallinson et al., 2017) to first translate the initial prompt into B candidates in another language, each of which is then…",impact-revealing,describing a method for paraphrasing using back-translation
3209,5efb0d5691e011063336d39c,0cee58946a13a5c2845647b4af8b9d2bf52a8b6b,BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision,5a9cb66717c44a376ffb8ac1,Deep contextualized word representations.,"To address the challenges in learning from distant supervision, our approach leverages the power of pre-trained language models (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLnet (Yang et al., 2019)) which are particularly attractive to this task due to the following merits: First , they are very large neural networks trained with huge amounts of unlabeled data in a completely unsupervised manner , which can be cheaply obtained; Second , due to their massive sizes (usually having hundreds of millions or billions of parameters), they have strong expressive power to capture general semantics and syntactic information e ﬀ ectively.###, ELMo (Peters et al., 2018), BERT (Devlin et al.###To address the challenges in learning from distant supervision, our approach leverages the power of pre-trained language models (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLnet (Yang et al., 2019)) which are particularly attractive to this task due to the following merits: First…",other,highlighting the advantages of pre-trained language models for distant supervision
853,5aed146117c44a4438152803,5f0da3cedda449b72fe36fa78798651a038f515c,MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects,53e99f8db7602d97028648c7,DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning,"Binary trees are well-suited for performing reductions and have been used in prior DNN implementations [3, 4, 17, 19, 21] to implement adder trees within the PE clusters described in Section 2.###Diannao [3] and Shidiannao [44] relied on mesh-based interconnects for data transfer.###Diannao [3], DaDiannao [15], ShiDiannao [44] are early spatial DNN accelerators.",impact-revealing,acknowledge prior implementations of binary trees in DNNs
560,5ce3af9aced107d4c65f6b25,f2bb7e2f5a1afad5370159c15760c44df93c0438,Very Deep Self-Attention Networks for End-to-End Speech Recognition,5bbacb4c17c44aecc4eac691,Speech-Transformer: A No-Recurrence Sequence-To-Sequence Model For Speech Recognition,"ith unsatisfactory results. [6] found that self-attention in the encoder (acoustic model) was not effective, but combined with an LSTM brought marginal improvement and greater interpretability, while [9] did not ﬁnd any notable improvement using the Transformer in which the encoder combines self-attention with convolution/LSTM compared to other model architectures. In this work, we show that the Tran###ention with LSTMs, while [30] uses self-attention as an alternative in CTC models. A variation of the Transformer has been applied to ASR with additional TDNN layers to downsample the acoustic signal [9]. Though self-attention has provided various beneﬁts such as training speed or model interpretability, previous works have not been able to point out any enhancement in terms of performance. Our work ",impact-revealing,highlighting the limitations of self-attention in acoustic models
833,53e9bc6eb7602d97048eba78,6c06f0023fce703f948ae41668875d2f7ad35ea6,data cache prefetching using a global history buffer,53e99b30b7602d97023cdea7,Prefetching using Markov predictors,"as in Markov prefetching [6] (specifics are described below).###Markov Prefetching [6] is an example of a correlation prefetching method.###Although they are simple, conventional table-based methods [4,6,7,9,10,11,12,14] are relatively inefficient, in that they reserve a fixed amount of history space per prefetch key.###As with most recent research on data prefetching [6,7,10,11,12], we focus on the lowest level data cache (in our case the L2) because modern out-of-order processors can tolerate most L1 data cache misses with relatively little performance degradation.",impact-revealing,providing context on data prefetching methods
2718,5ee8986891e011e66831c3b6,38f93092ece8eee9771e61c1edaf11b1293cae1b,Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,53e99e8cb7602d97027565e7,Slow feature analysis: unsupervised learning of invariances.,"Learning good image representations is a key challenge in computer vision [1, 2, 3] as it allows for efficient training on downstream tasks [4, 5, 6, 7].",other,highlighting the importance of learning good image representations in computer vision
1581,,45bbcf2d0bbf799c95edade59bec2671cb927860,Application of the path-repairing technique and virus optimization algorithm for the dimensional synthesis of four-bar mechanisms,,,"###Currently, more and more new metaheuristics are being proposed in the literature; these are inspired by various natural phenomena, such as bacterial foraging [5], dolphin echolocation [6], forest spreading [7], plant propagation [8], river formation dynamics [9] and the water cycle [10].",impact-revealing,acknowledging the trend of new metaheuristics inspired by natural phenomena
706,5b1643998fbcbf6e5a9bc32d,2fe2cfd98e232f1396f01881853ed6b3d5e37d65,Taskonomy: Disentangling Task Transfer Learning,57a4e91dac44365e35c98c54,Cross-Stitch Networks for Multi-task Learning,"Multi-task learning has experienced recent progress and the reported advantages are another support for existence of a useful structure among tasks [93, 100, 50, 76, 73, 50, 18, 97, 61, 11, 66].###…years of modern computer science, e.g. with Turing arguing for using learning elements [95, 98] rather than the ﬁnal outcome or Jean Piaget’s works on developmental stages using previously learned stages as sources [74, 39, 38], and have extended to recent works [76, 73, 50, 18, 97, 61, 11, 66].",impact-revealing,highlighting the progress and significance of multi-task learning
1628,,f808d39c0620f276fa8cb542c109c1cfa339dc56,An observation-based model for fault localization,,,"###In contrast to most approaches to software fault diagnosis, which present diagnosis candidates as single explanations [2, 10, 12], our approach also contains multiple fault explanations in the diagnostic ranking (typical of model-based approaches [13, 17]).###This represents an improvement over Ochiai of about 4% (6 versions), 6% over Taran-tula (8 versions), 10% over Sober and 13% over CT.###Currently, we study a model-based diagnosis approach that uses dynamic information to extract a model of program behavior [1], which is inspired by spectrum-based software localization (SFL) approaches [2, 10, 11, 12], which are based on analyzing run-time traces of component activity.###The values for Sober [12] (a similar technique is presented in [11], which is not included in our comparison because Sober consistently outperforms it [12]), Tarantula [10], and Ochiai [2] were obtained by running them in our own environment, those for Cause Transitions (CT) are, however, directly cited from [3].###The values for Sober [12] (a similar technique is presented in [11], which is not included in our comparison because Sober consistently outperforms it [12]), Taran-tula [10], and Ochiai [2] were obtained by running them in our own environment, those for Cause Transitions (CT) are, however, directly cited from [3].###For compatibility with previous work in fault localization, we use the effort/score metric [2, 12] which is the percentage of statements that need to be inspected to find the fault in other words, the rank position of the faulty statement divided by the total number of statements.",impact-revealing,highlighting the improvement of the proposed approach over existing methods in software fault diagnosis
1463,,b8f5a7b4b4d746135053872ec2f46bd210fbc21a,"What Is Word Meaning, Really? (And How Can Distributional Models Help Us Describe It?)",,,"###The first builds on research on the human concept representation that has shown that concepts in the human mind do not work like sets with clear-cut boundaries; they show graded membership, and there are typical members as well as borderline cases (Rosch, 1975; Hampton, 2007).###The ﬁrst builds on research on the human concept representation that has shown that concepts in the human mind do not work like sets with clear-cut boundaries; they show graded membership, and there are typical members as well as borderline cases (Rosch, 1975; Hamp-ton, 2007).###Some items are perceived as more typical than others (Rosch, 1975; Rosch and Mervis, 1975).",impact-revealing,highlighting the complexity of human concept representation
1065,,ef41f926bd51a9cc1f3ad0791708f432964cecea,Beyond the R&D effects on innovation: the contribution of non-R&D activities to TFP growth in the EU,,,"###…CDM model has been frequently applied by scholars using data from the Community Innovation Survey (CIS) launched by Eurostat, such as Lööf and Heshmati (2003) for Norway, Finland and Sweden, Janz et al. (2004) for Germany and Sweden, or Gri¢ th et al. (2006) for France, Germany, Spain and the UK.###The CDM model has been frequently applied by scholars using data from the Community Innovation Survey (CIS) launched by Eurostat, such as Lööf and Heshmati (2003) for Norway, Finland and Sweden, Janz et al. (2004) for Germany and Sweden, or Gri¢ th et al. (2006) for France, Germany, Spain and the UK.",impact-revealing,acknowledge the application of the CDM model in various studies
3750,53e9bafbb7602d9704734d1d,99a56abcea69a05eaa8effdfe1be01c283b08f4a,branch history matching: branch predictor warmup for sampled simulation,53e99e3eb7602d970270460f,Picking Statistically Valid and Early Simulation Points,"Note this is in the range of sampling units used in contemporary sampled simulation environments such as SMARTS [3,9] (sampling unit size of 10K instructions) and SimPoint [2,5,21] (sampling unit sizes from 1M to 100M instructions).###Various authors have proposed various approaches for achieving this, such as random sampling [1], periodic sampling as done in SMARTS [3] and targeted sampling based on program phase behavior as done in SimPoint [2].",other,acknowledge existing approaches in simulation environments
2668,599c797a601a182cd2642797,668db48c6a79826456341680ee1175dfc4cced71,Get To The Point: Summarization with Pointer-Generator Networks,5550410f45ce0a409eb384f8,Sequence to Sequence Learning with Neural Networks.,"to-sequence models (Sutskever et al., 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al.###However, the recent success of sequence-to-sequence models (Sutskever et al., 2014), in which recurrent neural networks (RNNs) both read and freely generate text, has made abstractive summarization viable (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015; Zeng et al., 2016).",other,highlighting the impact of sequence-to-sequence models on abstractive summarization
2827,5f993ec291e011a3fbe2fb5c,f1e5e65941617604923225cc4bf464e370fcae67,Combining Label Propagation and Simple Models Out-performs Graph Neural Networks,5d9edc8347c8f76646042a37,Simplifying Graph Convolutional Networks,"Our framework also complements the Simplified Graph Convolution (Wu et al., 2019), as well as algorithms designed to increase scalability (Bojchevski et al.###Our framework also complements the Simpliﬁed Graph Convolution (Wu et al., 2019), as well as algorithms designed to increase scalability (Bojchevski et al., 2020; Zeng et al., 2019; Rossi et al., 2020).###Instead, we hypothesize that GCNs gain performance by having smoothed outputs over the graph, a similar observation made by Wu et al. (2019).",other,highlighting the complementarity of the proposed framework with existing algorithms
3004,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,5550460645ce0a409eb5b6cf,Transfer Joint Matching for Unsupervised Domain Adaptation,"Ad-versarial penalty can be added to the loss function to make models learn domain-invariant feature only (Fernando et al., 2015; Long et al., 2014; Ming Harry Hsu et al., 2015).",other,reporting prior findings on adversarial penalties in loss functions
2240,5eccb534e06a4c1b26a83514,a9682a89b2fef793507c365a577f1521745db96c,Boosting the Transferability of Adversarial Samples via Attention,58d82fcbd649053542fd64c5,Adversarial Machine Learning at Scale.,"When it comes to the defended models, we adopt multiple state-of-the-art adversarially trained models as remote targets [37, 19], since adversarial training is arguably the most promising and effective defense to date [22].",other,highlighting the effectiveness of adversarial training as a defense method
3822,5d3ed25a275ded87f97deae1,f5252075bb34666863cd01cc82c2d941d4ffe6c6,robust graph convolutional networks against adversarial attacks,5c8d279f4895d9cbc6403a3b,Attention-based Graph Neural Network for Semi-supervised Learning,"Further improvements include adding an attention mechanism to assign different weights in aggregating node neighborhoods [28, 30, 31, 34], adding residual and jumping connections [32], sampling to improve efficiency [4, 5, 14, 33] considering edge attributes [15, 23, 26], disentangling node representations [20] and automatically selecting hyper-parameters [29].",other,acknowledge potential improvements in model architecture
1222,,de2462a72c2e9cafffbc4b605dbcb52ec9646305,Personal dietary assessment using mobile devices,,,"###INTRODUCTION Dietary intake provides valuable insights for mounting intervention programs for prevention of disease [1, 2].",impact-revealing,highlighting the importance of dietary intake for disease prevention
2851,5ebe685391e0117693a52241,99314a532a3358cb86064fc8917ed2c283227539,NAT: Noise-Aware Training for Robust Neural Sequence Labeling,5a73cbcc17c44a0b3035f3ea,Black-Box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers,"In a white-box attack scenario (Goodfellow et al., 2015; Ebrahimi et al., 2018) we assume that the attacker has access to the model parameters, in contrast to the black-box scenario (Alzantot et al., 2018; Gao et al., 2018), where the attacker can only sample model predictions on given examples.",other,providing context on attack scenarios in machine learning
1608,,ed34ef02f3bd178283350d75809fae57014aba96,The Performance Parameters Of Wireless Sensor Networks In Underground Mines,,,"###Since the radiation pattern of omnidirectional antennas is not suitable for vertical communication, the use of one omnidirectional antenna is not suitable for such a structure [11].",impact-revealing,highlighting the limitations of omnidirectional antennas for vertical communication
852,5aed14e217c44a4438159cc1,98eab599dd9cc25823569ab02537970994ce855a,a taxonomy of task-based parallel programming technologies for high-performance computing,53e9b8d4b7602d97044b5f0c,Scheduling multithreaded computations by work stealing,Work stealing [4] can be considered the most widely used load balancing technique in task-based runtime systems.,impact-revealing,providing context on load balancing techniques
3116,5e539eca3a55ac4db70a53f1,c529f5b08675f787cdcc094ee495239592339f82,learning to simulate complex physics with graph networks,56d8a00adabfae2eee5defc1,Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances,"We also investigated distributional metrics including optimal transport (OT) (Villani, 2003) (approximated by the Sinkhorn Algorithm (Cuturi, 2013)), and Maximum Mean Discrep-ancy (MMD) (Gretton et al., 2012).###…measuring differences between the distributions of particles: optimal transport (OT) (Villani, 2003) using 2D or 3D Wasserstein distance and approximated by the Sinkhorn Algorithm (Cuturi, 2013), and maximum mean discrepancy (MMD) (Gretton et al., 2012) with a Gaussian kernel bandwidth of σ = 0 .###So we also explored two metrics that are invariant under particle permutations, by measuring differences between the distributions of particles: optimal transport (OT) (Villani, 2003) using 2D or 3D Wasserstein distance and approximated by the Sinkhorn Algorithm (Cuturi, 2013), and maximum mean discrepancy (MMD) (Gretton et al., 2012) with a Gaussian kernel bandwidth of σ = 0 .",other,acknowledge existing distributional metrics in the context of measuring differences
347,5c5c55bfe1cd8e03e71689a9,5babbf2ed9f6e36b83ed246927b270db320fe866,A Simple Convolutional Generative Network for Next Item Recommendation,5a9cb60d17c44a376ffb3c6d,Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding.,"(3) ( see [20, 25, 29, 30]).###Inspired by the successful use of CNNs in image tasks, a newly proposed sequential recommender, referred to as Caser [29], abandoned RNN structures, proposing instead a convolutional sequence embedding model, and demonstrated that this CNN-based recommender is able to achieve comparable or superior…",impact-revealing,drawing inspiration from successful CNN-based recommender models
61,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,5a260c2817c44a4ba8a23746,Character-based Bidirectional LSTM-CRF with words and characters for Japanese Named Entity Recognition.,"Many recent sequence labeling frameworks (Ma and Hovy, 2016b; Misawa et al., 2017) share a very basic structure: a bidirectional LSTM network followed by a CRF tagging layer (i.e. BLSTM-CRF).",impact-revealing,acknowledge existing sequence labeling frameworks
2575,5ec49a639fced0a24b4de7ed,1eed0659d561354d5c471a723cf3381430561d04,Graph Neural News Recommendation with Unsupervised Preference Disentanglement,599c7950601a182cd262ecb7,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,"DeepFM (Guo et al., 2017), a general model that combines factorization machines and deep neural networks that share the input.",other,describing a specific model architecture
542,5fe4094e9e795e14f30e634a,94497472eecb7530a2b75c564548c540ebd61e9b,Learning to Pre-train Graph Neural Networks,5e5e18eb93d709897ce3ce41,Strategies for Pre-training Graph Neural Networks,"This ﬁnding conﬁrms previous observations (Hu et al. 2020; Rosenstein et al. 2005) that negative transfer results in limitations on the applicability and reliability of pre-trained models.###More recently, Hu et al. (Hu et al. 2020) propose different strategies to pre-train graph neural networks at both node and graph levels, although labeled data are required at the graph level.###This finding confirms previous observations (Hu et al. 2020; Rosenstein et al. 2005) that negative transfer results in limitations on the applicability and reliability of pre-trained models.###We split the downstream data with species split (Hu et al. 2020), and evaluate the test performance with average ROC-AUC (Bradley 1997) across the 40 tasks.###(3) We also notice that some baselines give surprisingly limited performance gain and yield negative transfer (Rosenstein et al. 2005) on the downstream task (i.e., EdgePred and AttrMasking strategies w.r.t. the GAT model).###(Hu et al. 2020) propose different strategies to pre-train graph neural networks at both node and graph levels, although labeled data are required at the graph level.###2020) to explore graph structures and (4) Attribute Masking strategy (denoted by AttrMasking) (Hu et al. 2020) to learn the regularities of the node and edge attributes distributed over graphs.###Existing methods either only take into account the node-level pre-training (Navarin, Tran, and Sperduti 2018; Hu et al. 2019), or still require supervised information for graph-level pre-training (Hu et al. 2020).###For biology data, as in (Hu et al. 2020), we use 306,925 unlabeled protein ego-networks for pre-training.###We conduct experiments on data from two domains: biological function prediction in biology (Hu et al. 2020) and research ﬁeld prediction in bibliography.###The primary goal of pre-training GNNs (Navarin, Tran, and Sperduti 2018; Hu et al. 2019, 2020) is to learn transferable prior knowledge from mostly unlabeled data, which can be generalized to downstream tasks with a quick ﬁne-tuning step.###We conduct experiments on data from two domains: biological function prediction in biology (Hu et al. 2020) and research field prediction in bibliography.###2019) to maximize local mutual information across the graph’s patch representations; (3) Context Prediction strategy (denoted by ContextPred) (Hu et al. 2020) to explore graph structures and (4) Attribute Masking strategy (denoted by AttrMasking) (Hu et al.###The goal of pre-training GNNs is to learn a generic initialization for model parameters using readily available graph structures (Hu et al. 2020, 2019).###Each subgraph is centered at a paper and contains the associated information of For biology data, as in (Hu et al. 2020), we use 306,925 unlabeled protein ego-networks for pre-training.###…the graph’s patch representations; (3) Context Prediction strategy (denoted by ContextPred) (Hu et al. 2020) to explore graph structures and (4) Attribute Masking strategy (denoted by AttrMasking) (Hu et al. 2020) to learn the regularities of the node and edge attributes distributed over graphs.###…et al. 2019) to maximize local mutual information across the graph’s patch representations; (3) Context Prediction strategy (denoted by ContextPred) (Hu et al. 2020) to explore graph structures and (4) Attribute Masking strategy (denoted by AttrMasking) (Hu et al. 2020) to learn the regularities…###2019), or still require supervised information for graph-level pretraining (Hu et al. 2020).###To contextualize the empirical results of L2P-GNN on the pre-training benchmarks, we compare against four self-supervised or unsupervised baselines: (1) the original Edge Prediction (denoted by EdgePred) (Hamilton, Ying, and Leskovec 2017) to predict the connectivity of node pairs; (2) Deep Graph Infomax (denoted by DGI) (Velickovic et al. 2019) to maximize local mutual information across the graph’s patch representations; (3) Context Prediction strategy (denoted by ContextPred) (Hu et al. 2020) to explore graph structures and (4) Attribute Masking strategy (denoted by AttrMasking) (Hu et al. 2020) to learn the regularities of the node and edge attributes distributed over graphs.",impact-revealing,highlighting the limitations and challenges in pre-trained models and their applicability
2160,,389bffbab73443815500e3eb14c3f8ed92e22054,Time Series Classification with Recurrent Neural Networks,,,"###It conceptually builds on the bag-of-patterns (BOP) [13,12] model and the WEASEL [15]###Networks, where interacting elements are denoted as nodes and interactions are denoted as edges, are a fundamental tool to study complex systems [1, 15], including social, communication, biology and economics networks.",impact-revealing,describing the foundational concepts of a model used in complex systems
3550,5c757416f56def979890a549,0ded7a6f60e160f6d6baed1dddc39371797f8fef,network-based prediction of protein interactions,53e9ba0bb7602d970460b955,Link Prediction in Complex Networks: A Survey,"[12] Lü L & Zhou T, Link prediction in complex networks: A survey, Physica A 390 1150-1170 (2011).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
486,573698456e3b12023e70ee1b,524664475292ad6cdbdda3992fe5dc8f036b6ce5,Deep learning for emotion recognition on small datasets using transfer learning,53e99b63b7602d970240a38e,Return of the Devil in the Details: Delving Deep into Convolutional Nets.,"01 [2, 12], so as not to drastically alter the pre-trained weights.###The CNN-M-2048 model from [2] (VGG-CNN-M-2048), which is a variant of the model introduced in [28].###Our approach to tackling this problem follows recent works [2, 8, 9, 26], which consistently show that supervised fine-tuning with a relatively small dataset on a network pre-trained with a large image dataset of generic objects (e.###2 Architectures The success of CNN for face emotion recognition motivated us to base our models on two representative CNN architectures, which we chose because of their nice tradeoffs between speed and accuracy [2]: 1.",impact-revealing,acknowledge existing CNN architectures for face emotion recognition
3296,5bdc315017c44a1f58a05e13,1717255b6aea01fe956cef998abbc3c399b5d7cf,AMC: AutoML for Model Compression and Acceleration on Mobile Devices,5a260c8617c44a4ba8a324e9,Practical Network Blocks Design with Q-Learning.,"Following [57, 4, 54], we use the following reward:###Following Block-QNN [54], which applies a variant form of Bellman’s Equation [50], each transition in an episode is ( s t , a t , R, s t +1 ), where R is the reward after the network is compressed.###Following Block-QNN [54], which applies a variant form of Bellman’s Equation [50], each transition in an episode is (st, at, R, st+1), where R is the reward after the network is compressed.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2775,5d3c234c3a55acd386d4e112,a9ec03dbe702f6909acd1f1f14a3395d0141043b,generative models for graph-based protein design,5a260c8117c44a4ba8a30f54,Graph Attention Networks.,"Our approach is similar to Graph Attention Networks [38], but augmented with edge features and an autoregressive decoder.",other,comparing the proposed approach to existing methods
2043,,c6edf03db0dfe369661f1ebb8f06ebda3ec932df,Graph-based optimal multi-surface segmentation with a star-shaped prior: Application to the segmentation of the optic disc and cup,,,"###[3, 4] developed a novel method for simultaneously segmenting multiple interacting surfaces, the geometric constraints are enforced in the resampled image, which may require interpolation of the input image, potentially introducing artifacts.",impact-revealing,reporting prior findings on a novel method for segmentation
1262,,c773fffc9334c8de4e77e56c494ef8182e37a8cc,Differences by Race in the Health Status of Rural Cognitively Impaired Arkansans,,,"###Using data on persons 80 years of age and older from the Supplement on Aging to the National Health Interview Study, Lee (1998) reported that rural men were in worse health than urban men in terms of self-rated health and in difficulties performing ADL tasks.###Previous research has emphasized the importance of controlling for other characteristics (e.g., private insurance coverage; dependency on social security; informal assistance from family members) (Ferraro & Feller, 1996; Herzog & Wallace, 1998; Lee, 1998; Mutchler & Burr, 1991).###Lee (1998) further called for future research on the apparent disadvantages of rural men by performing multivariate analyses that controls for the socioeconomic status among persons who are residing in rural areas (Lee, 1998).###The findings underscore the appeals of other studies for increased health intervention toward rural cognitively impaired African American men (Lee, 1998; Mitchell et al., 1997).###, private insurance coverage; dependency on social security; informal assistance from family members) (Ferraro & Feller, 1996; Herzog & Wallace, 1998; Lee, 1998; Mutchler & Burr, 1991).",impact-revealing,highlighting the health disparities between rural and urban men and calling for further research
3075,5d9ed2d847c8f76646f797b7,c3715947bbbf648dcf29a1aa4b35cfb68044f919,GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification,5a260c8417c44a4ba8a317a5,Natural Language Inference over Interaction Space.,"Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM.###Hanselowski et al. (2018) modify the ESIM 2 https://www.mediawiki.org/wiki/API: Main_page model to compute the relevance score between the evidence and the claim.###In the document retrieval and sentence selection stages, we simply follow the method from Hanselowski et al. (2018) since their method has the highest score on evidence recall in the former FEVER shared task.###In the document retrieval step, we adopt the entity linking approach from Hanselowski et al. (2018).###In addition to the original model (Hanselowski et al., 2018), we add a relevance score ﬁlter with a threshold τ .###After running the same model proposed by Hanselowski et al. (2018), we ﬁnd our OFEVER score is slightly lower, which may due to the random factors.###Nie et al. (2019); Yoneda et al. (2018) and Hanselowski et al. (2018) have achieved the top three results among 23 teams.###The Athene UKP TU Darmstadt team (Athene) (Hanselowski et al., 2018) combines ﬁve inference vectors from the ESIM model via attention mechanism to make the ﬁnal prediction.",other,acknowledge the effectiveness of existing models in evidence and claim relevance
2409,5f86cae991e011dbc7eba2fa,5e9bb0f3e74be4d8f75cca6ceb3ec87b3e04d7cc,piuma: programmable integrated unified memory architecture,58d83045d649053542fe8543,Graphicionado: A high-performance and energy-efficient accelerator for graph analytics.,"Graphicionado [12] is a graph analysis accelerator, implementing a vertex-centric compute paradigm.",other,providing context about a specific tool
1003,,01782eaf42fee661ffaf12edfe84a14bfa12c08b,Matching for Peer Support: Exploring Algorithmic Matching for Online Mental Health Communities,,,"###Peer support through online communities is able to fill gaps in accessing health services for many, helping those whose needs are unmet by traditional resources [49, 83, 96, 107] or individuals who lack adequate peer networks in their daily lives to achieve needed support [64, 67, 106, 117].",impact-revealing,highlighting the role of peer support in addressing health service gaps
2365,5736986b6e3b12023e730129,424561d8585ff8ebce7d5d07de8dbf7aae5e7270,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,573695ff6e3b12023e5133cb,Object-Proposal Evaluation Protocol is _x0091_Gameable_x0092_,"[19], [20], [21] related to the ultimate detection accuracy.###Comprehensive surveys and comparisons of object proposal methods can be found in [19], [20], [21].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1769,,acf1577fc71faabfb7d78c057182c9b5d33546f4,"Ecological Partitioning of Cercopithecus campbelli, C. petaurista, and C. diana in the Taï Forest",,,"###, 1983) as well as more recent studies (Ben-David et al., 1995; Fedriani et al., 1999; Ganzhorn, 1988; Kruuk et al., 1993; Vasey, 2000) have provided important data on community organization and species coexistence in several primate and other mammal communities.###Nevertheless, early ecological partitioning studies (Emmons et al., 1983) as well as more recent studies (Ben-David et al., 1995; Fedriani et al., 1999; Ganzhorn, 1988; Kruuk et al., 1993; Vasey, 2000) have provided important data on community organization and species coexistence in several primate…",impact-revealing,acknowledging prior findings on community organization and species coexistence
553,5cede0eeda562983788cd285,e4d99f390901df5caac0b587ff685f9cde100342,end-to-end speech translation with knowledge distillation,599c7987601a182cd2648373,Attention Is All You Need.,"Other parts are the same with Transformer model.###In this section, we first describe the core architecture of Transformer and then show how this model is applied to ASR/ST and MT task.###This sequence is finally treated as the input into Transformer model.###Transformer is an encoder-decoder architecture which entirely relies on self-attention mechanism including scaled dot-product attention and multi-head attention.###End-to-end model has already become a dominant paradigm in machine translation task, which adopts an encoder-decoder architecture and generates target words from left to right at each step [1, 3, 5].###Conventional speech translation system is a pipeline of two main components: an automatic speech recognition (ASR) model which provides transcripts of source language utterances, and a text machine translation (MT) model which translates the transcripts to target language [1, 2, 3, 4, 5].###It can be seen that Transformer model significantly outperforms in both ASR and MT tasks, with 0.92 WER reduction and 4.1 BLEU scores improvement in beam search compared to [10].###The model architecture is similar with Transformer [5], which is the state-of-art model in MT task.###We contribute it to the superior performance of Transformer model which is good at modeling long distance in sequence-to-sequence tasks, especially for MT
1https://www.ted.com 2http://ffmpeg.org 3https://www.statmt.org/moses/
tasks.###We also use Transformer to train a baseline MT model, as shown in the right part of Figure 1.",impact-revealing,describing the architecture and performance of the Transformer model in ASR and MT tasks
3868,5ddcf7f53a55ac1c5e8cce13,1f2577071ca2aa1086f4b1c12cd911061aeea960,meta-learning of neural architectures for few-shot learning,5b67b4b917c44aac1c867e30,Auto-Meta: Automated Gradient Based Meta Learner Search.,"This is in contrast to prior work that applied NAS to multi-task or few-shot learning, where a single neural architecture is optimized to work well on average across all tasks [24, 38] (see Figure 1, middle).###We first compare against the architectures from the original REPTILE [36] paper and from AutoMeta [24] when training all models with the same meta-learning algorithm, namely REPTILE.###By incorporating a NAS algorithm directly into the metalearning algorithm, we can search for architectures with a single run of the meta-learning algorithm, while prior work [24] required full meta-learning of hundreds of proposed architectures during the architecture search process.###[24] wrap neural architecture search around meta-learning as illustrated in Figure 1 (middle).###Middle: applying NAS to metalearning such as AutoMeta [24].###Also prior work using NAS for meta-learning [24] searches for a single architecture that is then shared across all tasks.###Prior work [16, 36, 24] considered a fixed, predefined architecture αfixed and chose Φ k to be an optimizer like SGD for the weights: w = w = Φ(w,αfixed, D Ti train)",other,highlighting the differences in approach to neural architecture search in meta-learning
3664,5d0616bd8607575390f86730,635b079447ee46acc813a08305988e77dfe3d1dd,Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi,5550484e45ce0a409eb6d436,Feasibility and limits of wi-fi imaging.,"…extract various parameters of signals reflected or shadowed by human, including DFS [26, 32, 44], ToF [2–4, 21], AoA / AoD [2 , On the human side, existing model-based works only tracks coarse human motion status, such as location [4, 41], velocity [26, 32], gait [43, 49] and figure [2, 19].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
150,5d9edc8347c8f76646042a37,7e71eedb078181873a56f2adcfef9dddaeb95602,simplifying graph convolutional networks,5a9cb66717c44a376ffb8c0f,Deeper insights into graph convolutional networks for semi-supervised learning,"There are many other graph neural models (Monti et al., 2017; Duran & Niepert, 2017; Li et al., 2018); we refer to Zhou et al.",impact-revealing,acknowledge the existence of various graph neural models
2113,,e604f29bdf193ffc5a03f3c1481a006f987ad88b,Model Selection for Big Data: Algorithmic Stability and Bag of Little Bootstraps on GPUs,,,"###For this purpose, in this paper we exploit two recent theoretical results, namely Bag of Little Bootstraps (BLB) [7, 8] and Fully-empirical Algorithmic Stability (FAS) [9, 10, 11].###The Bag of Little Bootstraps (BLB) approach [8, 7] represents an alternative to FAS, which builds on the conventional Bootstrap procedure [18] and tested on the corresponding T , so to deﬁne the following model selection procedure:",impact-revealing,reporting theoretical results used in the study
2425,573698486e3b12023e711478,c2fd72cb2a77941e655b5d949d0d59b01e173c3b,grarep: learning graph representations with global structural information,5736977f6e3b12023e66632b,LINE: Large-scale Information Network Embedding,"[25] later proposed a large-scale information network embedding, which optimizes a loss function where both 1-step and 2-step relational information can be captured in the learning process.###LINE [25].###According to [25], LINE yielded better results when the learned graph representations are L2 normalized, while DeepWalk and E-SGNS can achieve optimal performance without normalization.###As suggested in [25], for LINE, we set the mini-batch size of stochastic gradient descent (SGD) as 1, learning rate of starting value as 0.025, the number of negative samples as 5, and the total number of samples as 10 billion.###For LINE, the reconstruction strategy does help, since it can capture additional structural information of the graph beyond 1-step and 2-step local information.###LINE is a recently proposed method for learning graph representations on large-scale information networks.###For LINE and GraRep, the boundaries of each group become much clearer, with vertices of different colors appearing in clearly distinguishable regions.###As suggested in [25], for LINE, we set the mini-batch size of stochastic gradient descent (SGD) as 1, learning rate of starting value as 0.###It is worth mentioning that GraRep and LINE can achieve good performance with a small graph.###LINE defines a loss function based on 1-step and 2-step relational information between vertices.###The results for GraRep appear to be better, with clearer boundaries for each regions as compared to LINE.###For LINE, we employ the reconstruction strategy proposed by their work by adding neighbors of neighbors as additional neighbors to improve performance.###Besides, for tasks with more labels, such as 9NG, GraRep and LINE can provide better performances than other methods, with GraRep providing the best.###LINE will get the best performance, if concatenating the representation of 1- step and 2-step relational information and tuning the threshold of maximum number of vertices.###As suggested in [25], we set k-max as 0, 200, 500 and 1000, respectively, and we report the best performance with kmax=500.###Another recently proposed work is LINE [25], which has a loss function to capture both 1-step and 2-step local relational information.###For a fair comparison, the dimension d of representations is set as 128 for Blogcatalog network and DBLP network as used in [25] and is set as 64 for 20-NewsGroup network as used in [29].###Due to limited space, we do not report the detailed results with d=128 and d=256 for all baseline methods, as well as k-max=500, 1000 and without reconstruction strategy for LINE, since these results are no better than the results presented in Table 3.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1088,,01e6af38326638b7413f84b60cf7abec1629cd3f,Changing the interface with minimal disruption: The roles of layout and labels,,,"###…multiple analyses with a cognitive engineering tool that estimates the amount of time required for perception, action, and memory retrieval (e.g., CPM-GOMS; John & Kieras, 1996), was that participants in the Memory-Test condition would perform the best, followed by Free-Access, and then GrayBox.###Their prediction, generated through multiple analyses with a cognitive engineering tool that estimates the amount of time required for perception, action, and memory retrieval (e.g., CPM-GOMS; John & Kieras, 1996), was that participants in the Memory-Test condition would perform the best, followed by Free-Access, and then GrayBox.###Such findings could not be predicted by extant theories of cognitive control such as GOMS (John & Kieras, 1996), standing for Goals, Operators, Methods, and Selection Rules, and Hierarchical Task Analysis (Annett & Duncan, 1967), which are based on the hierarchical decomposition of goals in a task.###Thus, in contrast to the vast body of literature in the field that has emphasized the importance of label information (e.g., Polson & Lewis, 1990) and goal structure (e.g., John & Kieras, 1996) in computer-based tasks, these findings reveal that users quickly learn to rely on layout information.###Furthermore, the manuals presented the tasks to participants in a hierarchical form (Appendix A), in accordance to the ideas of theories such as GOMS (John & Kieras, 1996).",impact-revealing,highlighting findings that challenge existing cognitive control theories
1830,,4b4f577c40dadb3e5971217639d2c37c9ee6a9eb,Bidirectional relationships between weight stigma and pediatric obesity: A systematic review and meta‐analysis,,,"###This scale was developed to assess the frequency and effect of teasing related to both competence and high weight status.(51) The weight subscale was most frequently used for assessing the perception of weight-based teasing in children, with acceptable internal consistency and convergent validity with related constructs.",impact-revealing,reporting on the development and validation of a teasing assessment scale
1424,,a334f9897a330abddf99cfec0b5a70f751e9497b,Conservative Safety Critics for Exploration,,,"###CQL (Kumar et al., 2020) is a method for offline/batch RL (Lange et al., 2012; Levine et al., 2020) that aims to learn a Q-function such that the expected value of a policy under the learned Q function lower bounds its true value, preventing over-estimation due to out-ofdistribution actions as a…###…Gc,T )[ Ea∼πφold ( πφold πφold′ − 1 )]−1 (26)
Here, Gc,T is a constant depending on the concentration properties of the safety constraint function C(s, a) and the state transition operator T (s′|s, a) (Kumar et al., 2020). φold′ denotes the parameters of the policy π in the iteration before φold.###If we follow Algorithm 1, during policy updates via Equation 5, the following is satisfied with high probability ≥ 1− ω V πφold C (μ) + 1 1− γ Es∼ρφold ,a∼πφ [AC(s, a)] ≤ χ+ ζ − ∆ 1− γ Here, ζ captures sampling error in the estimation of V πφold C (μ) and we have ζ ≤ C′ √ log(1/ω) |N | , where C ′ is a constant independent of ω obtained from union bounds and concentration inequalities (Kumar et al., 2020) and N is the number of samples used in the estimation of VC .###Here, the RHS is precisely the term in equation 2 of (Kumar et al., 2020) that is bounded by CQL.###…E(s,a)∼Denv [QC(s, a)] ) + 1
2 E(s,a,s′,c)∼Denv
[( QC(s, a)− B̂πφQ̂kC(s, a) )2] (2) Here, B̂πφ is the empirical Bellman operator discussed in section 3.1 and equation 2 of Kumar et al. (2020). α is a weight that varies the importance of the first term in equation 2, and controls the magnitude of…###We describe the problem setting of a constrained MDP (Altman, 1999) specific to our approach and the conservative Q learning (Kumar et al., 2020) framework that we build on in our algorithm.###Here, Gc,T is a constant depending on the concentration properties of the safety constraint function C(s, a) and the state transition operator T (s′|s, a) (Kumar et al., 2020).###CQL (Kumar et al., 2020) is a method for offline/batch RL (Lange et al.###…χ+ ζ −
∆
1− γ
Here, ζ captures sampling error in the estimation of V πφold C (µ) and we have ζ ≤
C′ √ log(1/ω)
|N | , where C ′ is a constant independent of ω obtained from union bounds and concentration inequalities (Kumar et al., 2020) and N is the number of samples used in the estimation of VC .###We train a conservative safety critic that overestimates the probability of catastrophic failure, building on tools in the recently proposed conservative Q-learning framework (Kumar et al., 2020) for offline RL.###To train such a critic QC , we incorporate tools from CQL to estimate QC through updates similar to those obtained by reversing the sign of α in Equation 2 of CQL(H) (Kumar et al., 2020).###To train such a critic QC , we incorporate theoretical insights from CQL, and estimate QC through updates similar to those obtained by flipping the sign of α in equation 2 of the CQL paper (Kumar et al., 2020).",impact-revealing,reporting on the CQL method for offline reinforcement learning
748,58d82fced649053542fd692f,29e944711a354c396fad71936f536e83025b6ce0,categorical reparameterization with gumbel-softmax,56d8a6d5dabfae2eee928e3b,Statistical Theory of Extreme Values and Some Practical Applications : A Series of Lectures,"The Gumbel-Max trick (Gumbel, 1954; Maddison et al., 2014) provides a simple and efficient way to draw samples z from a categorical distribution with class probabilities π:
z = one_hot ( arg max
i [gi + log πi]
) (1)
where g1...gk are i.i.d samples drawn from Gumbel(0, 1)1.",impact-revealing,providing context for the Gumbel-Max trick
984,,8a9988e90d34269104cb1bd784e6db1dd0c3130f,Blogs in a Changing Social Media Environment: Perspectives on the Future of Blogging in Scandinavia,,,"###They have been shown to be motivated by gratification (Sepp, Liljander, & Gummerus, 2011), and a willingness to experience and share their experiences (Guadagno et al., 2008).",impact-revealing,acknowledging prior findings on user motivation
1833,,96db9cc6763ddd00d983e829974b70a7a78f0bfa,Uniform Protection for Multi-exposed Targets,,,"###Our work is inspired by a successful strand of literature in protocol verification, where a protocol is translated into a set of first-order Horn clauses and resolution-based theorem proving is used to establish security properties [4,5], and by the flow logic approach to static analysis [6].",impact-revealing,highlighting the inspiration drawn from existing literature in protocol verification and static analysis
2833,5bdc31b417c44a1f58a0ba6c,62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9,How Powerful are Graph Neural Networks,573696026e3b12023e5160cd,Molecular graph convolutions: moving beyond fingerprints,"…schemes have been proposed (Scarselli et al., 2009b; Battaglia et al., 2016; Defferrard et al., 2016; Duvenaud et al., 2015; Hamilton et al., 2017a; Kearnes et al., 2016; Kipf & Welling, 2017; Li et al., 2016; Velickovic et al., 2018; Santoro et al., 2017; Xu et al., 2018; Santoro et al., 2018;…",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2358,5bdc315017c44a1f58a05a1d,f9fb0ad85f903e3ecf5d4bafdf7c74f647e75f58,LEMNA: Explaining Deep Learning based Security Applications,573697826e3b12023e66923d,Devnet: A Deep Event Network For Multimedia Event Detection And Evidence Recounting,"For example, the back-propagation methods including “saliency map” [3, 18, 54, 57] and activation difference propagation [53] require special operations on the convolutional layers and pooling layers of CNN, which do not exist in RNN or MLP 1.###For image classifiers, the basic method is to compute a feature “saliency map” using the gradients of output with respect to the input pixels in images [54, 57] or video frames [18].",other,providing context on back-propagation methods in neural networks
283,5edcbb9a91e0110f1d6db1c2,c99e2877d4b1f171746a3aa73d5fd5fd680c982d,Hierarchical Bipartite Graph Neural Networks: Towards Large-Scale E-commerce Applications,5d68f4b8ed7e9c79265606c9,SHOAL: Large-scale Hierarchical Taxonomy via Graph-based Query Coalition in E-commerce.,"On the other hand, [22] proposes an approach that automatically constructs an easy-to-interpret taxonomy on a large-scale bi-partite graph in a unsupervised manner, facilitating an efﬁcient browsing navigation that enhances user search experiences with inherent high-order connections, and the…###2) Ofﬂine Experimental Results: To investigate the model effectiveness, we compare our proposed method with Al-ibaba’s current topic-driven taxonomy solution SHOAL [22].###[22] illstrates a topic-driven hierarchical taxonomy based on user-item bi-partite graph in presence of query interactions effectively expressing user intention.###However, learning hierarchical representations of graph enjoys its outstanding features in graph classiﬁcation and clustering, and becomes prevailing in several scenarios such as link prediction, e-commerce recommendation, etc, [19], [22].###SHOAL [22] is Alibaba’s current topic-driven taxonomy solution deployed on Taobao platform, which also considers a hierarchical graph-based strategy but only uses a well-deﬁned metric to calculate the query-item embeddings.",impact-revealing,describing a proposed approach for constructing a taxonomy
3190,5ecbc8eb9fced0a24b52a39b,2b9514be4679f68f97bbcf9671053ac2f03df2e4,Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning,5550415945ce0a409eb3a820,ImageNet Large Scale Visual Recognition Challenge,We use the ImageNet dataset [25] with a pre-trained VGG19 model [28] for our evaluation.,other,reporting data sources and methods used for evaluation
1407,,2c2180fbe7f38e88b1123e5fab43785b66814e5d,Extreme Q-Learning: MaxEnt RL without Entropy,,,"###In this section we provide additional theoretical details of our algorithm, X -QL, and its connection to conservatism in CQL (Kumar et al., 2020).###…log(1 + x ) ≤ x , to get a direct concentration bound on ˆ L β ( X ) , This bound also becomes tighter with increasing β , and asymptotically behaves as In this section we provide additional theoretical details of our algorithm, X -QL, and its connection to conservatism in CQL (Kumar et al., 2020).###Our ofﬂine results with ﬁxed hyperparameters for each domain outperform prior methods (Chen et al., 2021; Kumar et al., 2019; 2020; Kostrikov et al., 2021; Fujimoto & Gu, 2021) in several environments, reaching state-of-the-art on the Franka Kitchen tasks, as shown in Table 1.###In fact, theorems of CQL (Kumar et al., 2020) hold for our objective by replacing DCQL with DKL.###First, we establish the connection of our framework with conservative Q-learning (Kumar et al., 2020).###Furthermore, it can be noted that CQL conservatism can be derived as adding a χ(2) regularization to an MDP and although not shown by the original work (Kumar et al., 2020) or any follow-ups to our awareness, the last term of Eq.###Moreover, actor-critic approaches on their own have shown to be catastrophic in the ofﬂine settings where actions sampled from a policy are consistently out-of-distribution (Kumar et al., 2020; Fujimoto et al., 2018).###On the other hand, conservatism in CQL (Kumar et al., 2020) is motivated by lower-bounding the Q-function.###Here, L ( · ) does a conservative Q-update similar to CQL (Kumar et al., 2020) with the nice property that the implied conservative term is just the KL-constraint between π and µ .###14 in CQL’s Appendix B (Kumar et al., 2020), is simply χ 2 ( π || π D ) and what the original work refers to as D CQL is actually the χ 2 divergence.###First, we connect our framework to conservative Q-learning (Kumar et al., 2020).###Furthermore, it can be noted that CQL conservatism can be derived as adding a χ 2 regularization to an MDP and although not shown by the original work (Kumar et al., 2020) or any follow-ups to our awareness, the last term of Eq.###Moreover, actor-critic approaches on their own have shown to be catastrophic in the offline settings where actions sampled from a policy are consistently out-of-distribution (Kumar et al., 2020; Fujimoto et al., 2018).###…RL can largely be categorized as relying on constrained or regularized Q-learning (Wu et al., 2019; Fujimoto & Gu, 2021; Fujimoto et al., 2019; Kumar et al., 2019; 2020; Nair et al., 2020), or extracting a greedy policy from the known behavior policy (Peng et al., 2019; Brandfonbrener et…###Here, L(·) does a conservative Q-update similar to CQL (Kumar et al., 2020) with the nice property that the implied conservative term is just the KL-constraint between π and μ.###2 BRIDGING SOFT AND CONSERVATIVE Q-LEARNING Inherent Convervatism in X -QL Our method is inherently conservative similar to CQL (Kumar et al., 2020) in that it underestimates the value function (in vanilla Q-learning) V (s) by −β Ea∼π(a|s) [ log π(a|s) πD(a|s) ] , whereas CQL understimates values by a factor −β Ea∼π(a|s) [ π(a|s) πD(a|s) − 1 ] , where πD is the behavior policy.###Inherent Convervatism in X -QL Our method is inherently conservative similar to CQL (Kumar et al., 2020) in that it underestimates the value function (in vanilla Q-learning) V π ( s ) by − β E a ∼ π ( a | s ) log π ( a | s ) π D ( a | s ) , whereas CQL understimates values by a factor − β E a ∼ π (…###14 in CQL’s Appendix B (Kumar et al., 2020), is simply χ(2)(π||πD) and what the original work refers to as DCQL is actually the χ(2) divergence.",impact-revealing,highlighting the connection and improvements of the proposed algorithm to existing methods
612,5d8dded23a55acd1b54967df,1ecbaf7a2cd3059e07261e72a1195a7c70b3d664,GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning,5cf48a4ada56291d582adbae,GMNN: Graph Markov Neural Networks,"For baselines, we choose GCN (Kipf & Welling, 2017), and the recent state-of-the-art methods GAT (Veliˇckovi´c et al., 2018), GMNN (Qu et al., 2019) and Graph U-Net (Gao & Ji, 2019).###We use the standard benchmark architecture as used in GCN (Kipf & Welling, 2017), GAT (Veliˇckovi´c et al., 2018) and GMNN (Qu et al., 2019), among others.###Furthermore, the results of GraphMix(GCN) are comparable with the recently proposed state-of-the-art method GMNN (Qu et al., 2019).###More recent approaches include (Bruna et al., 2013; Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2016; Gilmer et al., 2017; Hamilton et al., 2017; Veli ˇ ckovi ´ c et al., 2018; 2019; Qu et al., 2019; Gao & Ji, 2019; Ma et al., 2019), among others.###Following (Qu et al., 2019), we treat edges with weights greater than 3 as positive instances, and edges with weights less than -3 are treated as negative ones.###More recent approaches include (Bruna et al., 2013; Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2016; Gilmer et al., 2017; Hamilton et al., 2017; Veličković et al., 2018, 2019; Qu et al., 2019; Gao & Ji, 2019; Ma et al., 2019), among others.###These convolution operator based method exhibit state-of-the-results for semi-supervised learning over graph data, hence much of the recent attention is dedicated to proposing architectural changes to these methods (Qu et al., 2019; Gao & Ji, 2019; Ma et al., 2019).###For Pubmed, GraphMix improved upon GCN and GAT but was worse than GMNN.",impact-revealing,reporting baseline methods and their performance
3971,5d04eeba8607575390f83f3a,b67fd0612e3f72faee0fed9b1e930b69ed7ee98d,sparse reram engine: joint exploration of activation and weight sparsity in compressed neural networks,5550460f45ce0a409eb5bbc1,Small-footprint keyword spotting using deep neural networks,"The matrix-vector multiplication of OU2 and the LSB of the rest of values in the input sliding window [1, 1] is then performed at the second cycle to yield the output [3, 4].###Deep neural networks (DNNs) have recently emerged as a highly effective solution for many classification and regression tasks, including image classification [26], object detection [39], and speech recognition [4].###The output of OU1 and OU2 are added together, resulting in the summed output [4, 4] before being assembled by the shift-and-add circuit.",other,describing the process of matrix-vector multiplication and its application in deep neural networks
1857,,07f51c210d140debe3fa4ef959cf28c1a72b530f,Fipronil-Induced Biochemical Alterations During Oral Subacute Toxicity in Buffalo Calves,,,"###Fipronil (5-amino-1-[2,6-dichloro-4-(trifluoromethyl)phenyl]4-[(trifluoromethyl) sulfinyl]-1H-pyrazole-3-carbonitrile) [1], is a highly effective, broad-spectrum phenylpyrazole insecticide.###Fipronil has become widely used for control of a wide range of crop, public hygiene, amenity and veterinary pests since its introduction to the market in 1993 [1].",impact-revealing,providing context and background on Fipronil's usage and effectiveness
768,5d1b2f5a3a55ac071793c55c,83b56c3c7a61767bd88d85796aa5dbc4976912c3,gpt-based generation for classical chinese poetry,5cede0e6da562983788c5226,The Curious Case of Neural Text Degeneration.,"For text generation, we implement truncated top-k sampling instead of beam-search to generate diverse text [6].",impact-revealing,reporting a method for text generation
3113,558c2b08e4b00c3c48e0a105,368e031ce85bee93ad5bda8c0970cda76c9cf140,the heterogeneous block architecture,53e9b042b7602d9703aa37a5,Composite Cores: Pushing Heterogeneity Into A Core,"It extensively evaluates an example HBA design in comparison to four baselines (out-of-order, clustered [18], coarse-grained heterogeneous [37], and clustered coarse-grained), showing higher energy efficiency than all previous designs across a wide variety of workloads###, [2, 3, 6, 10, 20, 23, 28, 33, 59, 61]), or combine an in-order and an out-of-order pipeline with a shared frontend in a single core [37].###For comparison, McPAT estimates 20% area overhead for [37].###The state-of-the-art coarse-grained heterogeneous core [37] saves energy in both the out-of-order logic (RAT, ROB, and RS) and execution resources (bypass buses and register file) as it can use the in-order backend a portion of the time.###Overall, these results show that HBA reduces core energy significantly compared to all baseline designs, including the non-clustered and clustered versions of a state-of-the-art heterogeneous core [37], by leveraging block atomicity and heterogeneity synergistically.###, programs have memory and compute phases, and such phases can be exploited by migrating a thread to “big” cores for computeintensive phases and “little” cores for memory-intensive phases [37, 61].###Table II shows average core power and Energy Per Instruction (EPI) for six core designs: baseline out-oforder, clustered out-of-order [18], coarse-grained heterogeneous [37], coarse-grained heterogeneous combined with clustered out-of-order, HBA with only out-of-order backends, and HBA with heterogeneous backends.###We model an ideal controller for this coarse-grained design, thus providing an upper bound on efficiency and performance relative to the real controller-based mechanism of [37].###Using a clustered out-of-order backend in the coarse-grained heterogeneous core (row 4) degrades performance over either the clustered or coarse-grained core alone (rows 2,3) due to the additive overheads of clustering [18] and coarse-grained heterogeneity [37].###Second, we compare to two variants of a coarse-grained heterogeneous design that combines an out-of-order and an in-order core [37] (iii) without clustering and (iv) with clustering.",other,comparing energy efficiency of HBA design with baseline designs
2191,5f3f917891e011d38f9242d9,14156438bafed28a626738630b5181b83ed5d79c,Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters,5e9ef9b69fced0a24b1b64cb,Financial Defaulter Detection on Online Credit Payment via Multi-view Attributed Heterogeneous Information Network,"To detect those fraudulent activities, graph-based methods have become an effective approach in both academic [7, 21, 38] and industrial communities [2, 28, 50].",other,highlighting the effectiveness of graph-based methods for detecting fraudulent activities
3962,5db929e947c8f766461fd005,11ead744b2edbdabe174258a8dfe4023be8e501d,Hierarchical Attention Prototypical Networks for Few-Shot Text Classification,573696086e3b12023e51b3fc,A C-LSTM Neural Network for Text Classification.,"C-LSTM (Zhou et al., 2015) combines CNN and RNN for sentence representation and text classification.",other,reporting prior findings on C-LSTM for sentence representation
2360,5aed14d117c44a4438158af2,30f86d38f0660af5ea2e16d996434c72eee8c5ee,espnet: end-to-end speech processing toolkit,58d82fc8d649053542fd5ae7,Regularizing Neural Networks by Penalizing Confident Output Distributions.,"We implemented unigram smoothing, where the distribution of remaining labels is set to be proportional to the unigram distribution of the labels [40].",other,describing a method used for label distribution
2353,5736982b6e3b12023e6fd099,684a466028785c39911770b96fb0c814e75b5b6d,DynaMOS: Dynamic schedule migration for heterogeneous cores,53e9b4e0b7602d9704007e0e,Increasing The Size Of Atomic Instruction Blocks Using Control Flow Assertions,Compilers use profile-based static scheduling mechanisms [32] or run-time binary optimization [33] to create optimized instruction schedules.,other,reporting methods used in compiler optimization
2697,5c20b1fcda5629702063aff8,482f0510d8f64fe972ec7a05ae5989c25e98ea48,STRAIGHT: Hazardless Processor Architecture Without Register Renaming,558b405ce4b0b32fcb3b96f8,Dark Silicon and the End of Multicore Scaling,"The operand of SLTi is ﬁxed as [2] by adding RMOV s. 3) Distance Bounding: The STRAIGHT architecture has the maximum distance of source registers.###For example, “ [1] ” of instruction I 2 means that the instruction uses the result value of the previous instruction, and “ [2] ” means that the other operand is the result value of the second previous instruction.###This strategy trades off energy efﬁciency against lower-utilized cores, reﬂecting the recent dilemma that even though the number of transistors can be increased, they cannot be switched simultaneously [2].###Therefore, this code calculates a Fibonacci series as long as the “ ADD [1] [2] ” instruction is repeated.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2610,5da2f8aa3a55ac3402d8c2e1,17f2f3f7e58b916175d495109bc74b2757ef952a,Barrage of Random Transforms for Adversarially Robust Defense,5a9cb65d17c44a376ffb81e9,Deflecting Adversarial Attacks with Pixel Deflection,"[15] claimed 81% accuracy under attack and Liao, Liang, Dong, et al.###Prior works have used the median filter [12], wavelet [15, 17], and non-local mean [27] as defenses, but all have since been defeated.",other,reporting prior findings on accuracy and defenses
3096,5d5e6b9a3a55acfce79a16dd,6303bac53abd725c3b458190a6abe389a4a1e72d,Deep High-Resolution Representation Learning for Human Pose Estimation,5550415945ce0a409eb3a820,ImageNet Large Scale Visual Recognition Challenge,"n the paper, shown in Table9. Results on the ImageNet Validation Set We apply our networks to image classiﬁcation task. The models are trained and evaluated on the ImageNet 2013 classiﬁcation dataset [54]. We train our models for 100 epochs with a batch size of 256. The initial learning rate is set to 0.1 and is reduced by 10 times at epoch 30, 60 and 90. Our models can achieve comparable performance ",other,reporting experimental setup and results on ImageNet
3079,5d3ed25a275ded87f97deae1,f5252075bb34666863cd01cc82c2d941d4ffe6c6,robust graph convolutional networks against adversarial attacks,5c8a11324895d9cbc6121c34,Neural Message Passing for Quantum Chemistry.,"State-of-the-art GCNs usually follow a “message-passing” framework [10, 35] where each node aggregates information from its immediate neighbors in each convolutional layer.###MPNNs [10] and GraphSAGE [12] unify these approaches using the “message-passing” framework, i.",other,providing context on graph convolutional networks
3914,5c20b1fcda5629702063aff8,482f0510d8f64fe972ec7a05ae5989c25e98ea48,STRAIGHT: Hazardless Processor Architecture Without Register Renaming,558b4f2684ae84d265c2ab1a,Scale-out processors.,"Therefore, the recent improvement in single-thread performance is relatively modest compared to those of GPUs and TLP technologies, which demonstrate a performance increase that is proportional to the increase in the number of transistors employed [3] [4].###In the example, the instruction ADD [4] [3] in the callee always refers arg0 and arg1 .###The counter variable is initialized in the BB0 ( ADDi [0] 0 ) and incremented before the conditional statement in the BB2 ( ADDi [4] 1 ).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1017,,2036c267991a595e5409964b4dc5c11cc2450151,Direct and indirect pathways between adult attachment style and marital satisfaction,,,"###For example, Carnelley et al. (1994) reported that women with mild depression were more likely to endorse preoccupied and fearful avoidant attachment styles than nondepressed women.###Although most previous research has documented associations between ambivalent attachment and both psychological symptomatology (e.g., Carnelley et al., 1994; Hammen et al., 1995) and reduced social support (Davis et al., 1998; Mikulincer et al., 1993), null or conflicting results regarding…###…examine relations among attachment style, social support, psychological symptomatology, and relationship satisfaction; this builds on the work of previous investigators who have predominantly focused on subsets of these factors (e.g., Carnelley et al., 1994; Mikulincer & Florian, 1998).",impact-revealing,highlighting the need for comprehensive examination of attachment styles and their relations
3138,599c782b601a182cd25a765e,ac553579eaaaabd1003bce9a2ad679e901b3ae73,Predicting drug–drug interactions through drug structural similarities and interaction networks incorporating pharmacokinetics and pharmacodynamics knowledge,55a6415365ce054aad61d6eb,Pharmacokinetic Drug-Drug Interaction And Their Implication In Clinical Management,"both binding to a same plasma protein are co-administered, the concentration of the free drugs in plasma may change [4].###For example, changes in gastric pH caused by a drug can affect the gastro-intestinal absorption of a co-administered drug [4].###DDI occurs when two drugs share the same mechanism of excretion [4].",other,providing context for drug interactions and their effects
825,5db92a0d47c8f766461ff307,9014c4e7bec425a117b514e41d9cdf9dec8bd6c1,meta relational learning for few-shot link prediction in knowledge graphs,5bbacbad17c44aecc4eb007e,One-Shot Relational Learning for Knowledge Graphs,"To do few-shot link prediction, Xiong et al.
(2018) made the first trial and proposed GMatching, learning a matching metric by considering both learned embeddings and one-hop graph structures, while we try to accomplish few-shot link prediction from another perspective based on the intuition that…###Traditional embedding models are heavily rely on rich training instances (Zhang et al., 2019b; Xiong et al., 2018), thus are limited to do few-shot link prediction.###As far as we know, work proposed by Xiong et al. (2018) is the first research on few-shot learning for knowledge graphs.###Generally, there are three kinds of meta-learning methods so far: (1)Metric-based meta-learning (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017; Xiong et al., 2018), which tries to learn a matching metric between query and support set generalized to all tasks, where the idea of matching is similar to some nearest neighbors algorithms.###Compared with GMatching (Xiong et al., 2018) which relies on a background knowledge graph, our MetaR is independent with them, thus is more robust as background knowledge graphs might not be available for few-shot link prediction in real scenarios.###The result under the third setting is copied from Xiong et al. (2018). It uses the triples from background graph, training tasks and one-shot training triples from validation/test set, so it’s neither BG:Pre-Train nor BG:In-Train.###The result under the third setting is copied from Xiong et al. (2018).###We use two datasets, NELL-One and WikiOne which are constructed by Xiong et al. (2018).###GMatching (Xiong et al., 2018), the first trial on one-shot link prediction in knowledge graphs, learns a matching metric based on entity embeddings and local graph structures which also can be regarded as a metric-based method.###The baseline in our experiment is GMatching (Xiong et al., 2018), which made the first trial on few-shot link prediction task and is the only method that we can find as baseline.###…there are three kinds of meta-learning methods so far: (1) Metric-based meta-learning (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017; Xiong et al., 2018), which tries to learn a matching metric between query and support set generalized to all tasks, where the idea of matching is…",impact-revealing,acknowledge prior work on few-shot link prediction and its limitations
4001,59ec02da0cf22f5df7319dc3,c27db32efa8137cbf654902f8f728f338e55cd1c,mastering the game of go without human knowledge,55d097ff69632219056c948c,Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit,"The neural network consists of many residual blocks 4 of convolutional layers 16,17 with batch normalization 18 and rectifier nonlinearities 19 (see Methods).",other,providing context on the structure of the neural network
3284,5ee9f15b91e01152af022eaf,a83902f8b3aadfda633968a840ca1738bedef837,modeling graph structure via relative position for text generation from knowledge graphs,5b8c9f4a17c44af36f8b6b8c,SentencePiece: A simple and language independent subword tokenizer and   detokenizer for Neural Text Processing,"For both datasets, we train a BPE vocabulary us-ing sentencepiece (Kudo and Richardson, 2018) on the train set, i.e., a concatenation of node labels and target texts.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3938,5ecbc7639fced0a24b502b60,326974bd97dea3278515bcb3551a59e304d6d755,Improving Low-Resource Speech Recognition Based on Improved NN-HMM Structures,573697016e3b12023e5fb275,Audio Augmentation For Speech Recognition,"As far, the method of changing speed has the lowest implementation cost and achieve state-of-the-art performance [23].",other,highlighting the effectiveness and cost-efficiency of the speed-changing method
3287,5cf48a48da56291d582ab75a,c5f5f179d80a3bf9b4f29750283a87eaca42e91b,neural graph collaborative filtering,53e9bc74b7602d97048f4169,Factorization meets the neighborhood: a multifaceted collaborative filtering model.,"rs or items. We perform experiments to investigate the impact of message dropout and node dropout on NGCF in Section 4.4.3. 2.5 Discussions In the subsection, we first show how NGCF generalizes SVD++ [19]. In what follows, we analyze the time complexity of NGCF. 2.5.1 NGCF Generalizes SVD++. SVD++ can be viewed as a special case of NGCF with no high-order propagation layer. In particular, we set L to ",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1860,,b25de1474c7f540c05dd9a4929e1cd954deef9a3,"Science for whom? Examining the data quality, themes, and trends in 30 years of public funding for global climate change and energy research",,,"###8 identify large bodies of evidence, and they can be systematic, aimed at meta-analysis, or more commonly used as qualitative sections of research papers [65,66].",impact-revealing,providing context on types of evidence in research
3472,5a260c8117c44a4ba8a30a57,908272f8e6340971600148d4e73f50e1e8843aaf,"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec",57d063c7ac443673542906dc,Max-Margin DeepWalk: Discriminative Learning of Network Representation.,"Discussion on Yang et al. [46, 47] Two related works we want to discuss here are [46] and [47] by the same authors, where they claim they prove a closed form matrix factorized by DeepWalk. conclusion is then referred and used in a few following papers [29, 42, 48].###…to this area, for example, heterogeneous network embedding [7, 11, 17, 34], direct network embedding [27], semi-supervised network embedding [18, 42, 49], network embedding with rich vertex attributes [41, 47], network embedding with high order structure [5, 15], network embedding via deep…",other,discussing related works and their contributions
1940,,15ca9d3a42f31c034094ad5b7f02e971d85db315,"A Comparison of Phonological Awareness, Lexical Compounding, and Homophone Training for Chinese Word Reading in Hong Kong Kindergartners",,,"###However, the effect of phonological skill at the syllable level may be somewhat dwarfed by the impact of morphological awareness (McBride-Chang et al., 2003; Shu et al., 2006).###Given this outstanding feature of Chinese, some studies have demonstrated the importance of homograph and=or homophone sensitivity for Chinese literacy skills (McBride-Chang et al., 2003; Shu et al., 2006; Tong, McBride-Chang, Shu, & Wong, 2009).###Homophone awareness was tested with the same method as that previously used in a test for older children (Shu et al., 2006).###We had based it on previous work on older children (Shu et al., 2006), but the test was not easy to administer to children this young.",impact-revealing,highlighting the importance of morphological awareness in Chinese literacy skills
2661,5ce3cd34e1cd8e3f7932b9ee,4f9ba5e89a7d23675ca65473ae85e352a6d2c379,Aging-aware Lifetime Enhancement for Memristor-based Neuromorphic Computing,57d063b9ac4436735428e998,Dot-product engine for neuromorphic computing: programming 1T1M crossbar to accelerate matrix-vector multiplication.,"For example, 32 levels are used in [14] and 64 levels in [15].###In practice, it is difficult to program the conductances of memristors to exact values and thus the resistances are usually programmed instead [13], [14].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
377,5c8ddce94895d9cbc6a97820,8fb5a6fe93a3d19ffee50677c0ae563e3377d2d4,Chinese NER Using Lattice LSTM,573696106e3b12023e5227c8,Neural Architectures for Named Entity Recognition.,"The current stateof-the-art for English NER has been achieved by using LSTM-CRF models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018) with character information being integrated into word representations.###Huang et al. (2015) uses hand-crafted spelling features; Ma and Hovy (2016) and Chiu and Nichols (2016) use a character CNN to represent spelling characteristics; Lample et al. (2016) use a character LSTM instead.###We follow the best English NER model (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016), using LSTM-CRF as the main network structure.###Finally, for each word wi, −→ hi and ←− hi are concatenated as its representation: Integrating character representations Both character CNN (Ma and Hovy, 2016) and LSTM (Lample et al., 2016) have been used for representing the character sequence within a word.###Finally, for each word wi, −→ hwi and ←− hwi are concatenated as its representation: Integrating character representations Both character CNN (Ma and Hovy, 2016) and LSTM (Lample et al., 2016) have been used for representing the character sequence within a word.",impact-revealing,reporting existing methods and approaches in English NER
3500,5c04966a17c44a2c74708401,51203e9d5620abdcdf6c9be93b1e221e79cda67d,Transfer Learning of Language-independent End-to-end ASR with Language Model Fusion,5550461b45ce0a409eb5c104,Multilingual deep neural network based acoustic modeling for rapid language adaptation,"There are various approaches to leverage other languages: (a) to train a model multilingually (multi-task learning with other languages), and then further fine-tune to a particular language [6], and (b) to adapt a multilingual model to a new language using transfer learning [6–9] and additional features obtained from the multilingual model such as multilingual bottleneck features (BNF) [10–13] and language feature vectors (LFV) [14] (cross-lingual adaptation).",other,describing various approaches to leverage multilingual models
2893,5ecbc7639fced0a24b502b60,326974bd97dea3278515bcb3551a59e304d6d755,Improving Low-Resource Speech Recognition Based on Improved NN-HMM Structures,573697006e3b12023e5faed1,A Time Delay Neural Network Architecture For Efficient Modeling Of Long Temporal Contexts,"To verify the effectiveness of our framework, we also compared it with the-state-of-art framework in a low-resource environment, TDNN-HMM based on Kaldi platform [39].",other,comparing effectiveness of frameworks in low-resource environments
941,5e524da993d709897cb69fde,09e48adf1a3f012ec56fbb4f92acae975ee43d87,a deep learning approach to antibiotic discovery,5c641de9e1cd8e77bc5faabe,A graph-convolutional neural network model for the prediction of chemical reactivity.,"However, emerging models in retrosynthesis and physicochemical property prediction may overcome these limitations in the near future (Coley et al., 2018; Gao et al., 2018).",impact-revealing,highlighting the potential of emerging models to address existing limitations
117,5dde4b463a55ac4c42972afc,43a8b2fd651c3783723f4265d7641f9601a5a6f4,One Man’s Trash Is Another Man’s Treasure: Resisting Adversarial Examples by Adversarial Examples,5a9cb66717c44a376ffb89eb,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,"Thus far, gradient obfuscation is generally considered vulnerable (and at least incomplete) [2].###[2], with random input transformation, adversarial examples can still be found using Expectation over Transformation [3], which estimates the network gradient by taking the average over multiple trials (more details in Sec.###Therefore, we conclude that 500 samples in EOT allow thor-
ough evaluation of our defense against EOT.###EOT attack first estimates the gradient of expected f(g(x)) with respect to x using the relationship ∇Eg̃∼T f(g̃(x)) = Eg̃∼T ∇f(g̃(x)), where g̃(·) is a deterministic version of g(·) sampled from the distribution of randomized transformations T .###[2], who introduced a set of attacking strategies, including a method called Backward Pass Differentiable Approximation (BPDA), to circumvent gradient obfuscation (see further discussion in Sec.###Backward Pass Differentiable Approximation (BPDA).###But they have been circumvented by Expectation Over Transformation (EOT) [2, 3].###[2] introduced a strategy called Backward Pass Differentiable Approximation (BPDA) to estimate the defense model’s gradients.###We examine a wide range of possible attacks, including those having successfully circumvented many previous defenses [2].###It causes the adversary to suffer from either exploding or vanishing gradients [2].###Our defense is randomized, and when using EOT to attack our method we take 500 samples of g̃(·) (recall experiments in Fig.###The estimated gradients are then used in PGD-type methods (if the defense is deterministic) or the EOT method (if the defense is randomized) for crafting adversarial examples.###To circumvent the defense using non-differentiable operators, Athalye et al. [2] introduced a strategy called Backward Pass Differentiable Approximation (BPDA) to estimate the defense model’s gradients.###However, when the perturbation size is set to 0.2, the value we consistently use throughout all our evaluations, EOT attacks became persistently inefficient, regardless of the sample size.###7, when the perturbation size ∆ is small and the number of samples is also small (e.g.,   100), increasing sample size indeed allows EOT to better attack our method.###One can use Backward Pass Differentiable Approximation [2] to easily construct effective adversarial examples.###Thus, the feasibility of EOT hinges on a reliable estimation of ∇f(g̃(x)).###Sufficiency of EOT samples.###those defenses have been proven vulnerable under a reparameterization strategy [2].###Unfortunately, many of these methods have proven vulnerable by Athalye et al. [2], who introduced a set of attacking strategies, including a method called Backward Pass Differentiable Approximation (BPDA), to circumvent gradient obfuscation (see further discussion in Sec.###[2] presented a suite of strategies for estimating network gradients in the presence of gradient obfuscation.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
119,5dd7b1be3a55ac97f763dd30,948839277bface5780896e8e8791906818aa41ac,Adversarial Examples Improve Image Recognition,58d82fcbd649053542fd64c5,Adversarial Machine Learning at Scale.,"However, as observed in former studies [7, 18], directly optimizing Eq.###Meanwhile, recent works [18, 16, 45] also suggest that training with adversarial examples on large datasets, e.###Experiments show that such disentangled learning framework enables networks to get much stronger performance than the adversarial training baseline [7, 18].###5 in [18] shows the result of training with random normal perturbations) or adversarial noise [16, 18, 42], fail to improve accuracy on clean images.###Therefore we treat adversarial images as additional training samples and train networks with a mixture of adversarial examples and clean images, as suggested in [7, 18],###Adversarial training, which trains networks with adversarial examples, constitutes the current foundation of state-of-the-arts for defending against adversarial attacks [7, 18, 23, 45].###These results contradict previous conclusions [18, 42, 16] that the performance degradation is always observed if adversarial examples are used for training.###We hypothesize this distribution mismatch between clean examples and adversarial examples is a key factor that causes the performance degradation in previous works [16, 18, 45].###Since the first discovery of the vulnerability of ConvNets to adversarial attacks [40], many efforts [2, 7, 15, 16, 18, 23, 29, 36, 42, 45, 50] have been made to improve network robustness.",impact-revealing,highlighting the evolution and challenges in adversarial training for neural networks
742,5f7fdd328de39f0828397ac2,a5fa6e7565dca654eab9372ace4b1ba7f63655f7,CogLTX: Applying BERT to Long Texts,53e997bab7602d9701fa1393,Working memory.,"The maximum length limit in BERT reminds us the limited capacity (5∼ 9 chunks) of the working memory of humans –— then how do human beings Cognize Long TeXts? Founded on the cognitive theory stemming from Baddeley [2], the proposed CogLTX 1 framework identifies key sentences by training a judge model, concatenates them for reasoning, and enables multi-step reasoning via rehearsal and decay.###The maximum length limit in BERT naturally reminds us the limited capacity of Working Memory [2], a human cognitive system storing information for logical reasoning and decision-making.###Experiments [27, 9, 31] already showed that the working memory could only hold 5∼9 items/words during reading, so how do humans actually understand long texts? “The central executive – the core of the (working memory) system that is responsible for coordinating (multi-modal) information”, and “functions like a limited-capacity attentional system capable of selecting and operating control processes and strategies”, as Baddeley [2] pointed out in his 1992 classic.###“The central executive – the core of the (working memory) system that is responsible for coordinating (multi-modal) information”, and “functions like a limited-capacity attentional system capable of selecting and operating control processes and strategies”, as Baddeley [2] pointed out in his 1992 classic.",impact-revealing,highlighting the connection between BERT's limitations and human cognitive capacity
2050,,97f386ea4a16ab36a943872cb37f4fd357ee2034,Graph-Based Retinal Fluid Segmentation from OCT Images,,,"###The key technique in our framework is graph-based optimization for image segmentation [10, 7, 6, 4].###We present a graph-based cyst/fluid-associated abnormality segmentation method that builds upon our prior work in optimal segmentation methods [17, 19, 9, 18, 12, 10, 13, 11, 14].###Left, results from the original Iowa Reference Algorithms [5, 1, 10, 9, 15, 3].###While the framework for the optimal fluid-associated abnormality segmentation is similar to the generic volumetric graph-based segmentation approaches [4, 10], we designed sophisticated cost functions to capture the expert domain knowledge regarding retinal fluid-associated abnormalities.###While our publicly available Iowa Reference Algorithms [5, 1, 10, 9, 15, 3] provide excellent accuracy and robustness for the segmentation of 11 retinal tissue layers (which can be combined to create a retinal mask) for images of eyes not exhibiting diseaserelated changes to layer topology, the pathological eyes that contain fluid-associated abnormalities can often be problematic, especially when these are close to the retinal boundary.",impact-revealing,highlighting the significance of graph-based optimization in image segmentation
2343,5db1765a3a55ac101c887e97,6c4b76232bb72897685d19b3d264c6ee3005bc2b,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,5b3d98cc17c44a510f801b91,Exploring the Limits of Weakly Supervised Pretraining,"…research argues that general methods that can leverage additional computation ultimately win out against methods that rely on human expertise [Sutton, 2019; Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Shazeer et al., 2018, 2017; Huang et al., 2018b].###it is often possible to achieve better performance simply by training a larger model on a larger dataset [Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019; Shazeer et al., 2018, 2017; Huang et al., 2018b].###…remarkable scalability, i.e. it is often possible to achieve better performance simply by training a larger model on a larger dataset [Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Radford et al., 2019; Shazeer et al., 2018, 2017; Huang et al., 2018b].###The “bitter lesson” of machine learning research argues that general methods that can leverage additional computation ultimately win out against methods that rely on human expertise [Sutton, 2019; Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Shazeer et al., 2018, 2017; Huang et al., 2018b].",other,highlighting the significance of leveraging computation in machine learning for better performance
3394,5db9294247c8f766461f1f4a,79c93274429d6355959f1e4374c2147bb81ea649,lxmert: learning cross-modality encoder representations from transformers,556f622a2401b4b38c23635c,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,"Pi-oneering works (Girshick et al., 2014; Xu et al., 2015) also show the generalizability of these pre-trained (especially on ImageNet) backbone models by ﬁne-tuning them on different tasks.",other,highlighting the generalizability of pre-trained models across tasks
2095,,c08c7dfb1fd5be34cfc6c8d40e04508aee210edc,Developing Teaching Materials on Artificial Intelligence by Using a Simulation Game (Work in Progress),,,"###However, the frequent mention of AI in public could create an ""illusion of explanatory depth"" [7], which can be reinforced by a popularisation and simplification of the topic [8].",impact-revealing,highlighting the potential misconception created by frequent AI mentions in public discourse
495,5db9298547c8f766461f8b65,ddb2aecf8777007414b1eb341c6c19ec799280d3,Frame attention networks for facial expression recognition in videos,599c7987601a182cd2648373,Attention Is All You Need.,"In this paper, inspired by the attention mechanism [14] of machine translation and the neural aggregation networks [15] of video face recognition, we propose the Frame Attention Networks (FAN) to adaptively aggregate frame features.",impact-revealing,introducing a novel method inspired by existing mechanisms
3557,5d08be648607575390f908ca,b30b850e726ee41f80a28c2a81da9c640a8541fc,QoSMT: supporting precise performance control for simultaneous multithreading architecture,53e9ac75b7602d97036466ed,Characterizing The Branch Misprediction Penalty,"They are not only hard to predict whether it is taken or not, but also highly depend on prior floating point instructions[15].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
90,5d3ed25a275ded87f97dea9b,0ddd7c9a1955c76eb831a230ef20a68b662886bb,Mining Algorithm Roadmap in Scientific Publications,5c8d30274895d9cbc643dc9e,Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction.,"[37] proposes a method using self-attention [36] and bi-affine scoring algorithm to predict biological relations between all mention pairs in the abstract simultaneously.###We adapt Transformer [36] to encode word sequences in a paragraph, where we calculate the self-attention of the words, and use a convolutional layer in self-attention blocks similar to [37] to alleviate the burden on the model to attend to local features.###Most existing researches for the supervised relation extraction focus on single sentence relation extraction with an exception of [25, 37], which focus on general documents while not targeting on a specific narration of algorithm abbreviations and comparative relation.",impact-revealing,acknowledge existing research on relation extraction methods
3616,5c8d4bf34895d9cbc64e3332,a60c69c2fae27ebbb73c87f7f2a4765556bd7f9f,Stochastic Training of Graph Convolutional Networks with Variance Reduction,5a260c8117c44a4ba8a30adf,Representation Learning on Graphs: Methods and Applications,"We cannot set D ( l ) = 1 because GraphSAGE explicitly need the activation of a node itself besides the average of its neighbors.###Hamilton et al. (2017a) propose to perform the approximate forward propagation as Eq.###Since most GCNs only have two graph convolution layers (Kipf & Welling, 2017; Hamilton et al., 2017a), this gives a signiﬁcant reduction of the receptive ﬁeld size and speeds up the computation.###On the largest Reddit dataset, the training time of our algorithm is 7 times shorter than that of the best-performing competitor among the exact algorithm (Kipf & Welling, 2017), neighbor sampling (Hamilton et al., 2017a) and importance sampling (Chen et al., 2018) algorithms.###Hamilton et al. (2017a) make an initial attempt to develop stochastic training algorithms for GCNs via a scheme of neighbor sampling (NS).###To reduce the receptive ﬁeld size, Hamilton et al. (2017a) propose a neighbor sampling (NS) algorithm.###GCNs and their variants (Hamilton et al., 2017a; Veliˇckovi´c et al., 2017) have been applied to semi-supervised node classiﬁcation (Kipf & Welling, 2017), inductive node embedding (Hamilton et al., 2017a), link prediction (Kipf & Welling, 2016; Berg et al., 2017) and knowledge graphs…###Hamilton et al. (2017a) choose D (1) = 10 and D (2) = 25 , and the receptive ﬁeld size D (1) × D (2) = 250 is much larger than that of MLP, which is 1 , so the training is still expensive.###• PPI and Reddit: We use the mean pooling architecture GraphSAGE-mean proposed by [3].###Our accuracy result for IS+PP can match the result reported by Chen et al. (2018), while their NS baseline, GraphSAGE (Hamilton et al., 2017a), does not implement the preprocessing technique in Sec.###Our algorithm is applicable to other models (Hamilton et al., 2017a) and tasks (Kipf & Welling, 2016; Berg et al., 2017; Schlichtkrull et al., 2017; Hamilton et al., 2017b) that involve computing the average activation of neighbors.###…et al., 2017a; Veliˇckovi´c et al., 2017) have been applied to semi-supervised node classiﬁcation (Kipf & Welling, 2017), inductive node embedding (Hamilton et al., 2017a), link prediction (Kipf & Welling, 2016; Berg et al., 2017) and knowledge graphs (Schlichtkrull et al., 2017),…###The model is GCN for the former 4 datasets and GraphSAGE (Hamilton et al., 2017a) for the latter 2 datasets, see Appendix E for the details on the architectures.###We examine the variance and convergence of our algorithms empirically on six datasets, including Citeseer, Cora, PubMed and NELL from Kipf & Welling (2017) and Red-dit, PPI from Hamilton et al. (2017a), as summarized in Table 1, with the same train / validation / test splits.###Training with the CV estimator is similar as with the NS estimator (Hamilton et al., 2017a).",other,discussing the application and performance of GraphSAGE and GCNs in various tasks
3106,5eb789d3da5629cf24430b41,1739466ac1411788cf1de60a3a6b59d739dc41ff,Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder,599c7cdf601a182cd27e33f3,Practical Black-Box Attacks against Machine Learning.,"Under the black-box condition, we separately train a simple three layers fully-connected network as the substitute network [23] for each network.",other,describing the method for training a substitute network
2273,5eabf3cd91e011664efc496f,916595fc4e701ac6125725408912a1fac3d7a60b,CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows,55323c6d45cec66b6f9dc0c5,A Front-End Execution Architecture for High Energy Efficiency.,"A straightforward solution is allocating a new physical register to every destination operand, which guarantees correct data communication between instructions through the physical register ﬁle (PRF) [6], [15].###FXA [6] employs an InO execution unit (IXU) that consists of the FUs and a bypass network, in front of an OoO back-end to execute ready-at-dispatch instructions in an energy-efﬁcient manner.###For decades, researchers have tried to make an OoO core more energy efﬁcient by addressing the complexity of the scheduling logic [2], [4] or reducing the accesses to power-hungry structures [5], [6], [7], [8].",other,discussing energy efficiency improvements in out-of-order execution cores
1579,,b6c9177ff881e80924f9146854928d9790cc5a33,"Particulate matter intake fractions for vehicular emissions at elementary schools in Hamilton, Canada: an assessment of outdoor and indoor exposure",,,"###Previous studies have suggested that the one-compartment model is accurate since it has generated similar results compared with more complex models (Apte et al. 2012; Ji et al. 2012).###This method has been widely used for sensitivity analysis on iF (Ji et al. 2012, 2015; Xu et al. 2015).",impact-revealing,highlighting the accuracy and reliability of the one-compartment model in comparison to complex models
3612,5d3044363a55ac8b59feaf5b,31f8b4ca13c0b333324044bd96aac20c3d0b67bd,JumpSwitches: Restoring the Performance of Indirect Branches In the Era of Spectre,55465e270cf2939c2fee9ed3,Optimizing binary translation of dynamically generated code,"Indirect call promotion is often employed by compilers [6, 9] to take advantage of profiling data, or binary translators and JIT engines to avoid expensive lookups in code caches [20].",other,acknowledge existing techniques in indirect call promotion
3237,5eede0b091e0116a23aafc15,9a75cb455b4e70c66f3b72e6bb1498d8cab72fb2,Big Self-Supervised Models are Strong Semi-Supervised Learners,5cede0fcda562983788db9a8,Selective Kernel Networks,"The largest model we train is a 152-layer ResNet [25] with 3× wider channels and selective kernels (SK) [28], a channel-wise attention mechanism that improves the parameter efficiency of the network.###2 Bigger Models Are More Label-Efficient In order to study the effectiveness of big models, we train ResNet models by varying width and depth as well as whether or not to use selective kernels (SK) [28].###002× sqrt(BatchSize)) for larger ResNets variants (with width multiplier larger than 1 and/or SK [28]).###Depth Width Use SK [28] Param (M) Fine-tuned on Linear eval Supervised 1% 10% 100%",other,describing the architecture and training of a large model
1445,,5a0b02bc4452695dd29febbf876a160b33110e89,Osteogenic stimulation of human adipose-derived stem cells by pre-treatment with fibroblast growth factor 2,,,"###…supported by aMusculoskeletal Bioorgan Center Grant (A040003) from the Korean Ministry of Health and Welfare, the Bio & Medical Technology Development Program of the National Research Foundation (NRF) funded by the Ministry of Science ICT & Future Planning (NRF-2012M3A9C6050499) awarded to…###This research was supported by aMusculoskeletal Bioorgan Center Grant (A040003) from the Korean Ministry of Health and Welfare, the Bio & Medical Technology Development Program of the National Research Foundation (NRF) funded by the Ministry of Science ICT & Future Planning (NRF-2012M3A9C6050499) awarded to Youngsook Son Midcareer Researcher Program (NRF-2015R1A2A2A04006172), and the Basic Science Research Program through the National Research Foundation of Korea (NRF) founded by the Ministry of Science ICT & Future Planning (2011-0009391) awarded to EunAh Lee.###Despite their advantages, the practical application of ADSCs in bone engineering is limited by their osteogenic differentiation potential, which is suboptimal compared with that of bone-marrow-derived stem cells (BMSCs), the gold standard of skeletal stem cells (Im et al. 2005).",impact-revealing,highlighting the limitations of ADSCs in bone engineering compared to BMSCs
352,5e79da4491e0115bb1157b77,8e74106415626a21bb909ff489ef310625c769e1,Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"As suggested in [15], the Transformer training is performed with batch size b = 4, dropout probability d = 0.###As the third model category, we employ two language models for deep contextual text representations based on the Transformer architecture [37], named BERT [15] and XLNet [41].###Recently, Transformer-based [37] neural language models introduced a shift from context-free word embeddings, like GloVe [31], to contextual embeddings as the ones used in BERT [15] and XLNet [41].###Second, we implement six different models using word-based document embeddings from GloVe [31] and Paragraph Vectors [23] (as Doc2vec implementation [33]), and deep contextual language models from BERT [15] and XLNet [41] in a vanilla and Siamese architecture [9].",impact-revealing,describing the implementation of various language models and their significance
3447,5a260c8117c44a4ba8a30b08,79cfb51a51fc093f66aac8e858afe2e14d4a1f20,Focal Loss for Dense Object Detection,573696086e3b12023e51af06,Learning To Refine Object Segments,", Selective Search [34], EdgeBoxes [37], DeepMask [23, 24], RPN [27]) rapidly narrows down the number of candidate object locations to a small number (e.###FPN improves multi-scale predictions from fully convolutional networks (FCN) [36], as shown by its gains for RPN [3] and DeepMask-style proposals [14], as well at two-stage detectors such as Fast R-CNN [2] or Mask R-CNN [5].###The proposal stage ( e.g ., Selective Search [12], EdgeBoxes [13], DeepMask [14], [15], RPN [3]) rapidly narrows down the number of candidate object locations to a small number ( e.g ., 1-2k), ﬁltering out most background samples.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2099,,80353f388c00b172dc063747699ddd4192cb01d8,POSGen: Personalized Opening Sentence Generation for Online Insurance Sales,,,"###However, SATL improves upon [23] by allowing not only such low-level feature sharing, but also high-level feature sharing.###SATL is inspired by knowledge transfer in recommendation systems [23].",impact-revealing,Highlighting improvements in SATL over previous methods
3804,5d3ed25a275ded87f97deb36,37f7eaf57b88ef03efd4196f2f57ae0b4c657a79,Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommendation,5bbacb6117c44aecc4ead40d,Recurrent knowledge graph embedding for effective recommendation.,"Existing KG-aware recommender systems can be classified into path-based methods [8, 33, 36], embedding-based methods [9, 26, 27, 34], and hybrid methods [18, 24, 28].###But in contrast to hybrid methods such as RKGE [18] or RippleNet [24], the computation complexity of our model scales well with the increase of KG size.###(3) Hybrid methods [18, 24] combine the above two categories and learn user/item embeddings by exploiting the structure of KGs.",other,acknowledge existing classifications of KG-aware recommender systems and highlight computational efficiency
3382,55a5d3fd65ce60f99bf6743c,8fdc3a400df0e4556efa907fea2306fb3ac1e799,high-throughput methods for combinatorial drug discovery,53e9a848b7602d9703190015,A systems biology approach to identify effective cocktail drugs,"Wu and colleagues ( 40 ) explored protein-protein, protein-DNA, and signaling pathways in the context of type 2 diabetes.",other,reporting prior findings in protein interactions related to type 2 diabetes
210,599c797a601a182cd2641df7,63a010c69f00e65c946a68b546bbd42cbed03564,MagNet: A Two-Pronged Defense against Adversarial Examples,5550417845ce0a409eb3b9b3,Explaining and Harnessing Adversarial Examples.,"Current defenses against adversarial examples follow three approaches: (1) Training the target classifier with adversarial examples, called adversarial training [34, 5]; (2) Training a classifier to###However, recent research showed that an attacker could generate adversarial examples to fool classifiers [34, 5, 24, 19].###More specifically, researchers showed that it was possible to generate adversarial examples to fool classifiers [34, 5, 24, 19].###For example, one may use a mixture of normal and adversarial examples in the training set for data augmentation [34, 22], or mix the adversarial objective with the classification objective as regularizer [5].###Researchers developed several methods for generating adversarial examples, most of which leveraged gradient based optimization from normal examples [2, 34, 5].###Given a normal image x , fast gradient sign method [5] looks for a similar image x ′ in the L∞ neighborhood of x that fools the classifier.",impact-revealing,acknowledge existing methods for generating adversarial examples
3847,5f7ee07491e011a5faf0feb2,c54fb59319288d0d17ad09e6b7dfee6d042f83a8,Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer,59ae3be32bbe271c4c71b8ba,Convolutional 2D Knowledge Graph Embeddings,"There are also various methods falling into the groups of Bilinear models such as RESCAL (Nickel et al., 2011) and DistMult (Yang et al., 2015), as well as neural models like HolE (Nickel et al., 2016) and ConvE (Dettmers et al., 2018).",other,acknowledge existing methods in relation to Bilinear and neural models
1607,,09a4d9ab6a305ecf04d8a2e8f4d979f1bfbe1dc3,URLTran: Improving Phishing URL Detection Using Transformers,,,###The attack framework proposed in our work is more in line with black-box attack frameworks such as DeepWordBug [26] and TextAttack [38] where the construction of adversarial data is motivated by,impact-revealing,highlighting alignment with existing black-box attack frameworks
3737,5fa909a591e011e83f7406b0,ba9f6368370ca07c1a0c9a5684b0908f6d2e0c6f,Sandslash: a two-level framework for efficient graph pattern mining,5b16426b8fbcbf6e5a9b5cbd,TurboFlux: A Fast Continuous Subgraph Matching System for Streaming Graph Data.,Sandslash-Lo user code for 4-MC using local counting.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3716,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,53e9aa09b7602d970337438a,Hierarchical clustering for graph visualization,"Furthermore, most works explore these mechanisms only within a semisupervised or supervised framework, ignoring the fact that unsupervised graph clustering is often an extremely useful end-goal in itself – whether for data exploration [44], visualization [11, 12], genomic feature discovery [7], anomaly detection [43], or for many other use-cases discussed e.",other,highlighting the importance of unsupervised graph clustering across various applications
3589,5f3268fb91e011bc1612aeab,dee8650c0a65588a09eb86751c600fb67a030bbc,Speech Driven Talking Face Generation From a Single Image and an Emotion Condition,55a6b4d765ce054aad7223c2,Auditory selective attention is enhanced by a task-irrelevant temporally coherent visual stimulus in human listeners.,"The presence of visual cues improves speech comprehension [1], [2], [3], [4] in noisy environments and for the hardof-hearing population.",other,highlighting the significance of visual cues in improving speech comprehension
2256,5edf5dd891e011bc656deb7d,14b65a86c82e38fce0eb3506e0d4084ad5cdb583,deberta- decoding-enhanced bert with disentangled attention,5c04967517c44a2c74708c19,ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension,"‚ ReCoRD is a commonsense Question Answering dataset.###0 [32], RACE [33], ReCoRD [34] and SWAG [35]; (2) Natural Language Inference: MNLI [36]; and (3) NER: CoNLL-2003.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
308,5f8ebbb99fced0a24b4e1966,d299c78121f39c3a4cd09e0994e47ec8cd0c20d6,Diversifying Search Results using Self-Attention Network,59a02ff0b161e8ad1a7b6eb9,Learning to Diversify Search Results via Subtopic Attention,"In the experiments we are using the same dataset as many previous diversification models(e.g.HxQuAD, PAMM-NTN, DSSA) which includes the Web Track dataset from TREC 2009 to 2012.###Nowadays both unsupervised and supervised explicit approaches are proposed e.g. xQuAD [7], PM2 [8], HxQuAD/HPM2 [9] and DSSA [14].###Inspired by previous work [14], we use the metric of α − nDCG@20 to tune the parameters.###Since the deep reinforced learning based models e.g. MDP-DIV [14] and M2DIV [15] are taking too much time to train, we do not take those models as baseline.###For example, the ERR-IA@5 of DESA and DSSA (doc2vec) is 0 .###As an explicit model, DSSA use the RNN and attention mechanism to select the best document satisfying the subtopics needed by the selected sequence.###xQuAD [7], PM2 [8], HxQuAD/HPM2 [9] and DSSA [14].###For a fair comparison, we are using the document relevance features and embeddings exactly the same as the DSSA, which have been released by Jiang et al. [14] in the repository on GitHub .###We use the same relevance features as the previous work [14] for xq and xqi , including BM25, TF-IDF, language model scores, Page Rank, the numbers of incoming links and outgoing links, et al.###Based on the reinforced learning approach MDP-DIV [14], Feng [15] proposed the M2DIV model with the Monte-Caro Tree Search (MCTS) to search a larger ranking space and minimize the gap between the local optimal and global optimal rankings.###Comparing the state-of-the-art supervised approach, DESA’s improvement over DSSA on 𝑎𝑙𝑝ℎ𝑎 − nDCG is about 3%.###We train the DSSA model with the code and data released by Jiang et al. on GitHub , and use the following optimized settings described in the work of DSSA: LSTM cells, max-pooling on subtopic attention, hidden size 50, doc2vec embedding dimension 100 and random permutation count 10 for the list-pairwise samples.###More details about these features can be found in [14] and we omit the details due to space limitation.###MDP-DIV [14] and M2DIV [15] are taking too much time to train, we do not take those models as baseline.###We calculate the total metric improvement of DESA comparing with DSSA.###DSSA [14] .###[14] in order to get enough training samples.###The result is denoted as DSSA (doc2vec).###We compare DESA with DSSA, the state-of-the-art greedy document sequential selection based model.###In the significance testing, DESA is compared with the DSSA as the SOTA explicit supervised model.",impact-revealing,reporting on dataset and model comparisons in diversification models
1149,,7e839c2667479d91e21e84583c27257dc7dc1a36,Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality,,,"###For every family of models we search over, we initialize the degrees of freedom such that training begins with a sampler matching DDPM with K substeps following Song et al. (2020); Nichol & Dhariwal (2021).###In particular, optimizing the DDIM sigma coefficients does not outperform the corresponding DDIM(η = 0) baseline on CIFAR10, which is not a surprising result as Song et al. (2020) show empirically that most choices of the σt degrees of freedom lead to worse FID scores than setting them all to 0.###Besides DDIM (Song et al., 2020), there have been more recent attempts at reducing the number of inference steps for DDPMs.###inference steps) through the denoising network (Song et al., 2020; Nichol & Dhariwal, 2021).###The success of our method hinges on searching a novel, wider family of Generalized Gaussian Diffusion Model (GGDM) than those identified in prior work (Song et al., 2020).###Given the importance of generation speed, recent work (Song et al., 2020; Chen et al., 2021a; Watson et al., 2021) has explored reducing the number of steps required for high quality sampling with pretrained diffusion models.###In particular, at inference time, DDPMs allow controlling the number of forward passes (a.k.a. inference steps) through the denoising network (Song et al., 2020; Nichol & Dhariwal, 2021).###Our class of parameteric samplers, which we call Generalized Gaussian Diffusion Model (GGDM), includes Denoising Diffusion Implicit Models (DDIM) (Song et al., 2020) as a special case and is motivated by the success of DDIM on fast sampling of diffusion models.###Besides DDIM (Song et al., 2020), there have been more recent attempts at reducing the number of inference steps for DDPMs. Jolicoeur-Martineau et al. (2021) proposed a dynamic step size SDE solver that can reduce the number of calls to the modeled score function to ∼ 150 on CIFAR10 (Krizhevsky et…###We start with a brief review on DDPM (Ho et al., 2020) and DDIM (Song et al., 2020).###Specifically, Song et al. (2020) note that is it possible to construct alternative ELBOs with only a subsequence of the original timesteps S ⊂ {1, ..., T} that shares the same marginals as the construction above (i.e., qS(xt|x0) = q(xt|x0) for every t ∈ S, so qS defines a faster sampler compatible…###The seminal work of Song et al. (2020) presents Denoising Diffusion Implicit Models (DDIM): a family of evidence lower bounds (ELBOs) with corresponding forward diffusion processes and samplers.###…...,xt) = q(x0)q(xT |x0) T−1∏ t=1 qσ(xt|xt+1,x0) (5)
and where the posteriors are defined as qσ(xt−1|xt,x0) = N ( xt−1 ∣∣∣∣√ᾱt−1x0 +√1− ᾱt−1 − σ2t · xt −√ᾱtx0√1− ᾱt , σ2t Id )
(6)
Song et al. (2020) empirically find that the extreme case of using all-zero variances (a.k.a. DDIM(η = 0))…",impact-revealing,highlighting the importance of the proposed method in relation to existing diffusion models
3817,5a260c0217c44a4ba8a1d1dd,28e4ae18a652e7d67df3e3fa6f4703ae9ef930e9,BestConfig: tapping the performance potential of systems via automatic configuration tuning,53e9a500b7602d9702e1ee68,Bigop: Generating Comprehensive Big Data Workloads As A Benchmarking Framework,"Besides, as the benchmarking community has proved theoretically and practically [2, 12, 30, 33, 38, 47], the long-running application workloads can be represented by some short-running workloads.",other,highlighting theoretical and practical insights from the benchmarking community
3441,5f7d893591e011346ad27d16,ff8a988d88ebe5d5ca116340baa34ae00ce011e8,PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation,58437722ac44360f1082ec27,Iterative Refinement for Machine Translation.,"Our proposed strategy substantially differs from prior solutions that rely on in-place word substitutions (Novak et al., 2016; Xia et al., 2017; Weston et al., 2018), as we leverage the seq2seq architecture to offer more ﬂexible edits.###Our proposed strategy substantially differs from prior solutions that rely on in-place word substitutions (Novak et al., 2016; Xia et al., 2017; Weston et al., 2018), as we leverage the seq2seq architecture to offer more flexible edits.",other,highlighting the differences between proposed strategy and prior solutions
2807,5db9297247c8f766461f6d13,9ec95c1130a6ac4238ac2e5c7b2b66047511ea92,long and diverse text generation with planning-based hierarchical variational model,53e9b196b7602d9703c236ee,Inducing Document Plans for Concept-to-Text Generation.,Konstas and Lapata (2013) proposed to plan content organization with grammar rules while Puduppully et al. (2019) planned by reordering input data.,other,reporting existing methods for content organization
1743,,e264cc9b89483f812fe1d1d04c6cd4056ae25a13,Global biogeography of coral recruitment: tropical decline and subtropical increase,,,"###LITs, typically selected by investigators (Harrison et al. 1984, Hughes et al. 1999), allow re - searchers to capture the cumulative recruitment in corals that utilize different reproductive strategies, including those that spawn, usually annually (‘spawners’), and corals that release offspring…###SITs were not widely used by the research community until synchronized mass spawning of corals was described in the early 1980s (Harrison et al. 1984).",impact-revealing,highlighting the historical context and significance of LITs in coral research
42,5ec49a639fced0a24b4de7ed,1eed0659d561354d5c471a723cf3381430561d04,Graph Neural News Recommendation with Unsupervised Preference Disentanglement,5bdc315017c44a1f58a05ba9,Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement.,"According to (Yang et al., 2018), the mutual information maximization can be converted to the following form.",impact-revealing,reporting a method from prior work
2342,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",53e9a4ebb7602d9702e0db14,Probabilistic Graphical Models: Principles and Techniques,"3) Graphical models: The focus in this area is on inference and learning from large datasets, [26], [27], [28], [29], [30].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2379,5ac1827b17c44a1fda915855,f96a5a9cfa1dbb01df6df749e093c0bad20240a8,Control Flow Checking at Virtual Edges.,53e9b51bb7602d970404e0d7,The Geometry Of Innocent Flesh On The Bone: Return-Into-Libc Without Function Calls (On The X86),"Moreover, the new attacks generalize the original return-into-libc attack by allowing the attacker arbitrary computation without calling any functions [8].",other,highlighting the generalization of new attacks in cybersecurity
3907,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",557c689cf66765fbb46b267c,Diffusion in Social Networks as SIS Epidemics: Beyond Full Mixing and Complete Graphs,"To study these processes [18], [19] one usually considers one of two asymptotic regimes: (1) long term behavior (time-asymptotics), attempting to find the equilibrium distribution of the process [20], [21]; or (2) large network asymptotics (mean-field approximation) [22] leading",other,describing asymptotic regimes in process studies
1804,,e3265afd4eb6f9b6c936a03dfcc165fe86526842,Glycemic control in adolescents with type 1 diabetes: Are computerized simulations effective learning tools?,,,"###12,13 To promote mechanistic reasoning, we propose to build upon previous research regarding the value of learning from a complex systems perspective.",impact-revealing,suggesting a foundation for mechanistic reasoning based on prior research
2806,53e9ad3bb7602d970372109a,8686368908956c506f6e3bbcaa2810adfda14914,NoC-sprinting: Interconnect for fine-grained sprinting in the dark silicon era,53e9bbc2b7602d970480ea61,Nord: Node-Router Decoupling For Effective Power-Gating Of On-Chip Routers,"Therefore, some complimentary techniques such as bypass paths [4] can be leveraged to avoid completely isolating cache banks from the network.###Recently researchers have proposed various schemes [4,5,14,18] to mitigate the latency overhead caused by frequent router wake-up.",other,acknowledge existing techniques for mitigating latency overhead
1723,,eca014d202a39fb1446c1ceeeae0457e8ef38ebd,Throughput rates of Simple Operations when Scaling-out with RDBMS and NoSQL Databases,,,"###However, as more machines are involved, the time needed to contact all the master machines rises and thus less write operations per second can be handled per machine[24].###However, it is shown that in either case the maximum possible throughput rate of write operation decreases dramatically when more machines are added[24].###However, Gray showed in 1996 that this model leads to enormous conflicts and deadlock situations [24].###In fact, the occurrence of conflicts seems to increase a thousand-fold if the number of machines and workload increases a ten-fold[24].###Because write operations are still handled at a single machine, it can not scale in write load [20, 24].###This was already shown by Gray in 1996[24].###However, such a system is hardly used in industry as the consensus is that it imposes even more computational overhead to the system than the asynchronous method[24].###We found that scaling-up is limited by hardware developments[28] and that the widely known RDBMS databases can not scale-out to an unlimited number of machines[24].###From Gray[24] we know that RDBMS systems do not scale-out well when it comes to write operations (as discussed in Section 2.###We found that scaling-up is limited by hardware developments[28] and that the widely known RDBMS databases can not scaleout unlimited[24].",impact-revealing,highlighting limitations in scaling RDBMS systems for write operations
1229,,542224f42044067eae11527d43d322f728889ddc,Clinical Pharmacokinetics of Verapamil,,,"###Clinical Pharmacokinetics of Verapamil
Verapamil is a calcium channel blocking drug (Fleckenstein, 1977) which is widely used as an antiarrhythmic agent to control supraventricular tachyarrhythmias (Krikler and Spurrel, 1974; Singh et a!., 1980).",impact-revealing,providing context on the use and application of Verapamil in clinical pharmacokinetics
107,5ede0553e06a4c1b26a83f63,1d81e7f428fea2b2e15ee3a96fe843ca603acc4c,Simple and Deep Graph Convolutional Networks,5ce2d032ced107d4c635260c,Predict then Propagate: Graph Neural Networks meet Personalized PageRank,"(Klicpera et al., 2019a) also proposes APPNP, which with an approximation derived by a truncated power iteration.###Finally, we recall that APPNP (Klicpera et al., 2019a) employs a similar approach to the initial residual connection in the context of Personalized PageRank.###We also include three state-of-the-art shallow models: GCN (Kipf & Welling, 2017), GAT (Veli ˇ ckovi ´ c et al., 2018) and APPNP (Klicpera et al., 2019a).###GDC (Klicpera et al., 2019b) further extends APPNP by generalizing Personalized PageRank (Page et al., 1999) to an arbitrary graph diffusion process.###(Klicpera et al., 2019a) uses Personalized PageR-ank to derive a ﬁxed ﬁlter of order K .###PPNP and APPNP (Klicpera et al., 2019a) replace the power of the graph convolution matrix with the Personalized PageRank matrix to solve the over-smoothing problem.###For example, APPNP (Klicpera et al., 2019a) and GDC (Klicpera et al., 2019b) set θ i = α (1 − α ) i for some constant 0   α   1 .###By decoupling feature transformation and propagation, PPNP and APPNP can aggregate information from multi-hop neighbors without increasing the number of layers in the neural network.###• It has been observed that frequent interaction between different dimensions of the feature matrix (Klicpera et al., 2019a) degrades the performance of the model in semi-supervised tasks.###However, (Klicpera et al., 2019a) also shows that performing multiple non-linearity operations to the feature matrix will lead to over-ﬁtting and thus results in the performance drop.",impact-revealing,reporting on various models and their approaches in graph convolution
3611,5e5e19c093d709897ce87ab9,9b240a87b11d085641d6640f73cc3cc2d678e305,automatically scheduling halide image processing pipelines,5550484945ce0a409eb6d218,Opentuner: An Extensible Framework For Program Autotuning,A more general Halide auto-tuner was later implemented within the OpenTuner framework [Ansel et al. 2014].,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1939,,86abd275d0667c2e9aef813f81bee649c297460b,The Stability of Literacy-Related Cognitive Contributions to Chinese Character Naming and Reading Fluency,,,"###The cognitive measures were developed from tasks in the previous research, specifically, phonological awareness from Lei et al. (2011) and Shu et al. (2006), morphological awareness and STM from Shu et al. (2006), orthographic awareness from Li et al. (2010).###The results replicated the findings by Shu et al. (2006) and Tong and McBride-Chang (2010) that morphological awareness is a core ability for Chinese character naming.###The task was carried out by following the same method as in Shu et al. (2006).###…(pseudo) 0.23 4.79 0.001
Morphological production 0.24 5.33 0.001
Reading fluency 0.10
RAN −0.13 −2.65 0.008 Digit STM 0.10 2.02 0.04
Morphological production 0.12 2.23 0.03
cognitive constructs are usually significantly correlated with literacy (e.g., Hulme et al. 2007; Shu et al. 2006).###Connected-text fluency has been neglected as a component of reading skill in most previous studies (e.g., Li et al. 2010; Shu et al. 2006), and less explored as a component of reading skill in Chinese literacy development.###Overall, according to the SEM model (Table 5), present further results demonstrated the importance of PA in word reading, which is consistent with previous findings (e.g., Ho et al. 2004; McBride-Chang et al. 2003; Shu et al. 2006; Snowling et al. 1997; Torgesen et al. 1997).###The present results are converging with previous research (e.g., McBride-Chang et al. 2003; Shu et al. 2006), in that morphological awareness predicted unique variance in Chinese character naming across grades, after controlling for phonological awareness and other cognitive skills.###The RAN was measured using the classical digit naming as in the previous research (e.g. Ho and Lai 1999a; Shu et al. 2006).###Shu et al. (2006) examined the fifth- and sixthgraders, while Shu et al. (2008) tested on preschool children.###It was measured using the digit naming test as in the traditional research (e.g. Ho and Lai 1999a; Shu et al. 2006).###It is found that morphological awareness is relatively more important in Chinese literacy (McBride-Chang et al. 2003, 2005; Shu et al. 2006; Wang et al. 2006).",impact-revealing,reporting findings on cognitive measures and their significance in literacy
181,5f7aeb7691e011983cc81e80,572c12e81319ccd47cc0c637c82efadd03fd05ab,Autoregressive entity retrieval,5b8c9f4a17c44af36f8b72cf,End-to-End Neural Entity Linking.,"Following Kolitsas et al. (2018), we considered only mentions that have entities in the KB (i.e., Wikipedia) and we used their candidate sets with the additions of the table computed by Hoffart et al. (2011).###Again, following previous works (Kolitsas et al., 2018), we considered only mentions that have entities in Wikipedia.###End-to-End Entity Linking (EL) For EL, we reproduce the setting of Kolitsas et al. (2018) us-ing the same in-domain and out-of-domain datasets as well as evaluating the InKB micro-F 1 on the GERBIL benchmark platform (R ¨ oder et al., 2018).",impact-revealing,acknowledge methodology and dataset used in entity linking
2337,58d82fcbd649053542fd5d36,81db3f78f346eecf2f378070712feade6d45d6b1,MOLIERE: Automatic Biomedical Hypothesis Generation System,55d06502696322190568a222,A network comprising short and long noncoding RNAs and RNA helicase controls mouse retina architecture,"development and progression mainly through regulation of the Wnt signaling pathway [13, 50] and associated regulation of Cellcell and Cell-matrix adhesion, tumor cells invasion, and metastasis [12, 24, 43, 48].",other,highlighting the role of the Wnt signaling pathway in tumor progression
228,5db92a1a47c8f76646200974,8330c7c98c6c3e338ca578e0ab47f41bc18d0019,HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs,5bdc31b417c44a1f58a0b894,Hypergraph neural networks,"Hypergraph neural networks [17] and their variants [23, 24] use the clique expansion to extend GCNs for hypergraphs.",impact-revealing,providing context on hypergraph neural networks and their methods
2782,5db1765a3a55ac101c887e97,6c4b76232bb72897685d19b3d264c6ee3005bc2b,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,53e99d8eb7602d970264b2fd,Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning.,"• Sentence completion (COPA [Roemmele et al., 2011])###…al., 2016], RTE [Dagan et al., 2005], CB [De Marneff et al., 2019])
• Coreference resolution (WNLI and WSC [Levesque et al., 2012])
• Sentence completion (COPA [Roemmele et al., 2011])
• Word sense disambiguation (WIC [Pilehvar and Camacho-Collados, 2018])
• Question answering (MultiRC [Khashabi…",other,acknowledge various tasks in NLP
3061,5ee7495191e01198a507f6a4,d2599ccb2401198b5e6e1d867c7d0f22b5055f5e,CausalVAE: Disentangled Representation Learning via Neural Structural Causal Models,53e9b2eab7602d9703da3451,On the identifiability of the post-nonlinear causal model,"Discovering the causal graph from pure observations has attracted large amounts of attention in the past decades [7, 33, 28].",other,highlighting the growing interest in causal graph discovery
1980,,b0b57799c1832c79c1a696773f987dc55d26b542,Semidefinite Programming Approaches to Network Clustering and Smoothing,,,"###, 2011; Karrer and Newman, 2011), partitioning methods (Lancichinetti and Fortunato, 2011; Newman, 2006), semidefinite programming methods (Amini and Levina, 2014; Cai and Li, 2015; Chen, Li, et al., 2016; Guédon and Vershynin, 2016), and spectral clustering (Amini, Chen, et al.###In current studies of SDP approaches for cluster analysis, ADMM algorithms for solving SDPs require a full spectral decomposition of a symmetric matrix in each iteration of ADMM (Amini and Levina, 2014; Cai and Li, 2015; Chen, Li, et al., 2016).###To create community structure, the notions of weak and strong assortativity are commonly assumed in the SBM literature (Abbe et al., 2016; Amini and Levina, 2014; Cai and Li, 2015).###However, this will introduce an additional tuning parameter and seems only necessary for theoretical analysis (see Remark 2.2 in Cai and Li, 2015).###Hence, many existing SDPs for network community detection including those proposed in Amini and Levina (2014), Cai and Li (2015), and
12
Chen and Xu (2016) are able to achieve arbitrary relative accuracy with high probability for sparse networks with bounded expected degrees under strongly…###ADMM is now widely used in many sparse and low-rank optimization problems (e.g., Amini and Levina, 2014; Cai and Li, 2015; Chen, Li, et al., 2016; Vu et al., 2013).###Recently, semidefinite programming approaches have been proposed for solving computationally intractable likelihood optimization problems (Amini and Levina, 2014; Cai and Li, 2015; Guédon and Vershynin, 2016) and partitioning problems (Chen, Li, et al., 2016).###Recently, semidefinite programming approaches have been proposed for solving computationally intractable likelihood optimization problems (Amini and Levina, 2014; Cai and Li, 2015; Guédon and Vershynin, 2016) and partitioning problems (Chen, Li, et al.###…and Newman, 2011), partitioning methods (Lancichinetti and Fortunato, 2011; Newman, 2006), semidefinite programming methods (Amini and Levina, 2014; Cai and Li, 2015; Chen, Li, et al., 2016; Guédon and Vershynin, 2016), and spectral clustering (Amini, Chen, et al., 2013; Lei and Rinaldo, 2015;…###The main part of the algorithm is an alternating direction method of multipliers (ADMM) algorithm.###The Semidefinite Program and its General Analysis Framework . . . . . 20
2.1 Problem Setup and A Semidefinite Program . . . . . . . . . . . . . 21 2.2 Optimality Conditions . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.3 A Sufficient Condition for Fisher Consistency of the SDP . . . . . 29 2.4 An Efficient Algorithm for Solving the SDP . . . . . . . . . . . . . 31
vii
2.4.1 The ADMM Algorithm . . . . . . . . . . . . . . . . . . . . 31 2.4.2 Frank-Wolfe Algorithm for Solving the Subproblem . . . . . 32 2.4.3 A Toy Example . . . . . . . . . . . . . . . . . . . . . . . . 35
3.###Are there any differences in performance of network community detection between SDP-BF and other SDPs under various scenarios? Furthermore, all existing SDPs for community detection use the adjacency matrix as the input matrix (Amini and Levina, 2014; Cai and Li, 2015; Chen, Li, et al., 2016; Guédon and Vershynin, 2016).###To show exact recovery, we adopt the commonly used technique, namely primal-dual witness construction (see, for exmaple, Amini and Levina, 2014; Awasthi et al., 2015; Cai and Li, 2015; Lei and Vu, 2015).###In Cai and Li (2015), the authors have shown that if the expected degrees grow no slower than log n, then X̂ = X0 with high probability under strongly assortative SBMs if the gap between within-community edge probabilities and between-community edge probabilities is sufficiently large.###On the other hand, the majority of existing SDP methods for network community detection are motivated by the likelihood maximization problem under the SBM and rely on SDP relaxations of the set of clustering matrices (Amini and Levina, 2014; Cai and Li, 2015; Guédon and Vershynin, 2016).###In Cai and Li (2015), the authors further modified the objective function by penalizing the trace to control the possible outliers.###In contrast, our ADMM algorithm only requires to compute the leading K−1 eigenvectors of symmetric matrices in each iteration of ADMM.###32 2 Frank-Wolfe algorithm for solving the subproblem (2.15) in ADMM .###Let us introduce the following SDP relaxation of (1.5), which is essentially the SDP proposed in Cai and Li (2015): maximize ⟨A− λEn, X⟩
subject to X ⪰ 0, 0 ≤ X ≤ 1 .###1 ADMM algorithm for solving SDP-BF . . . . . . . . . . . . . . . .###Given X and B, in matrix formulation, the likelihood maximization is equivalent to (see Amini and Levina, 2014; Cai and Li, 2015, for detail)
maximize ⟨A− λEn, X⟩ subject to X ∈ X , (1.5)
10
where λ := log(1−q)−log(1−p) log p−log q+log(1−q)−log(1−p) > 0.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1426,,33c1087b86025c7bc919f2fda817a491c0c350ab,Beyond Tabula Rasa: Reincarnating Reinforcement Learning,,,"###We tried two different values of CQL coefﬁcient λ ( 0 .###To do so, we use CQL [46], a widely used offline RL algorithm, which jointly minimizes the TD and behavior cloning on logged transitions in DT (Equation A.###However, ﬁne-tuning degrades performance with 1 -step returns, which is more pronounced with higher values of CQL loss coefﬁcient (Figure A.12).###• RL Pretraining : We use CQL, which optimizes the following loss: The choice of CQL is motivated by its simplicity as well as recent ﬁndings that ofﬂine RL methods that do not estimate the behavior policy are more suited for online ﬁne-tuning [58].###To do so, we use CQL [46], a widely used ofﬂine RL algorithm, which jointly minimizes the TD and behavior cloning on logged transitions in D T (Equation A.3).",impact-revealing,describing the use of a widely used offline RL algorithm
1119,,29f68790aeb8d4f0642660166e52dba3373bb1b6,ReNoise: Real Image Inversion Through Iterative Noising,,,"###For all models, we utilize the DDIM [44] sampler.###However, as we show in this paper, using current methods for the inversion of an image with a small number of steps degrades the reconstruction quality in terms of accuracy [12, 44] or editability [21, 51].###Specifically, they improve the reconstruction accuracy of DDIM inversion [44] with Stable Diffusion [40] without introducing a significant computational overhead.###Methods that approach the deterministic inversion commonly rely on the DDIM sampling method [44], and build upon DDIM inversion [12, 44].###Specifically, we compare DDIM Inversion [44] with one renoising iteration to Null-Text Inversion (NTI)[32] and Negative-Prompt Inversion (NPI)[31].###A common approach among these methods requires inversion [21, 32, 44, 50] to edit real images, i.e., obtaining a latent code z T such that denoising it with the pretrained diffusion model returns the original image.###This inversion depends on the sampler algorithm used during inference, which can be deterministic [44] or non-deterministic [20, 22].###We build upon the commonly used approach of reversing the diffusion sampling process, which is based on the linearity assumption that the direction from z t to z t +1 can be approximated by the negation of the direction from z t to z t − 1 [12, 44] (see Figure 2).",impact-revealing,highlighting the importance of DDIM inversion methods in image reconstruction
1074,,e0f42e0b573cd3c46c888d28c385d93ac96fd90d,Soft Cardinality in Semantic Text Processing: Experience of the SemEval International Competitions,,,"###Our 2012 approach was extended by building an additional similarity function for sentences using nPMI [14] as the comparator of words.###normalized Levenshtein similarity, nPMI [14], normalized path length in WordNet [15], etc.",impact-revealing,reporting an extension of a previous approach
2993,5f75eed591e0111c1eb4da5f,400389ca8b23ff77fa9ee96717fe6447df7469af,Uncertainty-Matching Graph Neural Networks to Defend Against Poisoning Attacks,5a260c8117c44a4ba8a30f54,Graph Attention Networks.,"As discussed earlier, RGCN provides only a marginal improvement over the vanilla GCN and GAT.###…i its feature representation (in the input layer For example, in a standard graph convolutional network (GCN), Here, the message computation is parameterized by α ij , which can be a symmetric normalization constant Kipf and Welling (2017) or a learnable attention weight Veliˇckovi´c et al. (2018).###In addition, we set the number of attention heads to 8 for GAT.###GAT (Veličković et al. 2018): This model uses a multi-head attention mechanism to learn the hidden representations for each node through a weighted aggregation of features in a closed neighborhood where the weights are trainable.###In comparison, GAT appears to be the most sensitive to random structural perturbations and its low performance strongly corroborates with the ﬁndings in Zhu et al. (2019).###We set the number of hidden neurons to 16 for both GCN and GAT baselines.###GAT Veli ˇ ckovi ´ c et al. (2018): This model uses a multi-head attention mechanism to learn the hidden representations for each node through a weighted aggregation of features in a closed neighborhood where the weights are trainable.###While there exist very few GNN formulations for speciﬁcally defending against adversarial attacks, the recent robust GCN (RGCN) approach Zhu et al. (2019) has been the most effective, when compared to standard GCN and GAT models.###RGCN Zhu et al. (2019): This is a recently proposed approach that explicitly enhances the robustness of GCNs. RGCN models node features as distributions as opposed to deterministic vectors in GCN and GAT models.###However, the large variance makes GAT unreliable in practice, particularly when the attack is severe.###Interestingly, under this attack, both GCN and RGCN perform poorly when compared to the GAT model.###We follow the typical transductive node classification setup (Kipf and Welling 2017; Veličković et al. 2018), while using the standard train, test, and validation splits for our experiments (see Table 1).###Here, the message computation is parameterized by αij , which can be a symmetric normalization constant (Kipf and Welling 2017) or a learnable attention weight (Veličković et al. 2018).",other,discussing the performance and characteristics of various graph neural network models
1945,,46938a85a8e208a040ef7c3e87d0d0b65ad98111,Automated Assessment of Inferences Using Pre-Trained Language Models,,,"###The stimulus text in Korean consisted of 10 sentences Three evaluators individually assessed each sentence–response pair according to the nine inference types de ﬁ ned in a previous study [21] (summarized in Table 1). taken from an elementary school reading textbook.###Specifically, this study was motivated by the fact that inference has been shown to play a key role in reading comprehension, serving as a critical component in the construction of meaning from text [19–21].###Previous research emphasizes its importance not only for comprehending literal content, but also for engaging with the text at a deeper level [20], allowing for the application of prior knowledge and the anticipation of subsequent narrative developments [21].",impact-revealing,highlighting the significance of inference in reading comprehension
2811,5e3940c73a55ace46ed436d2,845b4941d8c016aa5f8967da2f86d38ef6c18fa3,a survey on knowledge graphs representation acquisition and applications,599c7b59601a182cd272be4e,Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs.,"Know-evolve [141], a deep evolutionary knowledge network, investigates the knowledge evolution phenomenon of entities and their evolved relations.",other,reporting on a specific deep learning model for knowledge evolution
3026,5d3044363a55ac8b59feaf5b,31f8b4ca13c0b333324044bd96aac20c3d0b67bd,JumpSwitches: Restoring the Performance of Indirect Branches In the Era of Spectre,53e9afbab7602d9703a09d05,Fast Dynamic Binary Translation For The Kernel,"While binary translators [18, 27, 28] and JIT compilers [25, 33, 44] are capable of collecting branch targets at runtime, binary translators result in a significant overhead as they are designed to translate and instrument the entire binary, and may need to restrict the kernel’s functionality for best performance [27].###while restricting functionality, obviating the benefits of indirect call promotion [27].###retpolines [49] medium yes none EIBRS [48] low yes none PGO/FDO [54] low w/retpoline static DBT/JIT [27] high w/retpoline dynamic JumpSwitches low dynamic dynamic+semantic",other,highlighting the performance trade-offs of binary translators and JIT compilers
2981,5ecbc8eb9fced0a24b52a39b,2b9514be4679f68f97bbcf9671053ac2f03df2e4,Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning,5e7495c591e0111c7cee13c2,Backdooring and Poisoning Neural Networks with Image-Scaling Attacks,"In a concurrent work [24], we study the application for the poisoning scenario.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1847,,a2467bb2dc4b3ce3ec38cd4a2b4f17af05f23d16,On the relationships between models in protocol verification (extended version),,,"###The second difference is that [24] uses a combination of send and receive events, denoted a→ b : m, which at first sight looks like a synchronous communication model (where, in particular, the intruder cannot intercept messages), but this is not the case.###The second model is inspired by Paulson’s approach of message traces based on the Isabelle theorem prover [24].###There are further minor differences: in [24], we have two functions synth and analz that represent each a part of the DY closure, namely the synthesis and analysis rules, and instead of DY(IK ) the closure synth(analz (IK )) is employed, which in general is a proper subset of DY(IK ) (only when the protocol uses only atomic keys is executed in a typed model, these closures are equal).###Also, the way we use the set of permitted instances of agent names is not present in the rules of [24]; this is due to the fact that it does not need to be repeated in each step when given a concrete protocol, but necessary when we have an arbitrary protocol as a message pattern.###This formalization is close to the one of [24], but there are some differences.###However, there exists a wide variety of empirically succesful analysis techniques based on the models presented in this paper [7, 9, 10, 11, 13, 18, 25, 24, 2].###Besides a better understanding of the employed models, these results also pave the way for combining methods based on these models, in particular, connecting automated verification procedures with a formalization in the theorem prover Isabelle in the style of [24].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3283,599c797a601a182cd2642797,668db48c6a79826456341680ee1175dfc4cced71,Get To The Point: Summarization with Pointer-Generator Networks,53e99822b7602d9702041d96,Statistical Machine Translation,"Originating from Statistical Machine Translation (Koehn, 2009), coverage was adapted for NMT by Tu et al. (2016) and Mi et al. (2016), who both use a GRU to update the coverage vector each step.###Originating from Statistical Machine Translation (Koehn, 2009), coverage was adapted for NMT by Tu et al.",other,acknowledge adaptation of coverage in neural machine translation
1377,,9495aa05daf8b4267e9029258ed59f09fe126d49,A Cloud Eco-System: Reactive Demand Control and Dynamic Pricing Methodology,,,"###Program decomposition, distributed storage and computation are the fundamental techniques for parallel computing, which is widely adopted as a solution in big data processing and complex computing [39].",impact-revealing,highlighting fundamental techniques in parallel computing
3332,5cf48a3cda56291d5829eb69,a0852cd9a026bc90168fa85fa422cb0e48f98394,Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss,5c89e5304895d9cbc600f856,Out Of Time: Automated Lip Sync In The Wild,"All of them are trained on LRW dataset while Chung et al. [3] require extra VGG-M network pretrained on VGG Face dataset [25] and Wilels et al. [35] need extra MFCC feature extractor pretrained by [5].###For example, in web videos [5, 24] (e.g., LRW and VoxCeleb datasets), speakers move signiﬁcantly when they are talking.###We evaluate our model along with state-of-the-art methods on several popular datasets (e.g., GRID [6], LRW [5], VoxCeleb [24] and TCD [13]).###The ground truth videos are selected from different sources: we randomly select samples from the testing set of LRW [5], VoxCeleb [24], TCD [13], GRID [6] and real-world samples from YouTube (in total 38 videos).",other,reporting on datasets and evaluation methods used
363,58437722ac44360f1082f135,6729b698fc28d191f3006690d973974a13c2f501,Hiding Individuals and Communities in a Social Network,53e9b77db7602d970432385f,You are who you know: inferring user profiles in online social networks,"[24] demonstrated how, by analysing Facebook’s social network structure, as well as the attributes of some users, it is possible to infer otherwise-private information about other Facebook users.",impact-revealing,highlighting the implications of social network analysis on privacy
1139,,b9da81759f78eae50e191e88cfdbff86651a4308,DocDiff: Document Enhancement via Residual Diffusion Models,,,"###Followed by [10, 38], the HRR module executes the forward noise-adding process and the reverse denoising process to model the residual distributions.###Recently, Diffusion Probabilistic Models (DPMs) [10, 38] have been widely adopted for conditional image generation [18, 21, 33–35, 37, 43? ? ].###Followed by [38], we perform the deterministic reverse process q (xt−1 | xt , x0) with zero variance and the mean can be computed:",impact-revealing,describing the process of the HRR module and its application in image generation
2550,5e85c28491e0114016e821a6,21b11793b960a3e37c0eab7aae6127c28fd38e5c,Code Prediction by Feeding Trees to Transformers,57d063b9ac4436735428e58b,PHOG: Probabilistic Model for Code.,"Researchers have also investigated using the syntactic struc­ ture of code for prediction, as opposed to seeing code as text: both using probabilistic graphical models (probabilis­ tic context-free grammars [11] and probabilistic higher-order grammars [12]-[14]), as well as using deep learning [15].",other,highlighting research on syntactic structure in code prediction
3218,5f1022a091e01168a7d6fc4f,04e234c7e4f300b4e8a16370728d875f9f484b39,learning visual context by comparison,599c7949601a182cd262c3d6,ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on   Weakly-Supervised Classification and Localization of Common Thorax Diseases,"We demonstrate the effectiveness of ACM on three chest X-ray datasets [37] and COCO detection & segmentation dataset [24] with various architectures.###Chest X-ray14 [37] dataset is the first largescale dataset on 14 common diseases in chest X-rays.###We evaluate ACM in several datasets: internally-sourced Emergency-Pneumothorax (Em-Ptx) and Nodule (Ndl) datasets for lesion localization in chest X-rays, Chest X-ray14 [37] dataset for multi-label classification, and COCO 2017 [24] dataset for object detection and instance segmentation.###Recent releases of the large-scale chest X-ray datasets [37,18,19,3] showed that commonly occurring diseases can be classified and located in a weakly-supervised multi-label classification framework.###[37] ResNet-50 CE 1,024 74.###We split the dataset into training (70%), validation (10%), and test (20%) sets, following previous works [37,41].###With the recent presence of large-scale chest X-ray datasets [37,18,19,3], there has been a long line of works that find thoracic diseases from chest X-rays using deep learning [41,12,23,29].###ACM is validated over three chest X-ray datasets [37] and object detection & segmentation in COCO dataset [24] with various backbones such as ResNet [14], ResNeXt [40] or DenseNet [16].",other,demonstrating the effectiveness of ACM on various datasets
1709,,d71addf972e7928cf701d9f108de499cbb2982a5,"The Impact of Drivers’ Race, Gender, and Age During Traffic Stops",,,"###These profiles are reinforced by social identity theory (Hinton, 1993; Tajfel & Turner, 1979), the illusory correlation (Chapman, 1967), and the ecological fallacy (Robinson, 1950).",impact-revealing,providing theoretical context for social identity profiles
881,53e99967b7602d97021ac42b,41721de035c15528a7e35d3ab4d79b053633d763,Feedback-directed memory disambiguation through store distance analysis,53e9a6edb7602d9703026964,Instruction Based Memory Distance Analysis and its Application,"[9] have proposed a feedback-directed memory scheme which use memory distance prediction to determine whether or not to speculate a load instruction.###[9] define memory distance to include reuse distance, access distance and value distance.###Both whole-program [7, 28] and instructionbased [8, 9, 13] reuse distances have been predicted accurately across all program inputs using a few profiling runs.###[9] define the access distance of a load as the number of memory references between the load and the previous store to the same memory address.###[9] can be applied to enhance our mechanism by predicting store distances.###For the access distance based scheme [9] as described in Section 2.###[9] have observed that over 80% of the memory instructions in SPEC CPU2000 have constant memory distances.###[8, 9] introduce the notion of memory distance to encompass reuse distance, access distance and value distance.###Our experimental evaluations indicate that the store distance based method performs much better than the previous access distance based memory disambiguation scheme [9], and yields performance very close to perfect memory disambiguation, which uses exact knowledge of memory dependences for each dynamic load instance.###[9] propose a feedback-directed mechanism based upon access distance and value distance to determine whether or not to speculate a load.",impact-revealing,reporting on memory distance prediction methods and their applications
1305,,c57f64e2e4877e7d35b7fa0d2cb5acceef64d965,Culture's Consequences on Student Motivation: Capturing Cross-Cultural Universality and Variability Through Personal Investment Theory,,,"###A “cultural” imagination would behoove researchers to include other constructs in their studies and not just those derived from Western models (see King & Watkins, 2013, for the original formulation). We believe that PI theory could offer a unifying framework wherein constructs from Western theorizing could be studied in combination with more emically derived psychological processes. We are not arguing that the key motivational constructs identified by contemporary theories of motivation are unilaterally inapplicable to other non-Western settings. However, our claim is that what these theories deem to be central or focal might not occupy such a crucial role in other non-Western cultures. In a reversal of sorts, researchers in social psychology have examined how the culturally derived notion of filial piety, which is common among Chinese populations, is applicable to Western settings. They found that filial piety can also be found in Western populations as demonstrated through the responses to a survey. Face is another construct that is embedded deeply in East Asian social life, which generally refers to the integrity of one’s moral character and the way one’s public image is perceived (Gao, TingToomey, & Gudykunst, 1996). Face is more salient in Eastern compared to Western cultures. However, some scholars have emphasized the universality of this construct (Ho, 1976), and some researchers argue that it is also relevant in Western social interactions (Agassi & Jarvie, 1969). Recently, Zane and Yeh (2002) developed the Loss of Face Scale and administered it to both Chinese and European Americans.###In fact, many studies carried out by the authors have also used the imposed etic approach (e.g., King, Ganotice, & Watkins, 2012a, 2012b; King & Watkins, 2012b; McInerney, Roche, McInerney, & Marsh, 1997; Watkins, McInerney, & Boholst, 2003; Watkins, McInerney, & Lee, 2002). However, relying exclusively on this approach can prove to be a Procrustean bed for motivational psychologists whose horizons are unnecessarily constricted by constructs and models derived from Western theorizing. There is a need to complement the imposed etic approach with the emic approach, which involves in-depth studies within various cultures. In the emic phase of research, psychologists attempt to understand the cultures in their own terms. Instead of relying exclusively on Western-derived questionnaires and models, they conduct interviews, archival analysis, and ethnographic studies relying on key cultural informants. Li’s (2002) cultural model of learning for the Chinese is an example of knowledge generated through the use of the emic approach.###The first type of cross-cultural difference relates to how meanings are construed across different cultures. Although most mainstream researchers may unilaterally assume that abstract psychological constructs mean the same thing across all cultures and eschew the thick description favored by ethnographers, anthropologists, and indigenous psychological researchers, researchers using in-depth qualitative methods have found that some of the most prominent and well-studied psychological constructs in mainstream psychology actually take on different shades of meaning in various cultures. For example, cross-cultural differences in what IQ or intelligence means for different societies have emerged (Sternberg, 2004). In Africa, obedience is seen as a part of intelligence, but this is not the case in the West, which emphasizes the primacy of cognitive processes. S.-Y. Yang and Sternberg (1997) have also found that the Chinese conception of an intelligent person is different from the American conception.###However, some scholars have emphasized the universality of this construct (Ho, 1976), and some researchers argue that it is also relevant in Western social interactions (Agassi & Jarvie, 1969).###Subjective culture has been defined as the “how and why we behave in certain ways, how we perceive reality, what we believe to be true, what we build and create, and what we accept as good and desirable” (Westby, 1993, p. 9). It refers to the set of values, beliefs, and traditions that influence the behaviors of a social group and as it pertains to a society’s characteristic way of perceiving and interacting with the social environment. Triandis (2002) emphasized the importance of looking at both emic (culture-specific) and etic (universal) aspects when studying subjective culture.###The first type of cross-cultural difference relates to how meanings are construed across different cultures. Although most mainstream researchers may unilaterally assume that abstract psychological constructs mean the same thing across all cultures and eschew the thick description favored by ethnographers, anthropologists, and indigenous psychological researchers, researchers using in-depth qualitative methods have found that some of the most prominent and well-studied psychological constructs in mainstream psychology actually take on different shades of meaning in various cultures. For example, cross-cultural differences in what IQ or intelligence means for different societies have emerged (Sternberg, 2004). In Africa, obedience is seen as a part of intelligence, but this is not the case in the West, which emphasizes the primacy of cognitive processes. S.-Y. Yang and Sternberg (1997) have also found that the Chinese conception of an intelligent person is different from the American conception. In Chinese societies, intelligence is not only associated with general cognitive ability as it is in the United States, but also it has interpersonal (good at understanding and empathizing with others) and intrapersonal (knows the meaning of his or her own life) aspects. Li (2002) found that the meaning of something as common as “learning” also varies across cultures.###Despite the constructs of face and filial piety being found to be widely applicable (e.g., Ho, 1976; Zane & Yeh, 2002), they are clearly less central notions in the Western psyche.###The first type of cross-cultural difference relates to how meanings are construed across different cultures. Although most mainstream researchers may unilaterally assume that abstract psychological constructs mean the same thing across all cultures and eschew the thick description favored by ethnographers, anthropologists, and indigenous psychological researchers, researchers using in-depth qualitative methods have found that some of the most prominent and well-studied psychological constructs in mainstream psychology actually take on different shades of meaning in various cultures. For example, cross-cultural differences in what IQ or intelligence means for different societies have emerged (Sternberg, 2004). In Africa, obedience is seen as a part of intelligence, but this is not the case in the West, which emphasizes the primacy of cognitive processes. S.-Y. Yang and Sternberg (1997) have also found that the Chinese conception of an intelligent person is different from the American conception. In Chinese societies, intelligence is not only associated with general cognitive ability as it is in the United States, but also it has interpersonal (good at understanding and empathizing with others) and intrapersonal (knows the meaning of his or her own life) aspects. Li (2002) found that the meaning of something as common as “learning” also varies across cultures. She used prototype analysis in order to uncover what “learning” means in Chinese and American contexts. By collecting free associations of words and phrases related to learning among Chinese and Anglo-American participants and later subjecting them to cluster analysis, she found that there was little conceptual overlap in terms of how learning was construed in the two cultures. In the United States, elaborated conceptions of mental processes, internal learner characteristics, social contexts, and externally existing bodies of knowledge were found. However, in the Chinese context learning was more associated with “seeking knowledge,” which was closely related to personal attitudes, purposes, and action plans for learning. Learning also had an inherently moral and societal dimension in the Chinese context. Li (2002) concluded, “Whereas Americans elaborate on learner’s mental functioning and their related learning processes, the Chinese TABLE 2 Taxonomy of Possible Cross-Cultural Differences###A “cultural” imagination would behoove researchers to include other constructs in their studies and not just those derived from Western models (see King & Watkins, 2013, for the original formulation). We believe that PI theory could offer a unifying framework wherein constructs from Western theorizing could be studied in combination with more emically derived psychological processes. We are not arguing that the key motivational constructs identified by contemporary theories of motivation are unilaterally inapplicable to other non-Western settings. However, our claim is that what these theories deem to be central or focal might not occupy such a crucial role in other non-Western cultures. In a reversal of sorts, researchers in social psychology have examined how the culturally derived notion of filial piety, which is common among Chinese populations, is applicable to Western settings. They found that filial piety can also be found in Western populations as demonstrated through the responses to a survey. Face is another construct that is embedded deeply in East Asian social life, which generally refers to the integrity of one’s moral character and the way one’s public image is perceived (Gao, TingToomey, & Gudykunst, 1996). Face is more salient in Eastern compared to Western cultures. However, some scholars have emphasized the universality of this construct (Ho, 1976), and some researchers argue that it is also relevant in Western social interactions (Agassi & Jarvie, 1969). Recently, Zane and Yeh (2002) developed the Loss of Face Scale and administered it to both Chinese and European Americans. Responses were found to support both construct reliability and validity across both populations supporting its applicability in the Western setting. Consider then this thought experiment: In this alternative research world, Asian psychology is the dominant paradigm. A Western researcher wants to account for social behavior in Western samples and relies solely on constructs such as face and filial piety in his or her study. Finding that the scales measuring these constructs are reliable and valid, our imaginary researcher then proceeds to model the relationships among face, filial piety, and social behavior while ignoring other more crucial variables that are focal for Western peoples (e.g., self-efficacy, internal locus of control). Such an approach should be deemed, at the very least, to be quite limited. Despite the constructs of face and filial piety being found to be widely applicable (e.g., Ho, 1976; Zane & Yeh, 2002), they are clearly less central notions in the Western psyche. A more appropriate approach would be to utilize key constructs that are salient in Western settings while also including face and filial piety as something that could provide “added value” but not as the sole and only measures. Cross-cultural psychologists have demonstrated increases in R(2) or predictive power when salient indigenous constructs are added into the regression equation in addition to the etic constructs usually derived from Western theorizing (e.g., King, McInerney, & Watkins, 2010, 2012b, 2013). In a study conducted with Chinese students in Hong Kong, King et al. (2010) showed increases in predictive power when social goals (found to be salient for Chinese students) were added into the regression equation instead of just using mastery and performance goals. Similar results were found by King, McInerney, and Watkins (2012b) in a study with Filipino students.###Tao and Hong (2014) proposed that academic achievement has different meanings in Western and Chinese cultures.###A “cultural” imagination would behoove researchers to include other constructs in their studies and not just those derived from Western models (see King & Watkins, 2013, for the original formulation). We believe that PI theory could offer a unifying framework wherein constructs from Western theorizing could be studied in combination with more emically derived psychological processes. We are not arguing that the key motivational constructs identified by contemporary theories of motivation are unilaterally inapplicable to other non-Western settings. However, our claim is that what these theories deem to be central or focal might not occupy such a crucial role in other non-Western cultures. In a reversal of sorts, researchers in social psychology have examined how the culturally derived notion of filial piety, which is common among Chinese populations, is applicable to Western settings. They found that filial piety can also be found in Western populations as demonstrated through the responses to a survey. Face is another construct that is embedded deeply in East Asian social life, which generally refers to the integrity of one’s moral character and the way one’s public image is perceived (Gao, TingToomey, & Gudykunst, 1996). Face is more salient in Eastern compared to Western cultures. However, some scholars have emphasized the universality of this construct (Ho, 1976), and some researchers argue that it is also relevant in Western social interactions (Agassi & Jarvie, 1969). Recently, Zane and Yeh (2002) developed the Loss of Face Scale and administered it to both Chinese and European Americans. Responses were found to support both construct reliability and validity across both populations supporting its applicability in the Western setting. Consider then this thought experiment: In this alternative research world, Asian psychology is the dominant paradigm. A Western researcher wants to account for social behavior in Western samples and relies solely on constructs such as face and filial piety in his or her study. Finding that the scales measuring these constructs are reliable and valid, our imaginary researcher then proceeds to model the relationships among face, filial piety, and social behavior while ignoring other more crucial variables that are focal for Western peoples (e.g., self-efficacy, internal locus of control). Such an approach should be deemed, at the very least, to be quite limited. Despite the constructs of face and filial piety being found to be widely applicable (e.g., Ho, 1976; Zane & Yeh, 2002), they are clearly less central notions in the Western psyche. A more appropriate approach would be to utilize key constructs that are salient in Western settings while also including face and filial piety as something that could provide “added value” but not as the sole and only measures. Cross-cultural psychologists have demonstrated increases in R(2) or predictive power when salient indigenous constructs are added into the regression equation in addition to the etic constructs usually derived from Western theorizing (e.g., King, McInerney, & Watkins, 2010, 2012b, 2013). In a study conducted with Chinese students in Hong Kong, King et al. (2010) showed increases in predictive power when social goals (found to be salient for Chinese students) were added into the regression equation instead of just using mastery and performance goals.",impact-revealing,highlighting the need for a more inclusive approach in psychological research that considers both Western and non-Western constructs
1911,,f5085181a967f0ffefef89fb71610681888f3506,Meeting the Holistic Needs of K-12 Online Learners: Designing Schools for the Future,,,"###These efforts can build upon theoretical foundations established in higher education such as the CoI model (Garrison, 2011; Garrison et al., 2001) and the work of NSSE (Kuh, 1996).",impact-revealing,acknowledge theoretical foundations in higher education
1214,,bbe0dadbc9b28325c8038134cbb9f54b5304dca2,Effect of feedback regulation on stem cell fractions in tissues and tumors: understanding chemo-resistance in bladder cancer,,,"###, in the olfactory epithelium, where GDF11 and Activin βB negatively regulate self-renewal rates in progenitor and stem cells [12,13].###An ordinary differential equation model has been used to describe tissue hierarchy dynamics in a healthy tissue [13,16], and the models presented here build on these approaches.###This adds to the mathematical literature quantifying the role of feedback regulation for tissue and tumor dynamics [11,13,15-24], and builds upon the wider mathematical literature concerned with the dynamics of hierarchically structured cell populations, e.",impact-revealing,highlighting the contribution of mathematical models to understanding tissue dynamics
1771,,d280b95bb4394d343723fd766f6e409da7bea9c8,A minimum disclosure approach to authentication and privacy in RFID systems,,,"###In the scheme by Yeh et al. [20], Chien’s and Lo’s schemes are improved with the added security property of reader authentication.###This is due to the fact that we use the shared secret Ki to identify the tag and the reader entries in the database rather that depend on a shared secret that is updated during each access round such as in Yeh [20] and Chien [18].###Lo et al. [35] proposed
an improvement to Chien’s scheme but it still does not address the location privacy concern and can be compromised by collaborating readers [20].###[18], proposed a mutual authentication protocol that achieves EPC Class-1 Gen-2 compliance and is based on random nonces and CRC calculations.###Further as noted by Chien et al. [18], only a few hundred gates are required for implementing modular squaring operations; this is much cheaper than even the simplest hash function.###Chien et al. [18], proposed a mutual authentication protocol that achieves EPC Class-1 Gen-2 compliance and is based on random nonces and CRC calculations.###Further in both Chien’s and Lo’s schemes only security of the reader-tag channel is addressed with the server-reader channel assumed to be secure.###Chien and Laih have proposed a lightweight security scheme based on error correction codes with secret parameters [15].###In our mutual authentication scheme and those proposed by Chien [18], Chen [14], Yeh [16] and Chen [33] the channel between the reader and the server is assumed to be secure.###[18], only a few hundred gates are required for implementing modular squaring operations; this is much cheaper than even the simplest hash function.###Cryptanalysis of Chien’s scheme by Peris-Lopez et al. [34], shows that it cannot guarantee the unequivocal identification of tags, forward secrecy and location privacy of tags.###[18] 5 1CRC 2PRNG 1PRNG n + 1CRC 2PRNG Yes Yes OðnÞ###We follow an approach similar to [14] for the security analysis which is consistent with other research in this area [18,33,39].###All of these are within the capabilities of low-cost RFID tags [18,19].###Scheme Rounds Tag Reader Server Security assumption EPCC1G2 compliance Database loading (worst case)
Chen et al. [14] 5 3Hash 2Squaring 1PRNG
1PRNG 10Hash 1PRNG 2square root solving Yes No Oð1Þ
Cho et al. [39] 5 2Hash 1PRNG 1PRNG 1Hash 1PRNG No No OðnÞ Yeh et al. [16] 5 4Hash
2Squaring 2PRNG
1PRNG 14Hash 3Square root solving 1PRNG
Yes No Oð1Þ
Chien et al. [18] 5 1CRC 2PRNG 1PRNG n + 1CRC 2PRNG Yes Yes OðnÞ Chen et al. [33] 5 2CRC 1PRNG 2CRC,
1PRNG Not Involved Yes Yes OðnÞ
Yeh et al. [20] 5 7PRNG 1PRNG 6PRNG No Yes OðnÞ Our Mutual
Authentication Scheme 5 1Squaring 1PRNG 4CRC 1PRNG 1Square root solving 1PRNG 4CRC Yes Yes OðnÞ
Our Collaborative Authentication Scheme 5 1Squaring 1PRNG 4CRC
1PRNG 3Hash 1Square root solving 1PRNG 2Hash 4CRC
No Yes OðnÞ
to identify the tag uniquely.",impact-revealing,discussing improvements and limitations in mutual authentication schemes
1193,,71f1ea4afe8bf62c745c9690edab3776b121fe81,Workplace Issues in an Undergraduate Software Engineering Course,,,"###Weinberg introduced the idea of egoless programming in his classic book (first published in 1970) and Ledgard builds upon the egoless programming idea by stressing how important team work is for a successful software development project.###McLendon and Weinberg go on to describe the strengths of a congruent culture and how Software Engineers can strive to create a congruent work environment.###McLendon and Weinberg, as industry consultants, report that they can detect the tell-tale signs of an incongruent culture within minutes of entering a particular workplace.###This paper explicitly focuses on the importance of a good work culture (what McLendon and Weinberg call a “congruent” work
culture) for the success of software development projects.###Weinberg, Gerald M., The Psychology of Computer Programming, Dorset House, New York, 1998, 292 pp.
2.###These introductory lectures include materials relating to programmer psychology, with an emphasis on the ideas of Weinberg [1] and Ledgard [2].###After our discussion of the Software Engineering Code of Ethics, we move on to a discussion of a provocative paper by McLendon and Weinberg [17].###McLendon and Weinberg have the following to say about blaming ([17, p. 36): “Blaming is the dark secret underlying the failure of many projects.”###The author uses these materials (by Weinberg and Ledgard) to raise student awareness about the team work issues that might arise as they work on their team projects.###McLendon, Jean and Weinberg, Gerald M., “Beyond Blaming: Congruence in Large Systems Development Projects”, IEEE Software, July 1996, pp. 33-42.",impact-revealing,highlighting the importance of work culture in software development
3807,5e8da0c991e011f2de583820,a24aad407737645bf2ef0148462b2b23f657fab1,Let's Agree to Degree: Comparing Graph Convolutional Networks in the Message-Passing Framework,599c7985601a182cd2647a46,Deep Sets,"What follows is in fact an adaptation of Lemma 5 from [Xu et al., 2019] itself based on [Zaheer et al., 2017, Theorem 2].",other,acknowledging adaptation of prior work
2538,5bdc318017c44a1f58a08780,5ab5658a1666e26c66f0319a469228dbe19598a2,An e-learning recommendation approach based on the self-organization of learning resource,56d88bd4dabfae2eeebd7c53,A Personalized e-Learning material Recommender System,Lu et al.[15] pointed out the matching rules are important for discovering associations between student requirements and the learning material tree.,other,reporting prior findings on matching rules in education
4018,5c7a561ff56def9798e6a297,abe4e92d6e4868115aef57a4eba97ec4d48ac124,language-adversarial transfer learning for low-resource speech recognition,58d83008d649053542fe0714,Adversarial Multi-Task Learning Of Deep Neural Networks For Robust Speech Recognition,Shinohara [38] utilizes adversarial training to perform environment adaptation for robust speech recognition.,other,reporting prior findings on adversarial training for speech recognition
4000,5e15adca3a55ac47ab5b0729,97ebd482a78e6e6c1ba51da5e1b2f8e7640cc8b5,hyperbolic graph convolutional neural networks,5b67b47917c44aac1c863811,Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry,"Empirically, both models give similar performance but hyperboloid model offers more stable optimization, because Poincaré distance is numerically unstable [30].###Shallow embedding methods have also been developed in hyperbolic geometry [29, 30] for reconstructing trees [35] and graphs [5, 13, 22], or embedding text###Here, we work with the hyperboloid model for its simplicity and its numerical stability [30].",other,highlighting the advantages of the hyperboloid model in optimization
1044,,e52a5fac4ac99185337f48fc57e7a5aef58c11fe,Theory of Computing Systems,,,###This algorithm is inspired by the well-known fingerprint method of Karp and Rabin [7] for standard pattern matching.###In Section 4 we review a recent variation from Kalai [6] of the Karp–Rabin pattern matching method [7].,impact-revealing,describing the inspiration behind the algorithm
1773,,855cf0ee668ad37508b0718311034c2c97897ba6,Secure Mutual Authentication Protocol for Low-Cost RFID,,,"###c) [4] also does not guarantee location privacy of the tags.###As a result of its linearity, sensitive values can be gotten by performing an Exclusive OR operation on the two separate results that have a common variable [4] This therefore, does not guarantee the non – impersonation of the tags.###In contrast to [4] which updates the values on the server with every run, the new protocol does not allow the values on the server to be updated if the value of x = old which shows that the server and the tag are not in synchronization.###Taking into consideration of the weaknesses of these protocols, a new protocol is proposed which improves on the protocol in [4] because this protocol is especially suitable for low-cost RFID.###[9] also showed that a desynchronization attack was possible on the scheme [4] proposed.###b) Even though [4] asserts that the proposed protocol can resist DoS attack, if the message M2 in figure 2 is missing or tampered with up to two times, the database will not have any matching old authentication key and access key.###Authentication Phase The authentication phase is an adaptation and an improvement over the [4] protocol.###[4] then proposed a mutual authentication protocol under the EPC class 1 Generation 1 standard.###In this paper, we propose a new protocol that builds on an existing protocol that was proposed in [4].###However, [18] showed that the scheme [4] proposed has some weaknesses.",impact-revealing,highlighting weaknesses in existing protocols and proposing improvements
2204,5f7fdd328de39f0828397e22,21e33bd0ad95ee1f79d8b778e693fd316cbb72d4,Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs,5f75cce491e0111c1eb4d6e6,Graph Neural Networks With Heterophily,"Moreover, the recent CPGNN model [43] integrates the compatibility matrix as a set of learnable parameters into GNN, which it initializes with an estimated class compatibility matrix.###[43] Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai Koutra.",other,reporting advancements in GNN models
2289,57d063f6ac44367354296741,908f7931de8768786d9ef7d64f5a8156860709dd,Dynamic Pricing and Traffic Engineering for Timely Inter-Datacenter Transfers,558b4541e4b031bae1fc7821,Upward Max Min Fairness,"Recent papers consider the objective of imposing fairness in a shared network [10, 11].",other,highlighting recent research on fairness in shared networks
3375,5dd6604a3a55ac78684acf68,af54c890ffce96bae303310182be2ca301f2f97e,On Using SpecAugment for End-to-End Speech Translation,5ce3aaafced107d4c658c6cb,On using 2D sequence-to-sequence models for speech recognition,"Recent advancements in both ASR [1–6] and MT [7–11] have inspired the end-to-end direct ST models which can be trained using a translation speech corpus [12, 13].",other,highlighting the influence of advancements in ASR and MT on direct ST models
1908,,330049677048a47303d65d9305014aff77ca9764,Focusing on Social Presence in an Electronics Course at a Two-Year College: An Action Research Study,,,"###Finally, knowing they had to present the problems at the end of class and an exam was scheduled the next week provides the stimuli to complete the problems and commit to helping each other, which illustrates group cohesion (Garrison et al., 2000).###I thank them for their patience throughout the dissertation process.
v
Abstract
Using a phenomenological approach, this action research study explored the influence of social presence (Garrison et al., 2000) on the achievement of students who were enrolled in Electronics, a two-year college course.###included three interacting elements: teaching presence, cognitive presence, and social presence (Garrison et al., 2000).###During this review, I learned about the importance of establishing a social presence within the CoI framework for a successful experience in higher education (Garrison et al., 2000).###When participants in a CoI share their attitudes and feelings with one another, trust, support, and a sense of belonging develop (Garrison et al., 2000).###…achievement in my electrical circuits course, I
implemented reciprocal teaching (Green, 2000), a collaborative learning strategy (Stump et al., 2011) that has the potential to foster the development of social presence (Garrison et al., 2000), and is inherently culturally responsive (Gay, 2010).###learned about the importance of establishing a social presence within the CoI framework for a successful experience in higher education (Garrison et al., 2000).###observed communicating their emotions and attitudes, connecting with others (Garrison & Arbaugh, 2007), and showing their personalities (Garrison et al., 2000).###The community of inquiry (CoI) framework emphasized the importance of
establishing a social presence for a successful higher education experience (Garrison et al., 2000).###Finally, knowing they have to present the problem at the end of class provides the stimulus to complete the problems and commit to helping each other, which illustrates group cohesion (Garrison et al., 2000).###I also discuss the theories, social presence from the CoI (Garrison et al., 2000), culturally responsive teaching (Gay, 2010), and collaborative learning (Stump et al., 2011) that serve as the foundation for my theoretical framework.###In studies focused on understanding social presence, students were observed communicating their emotions and attitudes, connecting with others (Garrison & Arbaugh, 2007), and showing their personalities (Garrison et al., 2000).###Community of inquiry: Garrison, Anderson, and Archer (2000) defined the community of inquiry (CoI) as a group of students and instructors engaged in purposeful and meaningful interactions for an optimal educational experience.###In the CoI, the practical inquiry model grounded in Dewey’s work (Garrison et al., 2000) defines cognitive presence.###In reviewing the community of
60
inquiry framework (Garrison et al., 2000), culturally responsive teaching (Gay, 2010), and collaborative learning (Stump et al., 2011), I recognized how I could integrate them into a theoretical framework to address my problem of practice.###All of the phases of the practical inquiry model occur in an educational environment that encourages reflection, discussion, analysis, and synthesis (Garrison et al., 2000).###The second element, cognitive presence, was the ability to “construct meaning through sustained communication” (Garrison et al., 2000, p. 89).###During each cycle of the implementation of reciprocal teaching, I looked for meaningful interactions, which are an important part of social presence (Garrison et al., 2000) and an outcome of culturally responsive teaching (Gay, 2010).###During this review, I learned about the importance of establishing a social presence within the community of inquiry framework for a successful experience in higher education (Garrison et al., 2000).###Garrison, Anderson, and Archer (2000) defined the CoI as a group of students and
instructors engaged in purposeful and meaningful interactions for an optimal educational experience.###These theories include the community of inquiry framework (Garrison et al., 2000), culturally responsive teaching (Gay, 2010), and collaborative learning (Stump et###In addition, monitoring the student-participants’ progress showed that I was committed to helping them learn the content and complete the task (Gay, 2010), which aided in developing group cohesion (Garrison et al., 2000).###In an effort to improve student achievement, I synthesized social presence from the community of inquiry (Garrison et al., 2000) framework and elements of culturally responsive teaching (Gay, 2010) with a focus on collaborative learning (Stump et al., 2011).###These theories include the community of inquiry framework (Garrison et al., 2000), culturally responsive teaching (Gay, 2010), and collaborative learning (Stump et al., 2011).###Two examples of contributing factors to the expression of emotion are humor and self-disclosure (Garrison et al., 2000).###Using a phenomenological approach, this action research study explored the influence of social presence (Garrison et al., 2000) on the achievement of students who were enrolled in Electronics, a two-year college course.###I also discuss the theories, social presence from the CoI (Garrison et al., 2000), culturally responsive###To address the problem of practice in this study, I integrated social presence from
the CoI (Garrison et al., 2000) and elements of culturally responsive teaching (Gay, 2010) with a focus on collaborative learning (Green, 2000) into a theoretical framework that guided my decisions for this study.###This comfort level leads to open communication and exchanging ideas (Garrison et al., 2000).###This open communication leads to exchanging ideas and discussion about how to solve the problem (Garrison et al., 2000).###Cognitive presence is “the extent to which the participants in
any configuration of a community of inquiry are able to construct meaning through sustained communication” (Garrison et al., 2000, p. 89).###In order to support this student learning, the CoI included three interacting elements: teaching presence, cognitive presence, and social presence (Garrison et al., 2000).###Within the CoI framework, social presence, an essential element, fosters socio-emotional interactions and peer-to-peer support related to the educational experience (Garrison et al., 2000).###This illustrated my high expectations for the students and the opportunity to engage in open communication (Gay, 2010), an indicator of social presence (Garrison et al., 2000).###presence (Garrison et al., 2000) and an outcome of culturally responsive teaching (Gay, 2010).###To address the problem of practice in this study, I integrated social presence from the CoI (Garrison et al., 2000) and elements of culturally responsive teaching (Gay, 2010) with a focus on collaborative learning (Green, 2000) into a theoretical framework that guided my decisions for this study.###The second indicator of social presence, open communication, involves respectful
and reciprocal exchanges between participants (Garrison et al., 2000).###During each cycle of the implementation of reciprocal teaching, I looked for meaningful interactions, which is an important part of social presence (Garrison et al., 2000) and an outcome of culturally responsive teaching (Gay, 2010).",impact-revealing,highlighting the importance of social presence in educational experiences
571,5a73cb3517c44a0b303556bf,eac48f406c46527f5ca821de7fe8d62d6db56a27,"Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer",58437725ac44360f1082ffd7,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation.,"Therefore, as subword units, we use wordpieces as described in [16].###Another contribution of this work is to investigate the use of wordpieces [16], which have been explored previously in the context of machine translation, as a sub-word unit for end-to-end speech recognition.",impact-revealing,acknowledge the use of wordpieces in speech recognition
3055,5d3ed25a275ded87f97deae9,2c6097792ed9e4e8a664ce2dc7492377bfd57139,LightNet: A Dual Spatiotemporal Encoder Network Model for Lightning Prediction,57a4e91dac44365e35c98a45,Application of Deep Convolutional Neural Networks for Detecting Extreme Weather in Climate Datasets.,[13] applied deep convolutional neural networks to detect extreme weather in large historical climate datasets.,other,reporting prior findings on extreme weather detection
1909,,4af058b8788249a68726686ff4aed07b1002698e,Development and Validation of the Community of Inquiry Program-Level Inventory,,,"###The CoI is a model of effective online education suggesting that quality education occurs
within a community of learners who are actively engaged and invested in the learning process (Garrison et al., 1999).###When students feel emotionally involved with classmates and teachers, they have a sense of belonging and feel supported (Garrison et al., 1999).###In an effort to understand the dynamics of the increasingly popular online educational environments of higher education, Garrison, Anderson, and Archer (1999) proposed the Community of Inquiry (CoI) framework.###Social presence takes a supportive role of cognitive presence because critical thinking, intellectual engagement, and the finding of meaning are continuously engaged while conversing with and considering the perspectives of fellow learners (Garrison et al., 1999).###The CoI framework is a model of online education that emphasizes the roles of cognitive presence, social presence, and teaching presence in online learning environments (Garrison et al., 1999).###Social Presence
Social presence is one of the CoI presences, and it involves discussions about course
material and any class interactions that promote group cohesion among students (Garrison et al., 1999).###Similar to the social presence, the teaching presence is also an essential part of the phases
of cognitive presence (Garrison et al., 1999).###The final stage is resolution, which involves agreeing upon a consensual resolution to the problem and discussing applications of the resolution (Garrison et al., 1999).###For people to feel united as a group, they must engage in communication that meets the societal expectations for group members (e.g., being kind to each other; Garrison et al., 1999).###Affective statements help demonstrate the unique personalities and experiences of members of a community of inquiry and help students build confidence that they know and understand the authentic persons of their classmates and teacher (Garrison et al., 1999).###The three CoI presences (i.e., cognitive presence, social presence, and teaching presence)
are distinct constructs that influence and overlap with each other (Garrison et al., 1999).###During the third phase, integration, students start to brainstorm about solutions and continue to reflect on their personal ideas while also responding to the ideas of classmates (Garrison et al., 1999).###Consistent with the theoretical model of the CoI (Garrison et al., 1999) and with previous research (Kozan & Richardson, 2014), the subscales are strongly correlated with each other.###The separation of the teaching presence subscales is somewhat inconsistent with the theoretical framework of the CoI (Anderson et al., 2001; Garrison et al., 1999), but it is consistent with some previous research (Arbaugh, 2007; Arbaugh et al., 2008; Bangert, 2009; Shea, Li, & Pickett, 2006).###When students reference the statements of another group member, they are turning the attention of the group onto the contribution of one member and adding a sense of united agreement within the group (Garrison et al., 1999).###There are three categories of communication within
social presence that researchers use as they analyze transcripts of class interactions to determine the amount of social presence (Garrison et al., 1999).###The three CoI presences have individual effects and a combined effect on learning outcomes, and they make up the essential elements of an effective learning environment (Garrison et al., 1999).###The teaching presence construct consists of three categories that represent different roles of instructors, which are instructional design, facilitating discussion, and direct instruction (Anderson et al., 2001; Garrison et al., 1999).###…subscale were designed to measure the four phases of cognitive development from the CoI framework so this subscale’s emergence as a single factor is consistent with the CoI model and the conceptualization of the four phases of the cognitive presence (Garrison et al., 1999; Garrison et al., 2001).###The CoI was introduced as a conceptual framework to help guide educators as they seek to create online learning environments that promote learning (Garrison et al., 1999).###The CoI framework is a model for effective online education that emphasizes the importance of strong cognitive presence, social presence, and teaching presence for learning in an online learning environment (Garrison et al., 1999).###Cognitive presence is one of the three CoI presences, and it involves deep intellectual
engagement with the subject matter (Garrison et al., 1999).###The CoI framework, which was developed to give educators a conceptual model to guide their choices, posits that quality learning occurs in learning environments with strong cognitive presence, social presence, and teaching presence regardless of the delivery medium (Garrison et al., 1999).###Direct instruction is the aspect of teaching that involves knowledge contribution from the teacher, which may take the form of recorded lectures, written comments, or constructive feedback on assignments (Garrison et al., 1999).###The cognitive presence items formed a single subscale, which appears to be consistent with the cognitive presence construct of the CoI (Garrison et al., 1999; Garrison et al., 2001).###If the CoI presences are essential for learning in online academic environments (Garrison et al., 1999), it is important for online academic programs to have strong cognitive, social, and teaching presences throughout the programs.###Communication in the affective category of social presence expresses the personal or
emotional experiences of the learner in an attempt to build comradery through self-disclosure (Garrison et al., 1999).###Teaching presence is one of the three CoI presences, and it involves all of the typical instructional roles of a teacher (Garrison et al., 1999).###In a text-based environment, interactive communications may occur in response to an initial discussion prompt from the instructor or in response to specific comments of other students (Garrison et al., 1999).###The construct of cognitive presence has four stages, which are triggering event, exploration, integration, and resolution (Garrison et al., 1999; Garrison et al., 2001).###According to the CoI framework, the three categories of the teaching presence should all
intertwine with each other and collectively make up the role of the teacher in the learning community (Anderson et al., 2001; Garrison et al., 1999).###While there are some similarities between the NSSE construct of student engagement (Center for Postsecondary Research, n.d.c) and the CoI construct of social presence (Garrison et al., 1999), the NSSE does not measure cognitive presence or teaching presence.###The social presence loaded as one factor, which is consistent with the CoI model (Garrison et al., 1999; Rourke et al., 1999), but all of the items designed to measure the affective category were eliminated from the subscale.###, cognitive presence, social presence, and teaching presence) are distinct constructs that influence and overlap with each other (Garrison et al., 1999).###Affective content is anything that involves emotional expression or personal disclosure (Garrison et al., 1999).###Cohesive content consists of communication that builds unity within a group, which may involve using social niceties or the simple acknowledgment by a group member of the existence of a shared experience or goal (Garrison et al., 1999).###Both face-to-face and online learning environments must have each of the three presences to be successful, and stronger presences promote deeper and more meaningful learning (Garrison et al., 1999).###Social presence is one of the CoI presences, and it involves discussions about course material and any class interactions that promote group cohesion among students (Garrison et al., 1999).###The cohesive category is present in comments that emphasize the unity of members or
comments that meet a social responsibility for members of a group (e.g., greetings; Garrison et al., 1999).###…is a model built on the assumption that the aspects of traditional higher education that facilitate learning are the cognitive presence, social presence, and teaching presence and online higher education can produce quality learning if it possesses these three presences (Garrison et al., 1999).###Teaching presence is the function of the teacher to guide and facilitate learning
experiences (Garrison et al., 1999).###The purpose of interactive communication is to strengthen the cohesion of the group and build feelings of a shared learning experience among learners (Garrison et al., 1999).###The CoI model conceptualizes the social presence as consisting of the three categories of affective, interactive, and cohesive (Garrison et al., 1999; Rourke et al., 1999).###While teaching presence is not considered to be as fundamental to learning as cognitive
presence, it is thought to play a vitally important role in establishing and maintaining the other two presences (Garrison et al., 1999).###Vocative statements, which occur when group member address each other by name in their dialogue, help build feelings of belongingness (Garrison et al., 1999).###Cognitive presence is one of the three CoI presences, and it involves deep intellectual engagement with the subject matter (Garrison et al., 1999).###The facilitating discourse category of teaching presence includes activities that promote
group interactions and social investment (Garrison et al., 1999).###The CoI framework is a conceptual model that posits that effective learning environments require strong cognitive, social, and teaching presences (Garrison et al., 1999).###The Community of Inquiry (CoI) is a conceptual framework for educational
environments suggesting that for deep and meaningful learning to occur in any learning
environment, it is important to cultivate strong cognitive presence, social presence, and teaching presence (Garrison et al., 1999).###In the resolution phase, instructors must guide the class to a logical solution that is congruent with the teacher’s knowledge of the subject and challenge the class by asking questions to test the logic of the solution (Garrison et al., 1999).###The instructional design category of teaching presence encapsulates all of the teaching
activities that relate to managing the course (Garrison et al., 1999).###The CoI model consists of three overlapping, yet characteristically distinct, presences
(i.e., cognitive presence, social presence, and teaching presence) that form the essential elements of an effective learning environment (Garrison et al., 1999).###Cognitive presence involves deep consideration of the
content and can be facilitated by learning activities that promote cognitive engagement (Garrison et al., 1999).###Instructional design (also called instructional management) is defined by the teacher’s ability to manage the logistics of online education including setting up the course in the online medium, planning the course schedule, and organizing learning activities (Garrison et al., 1999).###This category of teaching presence requires administrative and organizational skills (Garrison et al., 1999).###The CoI is a conceptual framework of online education that emphasizes the role of the social learning environment in online education (Garrison et al., 1999).###Social presence also accomplishes the secondary role of promoting emotional
connectedness among students, which increases the level of commitment to the course and the academic program (Garrison et al., 1999).###In the exploration phase, students must gain a full understanding of the dilemma and decide what informational sources will be helpful for solving the dilemma (Garrison et al., 1999).###Teaching presence is one of the three CoI presences, and it involves all of the typical
instructional roles of a teacher (Garrison et al., 1999).###They have to be able to start discussing a solution in a collaborative manner, which involves using critical thinking skills to state and support their perspectives (Garrison et al., 1999).###As the students reflect and discuss the dilemma during the exploration phase, the teacher should guide them as they attempt to evaluate relevant information (Garrison et al., 1999).###facilitate learning are the cognitive presence, social presence, and teaching presence and online higher education can produce quality learning if it possesses these three presences (Garrison et al., 1999).###The initial social presence subscale of the CPI contained three items to measure each of the three categories of social presence in the CoI model, which are affective, interactive, and cohesive (Garrison et al., 1999; Rourke et al., 1999).###Social presence is the ability of members of a learning community to have genuine and
meaningful social interactions (Garrison et al., 1999).###Teachers should seek to bring attention to similarities and differences among students’ comments to help students gain the skill of discriminating between ideas (Garrison et al., 1999).###Cognitive presence is the fundamental component of learning; it is the process of gaining an understanding of the meaning of ideas (Garrison et al., 1999).###The cognitive presence involves active intellectual engagement with the learning material (Garrison et al., 1999) and is supported by the social presence and teaching presence (Garrison et al., 2010).###The direct instruction category of teaching presence is the aspect of the teaching role that
involves the presentation of knowledge from the teacher to the students (Garrison et al., 1999).###Cognitive presence is intellectual engagement and meaning construction, which is the
most essential element of learning in higher education (Garrison et al., 1999).###environment, it is important to cultivate strong cognitive presence, social presence, and teaching presence (Garrison et al., 1999).###The CoI framework is a model of online education that emphasizes the need for strong
cognitive, social, and teaching presences in educational environments for effective learning to occur (Garrison et al., 1999).###In the CoI framework, the presences are three variables that are characteristically distinct
yet highly entangled and indivisible (Garrison et al., 1999).###Even though the four phases of cognitive presence are specifically related to cognitive
presence, the importance of social presence and teaching presence in these phases is clear throughout the process which highlights the interconnection of the three CoI presences (Garrison et al., 1999).",impact-revealing,highlighting the importance of the Community of Inquiry framework in online education
1023,,d32fd62ed78d817f341db717dcee620addb6b4f4,Applying Person‐Environment Fit Theory to Identify Personality Differences between Prospective Social and Commercial Entrepreneurs: An Explorative Study,,,"###To measure altruism, we used five items from the Prosocial Personality Battery (PPB) developed by Penner et al. (1995), which has been widely used by other scholars when assessing aspects of prosocial behavior (e.g., Eisenberg et al. 2002; Graziano et al. 2007; Rioux and Penner 2001).",impact-revealing,reporting the use of a widely accepted measurement tool for altruism
1451,,d9ab74f67c5161301f240589e96e62c1b039684f,A Natural Language Processing Pipeline of Chinese Free-Text Radiology Reports for Liver Cancer Diagnosis,,,"###In addition, MetaMap [18] and MedLEE [19] are also widely used for information extraction.",impact-revealing,reporting widely used tools for information extraction
3732,5aed14d117c44a4438158af2,30f86d38f0660af5ea2e16d996434c72eee8c5ee,espnet: end-to-end speech processing toolkit,573696126e3b12023e5246d6,End-to-End Attention-based Large Vocabulary Speech Recognition,"• Attention-based: Attention-LVCSR [29], OpenNMT speech to text [30]###We can categorize the toolkits into two types based on CTC and attention architectures as follows: • CTC-based: EESEN[11], Stanford CTC [28], Baidu Deepsppech [12], • Attention-based: Attention-LVCSR [29], OpenNMT speech to text [30] Note of RNNLM during decoding, and a number of Kaldi-style ASR recipes, which make ESPnet unique to the other toolkits.",other,categorizing toolkits based on architectures
2967,57a4e91dac44365e35c9886f,52b2bca872dac6cd708d36c009daa6f90db371be,A Soft Processor Overlay with Tightly-coupled FPGA Accelerator,53e9bda6b7602d9704a51fed,Vector Processing As A Soft-Core Cpu Accelerator,"In addition, soft vector processors [16], [17], [18], soft VLIW processors [19], [20], multi-thread soft processors [21], [22] and application-specific soft processors [23], [24] are also extensively studied and developed on FPGA to analyze the performance and to demonstrate the benefits of soft processors.",other,acknowledge existing research on soft processors
1590,,aa327aa4892d45fae8d5e5215a78e5aad343be58,Crohn's and Colitis: Treatment of 5-ASA Responders,,,"###Infliximab has been shown to be a highly effective agent for the treatment of moderately to severely active, medically refractory, inflammatory76 and fistulizing CD.111 Four weeks after a single infusion of 5 mg/kg of infliximab, 81% of patients had a clinical response, compared with 17% of patients in the placebo group.76 The role of infliximab in maintenance therapy continues to be defined.###Because PML was associated with combination therapy in patients taking natalizumab, this therapy has been approved by the FDA as monotherapy in patients with CD.
Post hoc subgroup analyses of the ENACT 2 and ENCORE maintenance trials found consistently higher induction remission rates for nearly 25% of patients with short-duration CD (<3 years).126 Almost 33% of patients in the ENCORE trial were considered infliximab failures; 38% of these patients achieved a sustained response through weeks 8 and 12, compared with 17% of those on placebo.127 Remission rates at 12 weeks were unaffected by concomitant immunosuppressants for nearly 40% of patients.###In addition, antibodies against the CD-related bacterial sequence I2, Escherichia coli outer membrane porin C, and CBir1 flagellin identify a unique subset of immunologically vulnerable patients with complicated/aggressive CD.(15,16) Serologic diagnostic and biomarker testing provides a molecular snapshot of patients with IBD.###Poor therapeutic response is an indication for surgery in nearly 29% to 30% of patients with UC and approximately 50% to 70% of patients with CD. Patients with refractory left-sided colitis or IC may benefit from serologic testing, in addition to documentation of clubbing and oral aphthae.9 In these patients, if the markers are more consistent with a molecular pattern of CD, physicians may consider antitumor necrosis factor (TNF) therapy as an option rather than total colectomy.###The REACH study evaluated the safety and efficacy of infliximab in children with moderately to severely active CD.113 At week 54 of treatment, 63.5% and 55.8% of patients who received infliximab every 8 weeks achieved clinical response and were in clinical remission, respectively, and did not require dose adjustment.###Indeterminate colitis (IC) might represent part of an immunologic continuum, rather than a well-defined clinical subset of UC and CD.5,6 IBD is not an autoimmune disease; it is a dysregulated immune response to luminal microbial antigens.###Laparoscopic ileocolic resection has shortened recovery time and length of hospital stay for patients with medically refractory CD.132,133 Earlier surgical intervention may be of benefit with minimally invasive laparoscopic techniques, as well as novel bowel-sparing strictureplasty.134,135 The combination of infliximab and surgery for perianal disease (including seton placement) closed perianal fistulas in 68% of patients and reduced the rate of recurrent abscesses.136
The annual incidence of clinically significant postoperative recurrence is 8% to 10%; stricturing disease tends to develop more slowly, and fistulizing disease tends to recur faster.137 The severity of clinical postoperative recurrences can be predicted by the severity of endoscopic anastomotic lesions.###Reproduction i w
hole or in part w ithout perm
ission is prohibited.
compared with only 17% of children receiving infliximab every 12 weeks.113 Rare postmarketing cases of hepatosplenic T-cell lymphoma have been reported with both infliximab and adalimumab; all of the cases occurred in patients on concomitant treatment with AZA or 6-MP. Updated TREAT registry data found no increased risk with infliximab for CD.91
Adalimumab.###The rationale for using antibiotics in CD is related to the concept that environmental triggers that alter indigenous luminal bacteria (traveler’s diarrhea, gastroenteritis) or mucosal barrier function (acute infections, nonsteroidal anti-inflammatory drugs) increase mucosal inflammation and permeability.83 Metronidazole is effective for the induction and maintenance of remission in patients with perianal disease, and also for the postoperative prevention of endoscopic and clinical recurrences (when given for 3 months following ileal resection).84-86 The combination of ciprofloxacin (500 mg bid) and metronidazole (250 mg qid) has been shown to be almost as effective as methylprednisolone in the treatment of patients with active CD.87 Rifaximin is an orally administered, topically active nonabsorbable antibiotic with proven efficacy in traveler’s diarrhea.88 It also may prove to be effective for the induction and/or maintenance of remission in patients with CD.89
Steroids.###The SONIC trial does not address top–down steroid strategy—that is, steroids and weightbased AZA or 6-MP early in the course of aggressive CD—and also does not address AEs related to combination infliximab and AZA in long-standing CD.
Natalizumab.###This distinction becomes particularly difficult when attempting to classify CD.###Those in whom diffuse, deep new ileal recurrences developed within 1 year tended to experience early symptoms and complications.14
Postoperative recurrence may be prevented by the addition of 5-ASA,138,139 6-MP/AZA,140,141 VSL3,142 or infliximab.143 Postoperative infliximab significantly lowered endoscopic recurrence at 1 year in complex fistulizing CD.143 Only 9% of patients who had undergone an ileocolonic resection had documented endoscopic recurrence after 1 year of postoperative infliximab (3-dose induction followed every 8 weeks for 1 year).###The results of the ACCENT I trial, establishing the efficacy of infliximab for maintenance therapy in CD, challenges the lack of maintenance strategies previously established by the multicenter National Cooperative Crohn’s Disease Study and European Cooperative Crohn’s Disease Study.50-52 These trials did not demonstrate a statistically significant benefit of sulfasalazine in maintaining remission of CD. Notably, only 20% to 30% of patients with CD have colonic disease alone, and most sulfasalazine is released in the colon.###With the introduction of mesalamine, higher doses of active 5-ASA have shown increased efficacy for both the induction and maintenance of remission in CD.53 A meta-analysis of 10 randomized controlled trials evaluating mesalamine maintenance therapy in 1,371 patients with CD showed no statistically significant difference in
relapse rates between placebo and treatment groups.54 However, the analysis showed that mesalamine treatment significantly decreased the relapse rates in patients with limited ileal disease of long duration.54###Tacrolimus and cyclosporine still may have a role in the treatment of CD. Tacrolimus (0.10 mg/kg bid) has been shown to close perianal fistulas in patients with medically and surgically refractory CD.108,109 In a study by Brynskov et al, 59% of steroid-dependent or -intolerant patients responded to cyclosporine (7.5 mg/kg per day) over 3 months, compared with 32% in the placebo group.110
Infliximab.###Although there is controversy, high levels of perinuclear antineutrophil cytoplasmic antibodies (pANCA) have consistently correlated with postoperative pouchitis.11 Anti-CBir1 is an antibody to flagellin of the Clostridium species and increases the incidence of chronic pouchitis in patients who have high pANCA levels and predicts acute pouchitis when associated with low pANCA levels.12 Anti– Saccharomyces cerevisiae antibody positivity (expression of both immunoglobulin [Ig] subtypes A and G) correlates with a younger age at onset and more aggressive fibrostenotic disease.13,14 In addition, antibodies against the CD-related bacterial sequence I2, Escherichia coli outer membrane porin C, and CBir1 flagellin identify a unique subset of immunologically vulnerable patients with complicated/aggressive CD.15,16
Serologic diagnostic and biomarker testing provides a molecular snapshot of patients with IBD.",impact-revealing,highlighting the significance of serologic diagnostic and biomarker testing in understanding IBD
3855,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",58437722ac44360f1082ed2b,Network Topology Inference from Spectral Templates.,has also been addressed with help of spectral graph templates [148].,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3578,5ef0816891e0112aee042b88,5c126ae3421f05768d8edd97ecd44b1364e2c99a,denoising diffusion probabilistic models,5550443b45ce0a409eb4c39d,Stochastic Backpropagation and Approximate Inference in Deep Generative Models.,"While diffusion models might resemble flows [9, 46, 10, 32, 5, 16, 23] and VAEs [33, 47, 37], diffusion models are designed so that q has no parameters and the top-level latent xT has nearly zero mutual information with the data x0.",other,providing context on the characteristics of diffusion models
2389,5843777eac44360f108417ec,455afd748e8834ef521e4b67c7c056d3c33429e2,Hierarchical Attention Networks for Document Classification,555048d345ce0a409eb71be1,Recurrent Convolutional Neural Networks For Text Classification,"There are also some works that combine LSTM and CNN structure to for sentence classiﬁcation (Lai et al., 2015; Zhou et al., 2015).",other,acknowledge existing methods for sentence classification
1031,,2966a9c856f3cc32db2e619389bfe1491ffcff0d,An Investigation of Human Figure Drawings and the Use of Colours in Psychological Evaluation of Children with Emotional and Behavioral Disorders,,,"###However, Machover’s work was criticized for no scoring system and lack of empirical support (Swensen, 1968; Roback, 1968).###It was noted there was no one-to-one relationship between any emotional indicator and a definite emotional state of the children (Roback, 1968; Swensen, 1957, 1968).",impact-revealing,highlighting criticisms of Machover's work
1956,,0b1ff880e4018df5c924ad913a4de590d9f7a7ed,Mechanisms of DJ‐1 neuroprotection in a cellular model of Parkinson’s disease,,,"###…for the full DJ-1 neuroprotective effect against proteasome dysfunction, additional mechanisms not explored in this study are likely to be involved, including anti-apoptotic activities and signaling through the PI3K-Akt pathway (Junn et al. 2005; Xu et al. 2005; Yang et al. 2005; Fan et al. 2008).###Because iHsp70 up-regulation is not sufficient for the full DJ-1 neuroprotective effect against proteasome dysfunction, additional mechanisms not explored in this study are likely to be involved, including anti-apoptotic activities and signaling through the PI3K-Akt pathway (Junn et al. 2005; Xu et al. 2005; Yang et al. 2005; Fan et al. 2008).###In addition to eliminating potentially toxic a-synuclein aggregates via up-regulation of iHsp70, DJ-1 may activate various anti-apoptotic signaling pathways (Junn et al. 2005; Xu et al. 2005; Yang et al. 2005; Fan et al. 2008).",impact-revealing,highlighting the need for further exploration of mechanisms involved in neuroprotection
2416,53e99e61b7602d97027252c8,e3369553f399fb63b9f24261247afe64fb6c3f65,Virtual Program Counter (VPC) Prediction: Very Low Cost Indirect Branch Prediction Using Conditional Branch Prediction Hardware,53e9bd6fb7602d9704a0cd33,Pin: Building Customized Program Analysis Tools With Dynamic Instrumentation,"We use a Pin-based [38] cycle-accurate x86 simulator to###We use a Pin-based [38] cycle-accurate x86 simulator to evaluate VPC prediction.###iDNA [3] is a dynamic binary instrumentation tool similar to Pin [38], but capable of tracing Java virtual machines.###We use Pinpoints [45] to select a representative simulation region for each benchmark using the reference input set.",other,reporting tools used for evaluation
2417,5b1643998fbcbf6e5a9bc25e,080e1bb6bbebeb78f822b3998b7ed898ab6457aa,End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction,58d82fced649053542fd6b51,Deep clustering and conventional networks for music separation: stronger together.,"Following [22], our recent study [3] proposed a chimera++ network combining the two approaches via multi-task learning, as illustrated in the bottom of Fig.",other,describing a recent study and its approach
3826,5edf5dd891e011bc656deed4,ae0988c55095827384bf70e346e192a89216e75c,IterefinE: Iterative KG Refinement Embeddings using Symbolic Knowledge,57d063c3ac44367354290599,Representation Learning of Knowledge Graphs with Hierarchical Types.,"Therefore, we use the type labels for entities from [Xie et al., 2016] which also provides the domain and range information for relations.###There are some recent efforts to incorporate type hierarchy information in KG embeddings –e.g., TKRL [Xie et al., 2016] and TransC [Lv et al., 2018].###The resulting framework called IterefinE is based on the observation that the mispredictions by the embeddings based methods are often due to the lack of type compatibility between the entities due to their typeagnostic nature [Xie et al., 2016, Jain et al., 2018].",other,acknowledging prior work on type labels and their relevance in knowledge graph embeddings
2947,5f058d15dfae54570ec57ea1,fb93ca1e004cbdcb93c8ffc57357189fa4eb6770,Resnest: Split-attention networks,5550401245ce0a409eb3205c,Dropout: a simple way to prevent neural networks from overfitting,"To prevent this, dropout regularization randomly masks out some neurons during training (but not during inference) to form an implicit network ensemble [29, 49, 68].",other,providing context on dropout regularization in neural networks
1547,,8aecaef71770afd497e7b0eb2e373179b3214b1f,Understanding Protein Protocadherin-19 (PCDH19) Syndrome: A Literature Review of the Pathophysiology,,,"###Allopregnanolone is known to have anticonvulsive properties [18], which is why the use of ganaxolone, a synthetic analog, has gained clinical relevance showing promising results not only in mouse knockout models for fragile X syndrome [17] but also in adults and children [19-21].###The discovery of the alteration in steroidogenesis could help guide the treatment of this unique disease, probably also explaining the poor response to anticonvulsants and the purpose of corticoids validated by Higurashi et al. The link between epilepsy and fluctuations in sex steroid hormones has been made in the past as described by Reddy et al. in catamenial epilepsy, where a high ratio between estradiol:progesterone in the perimenstrual period increases seizure activity [15]; this association is also seen in female patients with PCHD19 [16,17].###The product of AKR1C3/AKR1C2 converts 5-alpha-dihydro-progesterone to allopregnanolone, the most potent positive modulator of GABA(A)R that increases the tonic and phasic inhibitory currents of the receptor [15-17].###The main clinical feature of PCHD19-related epilepsy is refractory epilepsy.###It is possible that a combination of all or some of these hypotheses plays a role in the pathophysiology of PCHD19 syndrome, with some contributions to various degrees.###in catamenial epilepsy, where a high ratio between estradiol:progesterone in the perimenstrual period increases seizure activity [15]; this association is also seen in female patients with PCHD19 [16,17].",impact-revealing,highlighting the clinical relevance of ganaxolone and its connection to allopregnanolone's properties in treating epilepsy
3063,5b67b45517c44aac1c86078b,e62ddf27659bc131968d2dcc3e2bd59de98c6917,"Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop.",53e9a310b7602d9702c1bbf4,Collective entity resolution in relational data,"It has many real applications, for example, matching records between enterprise databases with di erent schema [4], aligning protein-protein interaction networks to transfer biological knowledge across di erent species [30], constructing canonicalized knowledge base based on facts extracted from texts [6], and identifying users across multiple online social networks [34].###The problem of disambiguating who is who is referred to as name disambiguation, also named as entity resolution [3, 4], web appearance disambiguation [1, 11], name identi cation [16], and object distinction [31] from a broader viewpoint, and has been extensively studied for decades by di erent communities.",other,highlighting the significance and applications of name disambiguation
100,5ee8986891e011e66831c452,a9872078cc6dabd2428750543862b45f4a482dfc,Graph Meta Learning via Local Subgraphs,599c7974601a182cd263f01c,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.,"Meta learning methods generally fall into three categories, model-based [14, 50, 34], metric-based [37, 38, 47], and optimizationbased [32, 29, 12] techniques.###Meta-GNN t e a t - [63] applies MAML [12] to Simple Graph Convolution (SGC) [52].###This result confirms our analysis of using the prototypical loss to leverage the label inductive bias and MAML to transfer knowledge across graphs and labels.###Across meta-learning models, we observe G-META is consistently better than others such as MAML and ProtoNet.###Meta-GNNt -e at - [63] applies MAML [12] to Simple Graph Convolution (SGC) [52].###It allows direct adaptation to MAML since individual subgraphs can be considered as an individual image in the classic few-shot meta-learning setups.###To transfer the structural knowledge across graphs and labels, we use MAML, an optimization-based meta-learning approach.###MAML [12] switches ProtoNet to MAML as the meta-learner.###Note that baseline ProtoNet and MAML are the ablations of G-META.###MAML and ProtoNet have volatile results across different problems and tasks, whereas G-META is stable.###The goal of Model-Agnostic Meta-Learning (MAML) [12] is to obtain a parameter initialization ✓⇤ that can adapt to unseen tasks quickly, such as Dtest, using gradients information learnt during meta-training.###Note that the baselines ProtoNet and MAML can be considered as an ablation of G-META removing MAML and Prototypical loss respectively.###Finally, it uses prototypical loss for inductive bias and MAML for knowledge transfer across graphs and labels.",impact-revealing,describing categories and applications of meta-learning methods
3024,5f0d85c69fced0a24be4f019,6817b5d48cf8f665b680de07d23b91814d2923b1,Bouquet of Instruction Pointers: Instruction Pointer Classifier-based Spatial Hardware Prefetching,599c7e7c601a182cd28a5dc0,Prefetching For Cloud Workloads: An Analysis Based On Address Patterns,"It is well known that spatial prefetchers fail to improve performance for server workloads like CloudSuite [14], [33], [53], [58], [59] and additional prefetchers [12], [24], [52], [58], [59] can be used on top of IPCP to improve the performance.",other,highlighting limitations of spatial prefetchers and suggesting alternatives
3195,53e9bd8cb7602d9704a3405d,e23851a5ddec564325481eacd1669876e233ef73,A new case for the TAGE branch predictor,53e999b4b7602d97021f6710,"Low-Power, High-Performance Analog Neural Branch Prediction","However, tremendous progress were made during this period: the research of neural inspired predictors led by Jimenez [13, 9, 11, 1], the introduction of GEometric History Length predictors by Seznec [21] and the introduction of the TAGE predictor by Seznec and Michaud [26].###[9] D. Jimenez.###[10] D. Jiménez.###[12] D. Jiménez, S. W. 
Keckler, and C. Lin.###Since 2006, TAGE has been often considered as state-of-theart in terms of branch prediction accuracy [1, 5].###[13] D. Jiménez 
and C. Lin.###[11] D. Jiménez.###References [1] Renée St. Amant, Daniel A. Jiménez, and Doug 
Burger.###However, tremendous progress were made during this 
period: the research of neural in­spired predictors led by Jimenez [13, 9, 11, 1], the introduction of 
GEometric History Length predictors by Seznec [21] and the introduction of the TAGE predictor by Seznec 
and Michaud [26].###The TAGE predictor has often been considered as state-of-theart in conditional branch prediction in terms of prediction accuracy [1].###Both FTL++ and OH-SNAP are based on neural based techniques, respectively GEHL combined with LGEHL for FTL++ and piecewise linear [11] combined with dynamic weight adaptation [1] for OH-SNAP.###[14] Daniel A. Jiménez.",other,highlighting significant advancements in branch prediction research
660,5b1642388fbcbf6e5a9b5740,3913d2e0a51657a5fe11305b1bcc8bf3624471c0,learning structured representation for text classification via reinforcement learning,53e9a819b7602d970315f0f7,Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning,"Objective Function We optimize the parameters of PNet using REINFORCE algorithm (Williams 1992) and policy gradient methods (Sutton et al. 2000), aiming to maximize the expected reward as shown below.###Objective Function We optimize the parameters of PNet using REINFORCE algorithm (Williams 1992) and policy gradient methods (Sutton et al.",impact-revealing,describing the optimization method used for PNet
1005,,b187ae96d79bcebd81c5a513c64692730d28503e,"“We Feel Good”: Daily Support Provision, Health Behavior, and Well-Being in Romantic Couples",,,"###Social relationships are widely recognized for their protective role for physical health and psychological well-being (e.g., House et al., 1988; Kawachi and Berkman, 2001; Holt-Lunstad et al., 2010).",impact-revealing,acknowledging the significance of social relationships for health and well-being
1318,,76ed4805f3f8af26152faa91e61d08718fc8693f,Elastic Nailing for Pediatric Subtrochanteric and Supracondylar Femur Fractures,,,"###Elastic stable intramedullary nailing is widely used for the management of pediatric femur shaft fractures [6, 11, 15, 21].",impact-revealing,acknowledge common practice in pediatric fracture management
1158,,4e4ff93fb9c90fcb5958b4b02cae719a04cb5a45,Ultra-low latency cloud-fog computing for industrial Internet of Things,,,"###In [3], the low latency and low energy consumption performance of the fog computing is validated by comparing with the traditional cloud computing; In [4], authors focused on the service allocation problem in the Combined Fog-Cloud architecture to minimize the latency experienced; In [5], [6], authors proposed a novel C-RAN architecture with the mobile cloud computing.",impact-revealing,reporting prior findings on fog computing and cloud architecture
3254,5a9cb65d17c44a376ffb83f3,e06357ac23811054acb23e2ea30d087f5beaef90,an interpretable reasoning network for multi-relation question answering,58437722ac44360f1082f03b,ReasoNet: Learning to Stop Reading in Machine Comprehension.,"State-of-the-art memory-based Reading Comprehension models [ Sukhbaatar et al. , 2015; Kumar et al. , 2015; Shen et al. , 2016 ] make interactions between a question and the corresponding document in a multi-hop manner during reasoning.",other,reporting prior findings on memory-based reading comprehension models
1507,,1bd26bf7e7fbab10f8991dd23689b8ac751a11d0,Bayesian Learning of Graph Substructures,,,"###These models build on extensive work in random graphs and networks (Newman, 2011; Fortunato and Hric, 2016; Lee and Wilkinson, 2019) such as the stochastic blockmodel (Holland et al., 1983).###More recently, Newman (2011) advocates for more complex structures such as modularity.",impact-revealing,acknowledge foundational work in random graphs and networks
1309,,0ed6c7a75b40574a1ed154e4c97851cf4a82a13a,Politeness phenomena in Palestinian Arabic and Australian English: A cross-cultural study of selected contemporary plays,,,"###The first question was motivated by the work of many researchers, namely Nureddeen (2008), Schnurr, Marra and Holmes (2007), Vinagre (2008) and many others who made use of Brown and Levinson’s (1987) theory of politeness to explore politeness phenomena in specific cultures.",impact-revealing,highlighting the influence of prior research on the current question
3991,5c87a964da56296d04a90b97,a20bc2ecd8eb9bd735e6b3ba3d1a88f1c872d8eb,Shinjuku: Preemptive Scheduling for μsecond-scale Tail Latency,5390bfa220f70186a0f53dbb,Reconciling high server utilization and sub-millisecond quality-of-service,"We developed an open loop load generator similar to mutilate [36] that transmits requests over either TCP or UDP.###Unfortunately, thread management in modern operating systems such as Linux is not designed for microsecond-scale tasks and frequently produces long, unpredictable scheduling delays resulting in millisecond-scale tail latencies [36, 37].###Unfortunately, the Linux kernel scheduler operates at the millisecond scale because of preemption and context switch overheads and the complexity of simultaneously accommodating batch, background, and interactive tasks at different time scales [36, 37].###However, any service that uses one thread per request or connection and allows Linux to manage threads will experience millisecond-scale tail latencies, because the kernel employs preemption at millisecond granularities and its policies are not optimized for microsecond-scale tail latency [36, 37].",other,highlighting the limitations of thread management in modern operating systems for microsecond-scale tasks
3303,5a9cb66717c44a376ffb89eb,651adaa058f821a890f2c5d1053d69eb481a8352,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,5a260c8617c44a4ba8a32085,Certifiable Distributional Robustness with Principled Adversarial Training.,"We omit two defenses with provable security claims (Raghunathan et al., 2018; Sinha et al., 2018) and one that only argues black-box security (Tramèr et al.###We omit two defenses with provable security claims (Raghunathan et al., 2018; Sinha et al., 2018) and one that only argues black-box security (Tramèr et al., 2018).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2945,53e9af67b7602d97039a85ee,529e8c6e6b5a6cb4f1cf202c47d9d42f5889ec1d,Last-Touch Correlated Data Streaming,53e99d51b7602d970260624e,Speculative Precomputation: Long-Range Prefetching Of Delinquent Loads,"A number of proposals [7,14,16,21,22] advocate microarchitectural enhancements to the speculation window and allow instructions to flow speculatively beyond long-latency memory accesses.",other,acknowledge proposals for microarchitectural enhancements
2920,5fd3404791e01161cf73952c,e988e15d200faf64bb71e155b8354c4e127f7dab,Bipartite Graph Embedding via Mutual Information Maximization,5550401245ce0a409eb3205c,Dropout: a simple way to prevent neural networks from overfitting,Dropout [30] is applied to each layer of our encoder to regularize model parameters.,other,describing the application of dropout for regularization
332,5aed14e217c44a4438159759,c097be22f1e87a846385047346b73610d91fea4e,GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"Graph convolutional networks on large graph Applying graph convolution on large graphs is challenging because the memory complexity is proportional to the total number of nodes, which could be hundreds of thousands of nodes in large graphs (Hamilton et al., 2017a).###The GraphSAGE model used a 2-layer sample and aggregate model with a neighborhood size of S (1) = 25 and S (2) = 10 without dropout.###Our max pooling and avg. pooling baselines have higher scores on Reddit than that in the original GraphSAGE paper.###…because the full spectral treatment requires eigendecomposition of the Laplacian matrix, which is computationally intractable on large graphs, while the localized versions (Defferrard et al., 2016; Kipf and Welling, 2017) can be interpreted as graph aggrega-tors (Hamilton et al., 2017a).###Some models only integrate the node features of neighborhoods (Du-venaud et al., 2015; Kipf and Welling, 2017; Hamilton et al., 2017a), while others integrated edge features as well (Atwood and Towsley, 2016; Fout et al., 2017; Sch ¨ utt et al., 2017).###Also, we can see steady improvement with larger sampling sizes, which is consistent with the observation in (Hamilton et al., 2017a).###Traditionally, this is achieved by calculating various graph statistics like degree and centrality, using graph kernels, or extracting human engineered features (Hamilton et al., 2017b).###Many crucial machine learning tasks involve graph structured datasets, such as classifying posts in a social network (Hamilton et al., 2017a), predicting interfaces be-tween proteins (Fout et al., 2017) and forecasting the future trafﬁc speed in a road network (Li et al., 2018).###To reduce memory usage and computational cost, (Hamilton et al., 2017a) proposed the GraphSAGE framework that uses a sampling algorithm to select a small subset of the nodes and edges.###This includes GraphSAGE (Hamilton et al., 2017a), GAT (Veli ˇ ckovi ´ c et al., 2018), and FastGCN (Chen et al., 2018).###Extensive experiments on two node classiﬁcation datasets, PPI and Red-dit (Hamilton et al., 2017a), and one trafﬁc speed forecasting dataset, METR-LA (Li et al., 2018), show that GaAN consistently outperforms the baseline models and achieves the state-of-the-art performance.###Our approach follows that of (Hamilton et al., 2017a), where a mini-batch of nodes are sampled on each iteration during training and multiple layers of graph aggregators are stacked to compute the predictions.###…pivoted to solving these problems by graph convolution (Duvenaud et al., 2015; Atwood and Towsley, 2016; Kipf and Welling, 2017; Fout et al., 2017; Hamilton et al., 2017a; Veliˇckovi´c et al., 2018; Li et al., 2018), which generalizes the stan-∗ These two authors contributed equally. dard…###The training, validation, and testing splits are the same as that in (Hamilton et al., 2017a).###We performed a thorough comparison of GaAN with the state-of-the-art models, ﬁve aggregator-based models in our framework and a two-layer fully connected neural network on the PPI and Reddit datasets (Hamilton et al., 2017a).###We also improve the sampling strategy introduced in (Hamilton et al., 2017a) to reduce the memory cost and increase the run-time efﬁciency, in order to train our model and other graph aggregators on relatively large graphs.###On each iteration, GraphSAGE ﬁrst uniformly samples a mini-batch of nodes.###We refer to such an operator as a graph aggregator (Hamilton et al., 2017a) and the set of local nodes as the receptive ﬁeld of the aggregator.###Most existing graph aggregators are based on either pooling over neighborhoods (Kipf and Welling, 2017; Hamilton et al., 2017a) or computing a weighted sum of the neighboring features (Monti et al., 2017).",impact-revealing,discussing challenges and advancements in graph convolutional networks
2774,5d04e8d7da56295d08daef06,448b64df68335d3695a37c54770e7d5cd5f6fe68,Context Attentive Document Ranking and Query Suggestion,5736980d6e3b12023e6e4493,Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval.,"Inspired by these scenarios, recent works [2, 17, 27, 34, 41] have proposed to jointly model multiple types of user search activities.###[27] proposed a multi-task deep neural approach to combine query classification and document ranking, and showed improvement on both tasks.###Multi-task learning has been explored in information retrieval studies [17, 27, 34, 41].",other,highlighting advancements in multi-task learning for information retrieval
1475,,5879e2266b71906f8910ae172b422469f30fb78d,Predicting Acoustic Transmission Loss Uncertainty in Ocean Environments with Neural Networks,,,"###Clevert, D.-A.; Unterthiner, T.; Hochreiter, S. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). arXiv 2016, arXiv:1511.07289.###The hidden layers shared the same choice of non-linear activation function from the following options: (1) the rectified linear unit (ReLU) [26]; (2) the exponential linear unit (ELU) [27]; and (3) the Swish [28] activation function with a constant parameter value of one.###10−4.5 10−2.0 10−3.7 N/A Learning rate: LN3 10−5.0 10−3.5 N/A 10−4.5
AMSgrad : β1 1− (1/10)0.95 1− (1/10)2.5 1− (1/10)2.3 1− (1/10)1.0 AMSgrad : β2 1− (1/10)1.0 1− (1/10)4.0 1− (1/10)3.1 1− (1/10)1.1
Hyperparameter (Categorical) Choices Best Histogram Best LN3
Activation function ReLU ELU Swish Swish Swish Grid points in range 5 11 15 11 11 Grid points in depth 5 11 17 17 17 Spatial units in range 1 2 1 1 Spatial units in depth 0.5 1 1 1
Spatial unit 15 m 1 wavelength 15 m 15 m
For each hyperparameter configuration considered, four NNs were randomly initialized, trained on random subsets of the training dataset, and cross-validated by monitoring their performance on the remaining training examples.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2530,5bdc315817c44a1f58a05e88,9ea992f009492888c482d5f4006281eaa8b758e7,"X2Face: A network for controlling face generation by using images, audio, and pose codes",5a260c8417c44a4ba8a31bf1,Interpretable Transformations with Encoder-Decoder Networks,") by conditioning the generated image on known ground truth information which may be head pose, expression, or landmarks [5, 12, 21, 30, 42, 44].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2686,5c45b4d03a55ac25e7f0f55c,e5badcfa663c30a983da24dd682288141d00fcc3,GraphSAR: A Sparsity-Aware Processing-in-Memory Architecture for Large-Scale Graph Processing on ReRAMs,5a260c3517c44a4ba8a25526,A novel ReRAM-based processing-in-memory architecture for graph computing,"Many graph processing architectures/accelerators based on ReRAMs have been proposed (e.g., RPBFS [18], GraphR [1], and HyVE [19]), achieving great speedup and energy efficiency improvement.###RPBFS adopts a shared memory to store values of vertices.###RPBFS [18] uses a shared memory to store values of all vertices.###, RPBFS [18], GraphR [1], and HyVE [19]), achieving great speedup and energy efficiency improvement.###From the experimental results of RPBFS we can see, when the size of crossbar increases, larger graphs achieve more improvements against smaller graphs.###This means that, the performance of RPBFS will decrease drastically when graphs become larger compared with the fixed-size crossbar.###Instead, in the design of RPBFS, GraphR, and HyVE, edges in the graph are stored using the edge list format.",other,acknowledge advancements in graph processing architectures
1946,,902baf934e5fd2a22e8dbf732a649509ee52ba2a,Scheduling Coflows for Minimizing the Makespan in Identical Parallel Networks,,,"###These real traces have been widely used as benchmarks in various works, such as [11, 16, 18, 21].###[11] introduced a SmallestEffective-Bottleneck-First heuristic that allocates coflows greedily based on the maximum server loads.###In the literature, numerous heuristic algorithms have been proposed to tackle coflow scheduling problems, such as those discussed in [10, 11, 15, 22].",impact-revealing,acknowledge the use of real traces as benchmarks in various works
2111,,49873463cd0fd2670fdbf8acf89a40bba0eeaf8b,Variable Selection with Scalable Bootstrap in Generalized Linear Model for Massive Data,,,"###The method is inspired by the idea of Bag of Little Bootstraps (BLB) [8] which bootstraps multiple smaller subsets of a larger dataset, and then incorporates it with the method of variable selection.###According to [8], it is suggested to set γ ∈ [0 .",impact-revealing,describing the method inspired by Bag of Little Bootstraps
3528,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,59ae3c152bbe271c4c71e955,Aggregating And Predicting Sequence Labels From Crowd Annotations,"DS (Dawid and Skene, 1979), HMM (Nguyen et al., 2017) and BEA (Rahimi et al., 2019) induce consensus labels with probability models.###Nguyen et al. (2017) augments the LSTM 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 Annotator ID architecture with crowd vectors.###Nguyen et al. (2017); Rodrigues and Pereira (2018); Simpson et al. (2020) regards crowd annotations as noisy gold labels and constructs crowd components to model annotator-speciﬁc bias which were discarded during the inference process.",other,reporting methods for inducing consensus labels
1293,,e9f920bdf9e47d56e04d754bf1200a93a06f1669,Magnetic resonance spectroscopic imaging with 2D spectroscopy for the detection of brain metabolites,,,"###Linear Combination of Model Spectra LCModel [63] incorporates phase, chemical shift and line-width variations and is widely used for spectral fitting and accurate quantification of metabolites.",impact-revealing,describing a widely used method for spectral fitting
3631,5c2c7a9217c44a4e7cf3189c,9ae43e25b04f5c35173b0bf490612015bd86c08f,Face-Focused Cross-Stream Network for Deception Detection in Videos,5bdc31c217c44a1f58a0ca5c,Zero and Few Shot Learning with Semantic Feature Synthesis and Competitive Learning.,"To deal with the data scarcity problem, we propose to use meta learning [39, 52, 25, 42] to train our FFCSN (see Figure 2).###For the training data scarcity problem, we introduce meta learning [39, 52, 25] and adversarial learning [11, 23, 24] into the training process of our FFCSN.###However, meta-learning [39, 52, 25] was originally proposed for transfer learning.",other,highlighting the application of meta-learning to address data scarcity
1334,,05d4c4c66c7d327a87c7ec4f456e34e5596ec036,Colorblind Policies And The Discourses That Uphold Them: The Social Construction Of Bullying,,,"###…the Columbine shooting and the outcome of a lawsuit
stating schools could be held liable for continued sexual harassment against students, led to more than 120 bills passing between 1999 and 2010 either initiating or amending anti-bullying policies
5
in 49 of 50 states (Cornell & Limber, 2015).###Students of color are marginalized in schools similar to the ways Crenshaw (1991) describes women of color being marginalized in legal realms when those who define and choose policies fail to consider the effects of institutionalized racism and the ways identities are constructed in school settings.",impact-revealing,highlighting the impact of legal outcomes on anti-bullying policies
2317,57d063e0ac44367354294777,9c62532d78d6eb31f7b0489b6bcb4c2baf517860,CASH: Supporting IaaS Customers with a Sub-core Configurable Architecture,53e9b641b7602d97041a11b1,ControlWare: a middleware architecture for feedback control of software performance,", [22, 24, 25, 35, 38, 48, 51, 63]), CASH uses control theory to manage resources and meet performance goals.###Some prior work has provided more general implementations by implementing control systems at the middleware layer [18, 35, 63].###CASH is influenced by prior research that has effectively deployed control systems to meet QoS demands [17, 22, 24, 25, 35, 48, 51, 63].",other,acknowledge influence of prior research on control systems
989,,fbd945d94ab47de93a2e1d64d03cc9e0a1c31630,Enhancing knowledge sharing with information and communication technology.,,,"###The study adapted the questionnaire items that had been validated by Nonaka et al. (1994) to assess the modes of knowledge sharing, with changes in wording that were perceived as appropriate for the accounting profession setting .###The knowledge sharing constructs were those from Nonaka et al. (1994) and the firm performance construct was based on those used by Abernethy et al. (2004) and Bisbe and Otley (2004).",impact-revealing,describing the adaptation of validated questionnaire items for a specific context
3091,5b1643998fbcbf6e5a9bc32d,2fe2cfd98e232f1396f01881853ed6b3d5e37d65,Taskonomy: Disentangling Task Transfer Learning,573696f46e3b12023e5f160a,Unsupervised Visual Representation Learning by Context Prediction,"colorization) [65, 69, 15, 100, 97, 66].###compositional modeling [33, 8, 11, 21, 53, 89, 87], homomorphic cryptography [40], lifelong learning [90, 13, 82, 81], functional maps [68], certain aspects of Bayesian inference and Dirichlet processes [52, 88, 87, 86, 35, 37], few-shot learning [78, 23, 22, 67, 83], transfer learning [72, 81, 27, 61, 64, 57], un/semi/selfsupervised learning [20, 6, 15, 100, 17, 80], which are studied across various fields [70, 91, 10].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
138,5bdc318017c44a1f58a08780,5ab5658a1666e26c66f0319a469228dbe19598a2,An e-learning recommendation approach based on the self-organization of learning resource,57a4e912ac44365e35c961b7,A learner oriented learning recommendation approach based on mixed concept mapping and immune algorithm.,"The related study is presented in our previous paper [50].###Referring to other research [50], we design the elements of learning styles as: TU = {CL,MP,FP, PU,AT, FE,AC,DC,HP}.",impact-revealing,acknowledging prior work and its relevance
1807,,49e6b28bd63bd4b7a6486fd94ef182bbf2127c37,Effects of Explaining Anomalies on the Generation and Evaluation of Hypotheses,,,"###For example, a mathematics student might form tentative beliefs about how to solve a novel problem, but subsequently revise these beliefs in the face of anomalous data: observations that conflict with working assumptions and therefore signal a need to revise beliefs (Chinn & Brewer, 1993; Koslowski, 1996). Here we consider how beliefs are revised in light of anomalous observations, and in particular how explaining such observations influences learning. Generating explanations has been shown to promote learning across a range of tasks and domains, with evidence from experimental studies of category learning (Williams & Lombrozo, 2010), “self-explaining” in students (e.g., Fonseca & Chi, 2011), and conceptual development in children (e.g., Siegler, 2002; Wellman & Liu, 2007). Benefits of explanation are likely to derive from multiple sources, including increased engagement and increased accessibility of effective strategies (Siegler, 2002), better metacognitive monitoring (Chi et al, 1994), and the generation of inferences to fill gaps in understanding (Chi et al, 2000), among others. In the present work we build on the Subsumptive Constraints account of explanation developed in prior research (Williams & Lombrozo, 2010, 2013). According to this account, explaining a particular observation drives learners to interpret it as an instance of a broad pattern or generalization, and thereby facilitates learning about regularities that apply broadly (Williams & Lombrozo, 2010; 2013; Williams, Lombrozo & Rehder, 2013). To illustrate, consider the findings reported in Williams and Lombrozo (2010). Participants attempted to learn a new classification system involving two categories that could be differentiated by a rule with no exceptions (100% rule) or an alternative that accounted for most cases, but with two anomalies (75% rule). Participants who were prompted to explain were significantly more likely to discover the 100% rule than those prompted to describe the category members, think aloud, or engage in free study. These findings confirm the prediction that explaining facilitates learning about broad patterns, and also suggest that explaining could make learners especially sensitive to anomalies, as they signal that current beliefs are limited in scope if not false. Subsequent research, however, suggests a more complicated relationship between explanation and anomalies. Williams and Lombrozo (2013) found that participants prompted to explain favored patterns consistent with prior knowledge, even when such patterns had exceptions (anomalies) that were better explained by alternative patterns. Williams, Lombrozo, and Rehder (2013) found that participants prompted to explain were more likely to overgeneralize broad patterns, effectively ignoring exceptions, even when this resulted in slower and less accurate learning. Explaining can thus have seemingly opposite effects: by encouraging learners to seek broad patterns, explaining can sometimes lead to greater belief revision in light of anomalies, and at other times to the anomalies being effectively dismissed or “explained away” (see also Chinn & Brewer, 1993; Khemlani & Johnson-Laird, 2012; Koslowski, 1996). As a first step towards understanding the conditions under which explanation has each effect, Williams, Walker and Lombrozo (2012) investigated how changing the number of anomalous observations presented interacted with a prompt to explain.###…a mathematics student might form tentative beliefs about how to solve a novel problem, but subsequently revise these beliefs in the face of anomalous data: observations that conflict with working assumptions and therefore signal a need to revise beliefs (Chinn & Brewer, 1993; Koslowski, 1996).###…by encouraging learners to seek broad patterns, explaining can sometimes lead to greater belief revision in light of anomalies, and at other times to the anomalies being effectively dismissed or "" explained away "" (see also Chinn & Brewer, 1993; Khemlani & Johnson-Laird, 2012; Koslowski, 1996).###For example, a mathematics student might form tentative beliefs about how to solve a novel problem, but subsequently revise these beliefs in the face of anomalous data: observations that conflict with working assumptions and therefore signal a need to revise beliefs (Chinn & Brewer, 1993; Koslowski, 1996).###For example, a mathematics student might form tentative beliefs about how to solve a novel problem, but subsequently revise these beliefs in the face of anomalous data: observations that conflict with working assumptions and therefore signal a need to revise beliefs (Chinn & Brewer, 1993; Koslowski, 1996). Here we consider how beliefs are revised in light of anomalous observations, and in particular how explaining such observations influences learning. Generating explanations has been shown to promote learning across a range of tasks and domains, with evidence from experimental studies of category learning (Williams & Lombrozo, 2010), “self-explaining” in students (e.g., Fonseca & Chi, 2011), and conceptual development in children (e.g., Siegler, 2002; Wellman & Liu, 2007). Benefits of explanation are likely to derive from multiple sources, including increased engagement and increased accessibility of effective strategies (Siegler, 2002), better metacognitive monitoring (Chi et al, 1994), and the generation of inferences to fill gaps in understanding (Chi et al, 2000), among others. In the present work we build on the Subsumptive Constraints account of explanation developed in prior research (Williams & Lombrozo, 2010, 2013). According to this account, explaining a particular observation drives learners to interpret it as an instance of a broad pattern or generalization, and thereby facilitates learning about regularities that apply broadly (Williams & Lombrozo, 2010; 2013; Williams, Lombrozo & Rehder, 2013). To illustrate, consider the findings reported in Williams and Lombrozo (2010). Participants attempted to learn a new classification system involving two categories that could be differentiated by a rule with no exceptions (100% rule) or an alternative that accounted for most cases, but with two anomalies (75% rule).###Explaining can thus have seemingly opposite effects: by encouraging learners to seek broad patterns, explaining can sometimes lead to greater belief revision in light of anomalies, and at other times to the anomalies being effectively dismissed or “explained away” (see also Chinn & Brewer, 1993; Khemlani & Johnson-Laird, 2012; Koslowski, 1996).###For example, a mathematics student might form tentative beliefs about how to solve a novel problem, but subsequently revise these beliefs in the face of anomalous data: observations that conflict with working assumptions and therefore signal a need to revise beliefs (Chinn & Brewer, 1993; Koslowski, 1996). Here we consider how beliefs are revised in light of anomalous observations, and in particular how explaining such observations influences learning. Generating explanations has been shown to promote learning across a range of tasks and domains, with evidence from experimental studies of category learning (Williams & Lombrozo, 2010), “self-explaining” in students (e.g., Fonseca & Chi, 2011), and conceptual development in children (e.g., Siegler, 2002; Wellman & Liu, 2007). Benefits of explanation are likely to derive from multiple sources, including increased engagement and increased accessibility of effective strategies (Siegler, 2002), better metacognitive monitoring (Chi et al, 1994), and the generation of inferences to fill gaps in understanding (Chi et al, 2000), among others. In the present work we build on the Subsumptive Constraints account of explanation developed in prior research (Williams & Lombrozo, 2010, 2013). According to this account, explaining a particular observation drives learners to interpret it as an instance of a broad pattern or generalization, and thereby facilitates learning about regularities that apply broadly (Williams & Lombrozo, 2010; 2013; Williams, Lombrozo & Rehder, 2013). To illustrate, consider the findings reported in Williams and Lombrozo (2010). Participants attempted to learn a new classification system involving two categories that could be differentiated by a rule with no exceptions (100% rule) or an alternative that accounted for most cases, but with two anomalies (75% rule). Participants who were prompted to explain were significantly more likely to discover the 100% rule than those prompted to describe the category members, think aloud, or engage in free study. These findings confirm the prediction that explaining facilitates learning about broad patterns, and also suggest that explaining could make learners especially sensitive to anomalies, as they signal that current beliefs are limited in scope if not false. Subsequent research, however, suggests a more complicated relationship between explanation and anomalies. Williams and Lombrozo (2013) found that participants prompted to explain favored patterns consistent with prior knowledge, even when such patterns had exceptions (anomalies) that were better explained by alternative patterns. Williams, Lombrozo, and Rehder (2013) found that participants prompted to explain were more likely to overgeneralize broad patterns, effectively ignoring exceptions, even when this resulted in slower and less accurate learning.###For example, a mathematics student might form tentative beliefs about how to solve a novel problem, but subsequently revise these beliefs in the face of anomalous data: observations that conflict with working assumptions and therefore signal a need to revise beliefs (Chinn & Brewer, 1993; Koslowski, 1996). Here we consider how beliefs are revised in light of anomalous observations, and in particular how explaining such observations influences learning. Generating explanations has been shown to promote learning across a range of tasks and domains, with evidence from experimental studies of category learning (Williams & Lombrozo, 2010), “self-explaining” in students (e.g., Fonseca & Chi, 2011), and conceptual development in children (e.g., Siegler, 2002; Wellman & Liu, 2007). Benefits of explanation are likely to derive from multiple sources, including increased engagement and increased accessibility of effective strategies (Siegler, 2002), better metacognitive monitoring (Chi et al, 1994), and the generation of inferences to fill gaps in understanding (Chi et al, 2000), among others. In the present work we build on the Subsumptive Constraints account of explanation developed in prior research (Williams & Lombrozo, 2010, 2013). According to this account, explaining a particular observation drives learners to interpret it as an instance of a broad pattern or generalization, and thereby facilitates learning about regularities that apply broadly (Williams & Lombrozo, 2010; 2013; Williams, Lombrozo & Rehder, 2013). To illustrate, consider the findings reported in Williams and Lombrozo (2010). Participants attempted to learn a new classification system involving two categories that could be differentiated by a rule with no exceptions (100% rule) or an alternative that accounted for most cases, but with two anomalies (75% rule). Participants who were prompted to explain were significantly more likely to discover the 100% rule than those prompted to describe the category members, think aloud, or engage in free study. These findings confirm the prediction that explaining facilitates learning about broad patterns, and also suggest that explaining could make learners especially sensitive to anomalies, as they signal that current beliefs are limited in scope if not false. Subsequent research, however, suggests a more complicated relationship between explanation and anomalies. Williams and Lombrozo (2013) found that participants prompted to explain favored patterns consistent with prior knowledge, even when such patterns had exceptions (anomalies) that were better explained by alternative patterns.",impact-revealing,highlighting the role of explanation in belief revision and learning
62,5ebe685391e0117693a52241,99314a532a3358cb86064fc8917ed2c283227539,NAT: Noise-Aware Training for Robust Neural Sequence Labeling,57a4e91aac44365e35c976e2,Improving the Robustness of Deep Neural Networks via Stability Training,"Zheng et al. (2016) pointed out the output instability issues of deep neural networks.###Inspired by recent research in computer vision (Zheng et al., 2016), Neural Machine Translation (NMT; Cheng et al., 2018), and ASR (Sperber et al., 2017), we propose two Noise-Aware Training (NAT) objectives that improve the accuracy of sequence labeling performed on noisy input without reducing…###• We implement a stability training method (Zheng et al., 2016), adapted to the sequence labeling scenario, which explicitly addresses the noisy input data problem by encouraging the model to produce a noise-invariant latent representation ( § 3.4).###Zheng et al. (2016) presented a general method to stabilize model predictions against small input distortions.",impact-revealing,highlighting the significance of addressing output instability in deep neural networks
443,573697f96e3b12023e6d2f31,97acdfb3d247f8250d865ef8a9169f06e40f138b,EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding,5550443b45ce0a409eb4c3b9,Towards end-to-end speech recognition with recurrent neural networks,"Meanwhile, another line of work [6, 7, 8, 9, 10, 11, 12] has focused on end-to-end ASR, i.###Our experiments with the Wall Street Journal (WSJ) benchmark show that Eesen results in superior performance than the existing end-to-end ASR pipelines [6, 8].###To be consistent with previous work [6, 8], we report our results on the eval92 set.###This CTC technique is further investigated in [6, 7, 8, 14] on large-scale acoustic modeling tasks.###When decoding CTC-trained models, past work [6, 8, 10] has successfully constrained search paths with lexicons.###Previous work has introduced a variety of methods [6, 8, 10] to decode CTC-trained models.###Instead, following [6, 8, 10], we adopt the CTC objective [13] to automatically learn the alignments between speech frames and their label sequences (e.###For example, [6] and [8] adopt two distinct versions of beam search for decoding CTC models.###CTC experiments in past work [6] have adopted an expanded vocabulary, and re-trained the language model using text data released together with the WSJ corpus.###When only the lexicon is used, our decoding behaves similarly as the beam search in [6].",impact-revealing,acknowledge existing methods and results in end-to-end ASR
733,53e9b9fbb7602d97045fabff,f89facea7ae5a3f51af96b549f04f3dbe8c884b3,SHIFT: Shared history instruction fetch for lean-core server processors,53e9982cb7602d97020504ec,Proactive instruction fetch,"Moreover, by embedding the shared instruction history in the LLC, SHIFT obviates the need 
for dedi­cated instruction history storage, while transparently enabling multiple instruction histories 
in the presence of workload consoli­dation.###For instance, in 40nm technology, an ARM Cortex-A8 
(a dual-issue in-order core) together with its L1 caches occupies an area of 1.3mm2, whereas PIF s per-core 
storage cost is 0.9mm2 .",impact-revealing,providing context on the benefits of the SHIFT method in instruction history management
2261,5bbacb9e17c44aecc4eaff64,5b1516c87818084dc5d195cc274e1ee8923210d2,Neural Cross-Lingual Named Entity Recognition with Minimal Resources,59ae3bf12bbe271c4c71bcf9,Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection,", 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017).###This transfer can be performed using a variety of resources, including parallel corpora (T¨ackstr¨om et al., 2012; Ni et al., 2017), Wikipedia (Nothman et al., 2013), and large dictionaries (Ni et al., 2017; Mayhew et al., 2017).###This transfer can be performed using a variety of resources, including parallel corpora (Täckström et al., 2012; Ni et al., 2017), Wikipedia (Nothman et al.###To cope with the first challenge of lexical mapping, a number of methods use parallel corpora to project annotations between languages through word alignment (Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014; Ni et al., 2017).###In this work, we limit ourselves to a setting where we have the following resources, making us comparable to other methods such as Mayhew et al. (2017) and Ni et al. (2017): • Labeled training data in the source language.###• Common space This is the most common setting for using bilingual word embeddings, and has recently been applied in NER (Ni et al., 2017).###Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Am-mar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017).###Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features.###To cope with the ﬁrst challenge of lexical mapping, a number of methods use parallel corpora to project annotations between languages through word alignment (Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014; Ni et al., 2017).###This indicates that models trained directly on data using the source Instead of directly modeling the shared embedding space (Guo et al., 2015; Zhang et al., 2016; Fang and Cohn, 2017; Ni et al., 2017), we leverage the shared embedding space for word translation.###Instead of directly modeling the shared embedding space (Guo et al., 2015; Zhang et al., 2016; Fang and Cohn, 2017; Ni et al., 2017), we leverage the shared embedding space for word translation.",other,acknowledge existing methods for cross-lingual applications
3816,599c7959601a182cd2633b3e,c0c0990b84a350d5efde8d3b2cb2636b6b57c21c,On Sampling Strategies for Neural Network-based Collaborative Filtering,57a4e91dac44365e35c9859c,A Neural Autoregressive Approach to Collaborative Filtering.,"Hence, it is natural to combine deep learning with traditional collaborative filtering for recommendation tasks, as seen in recent studies [1, 4, 32, 37].",other,acknowledge the integration of deep learning and collaborative filtering in recommendation tasks
1838,,f90966987c87047c717cd21d126c4e07110e75fd,Verifying Smart Card Applications: An ASM Approach,,,"###5 docset doc is equivalent to doc ∈ synth(docset )i n [ Pau98 ].###Our representation of documents and the attacker’s knowledge is inspired by this approach: we use documents and functions very similar to analz and synth in [ Pau98 ].###Ports can be linked by channels and the agents communicate by sending messages (described by a freely generated data type document similar to msg used in [ Pau98 ]) over these channels.###Paulson uses Isabelle to verify security protocols [ Pau98 ].###The difference is that of state based vs. event based representation: recovering the current points loaded onto a card could be done in [ Pau98 ] by accumulating the points of all successful load and pay messages and adding/subtracting them.###A middle ground between [ Pau98 ] and our approach is also possible as the case study [BR98] on the Needham-Schroeder protocol shows: there an ASM is used to formalize runs, but a global execution trace is used instead of agent states.###Such a technique could be combined with our model of the possible runs of the application, but so far reasoning based on [ Pau98 ] was sufficient.###Our approach differs in how system runs are describ ed. First, [ Pau98 ] describes protocol runs declaratively using an inductive definition of the set of possible traces while our approach describes runs using iterative application of (operationally specified) ASM rules.###Distinct from most other approaches to security protocol verification (for example [ Pau98 ]) which model the state of an agent implicitly by the history of the steps it has performed, we explicitly model the internal state of the agents.###A second difference is that we use an explicit representation of the agents’ states, while [ Pau98 ] instead defines an explicit system trace consisting of all messages that have been sent.",impact-revealing,comparing and contrasting different approaches to security protocol verification
2394,53e99e61b7602d97027252c8,e3369553f399fb63b9f24261247afe64fb6c3f65,Virtual Program Counter (VPC) Prediction: Very Low Cost Indirect Branch Prediction Using Conditional Branch Prediction Hardware,53e9b5edb7602d97041417c7,Diverge-Merge Processor (DMP): Dynamic Predicated Execution of Complex Control-Flow Graphs Based on Frequently Executed Paths,"We [31], [30] recently proposed handling hard-to-predict indirect branches using dynamic predication [36].###Unlike VPC prediction, dynamic predication of indirect branches requires compiler support, new instructions in the instruction set architecture, and significant hardware support for dynamic predication (as described in [36]).",other,reporting on recent proposals for handling indirect branches
2972,5a260c8117c44a4ba8a30a57,908272f8e6340971600148d4e73f50e1e8843aaf,"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec",57aa28de0a3ac518da9896d6,Structural Deep Network Embedding,"…[7, 11, 17, 34], direct network embedding [27], semi-supervised network embedding [18, 42, 49], network embedding with rich vertex attributes [41, 47], network embedding with high order structure [5, 15], network embedding via deep neural network [6, 22, 44], signed network embedding [9], etc.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2481,5f896fa591e01149071e45df,1d16d4cdc3fcce26e2c2097d13896ec09683eee3,Self-training for Few-shot Transfer Across Extreme Task Differences,5df8a7763a55ac792c90fc56,A New Benchmark for Evaluation of Cross-Domain Few-Shot Learning,"For simplicity and based on results reported by Guo et al. (2020), we freeze the representation φ after performing STARTUP and train a linear classiﬁer on the support set and evaluate the classiﬁer on the query set.###In a recently released benchmark consisting of datasets from extremely different domains (Guo et al., 2020), we show that STARTUP provides signiﬁcant gains (up to 2.9 points on average) over few-shot learning, transfer learning and self-supervision state-of-the-art.###As such, recent work has found that all few-shot learners fail in the face of such extreme task/domain differences, underperforming even naive transfer learning from ImageNet (Guo et al., 2020).###When the domain gap between the base and novel dataset is large, recent work (Guo et al., 2020; Chen et al., 2019a) has shown that existing state-of-the-art few-shot learners fail to generalize.###∗ are methods reported in (Guo et al., 2020 The verdict is clear - STARTUP beneﬁts from more unlabeled data (Figure 3).###∗ Numbers re-ported in (Guo et al., 2020) re-initializing the classiﬁer head works better than using the pre-trained classiﬁer ( STARTUP vs STARTUP -###We compare to the techniques reported in Guo et al. (2020), which includes most state-of-the-art approaches as well as a cross-domain few-shot technique Tseng et al. (2020).###Following Guo et al. (2020), we evaluate 5-way k-shot classiﬁcation tasks (the support set consists of 5 classes and k examples per class) for k ∈ { 1 , 5 } and report the mean and 95% conﬁdence interval over 600 few-shot tasks (conclusions generalize to k ∈ { 20 , 50 } .###We experiment with the challenging (BSCD-FSL) benchmark introduced in Guo et al. (2020).",other,highlighting the effectiveness of the STARTUP method in few-shot learning
829,5e54f1813a55acae32a25da5,76b8d5f2ef97d71167aa78309918bf3f7d633c96,Author Name Disambiguation on Heterogeneous Information Network with Adversarial Representation Learning,59939d49ffdae9cf10039e6f,IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models,"IRGAN (Wang et al. 2017) unifies generative model and discriminative model in information retrieval, where the discriminative model provides guidance to the generative model, and the generative model generates difficult examples for the discriminative model.",impact-revealing,reporting prior findings on the unification of generative and discriminative models in information retrieval
2176,,fa60d8a70d27d87c6a12cd11e84a015883bf2f47,Defining Cloud Computing in Business Perspective: A Review of Research,,,"###Vouk (2008) opined that ""cloud computing embraces cyber-infrastructure, and builds upon virtualization, distributed computing, grid computing, utility computing, networking, and web and software services"".",impact-revealing,providing a definition of cloud computing
2641,5550411c45ce0a409eb3897f,fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5,Neural Machine Translation by Jointly Learning to Align and Translate,53e9a169b7602d9702a5aa0e,Domain Adaptation via Pseudo In-Domain Data Selection.,"(2014a), we reduce the size of the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).5 We do not use any monolingual data other than the mentioned parallel corpora, although it may be possible to use a much larger monolingual corpus to pretrain an encoder.###Following the procedure described in Cho et al. (2014a), we reduce the size of the combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).",other,reporting data selection method and corpus size reduction
3036,5f0277e911dc830562231dab,6deae79dec438eaaa524bca3b82c6b8d93553b20,A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data,5bbacb6117c44aecc4ead3f5,Unbiased offline recommender evaluation for missing-not-at-random implicit feedback.,"Previous studies have shown that models and evaluation metrics that ignore the biases do not reflect the true performance of a recommender system, and that explicitly handling of the biases may help improve the performance [16, 28, 31].###Sample-based distillation includes the IPS method [24, 31] and other approaches as described in Section 2.###IPS is one of the most popular counterfactual approaches for recommendation [24, 31], where each sample is weighted with an IPS, referring to the likelihood of the sample being logged.###This can be considered as a stochastic logging policy by following [24, 31], and thus the user set is biased.",other,highlighting the importance of addressing biases in recommender systems
2199,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,53e9bb4bb7602d970478cd07,"Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification","0 (Blitzer et al., 2007) is used for text classiﬁcation, which is built on Amazon reviews from 4 domains: books, dvd, electronics, and kitchen.",other,reporting the application of a specific model for text classification
1538,,c6c60c7813b58024da509f21f3f0594855d2dd4f,Portion control plate for weight loss in obese patients with type 2 diabetes mellitus: a controlled clinical trial.,,,"###Despite the plethora of diet strategies available, it is primarily the net caloric intake that determines the net change in weight.(10,11) The principles of portion control are commonly used by diabetes educators to effect a hypocaloric diet; patients are often taught to use reference sizes relative to their hand size to guide protein, carbohydrate, and fat portions.",impact-revealing,highlighting the importance of caloric intake in weight management
3112,5c04966a17c44a2c74708959,6b98bef930182a848c027dece1bfb58ca706449d,Improving End-to-end Speech Recognition with Pronunciation-assisted Sub-word Modeling,5b3d98cc17c44a510f801eb2,Improved training of end-to-end attention models for speech recognition,"Sub-word representations have recently seen their success in ASR [6].###In Table 3, we report the WER results on the LibriSpeech dataset, using the parameters described in [6].",other,reporting results on sub-word representations in ASR
3413,5dcbd5da3a55ac789b0dbc7f,f0efc23ecb6d4fb9745d555450b2c4a97e8ac4d5,Robust Design of Deep Neural Networks Against Adversarial Attacks Based on Lyapunov Theory,5c04966a17c44a2c747086c1,Efficient Neural Network Robustness Certification with General Activation Functions.,"Works such as [35, 40] focus on certifying robustness for a DNN by calculating the bounds for the activation functions’ responses to the inputs.",other,acknowledge existing research on DNN robustness certification
456,5ea2b8c391e01167f5a89e2d,38643c2926b10f6f74f122a7037e2cd20d77c0f1,Supervised Contrastive Learning,58d83051d649053542fe9c5b,Improved Deep Metric Learning with Multi-class N-pair Loss Objective.,"Our loss is directly inspired by the family of contrastive objective functions, which have achieved excellent performance in self-supervised learning in recent years in the image and video domains [50, 25, 21, 19, 46, 6, 43] and have connections to the large literature on metric learning [48, 5].###4) is largely motivated by noise contrastive estimation and N-pair losses [17, 43], wherein the ability to discriminate between signal and noise (negatives) is improved by adding more examples of negatives.###In these works, the losses are inspired by noise contrastive estimation [17, 32] or N-pair losses [43].",impact-revealing,highlighting the influence of contrastive objective functions in self-supervised learning
1902,,0404463052c3e699e448d3cbcefb9b1a58ebceda,Strategies for Designing Engaging Online Kinesiology Courses Based on the Community of Inquiry Model,,,"###serves as a support to cognitive presence because it indirectly facilitates the development of critical inquiry that occurs within the community of learners as they support and interact with one another (Garrison et al., 2000).###This review has shown the strong tie between the three constructs of the CoI framework and their interdependencies (Garrison et al., 2000; Garrison, Cleveland-Innes, & Shing Fung, 2010).###nions can help learners move from the exploration phase into the integration phase, where they are able to build on other learners’ ideas and provide justification for their beliefs (Garrison et al., 2000).###This function serves as a support to cognitive presence because it indirectly facilitates the development of critical inquiry that occurs within the community of learners as they support and interact with one another (Garrison et al., 2000).###Cognitive presence is considered a basic element to success in higher education and is highly dependent on how the flow of communication occurs within the course (Garrison et al., 2000).###The CoI framework was originally presented by Garrison, Anderson, and Archer (2000) as a framework that identified the important and necessary components of what they considered a successful college experience.###However, verbally voicing their opinions can help learners move from the exploration phase into the integration phase, where they are able to build on other learners’ ideas and provide justification for their beliefs (Garrison et al., 2000).",impact-revealing,highlighting the importance of cognitive presence in the community of inquiry framework
1654,,3dcd12807dee9f5f65b7bae76ca76b121b728682,Do Superordinate Identification and Temporal/Social Comparisons Independently Predict Citizens’ System Trust? Evidence From a 40-Nation Survey,,,"###TSST is, therefore, currently unique in its emphasis on the social comparison provision of SIT, arguing that so long as a given social or temporal stratification allows for intermediate positioning (whether it be within groups or between time points), that people may be motivated by the need for positive self-worth to support the status quo in which this was made possible.###But, this superordinate in-group bias explanation is not the only one on offer under the social identity umbrella (i.e., social identity theory (SIT); Tajfel and Turner, 1979), especially given that SIMSA and its predecessor (SJT) do not currently say much about system justification of members of groups that are placed in an intermediate position (i.e., those who are disadvantaged but can nevertheless realize downward comparison) relative to those who are clearly advantaged or disadvantaged.###TSST, similar to its parent framework (SIT), assumes that people are motivated by a need for positive self-worth to improve their social position both personally (i.e., by upward individual mobility) and collectively (by upward social mobility) and that sometimes this goal can be reached by comparing the outcomes of an individual (or the outcomes of an individual’s social group) with those of others.###…this superordinate in-group bias explanation is not the only one on offer under the social identity umbrella (i.e., social identity theory (SIT); Tajfel and Turner, 1979), especially given that SIMSA and its predecessor (SJT) do not currently say much about system justification of members of…###This is the vacuum that the Triadic Social Stratification Theory (TSST, Caricati, 2018; Caricati and Owuamalam, 2020) fills, also drawing from SIT (Tajfel and Turner, 1979).",impact-revealing,highlighting the unique contribution of TSST to social identity theory
3876,5f3f917891e011d38f9242d9,14156438bafed28a626738630b5181b83ed5d79c,Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters,5736973b6e3b12023e62b177,Collective Opinion Spam Detection: Bridging Review Networks And Metadata,"Since the Yelp dataset has imbalanced classes, and we focus more on fraudsters (positive instances), like previous work [29], we utilize ROC-AUC (AUC) and Recall to evaluate the overall performance of all classifiers.###We take 32 handcrafted features from [29] (25 handcrafted features from [47] resp.###Yelp: based on previous studies [27, 29] which show that opinion fraudsters have connections in user, product, review text, and time, we take reviews as nodes in the graph and design three relations: 1) R-U-R: it connects reviews posted by the same user; 2) R-S-R: it connects reviews under the same product with the same star rating (1-5 stars); 3) R-T-R: it connects two reviews under the same product posted in the same month.###For example, it can be a review on the review website [19, 29] or a transaction in the trading system [23, 37].###We use the Yelp review dataset [29] and Amazon review dataset [26] to study the fraudster camouflage and GNNbased fraud detection problem.",other,acknowledge dataset characteristics and evaluation methods
2518,5fae6dced4150a363cec41f7,71c97e7d4a529a21f32d98aad72fbbf32c9ee32e,Anomaly-Based Intrusion Detection From Network Flow Features Using Variational Autoencoder,5d035e488607575390f7c793,Testing Intrusion detection systems: a critique of the 1998 and 1999 DARPA intrusion detection system evaluations as performed by Lincoln Laboratory,"But, another issue, which is not addressed by NSL-KDD, is that this dataset is not a realistic representation of network trafﬁc [14].###Main drawbacks of these studies could be listed as follows: a) These studies focus on the detection of intrusions in content-based features. b) Dataset used is very outdated and does not reﬂect the real network trafﬁc [14].",other,highlighting limitations of the NSL-KDD dataset in representing realistic network traffic
2525,5736982b6e3b12023e6fd332,d0b8c5464b582e1a582a4affb270e8b02f8414a2,a scalable architecture for ordered parallelism,5550484945ce0a409eb6d225,Ilp And Tlp In Shared Memory Applications: A Limit Study,"Prior work has studied the limits of instruction-level parallelism under several idealizations, including a large or infinite instruction window, perfect branch prediction and memory disambiguation, and simple program transformations to remove unnecessary data dependences [4, 9, 18,20,24,42,49,57,74].",other,acknowledge prior research on instruction-level parallelism limits
2369,5c96086e3cb210d2716c4a08,bb76749fab841ccbe20ace74111a518f65d9870b,make the most out of last level cache in intel processors,5c75685ff56def97981eba2c,100g Cwdm4 Smf Optical Interconnects For Facebook Data Centers,This cache memory becomes even more valuable due to the explosion of data and the advent of hundred gigabit per second networks (100/200/400 Gbps) [9].,other,highlighting the importance of cache memory in the context of increasing data and network speeds
3368,5f0d85c69fced0a24be4f052,0dd3e9f581c617eb826bc0fabac5ae1394f9cef1,Data Compression Accelerator on IBM POWER9 and z15 Processors : Industrial Product,599c7c82601a182cd27b6ffb,Page Fault Support for Network Controllers.,Related works include [29]–[32].###The value of user-mode access and page fault handling is also recognized in [29] for network adapters.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2487,5e539eca3a55ac4db70a53f1,c529f5b08675f787cdcc094ee495239592339f82,learning to simulate complex physics with graph networks,5550437145ce0a409eb477c1,Unified particle physics for real-time applications,"(2018)’s BOXBATH, which simulates a container of water and a cube floating inside, all represented as particles, using the PBD engine FleX (Macklin et al., 2014).###For one domain, we use Li et al. (2018)’s BOXBATH, which simulates a container of water and a cube floating inside, all
Learning to Simulate
represented as particles, using the PBD engine FleX (Macklin et al., 2014).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
475,5e68b99493d709897cd373ed,d08b35243edc5be07387a9ed218070b31e502901,group normalization,573696ce6e3b12023e5ce95a,Batch normalization: accelerating deep network training by reducing internal covariate shift,"Unlike recent meth-ods [26, 3, 61], LRN computes the statistics in a small neighborhood for each pixel.###Batch Normalization [26] performs more global normalization along the batch dimension (and as importantly, it suggests to do this for all layers).###In Batch Norm [26], the set S i is deﬁned as: where i C (and k C ) denotes the sub-index of i (and k ) along the C axis.###For example, batch-wise normalization is not legitimate at inference time, so the mean and variance are pre-computed from the training set [26], often by running average; consequently, there is no normalization performed when testing.###This is understandable, because BN’s mean and variance computation introduces uncertainty caused by the stochastic batch sampling, which helps regularization [26].###Batch Normalization (Batch Norm or BN) [26] has been established as a very effective component in deep learning, largely helping push the frontier in computer vision [59, 20] and beyond [54].###We ﬁrst experiment with a regular batch size of 32 images (per GPU) [26, 20].###In particular, it is required for BN to work with a sufﬁciently large batch size ( e.g ., 32 per worker 2 [26, 59, 20]).###As in [26], all methods of BN, LN, and IN learn a per-channel linear transform to compensate for the possible lost of representational ability: where γ and β are trainable scale and shift (indexed by i C in all case, which we omit for simplifying notations).",impact-revealing,providing context on Batch Normalization and its significance in deep learning
158,5fc4cfdf91e011abfa2faf94,633e2fbfc0b21e959a244100937c5853afca4853,score-based generative modeling through stochastic differential equations,5db929f247c8f766461fd4f1,Generative Modeling by Estimating Gradients of the Data Distribution,"Song & Ermon (2019) propose to train a Noise Conditional Score Network (NCSN), denoted by sθpx, σq, with a weighted sum of denoising score matching (Vincent, 2011) objectives:
θ˚ “ arg min θ
N ÿ i“1 σ2i EpdatapxqEpσi px̃|xq “ ‖sθpx̃, σiq ´∇x̃ log pσipx̃ | xq‖ 2 2 ‰ .###For example, when using the reverse diffusion SDE solver (Appendix E) as the predictor, and annealed Langevin dynamics (Song & Ermon, 2019) as the corrector, we have Algorithms 2 and 3 for VE and VP SDEs respectively, where t iuN´1i“0 are step sizes for Langevin dynamics as specified below.###Here in order to match the convention used in Karras et al. (2018); Song & Ermon (2019) and Ho et al. (2020), we report the lowest FID value over the course of training, rather than the average FID value over checkpoints after 0.5M iterations (used for comparing different models of VE SDEs) or…###For sampling, Song & Ermon (2019) run M steps of Langevin MCMC to get a sample for each pσipxq sequentially:
xmi “ xm´1i ` isθ˚px m´1 i , σiq ` ?###…models, and related techniques (Bordes et al., 2017; Goyal et al., 2017; Du & Mordatch, 2019), have proven effective at generation of images (Song & Ermon, 2019; 2020; Ho et al., 2020), audio (Chen et al., 2020; Kong et al., 2020), graphs (Niu et al., 2020), and shapes (Cai
˚Work partially…###Although DDPM (Ho et al., 2020) was recently reported to achieve higher sample quality than SMLD (Song & Ermon, 2019; 2020), we show that with better architectures and new sampling algorithms allowed by our framework, the latter can catch up—it achieves new state-of-the-art Inception score (9.89)…###Score matching with Langevin dynamics (SMLD) (Song & Ermon, 2019) estimates the score (i.e., the gradient of the log probability density with respect to data) at each noise scale, and then uses Langevin dynamics to sample from a sequence of decreasing noise scales during generation.###The corrector algorithms We take the schedule of annealed Langevin dynamics in Song & Ermon (2019), but re-frame it with slight modifications in order to get better interpretability and empirical performance.###We name this model “NCSN++”, following the naming convention of previous SMLD models (Song & Ermon, 2019; 2020).###This allows us to interpolate between noise scales at test time in an ad-hoc way (while it is hard to do so for other architectures like the one in Song & Ermon (2019)).",impact-revealing,reporting a method and its application in noise conditional score networks
527,5b67b47917c44aac1c8637c6,5aea95e1ae78a66474051a330ded374e199b658c,Representation Learning on Graphs with Jumping Knowledge Networks,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"(III) For protein-protein interaction networks (PPI) (Hamilton et al., 2017), the task is to classify protein functions.###Many of these approaches broadly follow a neighborhood aggregation (or “message passing” scheme), and those have been very promising (Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Veličković et al., 2018; Kearnes et al., 2016).###The communities in the Reddit dataset were explicitly chosen from the well-behaved middle-sized communities to avoid the noisy cores and tree-like small communities (Hamilton et al., 2017).###We demonstrate the power of adaptive JK-Nets, e.g., JK-LSTM, with experiments on the PPI data, where the sub-graphs have more diverse and complex structures than those in the Reddit community detection dataset.###We compare against three baselines: Graph Convolutional Networks (GCN) (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017) and Graph Attention Networks (GAT) (Veličković et al.###Our experiments on PPI are inductive.###For COMBINE, GraphSAGE (Hamilton et al., 2017) uses concatenation after a feature transform.###We follow exactly the same setting of GraphSAGE as in the original paper (Hamilton et al., 2017), where the model consists of 2 hidden layers, each with 128 hidden units and is trained with Adam with learning rate of 0.###(II) On Reddit (Hamilton et al., 2017), the task is to predict the community to which different Reddit posts belong.###We compare against three baselines: Graph Convolu-tional Networks (GCN) (Kipf & Welling, 2017), Graph-SAGE (Hamilton et al., 2017) and Graph Attention Networks (GAT) (Veliˇckovi´c et al., 2018).###Many of these approaches broadly follow a neighborhood aggregation (or “message passing” scheme), and those have been very promising (Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Veliˇckovi´c et al., 2018; Kearnes et al., 2016).###PPI consists of 24 graphs, each corresponds to a human tissue.###We follow exactly the same setting of GraphSAGE as in the original paper (Hamilton et al., 2017), where the model consists of 2 hidden layers, each with 128 hidden units and is trained with Adam with learning rate of 0.01 and no weight decay.###The max-pooling operation in GraphSAGE (Hamilton et al., 2017) implicitly selects the important nodes.###Such schemes have been shown to generalize the Weisfeiler-Lehman graph isomorphism test (Weisfeiler & Lehman, 1968) enabling to simultaneously learn the topology as well as the distribution of node features in the neighborhood (Shervashidze et al., 2011; Kipf & Welling, 2017; Hamilton et al., 2017).###Hamilton et al. (2017) derived a variant of GCN that also works in inductive settings (previously unseen nodes), by using a different normalization to average: f e where deg ( v ) is the degree of node v in G . f e Neighborhood Aggregation with Skip Connections.",impact-revealing,Highlighting the effectiveness of neighborhood aggregation methods in protein-protein interaction networks
72,5ce3ad3fced107d4c65b6bd9,eeecea3097cf5629eb72a06e5caaf24d774adce7,Unsupervised label noise modeling and loss correction,5550413b45ce0a409eb397e0,Training Deep Neural Networks on Noisy Labels with Bootstrapping.,13 ● Bootstrapping loss correction [5] + mixup data augmentation [6] ● Our Beta Mixture Model drives our learning approach a step further by: ○ Preventing memorization ○ Correcting noisy labels to learn from them [5] Reed t al.###12 ● Bootstrapping loss correction [5] + mixup data augmentation [6] [5] Reed t al.,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3200,5cf48a30da56291d58292a2f,ed45bc75ca3406866e1bb9e95ad251536e9b985c,few-shot adversarial learning of realistic neural talking head models,57a4e921ac44365e35c98ea1,Instance Normalization: The Missing Ingredient for Fast Stylization.,"[19], but replace downsampling and upsampling layers with residual blocks similarly to [2] (with batch normalization [15] replaced by instance normalization [36]).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2504,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",558c06e3e4b0cfb70a1b341d,Laplacian Eigenmaps for Dimensionality Reduction and Data Representation,"based GSP: References [54], [55], [56], [57] develop lowdimensional representations for large high-dimensional data through spectral graph theory [58], [56] and the graph Lapla-",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1368,,2c7c9c7fea52d0df8b9566867c2570f540d1da91,Contributions of the 12 Neuron Classes in the Fly Lamina to Motion Vision,,,"###For all tethered flight experiments, we used female Drosophila (3–5 days old), which were heterozygous for both GAL4 and UAS transgenes (effectors backcrossed into Dickinson Laboratory [DL] or Canton-S [CS] wild-type backgrounds ).###In a complementary set of experiments, we genetically expressed the temperature-gated cation channel dTrpA1 (Hamada et al., 2008), which depolarizes Drosophila neurons (Pulver et al., 2009).###With the recent availability of a large collection of defined GAL4 driver lines (Jenett et al., 2012), this approach can now be readily applied to other parts of the Drosophila brain.###In the early visual system of Drosophila, input from photoreceptor neurons in the retina is initially processed in the optic lobes, which consist of a series of optic ganglia called the lamina, medulla, and lobula complex (comprising the lobula and lobula plate).###Because the reference pattern remained constant (and at a speed close to Drosophila's temporal frequency optimum), peak contrast sensitivity occurred when the reference and test pattern were moving at the same speed (5.33 Hz).###The lamina is organized into an array of 750 retinotopic ''cartridges,'' each of which corresponds to a discrete sample of the visual world, 5 in Drosophila (Braitenberg, 1967; Buchner, 1971; Kirschfeld, 1967).###In this study, we combined psychophysical measurements with targeted genetic manipulations in order to understand how lamina-associated neurons in Drosophila shape visual perception .###Though this approach is widely used in Drosophila and other genetic model organisms, its utility has been limited by two main experimental challenges.###) for expression in the Drosophila lamina and further examined expression patterns of selected lines by reimaging at higher resolution or with single-cell labeling techniques.",impact-revealing,describing the experimental approach and its significance in understanding visual perception in Drosophila
1753,,84b070cebf146e5e2df8d28f3654faf57b5e2b5c,CNNFlow: Memory-driven Data Flow Optimization for Convolutional Neural Networks,,,"###We also compare VGG-16 with Eyeriss, as shown in Table 2.###In Eyeriss, data are stored using a sparse representation in DRAM and are only decoded into full length before they are loaded into on-chip memory.###Given this difference, we cannot fairly compare the decrease in the percentage of DRAM accesses but, overall, we still do better even considering the compression due to the sparse representation in Eyeriss.###Eyeriss has a 108 KB global buffer, 12 × 14 PE array, and a scratchpad with size 24 B, 448 B, and 48 B for Ifmap, filter, and partial sum in each PE.###The solutions given by CNNFlow can reduce 86% SRAM accesses compared to Eyeriss.###We compare the solution generated by CNNFlow with Eyeriss[1] for AlexNet.###As in Eyeriss, the number of batches in CNNFlow, B , is set to 4.###This architecture can help with the high degree of parallelism and data reuse in our targeted applications and is commonly used by other CNN accelerators [1, 47, 48].###The hardware setting is the same as in the AlexNet experiment but the number of batches is 3, as used in Eyeriss.###Table 1 shows the results of the convolution layers in AlexNet given by Eyeriss and CNNFlow.###Eyeriss [1] configures a set of mapping parameters that are essentially blocking size.###Eyeriss [1] config-ures a set of mapping parameters that are essentially blocking size.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2724,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",599c7d08601a182cd27f700c,Brain Signal Analytics From Graph Signal Processing Perspective,brain networks (characterized by their spectral properties) and the level of exposure of subject to different tasks [173].,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3471,5f0277e911dc830562231dac,72a5feb4835b3e6cab3754437bb033371c5df154,DVGAN: A Minimax Game for Search Result Diversification Combining Explicit and Implicit Features,59a02ff0b161e8ad1a7b6eb9,Learning to Diversify Search Results via Subtopic Attention,"method we adapt the DSSA [12] score function for the generator.###In our experiments, we conduct the list-pairwise loss [12] to train DSSA method.###DSSA [12] introduces the machine learning method into explicit approaches.###We use ListMLE [22], R-LTR [26], PAMM [6], R-LTR-NTN, PAMM-NTN [24], and DSSA [12] as supervised baseline methods.###Similar to implicit approaches, explicit diversification approaches can also be categorized into heuristic approaches such as xQuAD [17, 18] and PM2 [5, 6, 9] and supervised approaches such as DSSA [12].###Different from implicit approaches which mainly model document novelty based on similarities between documents, explicit approaches regard the query as several subtopics, and explicitly leverage subtopics to determine the diversity of results [6, 12, 17, 18].###In the training process, we first train R-LTR [26] and DSSA [12] respectively using MLE loss in both ways.###Studies have shown that supervised approaches [12, 23, 24, 26] are able to outperform the heuristic approaches [1, 6, 18] by learning an optimized ranking function.###The explicit approaches [6, 12, 17, 18, 25] stress the relevance between the documents and the subtopics of the query, which infers that the selected document should cover the subtopics which the previously selected documents do not cover.",other,describing the adaptation of a score function for the generator
1442,,327d948b170f8f442fea623ec917a4be1a5df530,Real‐time presurgical resting‐state fMRI in patients with brain tumors: Quality control and comparison with task‐fMRI and intraoperative mapping,,,"###This has also been demonstrated in a recent study using a real-time motion analytics tool that allows to censor data frames affected by head movement and provides statistics to determine the scan time necessary to collect a desired amount of low movement data (Dosenbach et al., 2017).###A recent study by Dosenbach et al. (2017) described a real-time motion analytics tool that allows censoring of data frames affected by head movement and provides statistics to determine the scan time necessary to collect a desired amount of low movement data (Dosenbach et al., 2017).###The first objective was validated by real-time assessment of motion parameters and artifacts described by Dosenbach et al. (2017) and by additionally monitoring quantifiable metrics of resting-state connectivity and task-activation to determine overall scan success.###(2017) described a real-time motion analytics tool that allows censoring of data frames affected by head movement and provides statistics to determine the scan time necessary to collect a desired amount of low movement data (Dosenbach et al., 2017).",impact-revealing,reporting findings on a real-time motion analytics tool
1213,,00b5dbbfe906c18e2f687eba9c63332879ebee63,Dynamics of nevus development implicate cell cooperation in the growth arrest of transformed melanocytes,,,"###Arrows show deviations in neighbor size distribution greater than expected at random, and substantially different for large versus small nests. al., 2015; Lander, 2011; Lander et al., 2009).###This mechanism describes a dynamically well-understood feedback process that normal tissues use to control size (Lander, 2011; Lander et al., 2009).###This type of feedback is commonly used by adult tissues to maintain constant size, and also enables developing tissues to produce precise numbers of differentiated cells (Kunche et al., 2016; Lander, 2011; Lander et al., 2009).###This sort of behavior—where differentiated cells drive the differentiation of their progenitors—is exactly the sort of behavior that drives feedback models of renewal (Buzi et al., 2015; Lander, 2011; Lander et al., 2009).###The latter result is inherently problematic for any non-feedback model, but is precisely what renewal feedback predicts (Lander, 2011; Lander et al., 2009).",impact-revealing,highlighting the role of feedback mechanisms in tissue size control
3959,5cd7fa07ced107d4c65bf34f,371c799bde8b162e7f8fa2b2a0a8cfb29765f89f,Knowledge Graph Convolutional Networks for Recommender Systems,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"rty of CNN, researchers propose learning a weight matrix for each node degree [6], extracting locally connected regions from graphs [13], or sampling a fixed-size set of neighbors as the support size [7]. Our work can be seen as a non-spectral method for a special type of graphs (i.e., knowledge graph). Our method also connects to PinSage [21] and GAT [15]. But note that both PinSage and GAT are desi###owed by a nonlinear transformation: aддsum = σ  W ·(v+vu S(v) )+b  , (4) where W and b are transformation weight and bias, respectively, and σis the nonlinear function such ReLU. •Concat aggregator [7] concatenates the two representation vectors first before applying nonlinear transformation: aддconcat = σ  W ·concat(v,vu S(v) )+b  . (5) 2The knowledge graph Gis treated undirected. 3Technically, ",other,providing context for a method in knowledge graph analysis
1498,,f5a0c57f90c6abe31482e9f320ccac5ee789b135,Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion Models,,,"###To further enhance the spatial resolution, we also temporally align pixel-space and latent DM upsamplers [29], which are widely used for image super resolution [43, 65, 68, 69], turning them into temporally consistent video super resolution models.",impact-revealing,describing the enhancement of spatial resolution in video super resolution models
1492,,56bfccdc2b35d8e976a978d2c382b3c95f7af0f0,Underwater Image Enhancement by Diffusion Model with Customized CLIP-Classifier,,,"###2 (b)), and experiments have shown that simple concatenation produces similar generation quality and enhances the model’s robustness [25].###This approach is inspired by recent works on Denoising Diffusion Probabilistic Models [22]–[25] and Contrastive Language-Image Pretraining (CLIP) guidance for image manipulation [26]–[28].###1) Multi-Guidance for Diffusion Models : Our method leverages a diffusion model for underwater image enhancement, and we use the image-to-image model SR3 [25] as the base diffusion model framework.###An alternative strategy is to establish the relationship between input and conditional information in the latent space of the model [25], [40].###For special image-to-image tasks that require consistent content, such as super-resolution [25], image restoration [47], repaint [48], etc., the input image and the corresponding image share low-frequency semantic information in each transition from x t to x t − 1 during the generation process [49].###The image-to-image diffusion model was first introduced by Saharia et al. [25] and has been exploited for image enhancement [20], [41], inpainting [42], and super-resolution [25], etc. Conditional diffusion models have the ability to acquire mapping transitions between diverse image domains [21].",impact-revealing,describing the method and its inspirations for image enhancement
2867,5edb32399e795ec54fd81737,6c6c265c6d1de08f03fed0da0604bef5307fbcec,adagcn: adaboosting graph convolutional networks into deep models.,5a9cb66717c44a376ffb8667,FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling,"In the computation part, we additionally compare AdaGCN with FastGCN [6] and GraphSAGE [12].",other,comparing different graph convolutional network methods
24,53e9a710b7602d970304482e,941f318e41147773ae69d9da4f8de9b8dbea70f4,Learning semantic representations using convolutional neural networks for web search,53e9affab7602d9703a4f291,Learning deep structured semantic models for web search using clickthrough data,"In a separate line of research, deep learning based techniques have been proposed for semantic understanding[3][6][9][10].",impact-revealing,acknowledge existing research in semantic understanding
2359,5ee8986891e011e66831c556,73366d75289c5e37481639fb54fdee28a664e2b3,GNNGUARD: Defending Graph Neural Networks against Adversarial Attacks,5b8c9f4a17c44af36f8b70fa,Signed Graph Convolutional Network,"GNNGUARD works with many GNNs, including Graph Convolutional Network (GCN) [3], Graph Attention Network (GAT) [19], Graph Isomorphism Network (GIN) [7], Jumping Knowledge (JK-Net) [20], GraphSAINT [21], GraphSAGE [36], and SignedGCN [42].",other,reporting the compatibility of GNNGUARD with various GNN architectures
98,5f3f917891e011d38f9242d9,14156438bafed28a626738630b5181b83ed5d79c,Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters,5d8dded23a55acd1b54967df,GraphMix: Regularized Training of Graph Neural Networks for  Semi-Supervised Learning,"To train the similar measure with a direct supervised signal from labels, like [35], we define the cross-entropy loss of the MLP at l-layer as:",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1385,,5e30e51074415c2c9d5f3f48422221907e15b392,Rotated Test Problems for Assessing the Performance of Multi-objective Optimization Algorithms,,,"###Previous work in [12] demonstrated the importance of evaluating single objective Evolutionary Algorithms on rotated problems in order to test their rotationally invariant behaviour , the contention being that evaluations of EAs should be independent of any coordinate system.",impact-revealing,highlighting the significance of evaluating Evolutionary Algorithms on rotated problems
3768,5f8d6be69fced0a24bbab01e,a87e4124f7305a97a8efaa574c1b270dccf4a563,DisenHAN: Disentangled Heterogeneous Graph Attention Network for Recommendation,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"Followed by the previous work[9], we sample fixed number of source nodes and pad zeros when source nodes less than the fixed number.###Graph neural networks (GNNs) , especially gated graph neural network (GGNN)[19], graph convolutional network (GCN)[17], graph inductive representation learning (GraphSAGE)[9] and graph attention network (GAT)[32], have been attracting considerable attention recently.",other,acknowledge the growing interest in graph neural networks and their methods
3302,5dd7b1be3a55ac97f763dd30,948839277bface5780896e8e8791906818aa41ac,Adversarial Examples Improve Image Recognition,5550415945ce0a409eb3a820,ImageNet Large Scale Visual Recognition Challenge,"ing [7,20], or on larger datasets but in the semi-supervised setting [26,30]. Meanwhile, recent works [18,16,45] also suggest that training with adversarial examples on large datasets, e.g., ImageNet [33], with supervised learning results in performance degradation on clean images. To summarize, it remains an open question of how adversarial examples can be used effectively to help vision models. arXi###.....78#60 +),-./01 203/456666666666?A8A:62+78B:5 Figure 1. AdvProp improves image recognition. By training models on ImageNet, AdvProp helps EfﬁcientNet-B7 [41] to achieve 85.2% accuracy on ImageNet [33], 52.9% mCE (mean corruption error, lower is better) on ImageNet-C [9], 44.7% accuracy on ImageNet-A [10] and 26.6% accuracy on StylizedImageNet [6], beating its vanilla counterpart by 0.7%, 6.5%, 7.0###anging from 1 to 4. We set the number iteration for the attackers n=+1, except for the case =1 where nis set to 1. The attack step size is ﬁxed to =1. Datasets. We use the standard ImageNet dataset [33] to train all models. In addition to reporting performance on the original ImageNet validation set, we go beyond by testing the models on the following test sets: ImageNet-C [9]. The ImageNet-C datas",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1020,,7f9be73b25ea608aab82919cd53cdc5e0d8e7fc7,Leveraging Large Language Models to Measure Gender Bias in Gendered Languages,,,"###The choice of this sample subset size was motivated by the lower bound calculated using the formula from Daniel and Cross (2018): n = z 2 p (1 − p ) /e 2 , where z = 2 .",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3465,5550414745ce0a409eb39ec8,6010ebf22c6cc07e93c5335fa1d128be8c6b190b,understanding big data analytics workloads on modern processors,5550412c45ce0a409eb390b0,Characterizing and subsetting big data workloads,"On one hand, the big data analytics software stack facilitates the programmer to write a big data analytics application without considering the messy details of data partitioning, task distribution, load balancing, failure handling and other warehouse-scale system details [35, 23].",other,highlighting the advantages of big data analytics software
3125,5aed14d617c44a4438158e20,9e788f1530af08a1f2140e6016fd4aeaa8b29033,Recovering Realistic Texture in Image Super-Resolution by Deep Spatial Feature Transform,53e9b1eab7602d9703c7cdec,Single-Image Super-Resolution Using Sparse Regression and Natural Image Prior,"Image priors such as edge features [13, 41], statistics [24, 1] and internal patch recurrence [16] are employed to improve performance.",other,reporting prior findings on image priors
966,,12dcf3cd9b0e9da5490dcb51ce8ba66109fc3086,Deception and reciprocity,,,"###In the No-Intentions treatments, we build on previous literature (e.g., Blount 1995; Bolton et al. 1998; Charness 2004; Falk et al. 2008) and determine the payoff outcome based on the distribution of the first-stage outcomes of previous sessions of the other treatments.",impact-revealing,acknowledging prior literature in the context of No-Intentions treatments
3191,5d3ed25a275ded87f97deb36,37f7eaf57b88ef03efd4196f2f57ae0b4c657a79,Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommendation,53e9bc74b7602d97048f4169,Factorization meets the neighborhood: a multifaceted collaborative filtering model.,"The learning rate are the same as in SVD. • RippleNet [24] is a representative of hybrid methods, which is a memory-network-like approach that propagates users’ preferences on the KG for recommendation.###The settings of dimension and learning rate are the same as SVD. • CKE [34] is a representative of embedding-based methods, which combines CF with structural, textual, and visual knowledge in a unified framework.###• SVD [12] is a classic CF-based model using inner product to model user-item interactions.",other,providing context on various recommendation methods
1603,,45e175a1bde8553b02d1675a3467112c2dcb535f,Reducing Sentiment Bias in Pre-trained Sentiment Classification via Adaptive Gumbel Attack,,,"###However, the effectiveness of such methods is limited by the uniqueness of entities, that is, the word-level (Swenor 2022) and character-level (Morris et al. 2020) transformations of entities have the potential to drastically change the semantics of entities.",impact-revealing,highlighting limitations in entity transformation methods
1128,,fdc7600ee0578b841f0359abd0a2ce081b0a9fb8,Privacy through Diffusion: A White-listing Approach to Sensor Data Anonymization,,,"###To accelerate the synthesis process, Song et al. [21] propose Denoising Diffusion Implicit Models (DDIM) that model the diffusion process as a non-Markov process, enabling one-step computation for any timestamp.###1 The design of this obfuscation model is inspired by the denoising diffusion model [5, 8, 9, 21] – a generative AI model that achieves superb performance in the image synthesis task and exhibits better training stability than the generative adversarial network (GAN).",impact-revealing,Highlighting the innovation in synthesis process acceleration through DDIM
3688,5ce3ad3fced107d4c65b6bd9,eeecea3097cf5629eb72a06e5caaf24d774adce7,Unsupervised label noise modeling and loss correction,573697826e3b12023e6690c5,Learning From Massive Noisy Labeled Data For Image Classification,"Other approaches seek to relabel the noisy samples by modeling their noise through directed graphical models (Xiao et al., 2015), Conditional Random Fields (Vahdat, 2017), or CNNs (Veit et al.###, 2009)) and Clothing1M (Xiao et al., 2015) datasets to test the generality of our approach far from CIFAR data (Subsection 4.###Furthermore, we tested our approach in real-world label noise by evaluating our method on Clothing1M (Xiao et al., 2015), which contains non-uniform label noise with label flips concentrated in classes sharing similar visual patterns with the true class.###We further experiment on TinyImageNet (subset of ImageNet (Deng et al., 2009)) and Clothing1M (Xiao et al., 2015) We follow (Zhang et al., 2017; 2018; Tanaka et al., 2018) criterion for label noise addition, which consists of randomly selecting labels for a percentage of the training data using all…###Other approaches seek to relabel the noisy samples by modeling their noise through directed graphical models (Xiao et al., 2015), Conditional Random Fields (Vahdat, 2017), or CNNs (Veit et al., 2017).###Furthermore, we tested our approach in real-world label noise by evaluating our method on Clothing1M (Xiao et al., 2015), which contains non-uniform label noise with label ﬂips concentrated in classes sharing similar visual patterns with the true class.###We further experiment on TinyImageNet (subset of ImageNet (Deng et al., 2009)) and Clothing1M (Xiao et al., 2015) We follow (Zhang et al., 2017; 2018; Tanaka et al., 2018) criterion for label noise addition, which consists of randomly selecting labels for a percentage of the training data using all possible labels (i.e. the true label could be randomly maintained).###The approach generalizes well to TinyImageNet but shows some limitations under non-uniform noise in Clothing1M that we will explore in future research.",other,acknowledge existing methods for handling noisy samples
4039,5ee9f15b91e01152af022eb9,6360aaece0d6bf153183b9ecd075f42f7b127cc9,Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting,53e9b360b7602d9703e41a15,ZINC: A Free Tool to Discover Chemistry for Biology.,"water-octanol partition coefficient - logP” (see [111], [112], [113] for details) of molecules from the ZINC database [25], [114].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2627,5eabf3cd91e011664efc496f,916595fc4e701ac6125725408912a1fac3d7a60b,CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows,53e99d51b7602d970260624e,Speculative Precomputation: Long-Range Prefetching Of Delinquent Loads,"A large body of prior work has focused on extracting MLP on an OoO core, such as prefetching, runahead execution [51], [52], and speculative pre-execution [53], [54], [55].",other,acknowledge existing research on MLP extraction
3533,5bbacb9e17c44aecc4eaff64,5b1516c87818084dc5d195cc274e1ee8923210d2,Neural Cross-Lingual Named Entity Recognition with Minimal Resources,53e9aa23b7602d97033902f0,Multilingual Models For Compositional Distributed Semantics,"Approaches based on parallel corpora usually learn bilingual word embeddings that can produce similar representations for aligned sentences (Hermann and Blunsom, 2014; Chandar et al., 2014).",other,acknowledge existing approaches in bilingual word embeddings
1394,,8f96d1b208761a82197f80ee6eb642dfa356dce6,A Feedback Information-Theoretic Transmission Scheme (FITTS) for Modelling Aimed Movements,,,"###Crossman and Goodeve [5] proposed a discrete-time model where the change of position from one sampling instant to the next is proportional to the current error, implying that the distance to the target is reduced exponentially.###Woodworth’s two-component movement, Schmidt’s linear speed-accuracy tradeoff for ballistic movements and Crossman and Goodeve’s model are combined in the SOS model [6], [41].###For each condition of ID there were 24 profiles, the number of non unimodal profiles were for ID in [2,4,6,8] respectively [13,9,8,2].###This explains why some theoretic models [4], [5], [25] are derived by segmenting a single movement into several submovements, where each submovement is obtained as the response to a single control “impulse”.###The discrete corrections that characterize the second component are typically identified from the kinematics of movement by searching for zero-crossings of the velocity [4], [5], [25], acceleration [48] or peaks of jerk profiles [49].###c) Human movement can be decomposed into submovements, indicating intermittent control: Movements can be segmented into submovements [4], [5] or into two components [23].###Crossman and Goodeve [4] proposed a scheme of intermittent proportional correction of position.###None of the models above capture all of the characteristics a) – c) of human aimed movement described in Section I. Deterministic models, such as Crossman-Goodeve’s and VITE, fail to account for the variability of human movements (a) and the derivation of Fitts’ law in [33] does not include ways to integrate feedback (b).",impact-revealing,highlighting the limitations of existing movement models
399,58d82fced649053542fd729f,fddc32f3880688238847077fd927ab3025db7a6a,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,5550415645ce0a409eb3a69e,Very Deep Convolutional Networks for Large-Scale Image Recognition.,"Alternative perceptual losses have been proposed for CNNs [10, 24] where the idea is to shift the loss from the image-space to a higher-level feature space of an object recognition system like VGG [49], resulting in sharper results despite lower PSNR values.###For the feature map φ , we use a pre-trained implementation of the popular VGG-19 network [1, 49].###The exclusive use of 3 × 3 ﬁlters is inspired by the VGG architecture [49] and allows for deeper models at a low number of parameters in the network.",impact-revealing,providing context on perceptual losses in CNNs
1610,,fbf75372c7496744e7c8d824821665bbeecf5ee3,Interference-aware resource management in multihop wireless networks,,,"###, MSR’s mesh network project [10], MIT’s Roofnet project [12], IIT’s DGP project, Rice’s TFA project [13] and Purdue’s wireless mesh testbed project [14].###Although this metric accounts for linkquality in terms of loss-rate and also implicitly for interference between consecutive hops, it does not take data-rate and link-load into consideration for selecting a path [10,102].###In a large network, CSMA systems may suffer from high packet queuing delays due at least in part to considerable amounts of collision and associated capture phenomena, despite measures [4, 10, 17, 18] in IEEE 802.###These solutions involve techniques such as Multichannel MAC (MMAC) [10, 19–21], multiple radios [22] (or a hybrid of both [23, 24]) and directional and steerable antennas [10].###In the context of narrow-band systems, there are several existing methods to accurately find the conflict graph of a network with fixed transmit powers, see [10, 56] and their related references.###Our approach was motivated by persistent practical difficulties with the effectiveness of routing algorithms, despite the many routing protocols that have been suggested for multihop wireless networks [10].###Also, note that similar to TDMA-based settings, the self-interference avoidance restriction can be relaxed when two (or more) radios are used [10,33].",impact-revealing,highlighting practical difficulties with existing routing algorithms in multihop wireless networks
3101,58d83045d649053542fe853e,51898a5e35b2646a4e5121ca74f612fab831ad6a,path confidence based lookahead prefetching,5736982b6e3b12023e6fd15e,Efficiently Prefetching Complex Address Patterns,"Lookahead prefetchers [9], [12] efficiently encode the relationship between accesses to yield future predictions, enabling further speculative lookahead accesses.###Moreover, most hardware prefetchers work in the physical address space [4], [9], [10], [11], where the mapping between virtual and physical memory is not known.###These studies, however, suffer from high hardware complexity [12], [13], or do not implement adaptive throttling [9].###To address both prefetching coverage and accuracy, prior work has adopted lookahead mechanisms [9], [12], [13].###Moreover, SPP outperforms recent, best of class, lookahead and non-lookahead prefetchers [8], [9], [10], including the winner of the most recent data prefetching competition, by 6.###Since lbm has a variety of memory access patterns [9], deeper prefetching with a simple delta predictor wastes bandwidth, and pollutes 978-1-5090-3508-3/16/$31.###Previously proposed history-based techniques [4], [9] address this by making predictions using the first offset in a page, or with very short delta histories.###To avoid over-prefetching, existing lookahead prefetchers [12], [9] globally and statically limit the depth to which lookahead is pursued ahead of the current demand access stream.###This recursion allows lookahead prefetchers [9], [12] to prefetch far ahead of the current program execution, and generate timely prefetches for as long as their predictions remain accurate.###attempt to predict complex, irregular access patterns [6], [7], [8], [9], [10].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2155,,4e9a9bdb90836d80e443dcab736b9117dc0a41ba,A power efficient MAC protocol for implant device communication in Wireless Body Area Networks,,,###Radiotriggered hardware component [18] can be used in sensor devices that are inspired by the observation that the wakeup radio signal contains enough energy to trigger a wakeup process.,impact-revealing,reporting prior findings on radiotriggered hardware components
354,5e79da4491e0115bb1157b77,8e74106415626a21bb909ff489ef310625c769e1,Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles,5e2c137c3a55aca931ba5a35,Towards Discourse Parsing-inspired Semantic Storytelling.,"In prior work [32], we also utilized a Siamese BERT model to determine the discourse relations between text segments to generate a story for the segments.",impact-revealing,reporting prior findings on discourse relations using Siamese BERT
3467,5de4e0b73a55ac2224ba53a6,a75649771901a4881b44c0ceafa469fcc6e6f968,how can we know what language models know?,58437725ac44360f1082f754,A Neural Knowledge Language Model.,"Orthogonally, some previous works integrate external knowledge bases so that the language generation process is explicitly conditioned on symbolic knowledge (Ahn et al., 2016; Yang et al., 2017; Logan et al., 2019; Hayashi et al., 2020).###Orthogonally, some previous works integrate external knowledge bases so that the language generation process is explicitly conditioned on symbolic knowledge (Ahn et al., 2016; Yang et al., 2017; IV et al., 2019; Hayashi et al., 2020).",other,acknowledge prior work integrating external knowledge bases
384,58d82fcbd649053542fd5d36,81db3f78f346eecf2f378070712feade6d45d6b1,MOLIERE: Automatic Biomedical Hypothesis Generation System,55323bd345cec66b6f9daa9d,Scalable topical phrase mining from text corpora,"word phrases from that corpus such as “asthma a_x008a_ack,” allowing us to treat phrases as single tokens [16].###By using state-of-the-art tools, such as ToPMine [16] and FastText [9], we###It is also important to note that we modify the version of ToPMine distributed by El-Kishky in [16] to allow phrases containing numbers, such as gene names like p53.###We use natural language processing methods, such as Latent Dirichlet Allocation (LDA) [8] and topical phrase mining [16], along with other data mining techniques to KDD 2017 Applied Data Science Paper KDD’17, August 13–17, 2017, Halifax, NS, Canada###However, phrase mining approaches that recover n-grams, such as [16], produce accurate methods without limiting the dictionary.###NLP toolset [1] as well as ToPMine [16] and FastText [9, 23].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
661,5edf5dd891e011bc656deb7d,14b65a86c82e38fce0eb3506e0d4084ad5cdb583,deberta- decoding-enhanced bert with disentangled attention,5aed14d617c44a4438159341,Self-Attention with Relative Position Representations.,"For an input sequence of length N , it requires a space complexity of OpN2dq [19, 20, 21] to store the relative position embedding for each token.###Existing approaches [19, 21] to relative position encoding use a separate embedding matrix to compute the relative position bias in computing attention weights.###For an input sequence of length N , it requires a space complexity of OpN(2)dq [19, 20, 21] to store the relative position embedding for each token.###It has been shown that relative position representations are more effective for natural language understanding and generation tasks [20, 21].",impact-revealing,highlighting the effectiveness of relative position representations in NLP tasks
225,5cf48a3eda56291d582a1174,05c4eb154ad9512a69569c18d68bc4428ee8bb83,Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,"In [9], they adopt a technique similar to residual###An L-layer GCN [9] consists of L graph convolution layers and each of them constructs embeddings for each node by mixing the embeddings of the node’s neighbors in the graph from the previous layer:###For example, [9] considered a graph with only a few hundreds of training nodes for which overfitting can be an issue.###Previous attempts of training deeper GCNs [9] seem to suggest that adding more layers is not helpful.###In the original paper [9], full gradient descent is used for training GCN, but it suffers from high computational and memory cost.###• Full-batch gradient descent is proposed in the first GCN paper [9].###GCN [9] Vanilla SGD GraphSAGE [5] FastGCN [1] VR-GCN [2] Cluster-GCN###Graph convolutional network (GCN) [9] has become increasingly popular in addressing many graph-based applications, including semi-supervised node classification [9], link prediction [17] and recommender systems [15].###As a consequence, the original full gradient descent [9] only needs to computeO(NL) embeddings per epoch, which means on average onlyO(L) embedding computation is needed to acquire the gradient of one node.",impact-revealing,describing the architecture and training methods of graph convolutional networks
3258,5a73cb7417c44a0b3035a202,c1cb7a1efb1a47348e3a25c21ff0a3ff192d7058,Image Super-Resolution Using Dense Skip Connections,53e9b195b7602d9703c1dd78,Image super-resolution as sparse representation of raw image patches,"Sparsity-based techniques [28, 24] have recently developed to enhance linear models with rich image priors.",other,reporting recent advancements in sparsity-based techniques
163,5a9cb60d17c44a376ffb3c6d,75a927501749c2cbc0e19a58f798f04de59df64a,Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding,5c79a19c4895d9cbc65c34b3,Session-based Recommendations with Recurrent Neural Networks.,"Recurrent neural networks (RNN) was used for session-based recommendation [8, 10].###In fact, MovieLens has more sequential signals than the other three data sets, thus, the RNN-based GRU4Rec could perform well on MovieLens but can easily get biased on training sets of the other three data sets despite the use of regularization and dropout as described in [8].###This is the session-based recommendation proposed by [8].",impact-revealing,highlighting the performance of RNN in session-based recommendation
3992,5d9ed2d847c8f76646f797b7,c3715947bbbf648dcf29a1aa4b35cfb68044f919,GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification,599c7978601a182cd2641b24,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,"Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018).",other,acknowledge existing datasets for research promotion
428,5f7fdd328de39f0828397e7f,9fa283d4f9c2ed991383c0434ef6043bee0dc8e2,Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification,5550412e45ce0a409eb39182,Recurrent Models of Visual Attention.,"In fact, CNNs are shown to be able to produce correct classification results with only a few class-discriminative patches, such as the head of a dog or the wings of a bird [38, 12, 8].###One similar work to our GFNet is the recurrent visual attention model proposed in [38].###However, large images usually come at a high computational cost and high memory footprint, both of which grow quadratically with respect to the image height (or width) [38].###This differentiates our method from early recurrent attention methods [38] which adopt pure recurrent models.",impact-revealing,highlighting the efficiency of CNNs in classification tasks
928,5d04e908da56295d08dd9fcb,c2e3caf690fd5f4d514236a430e6a1370c5080d7,Pangloss: a novel Markov chain prefetcher,53e9ac6ab7602d970363e7c6,Storage Efficient Hardware Prefetching using Delta-Correlating Prediction Tables.,"3) Instead of only supporting a limited coverage of deltas [3], it seems worthwhile to be unbiased, including negative deltas [4] as well.",impact-revealing,suggesting a more comprehensive approach to deltas
2819,5f86cae991e011dbc7eba2fa,5e9bb0f3e74be4d8f75cca6ceb3ec87b3e04d7cc,piuma: programmable integrated unified memory architecture,5c2c7a9217c44a4e7cf317b4,SIMD-X: Programming and Processing of Graph Algorithms on GPUs,"Nevertheless, GPUs usually perform better on graph algorithms than CPUs for small graphs [5], because they have more threads, which hides memory latency, and much higher memory bandwidth, brute-forcing the inefficient bandwidth utilization.",other,highlighting performance differences between GPUs and CPUs for graph algorithms
3837,53e9b253b7602d9703cf4028,fff114cbba4f3ba900f33da574283e3de7f26c83,DeepWalk: online learning of social representations,53e9aae5b7602d9703463bae,Scalable learning of collective behavior based on sparse social dimensions.,"• YouTube [40] is a social network between users of the popular video sharing website.###When possible we report the original results [39,40] here directly.###To facilitate the comparison between our method and the relevant baselines, we use the exact same datasets and experimental procedure as in [39, 40].###• EdgeCluster [40]: This method uses k-means clustering to cluster the adjacency matrix of G.",other,acknowledge the use of the same datasets and experimental procedures for comparison
3960,53e9ba8ab7602d97046ac03f,67b3d45164531806e14697a3b4d268d5f294bb82,Object Storage on CRAQ: High-Throughput Chain Replication for Read-Mostly Workloads,53e99adcb7602d9702360f1e,The Google file system,"This strong focus on availability and performance—especially as such properties are being codified in tight SLA requirements [4] [24]— has caused many commercial systems to sacrifice strong consistency semantics due to their perceived costs (as at Google [22], Amazon [15], eBay [46], and Facebook [44], among others).###Furthermore, when workloads are read mostly—an assumption used in other systems such as the Google File System [22] and Memcached [18]—the performance of CRAQ rivals systems offering only eventual consistency.###…availability and performance—especially as such properties are being codified in tight SLA requirements [4] [24]— has caused many commercial systems to sacrifice strong consistency semantics due to their perceived costs (as at Google [22], Amazon [15], eBay [46], and Facebook [44], among others).###Multiple chains can be constructed across a cluster of nodes for better load balancing—via consistent hashing [29] or a more centralized directory approach [22]—but these algorithms might still find load imbalances if particular objects are disproportionally popular, a real issue in practice [17].",other,"highlighting the trade-offs between availability, performance, and consistency in commercial systems"
292,555048d345ce0a409eb71be1,eba36ac75bf22edf9a1bfd33244d459c75b98305,Recurrent convolutional neural networks for text classification,53e99ab2b7602d970232bf0b,Finding structure in time,This model analyzes a text word by word and stores the semantics of all the previous text in a ﬁxed-sized hidden layer (Elman 1990).,impact-revealing,describing the functionality of a specific model
2509,53e99845b7602d9702072224,00ae1c36f7eb925873322e548073a64d6795787e,multiple stream prediction,53e9b2eab7602d9703da3a7f,Tolerating Branch Predictor Latency On Smt,It is possible to reduce this complexity in the fetch engine of a simultaneous multithreaded processor by pipelining the branch predictor and interleaving prediction requests from different threads each cycle [2].,other,providing context for reducing complexity in processor design
2654,5c6a37d03a69b1c9e12a9fc4,81b6d24e8f313fd88b0fe5ff6c21dd154fbe32d2,SimGNN: A Neural Network Approach to Fast Graph Similarity Computation,5a260c8117c44a4ba8a30a57,"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec.","Over the years, there are several categories ofmethods that have been proposed for learning node representations, includingmatrix factorization basedmethods (NetMF [38]), skip-gram basedmethods (DeepWalk [37], Node2Vec [15], LINE [47]), autoencoder basedmethods (SDNE [50]), neighbor aggregation based methods (GCN [9, 26, 27], GraphSAGE [16]), etc.",other,acknowledge various methods for learning node representations
3848,5d04e8d7da56295d08daef06,448b64df68335d3695a37c54770e7d5cd5f6fe68,Context Attentive Document Ranking and Query Suggestion,573696c46e3b12023e5c5827,Exploring Session Context Using Distributed Representations Of Queries And Reformulations,[32] studies session context with a distributed representation of queries and reformulations and uses the learned embeddings to improve query prediction.,other,reporting prior findings on session context and query prediction
3877,5736973b6e3b12023e62b0a8,97e3bb4af723f43927317e9b9f2d794a9e398e8e,PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks,53e9afe8b7602d9703a3c50f,Node Classification in Social Networks,", node classification [3] and link prediction [13].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1264,,e2f1f37fc4b79d9c3e8a9911f8e3b21e3278efc0,Non-blind Deblurring: Handling Kernel Uncertainty with CNNs,,,"###6 reveals performance on high resolution images from [19].###To evaluate the performance of our proposed method, we use the publicly available datasets in [22, 32, 19].###Since [19] comprises of many challenging examples, there exist examples on which BD algorithms ([38, 40, 32, 26]) fail to recover the blur kernels.###Hence, most existing blind deblurring (BD) approaches [10, 5, 38, 17, 40, 32, 35, 25] try to recover an accurate motion estimate from the blurred image [19].###Use of IFC for [19] is motivated by the fact that on the images from this dataset, IFC has the highest correlation with respect to human subject scores [19].###Because of the highly ill-posed nature of BD, an approach that can deliver accurate (close to ground truth) motion estimate is still far from reality [19].###Synthetic (Row 1) and real (Row 2) examples from the dataset in [19].###Performance comparison on dataset of [32] and [19]###PSNR/SSIM for images from [32] SSIM/IFC for images from [19]",impact-revealing,highlighting the challenges in blind deblurring approaches and the motivation for using IFC
2255,5ed12ca69e795e8ab1c11568,b3b60a15502abca31ae7b20ce16bf50049183c5a,knowledge enhanced personalized search,5843777aac44360f10840697,Bag-of-Entities Representation for Ranking,"[41] consider the bags of entity representations in search model, and the interaction between bags of word representations and bags of entity representations is also studied in [42].###Queries are often short and ambiguous[42], making query entity linking a challenging task: A recent study shows that state-of-the-art entity linking techniques only have 50% accuracy onweb queries [41].",other,highlighting challenges in query entity linking accuracy
3637,5550470045ce0a409eb63934,00dd475a3966857498b4404e48c118d88c8838f4,Psychological stress detection from cross-media microblog data using Deep Sparse Neural Network,53e9a82bb7602d9703173044,Automatic Detection of Psychological Distress Indicators and Severity Assessment from Online Forum Posts.,[4] and [5] presented methods to detect psychological stress from forum posts and microblog tweets respectively.,other,reporting prior findings on psychological stress detection methods
3619,5bdc315817c44a1f58a05e88,9ea992f009492888c482d5f4006281eaa8b758e7,"X2Face: A network for controlling face generation by using images, audio, and pose codes",599e96ec9c05cae4992b4ef3,Audio-driven facial animation by joint end-to-end learning of pose and emotion,"Examples are controlling the mouth with speech [8, 38], controlling a head with audio and a known emotional state [16], and controlling body movement with music [36].",other,providing context for examples of control methods
69,5f03f3b611dc830562232090,aa2c0bd8345c7698f13ab7cc7e0fe9eaa8e4f108,Cracking Tabular Presentation Diversity for Automatic Cross-Checking over Numerical Facts,5aed148b17c44a44381550b0,Towards Automatic Numerical Cross-Checking: Extracting Formulas from Text.,"A recent study [1] published a system called AutoDoc, and introduced the module of cross-checking among only textual paragraphs.###Some recent news reported that these numerical errors brought about huge reputation risk, and even economic losses [1].###Therefore, as an important extension to [1], we propose Automatic Numerical Cross-Checking over Tables (ANCOT) in this study.###[1] propose a system to cross-check numerical facts by extracting structured formulas from textual paragraph in financial documents.",impact-revealing,highlighting the significance of the proposed extension to existing work
1658,,3446c392d79d7b899d73a3129ba706963bb49170,"Gone For Good: Deindustrialization, White Voter Backlash, and US Presidential Voting",,,"###Our argument builds on social identity theory, which holds that society consists of various groups with differing levels of power and status relative to one another (Shayo 2009; Tajfel et al. 1979). Social identity encompasses an individual’s association with, or attachment to, a particular group and the value placed on being a part of the group (Tajfel 1974). Individuals who are strongly affiliated with their group assess political, economic, and cultural outcomes through the lens of their identity: it shapes their stances on issues and political candidates (Akerlof and Kranton 2010; Ansolabehere and Puy 2016; Conover 1984; Jardina 2019; Sides, Tesler, and Vavreck 2018). While voters may consider the interests of others, they tend to care most about the well-being of those with whom they most closely identify (Bobo 1983). In turn, they tend to favor candidates and policies that are consistent with their group’s interests (Jardina 2019; Mansfield and Mutz 2009; 2013;Mutz andKim 2017; Shayo 2009); economic hardship can solidify their political preferences (Mansfield, Mutz, and Brackbill 2019). The decline of manufacturing in a locality can create a unique social status threat for some whites in that area. This is because the negative economic and social consequences of deindustrialization upend the settled expectations of whiteness: they challenge whites’ privileged status as the dominant group. For whites who perceive manufacturing jobs as historically important sources of employment and economic security mainly for members of their own group (Guisinger 2017), layoffs, stagnant incomes, and localized social decay all contribute to the sense of diminished status.6 Put differently, deindustrialization is a source of “nostalgic deprivation,” which Gest, Reny, and Mayer (2018) describe as the discrepancy between individuals’ understanding of their current economic, social, and political status and perceptions about their past.###Our argument builds on social identity theory, which holds that society consists of various groups with differing levels of power and status relative to one another (Shayo 2009; Tajfel et al. 1979).",impact-revealing,building on social identity theory to explain political preferences and group dynamics
2291,5f3e44b791e011c0de1c29bc,61325245e98920a69b40e18c069fda0c1cf00f21,MEANTIME: Mixture of Attention Mechanisms with Multi-temporal Embeddings for Sequential Recommendation,53e9b59ab7602d97040dc8c3,Sequential Click Prediction for Sponsored Search with Recurrent Neural   Networks,"Since the first suggestion by GRU4Rec [9], many RNN-based methods [8, 17, 23, 24, 31, 34, 35] brought the success of RNN into item sequence understanding.",other,acknowledge the evolution and success of RNN-based methods in item sequence understanding
1748,,7747e65363b954c2a30d336e482e6f4c595b2478,A Deep Learning Approach to the Malware Classification Problem using Autoencoders,,,"###The use of the TF-IDF together with different configurations of n-gram frequencies were inspired by [29] and [28]; however, another difference between this work and the related ones is, in fact, the use of not only the opcodes but also its operands as possible features.###In [29], the authors had the goal to explore data mining and machine learning approaches to develop models with good performance in detecting malware binary files not previously observed.###In previous works [4, 28, 29], authors investigate the use of Operational Codes – OpCodes – frequencies and statistics, as determinant features for the correct identification of malware patterns.",impact-revealing,highlighting the inspiration and differences in approach for malware detection
3249,5ee8986891e011e66831c3b6,38f93092ece8eee9771e61c1edaf11b1293cae1b,Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,5736960c6e3b12023e51ec0c,Semi-Supervised Learning with Ladder Networks,"In the semi-supervised setting [55, 56], an unsupervised loss is combined with a classification loss over a handful of labels to ground the training [19, 20, 57, 58, 59, 60, 61, 62].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2959,5dd6604a3a55ac78684acf68,af54c890ffce96bae303310182be2ca301f2f97e,On Using SpecAugment for End-to-End Speech Translation,599c796f601a182cd263d4e6,Sequence-To-Sequence Models Can Directly Translate Foreign Speech,"Recent advancements in both ASR [1–6] and MT [7–11] have inspired the end-to-end direct ST models which can be trained using a translation speech corpus [12, 13].",other,highlighting the influence of advancements in ASR and MT on direct ST models
208,5c5ce4fd17c44a400fc38abb,d524f10d653ba09e36456475da0aed92d244f795,Image Super-Resolution As A Defense Against Adversarial Attacks,5b3d98cc17c44a510f8018e7,Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models.,"Closely related to our approach are the Defense-GAN [38] and MagNet [26], which first estimate the manifold of clean data to detect adversarial examples and then apply a mapping function to reduce adversarial noise.",impact-revealing,acknowledge related approaches to adversarial example detection
2721,5d3c234c3a55acd386d4e112,a9ec03dbe702f6909acd1f1f14a3395d0141043b,generative models for graph-based protein design,558cd0d615f38090e2f2a1f0,Direct-coupling analysis of residue coevolution captures native contacts across many protein families.,"We accomplish the first two by leveraging a well-evidenced finding in protein science, namely that long-range dependencies in sequence are generally short-range in 3D space (Marks et al., 2011; Morcos et al., 2011; Balakrishnan et al., 2011).",other,highlighting a well-evidenced finding in protein science
2804,5f7aeb7691e011983cc81e80,572c12e81319ccd47cc0c637c82efadd03fd05ab,Autoregressive entity retrieval,5c8dc31f4895d9cbc69e72ee,Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation.,"Lexically constrained decoding has been used to force the inclusion of pre-speciﬁed words for machine translation (Hokamp & Liu, 2017; Post & Vilar, 2018), and image captioning (Anderson et al., 2017).",other,acknowledge applications of lexically constrained decoding
1162,,accfbdb4d66039cb649c9dbf7e7cd1746ac45775,Morphos Configuration Engine: the Core of a Commercial Configuration System in CLP(FD),,,"###Finally, there have been various attempts to combine different solutions in order to enhance the modeling and solving capabilities of model-based reasoning configurators.###The configuration problem has recently attracted a significant amount of attention not only from the application point of view, but also from the methodological one [30].###The introduction of the class of rule constraints is motivated by the fact that they make it possible to adopt a rule-based style in the specification of the dependencies among product features, which is quite common in existing configuration systems (Morphos included).###Different proposals can be situated in the setting of model-based reasoning, including approaches based on description logic [23], constraint programming [24, 36], and resource models [16].",impact-revealing,highlighting the attention on configuration problems and rule-based approaches
2624,5aed14d117c44a4438158af2,30f86d38f0660af5ea2e16d996434c72eee8c5ee,espnet: end-to-end speech processing toolkit,53e9af8db7602d97039d31ed,The Rwth Aachen University Open Source Speech Recognition System,"Especially, these efforts have been driven by popular products including Google voice search, Amazon Alexa, and Apple Siri and open source activities including Kaldi [1], HTK [2], Sphinx [3], Julius [4], RASR [5] in addition to general research activities.",other,acknowledging the influence of popular products and open-source activities on speech recognition research
3586,5e3d353b3a55ac4de4104f40,3024f58826a5bce3378af94f677e8fb90cbb49e0,LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"Later on, GraphSage [14] and GCN [23] re-define graph convolution in the spatial domain, i.###Many work have specified the AGG, such as the weighted sum aggregator in GIN [42], LSTM aggregator in GraphSAGE [14], and bilinear interaction aggregator in BGNN [48] etc.###It takes inspiration from the Graph Convolution Network (GCN) [14, 23], following the same ar X iv :2 00 2.###Recently emerged graph neural networks (GNNs) shine a light on modeling graph structure, especially highhop neighbors, to guide the embedding learning [14, 23].###This is different from most existing graph convolution operations [14, 23, 36, 39, 48] that typically aggregate extended neighbors and need to handle the self-connection specially.",other,describing advancements in graph neural networks and their methodologies
1151,,ff85a9d7182063bb71b47ab239d662fd2975c4fc,Score-based diffusion models for accelerated MRI,,,"###Therefore, while DDPM was developed in a separate variational framework, it can also be seen as a realization of SDE.###In fact, VP-SDE can be seen as the continuous version of DDPM (Song et al., 2021b; Kingma et al., 2021).###Song et al. (2022) is perhaps the most related to our work, in that the authors also propose to use VE-SDE of (Song et al., 2021b), and they use the same network architecture together with the PC sampler, as in our work.###(6)
Then, we can solve the SDE numerically, for example, with Euler-Maruyama discretization (Song et al., 2021b).###We refer to score-based SDEs as score-based diffusion models henceforth to emphasize that our proposed methodology can be flexibly used with any realizations from the two model classes:score-based generative models, and diffusion models.###Conditional generation from p(x|y) has also been studied in the context of widely known computer vision problems: in-painting (Song and Ermon, 2019; Song et al., 2021b), superresolution (Choi et al., 2021; Saharia et al., 2021), and image editing (Meng et al., 2021).###However, we note that the use of VP-SDE (including family of DDPMs developed under the variational framework) is also straightforward under our framework.###This was further developed in (Song et al., 2021b) to image colorization, and class-conditional image synthesis, using continuous-time score models.###Unconditional generation of samples from p(x) using these score-based diffusion models have found their applications in image (Song et al., 2021b; Nichol and Dhariwal, 2021; Dhariwal and Nichol, 2021), audio (Kong et al., 2021), and even graph (Niu et al., 2020) synthesis.###The same
group published a work for image editing (Meng et al., 2021) using VE-SDEs, which uses a similar algorithm to image inpainting used in (Song and Ermon, 2019; Song et al., 2021b).###This introduces a caveat when reconstructing the data with the score function, because the original theory of scorebased SDEs (Song et al., 2021b) did not consider complex signals.###Interestingly, the reverse process of (1) can be constructed with another stochastic process (Song et al., 2021b):
dx = [ f (x, t) − g(t)2 ∇x log pt(x)︸ ︷︷ ︸ score function ]dt + g(t)dw̄ (4)
= d[σ2(t)] dt ∇x log pt(x)︸ ︷︷ ︸
score function
+
√ d[σ2(t)]
dt dw̄,
where dt is the infinitesimal negative…###Note that this specific choice of λ(t) stabilizes the noise scale across t, and theoretically corresponds to likelihood weighting, as proven in (Song et al., 2021a).###Recently, score-based models (Hyvärinen and Dayan, 2005; Song and Ermon, 2019), and denoising diffusion probabilistic models (DDPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) have gained wide interest as a new class of generative model which achieves surprisingly high sample quality without adversarial training (Song et al., 2021b; Nichol and Dhariwal, 2021; Dhariwal and Nichol, 2021).###It is also worth mentioning that we use an advanced sampler (PC), and a more efficient network architecture, which was shown to improve the performance of generative modeling by a large margin (Song et al., 2021b).###…θ of the score network with the following cost:
min θ
Et∼U(0,1) [ λ(t)Ex(0)Ex(t)|x(0) [ (5)∥∥∥sθ(x(t), t) − ∇x log p0t(x(t)|x(0))∥∥∥22 ]],
where λ(t) is an appropriate weighting function, e.g. likelihood weighting of Song et al. (2021a), which puts different emphasis according to the time t.###We follow similar procedures to train VE-SDE as advised in (Song et al., 2021b).###…diffusion probabilistic models (DDPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) have gained wide interest as a new class of generative model which achieves surprisingly high sample quality without adversarial training (Song et al., 2021b; Nichol and Dhariwal, 2021; Dhariwal and Nichol, 2021).###One can construct different SDEs by choosing different functions for f and g.###We base the implementation of the time-dependent score function model ncsnpp 2 as suggested in (Song et al., 2021b).###On the other hand, variance exploding (VE) SDEs choose
f = 0, g =
√ d[σ2(t)]
dt , (3)
where σ(t) > 0 is again a monotonically increasing function, typically chosen to be a geometric series (Song and Ermon, 2019; Song et al., 2021b).###For the step size i used in the Langevin MC corrector step, we follow what is advised in (Song et al., 2021b), and set
i = 2r ‖z‖2
‖sθ(xi, σi)‖2 , (18)
1https://fastmri.org/
where r = 0.16 is set to a constant value.###Iteratively applying predictor and corrector steps yield the predictor-corrector (PC) sampling algorithm (Song et al., 2021b), as presented in Algorithm 1.###Among many works, Song et al. (2021b) generalized discrete score-matching procedures
∗Corresponding author. e-mail: jong.ye@kaist.ac.kr (Jong Chul Ye)
to a continuous stochastic differential equation (SDE), which in fact also subsumes diffusion models into the same framework.###With the trained score model at hand using the de-noising score matching loss, we construct a solver for the reverse SDE from the variance exploding (VE)-SDE (Song et al., 2021b), which enables us to sample from the distribution p(x|y), conditioned on the measurement y.###Furthermore, our work is based on the continuous version of score matching (Song et al., 2021b), whereas the work of Jalal et al. (2021) is based on a discrete version (Song and Ermon, 2020).",impact-revealing,highlighting the significance of diffusion probabilistic models and their applications in generative modeling
790,5dc9327d3a55acc1042498de,2cf3bd0cc1382f35384e259d99e4f9744eeaed28,Blockwise Self-Attention for Long Document Understanding,599c7987601a182cd2648373,Attention Is All You Need.,"…the paradigm of language model pre-training and down-stream task ﬁne-tuning, BERT (Devlin et al., 2019) consists of multiple layers of bidirectional Transformers (Vaswani et al., 2017), where each Transformer encoder has a multi-head self-attention layer and a position-wise feed-forward layer.###2556 shrinking the model by lowering the number of layers L, attention heads A, or hidden units H leads to significant performance degradation (Vaswani et al., 2017; Devlin et al., 2019) and does not address the long sequence issue.###For instance, shrinking the model by lowering the number of layers L , attention heads A , or hidden units H leads to signiﬁcant performance degradation (Vaswani et al., 2017; Devlin et al., 2019) and does not address the long sequence issue.###, 2019) consists of multiple layers of bidirectional Transformers (Vaswani et al., 2017), where each Transformer encoder has a multi-head self-attention layer and a position-wise feed-forward layer.###Following (Vaswani et al., 2017), the dot-product attention in Transformer is defined as:###Since the invention of Transformer (Vaswani et al., 2017; Dai et al., 2019) and its successful application on language model pre-training (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019), there have been several studies attempted to simplify it from different…###Since the invention of Transformer (Vaswani et al., 2017) and its successful application to masked language model pre-training (Devlin et al.###Analogous to Multi-head Attention (Vaswani et al., 2017), we allow queries, keys, and values to be projected multiple times and perform blockwise attentions in parallel.###1) shows that one of the main bottlenecks is actually dot-product selfattention, operated in multiple layers of Transformers (Vaswani et al., 2017), the building block of BERT.###…” Although one may think that model size is the main contributor to the large memory consumption, our analysis (Section 2.1) shows that one of the main bottlenecks is actually dot-product self-attention, operated in multiple layers of Transformers (Vaswani et al., 2017), the building block of BERT.###3 M ODEL : B LOCK B ERT Following (Vaswani et al., 2017), the dot-product attention in Transformer is deﬁned as: where Q , K , V ∈ R N × d with N to be the sequence length and d to be a hidden dimension.",impact-revealing,providing context on the architecture and components of BERT
16,5dce788a3a55ac9580a162f8,56cafbac34f2bb3f6a9828cd228ff281b810d6bb,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"For the MLM objective, we follow the approach of existing PLMs (Devlin et al., 2019; Liu et al., 2019d).###Recent pre-trained language representation models (PLMs), such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019d), learn effective language representation from large-scale unstructured corpora with unsupervised language modeling objectives.###We extract text from these two sources in the same way as Devlin et al. (2019).###Recent pre-trained language representation models (PLMs) such as BERT (Devlin et al., 2019) and###Later, Devlin et al. (2019) release a pre-trained deep Bidirectional Encoder Representation from Transformers (BERT), achieving state-of-the-art performance on a wide range of NLP benchmarks.###Usually, there is a special token [CLS] added to the beginning of the text (Devlin et al., 2019), and the output at [CLS] is regarded as the representation for the whole sentence.###For the text encoder, we use Transformer architecture (Vaswani et al., 2017) in the same way as Devlin et al. (2019); Liu et al. (2019d).###For the MLM objective, we follow the approach of existing PLMs (Devlin et al., 2019; Liu et al., 2019c).",impact-revealing,reporting on the methodology of existing pre-trained language models
1661,,595cd0cd9df4ce80e512cf0699c0037270b462d2,Online brand advocacy and brand loyalty: a reciprocal relationship?,,,"###The theory suggests people’s self-concepts have individual and social aspects (Tajfel and Turner, 1979) that are reinforced by their identification with social referents, such as organisations or brands (Bhattacharya and Sen, 2003) and by experiencing their successes and failures as one’s own (Ashforth and Mael, 1989).",impact-revealing,highlighting the theoretical framework of self-concept and its social aspects
1489,,c63a14b04a7b14a022fb09ac132d44dd62195d7d,Denoising Graph Super-Resolution towards Improved Collider Event Reconstruction,,,"###Super-resolution (SR) techniques, which have been extensively studied in the field of image processing [5–11], offer the potential to go beyond the spatial resolution intrinsic to the calorimeter design.###The core concept of our super-resolution model is inspired by the work [11].",impact-revealing,highlighting the potential of super-resolution techniques in calorimeter design
2442,5fe4094e9e795e14f30e634a,94497472eecb7530a2b75c564548c540ebd61e9b,Learning to Pre-train Graph Neural Networks,5b67b4b917c44aac1c867dbc,Hierarchical Graph Representation Learning with Differentiable Pooling.,"R EADOUT is typically implemented as a simple pooling operation like sum, max or mean-pooling (Atwood and Towsley 2016; Duvenaud et al. 2015) or more complex approaches (Bruna et al. 2014; Ying et al. 2018b).###Various GNN architectures with different aggregation schemes have been proposed (Kipf and Welling 2017; Hamilton, Ying, and Leskovec 2017; Velickovic et al. 2018; Ying et al. 2018b; Hasanzadeh et al. 2019; Qu, Bengio, and Tang 2019; Pei et al. 2020; Munkhdalai and Yu 2017).###Various GNN architectures with different aggregation schemes have been proposed (Kipf and Welling 2017; Hamilton, Ying, and Leskovec 2017; Velick-ovic et al. 2018; Ying et al. 2018b; Hasanzadeh et al. 2019; Qu, Bengio, and Tang 2019; Pei et al. 2020; Munkhdalai and Yu 2017).###Empirically, these GNNs have achieved impressive performance in many tasks, such as node and graph classiﬁcation (Kipf and Welling 2017; Hamilton, Ying, and Leskovec 2017), recommendation systems (Fan et al. 2019; Ying et al. 2018a) and graph generation (Li et al. 2018; You et al. 2018).",other,acknowledge various GNN architectures and their applications
180,5f7aeb7691e011983cc81e80,572c12e81319ccd47cc0c637c82efadd03fd05ab,Autoregressive entity retrieval,5b1643ba8fbcbf6e5a9bc5b8,Improving Entity Linking By Modeling Latent Relations Between Mentions,"Entity Disambiguation (ED) We reproduce the setting of Le & Titov (2018) using the same candidate sets, in-domain and out-of-domain datasets, and evaluating using the InKB micro-F 1 .###Following previous works (Yamada et al., 2016; Ganea & Hofmann, 2017; Le & Titov, 2018), we considered only mentions that have entities in the KB (i.e., Wikipedia).###Although there has been extensive previous work on entity retrieval (e.g. Hoffart et al., 2011; Pic-cinno & Ferragina, 2014; Huang et al., 2015; Le & Titov, 2018; Logeswaran et al., 2019; Broscheit, 2019; Wu et al., 2019, to name just a few) there is a common design choice to most current…",impact-revealing,acknowledge existing work on entity disambiguation and retrieval
231,5e5e18de93d709897ce37796,0a6a9e6d4e3efd7c69357769305b70097281655f,DropEdge: Towards Deep Graph Convolutional Networks on Node Classification,5d04e8e2da56295d08db7ff1,On Asymptotic Behaviors of Graph CNNs from Dynamical Systems Perspective.,"Later, Kipf & Welling (2017); Defferrard et al. (2016); Henaff et al. (2015); Li et al. (2018b); Levie et al. (2017) apply improvements, extensions, and approximations on spectralbased GCNs.###Our interpretations on why DropEdge can impede over-smoothing is based on the concepts proposed by Oono & Suzuki (2019).###Then, we recall Corollary 3 and Proposition 1 in Oono & Suzuki (2019) as Corollary 1 below.###(3) As mentioned in § 4.3, FastGCN, AS-GCN and GraphSAGE are considered as the DropNode extensions of GCNs.###This paper will use the concept of subspace by Oono & Suzuki (2019) for more generality.###This unwanted convergence restricts the output of deep GCNs to be only relevant to the graph topology but independent to the input node features, which as a matter of course incurs detriment of the expressive power of GCNs. Oono & Suzuki (2019) has generalized the idea in Li et al. (2018a) by taking both the non-linearity (i.e. the ReLu function) and the convolution filters into account; they explain over-smoothing as convergence to a subspace rather than convergence to a fixed point.###GCNs Inspired by the huge success of CNNs in computer vision, a large number of methods come redefining the notion of convolution on graphs under the umbrella of GCNs.###According to the conclusions by the authors in Oono & Suzuki (2019), a sufficiently deep GCN will certainly suffer from the -smoothing issue for any small value of under some mild conditions (the details are included in the supplementary material).###To prove Theorem 1, we need to borrow the following definitions and corollaries from Oono & Suzuki (2019).###The proof of Theorem 1 is based on the derivations in Oono & Suzuki (2019) as well as the concept of mixing time that has been studied in the random walk theory (Lovász et al., 1993).###JKNet (Xu et al., 2018a) employs dense connections for multi-hop message passing which is compatible with DropEdge for formulating deep GCNs. Oono & Suzuki (2019) theoretically prove that the node features of deep GCNs will converge to a subspace and incur information loss.###We also explain how the proposed DropEdge can prevent over-fitting and over-smoothing in generic GCNs.###…to the graph topology but independent to the input node features, which as a matter of course incurs detriment of the expressive power of GCNs. Oono & Suzuki (2019) has generalized the idea in Li et al. (2018a) by taking both the non-linearity (i.e. the ReLu function) and the convolution…",impact-revealing,highlighting the challenges and developments in graph convolutional networks
1583,,b2f0eeec41dd0b445c9b1dad0ca81c6b19c316c1,Improved Whale Optimization Algorithm Based on Nonlinear Adaptive Weight and Golden Sine Operator,,,"###Such algorithms mainly include Particle Swarm Algorithm (PSO) [6], Fireflies Algorithm (FA) [7], Bat Algorithm (BA) [8], Artificial Fish SwarmAlgorithm (AFSA) [9], Bird Mating Optimization Algorithm (BMO) [10], Gray Wolf Optimization Algorithm (GWO) [11], Naked Mole-Rat algorithm (NMR) [12], Cuckoo Search (CS) [13], Salp Swarm Algorithm (SSA) [14], Grasshopper Optimization Algorithm (GOA) [15], Moth-Flame Optimization Algorithm (MFO) [16], Ant Lion Optimization Algorithm (ALO) [17], Dolphin Echo Localization Algorithm [18], Fruit fly Optimization algorithm (FOA) [19], Hunting Search (HS) [20], Krill Herd (KH) [21], Wasp Swarm Algorithm (WSA) [22], Bee Collecting Pollen Algorithm (BCPA) [23], Monkey Search (MS) [24], etc.###For more information, see https://creativecommons.org/licenses/by/4.0/ 77013
Such algorithms mainly include Particle Swarm Algorithm (PSO) [6], Fireflies Algorithm (FA) [7], Bat Algorithm (BA) [8], Artificial Fish SwarmAlgorithm (AFSA) [9], Bird Mating Optimization Algorithm (BMO) [10], Gray Wolf Optimization Algorithm (GWO) [11], Naked Mole-Rat algorithm (NMR) [12], Cuckoo Search (CS) [13], Salp Swarm Algorithm (SSA) [14], Grasshopper Optimization Algorithm (GOA) [15], Moth-Flame Optimization Algorithm (MFO) [16], Ant Lion Optimization Algorithm (ALO) [17], Dolphin Echo Localization Algorithm [18], Fruit fly Optimization algorithm (FOA) [19], Hunting Search (HS) [20], Krill Herd (KH) [21], Wasp Swarm Algorithm (WSA) [22], Bee Collecting Pollen Algorithm (BCPA) [23], Monkey Search (MS) [24], etc. Physical phenomenon-based algorithms are inspired by physical phenomena, including Black Hole Algorithm (BH) [25], Center Force Optimization Algorithm (CFO) [26], Gravitational Search Algorithm (GSA) [27].",impact-revealing,listing various optimization algorithms
1383,,87ef59eccd797bdc1405b2f4da05f452a11c004f,A Comparative Study of CMA-ES on Large Scale Global Optimisation,,,###Delta grouping is inspired by the idea of improvement interval under coordinate rotation explained in detailed in [18].,impact-revealing,describing the inspiration behind delta grouping
663,5fae6dced4150a363cec41f7,71c97e7d4a529a21f32d98aad72fbbf32c9ee32e,Anomaly-Based Intrusion Detection From Network Flow Features Using Variational Autoencoder,5c8b9ec64895d9cbc69dfbb6,Autoencoder-based feature learning for cyber security applications,"In this active research area, the studies using deep learning approaches mostly focus on dimensionality reduction [2]–[6] and anomaly-based intrusion detection [7]–[10].",impact-revealing,acknowledge focus areas in deep learning research
2283,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,5c8904934895d9cbc6a90d1d,ThinLTO: scalable and incremental LTO.,"As we will show, these microservices could benefit from larger Icache and ITLB and other techniques that address instruction misses [64, 65].",other,suggesting improvements for microservices performance
2232,558ab239e4b037c08758a249,c1757f34c23241960a0089c5117fa3b676902951,the effect of code reordering on branch prediction,539087f320f70186a0d6faee,Software Trace Cache,"Both factors prove important at increasing the fetch performance, as shown in [19, 18].###We examine the interaction of these optimizations with both static and dynamic branch predictors using the Software Trace Cache layout optimization [19].###In order to simulate the optimized code layout we generate an address translation table using the Software Trace Cache algorithm [19] and feed the simulator with translated PC’s and recomputed branch outcomes.###Code layout optimizations usually target a better utilization of the instruction cache, and use profile data or heuristics to lay out the routines in a program [17, 7, 6], and the basic blocks in a routine [8, 17, 25, 19] to minimize the number of conflict misses.###We optimize the code layout using the Software Trace Cache (STC) algorithm [19], which targets an increase in the sequentiality of the code, that is, it reorders basic blo ks so that branches tend to be not taken.###By aligning basic blocks so that they execute sequentially, we can further increase spatial loca ity increasing both cache performance and fetch bandwidth [8, 17, 25, 19].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3270,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,5c8a11324895d9cbc6121c34,Neural Message Passing for Quantum Chemistry.,"Graph Neural Networks (GNNs) [15, 21, 29, 39, 48] allow end-toend differentable losses over data with arbitrary structure.###They have been applied to an incredible range of applications, from social networks [46], to recommender systems [65], to computational chemistry [21].",other,highlighting the versatility and broad application of Graph Neural Networks
1637,,9710707dcbc71a0f12e0964785cd1d123f8f9cfb,Neural Networks based approaches for Major Depressive Disorder and Bipolar Disorder Diagnosis using EEG signals: A review,,,"###The Center for Epidemiologic Studies Depression Scale (CES-D) was initially designed for the general population.###Wet electrodes are often made of silver chloride (AgCl) and use a gel to create a conductive path between the electrodes and the skin by reducing the impedance value.###[78] No drug history DSM-IV,(BDI)II (BDI)II-score>14 [123] No psychiatric drug [124] No mental disorder Written consent [125] No mental disease (DSM)-IV,HUSM (DSM)-score [126] Free of any medications (BDI)II (BDI)II-score>14 [127] Free of any medications Written consent [128] Free of drugs (BDI)II (BDI)II-score>14 [138] No psychopathology.###[120] SynAmps 61 10-20 Wet [121] Mobile EEG belt 3(Fp1, Fp2 and Fpz) Frontal lobe 10-20 Dry [122] 3(FP1,FP2,FPz) Frontal lobe 10-20 Wet [78] 7(FP1-FP8) Frontal lobe 10-20 Wet [123] 2 Channel pair Left/right half 10-20 Wet [124] Bipolar Montage 2 Channel pair Left/right half 10-20 Wet [125] ProComm 1(F4) Frontal lobe Wet [126] EEG belt 3 Frontal lobe 10-20 Wet [127] Brain amp 6(Fp1, Fp2, F3, F4, P3 and P4) Prefrontal,parietal cortex 10-20 Wet###15âĂŞ30Hz) [73] FastICA,hanning filter [120] Off line ICA [121] Low pass filter at 50HZ [116] Notch filter at 50HZ [122] Band pass filter [78] Wavelet filter [123] Notch and low pass filter at 50HZ [124] Notch filter at 50HZ [127] Convolutional filter [128] ANFIS [138] Net station waveform tools [129] Low pass,high pass filter [130] ICA,notch,low pass,high pass filter [131] Notch filter [116] Visual inspection,50-Hz notch filter [132] EEGLAB toolbox [133] Stationary wavelet transform (SWT) [134] Lowpass and highpass filter [135] Lowpass,highpass and notch filter [136] MSEC [137] ICA and butterworth filter###Depression is a leading source of disability worldwide and significantly contributes to the global burden of disease.###Wet electrodes are usually made of silver and silver chloride material and applied on a scalp by using the electrolytic gel material that works as a conductor between the skin and the wet electrodes.###The Hamilton Depression Rating Scale (HAM-D) [81] is a depression assessment tool that consists of 17 items that are used for scoring.###Wehave searched IEEEXplore, PubMed, Embase, Springer, ScienceDirect and Web of Science, for articles published between January 2010, and May 2020 by using the following keywords: (âĂĲDepressionâĂİ OR âĂĲshallow neural networkâĂİ OR âĂĲDeep learningâĂİ OR âĂĲElectroencephalogramâĂİ OR âĂĲCross-validationâĂİ OR âĂĲBipolar depressionâĂİ OR âĂĲArtificial neural networkâĂİ OR
Sana Yasin et al.: Preprint submitted to Elsevier Page 2 of 29
âĂĲuni polar depressionâĂİOR âĂĲEEGbase depressionâĂİ OR âĂĲbipolar depressionâĂİ OR âĂĲMajor depressive disorderâĂİ OR âĂĲâĂİ OR âĂĲRecurrent neural networkâĂİ OR âĂĲDeep neural networkOR âĂĲBDI-IIâĂİOR âĂĲDSMIVâĂİ OR âĂĲPHQ-9âĂİ OR âĂĲPersistent depressive disorderâĂİ OR âĂĲDeep Learning ModelsâĂİ OR âĂĲEEG bio-markersâĂİ OR âĂĲDeep Feature ExtractionâĂİ OR âĂĲFFNN OR Feed forward neural networkâĂİ OR âĂĲFBNN OR Feed backword neural networkâĂİ OR âĂĲDiscriminatives Deep learning modelsâĂİ OR âĂĲRepresentative Deep learning ModelsâĂİ OR âĂĲGenerative Deep Learning ModelsâĂİ OR âĂĲHybrid Deep learning ModelsâĂİ OR âĂĲDepression diagnosis’s techniquesâĂİ OR âĂĲConvolutional neural networkORCNNâĂİOR âĂĲMild DepressionâĂİ OR âĂĲMLPâĂİ OR âĂĲMental StatesâĂİ OR âĂĲ EEG ArtifactsâĂİ OR âĂĲBipolar DepressionâĂİ OR âĂĲManic DisorderâĂİ OR âĂĲDeep Belief NetworksâĂİ OR âĂĲLSTM OR Long Short Term Memory.###Multi-layer FFNN, Back Propagation Neural Network (BPNN), and Enhanced probabilistic neural network (EPNN) have been used in [60, 121, 78] for discriminating MDD and non-MDD patients.###Wet and dry types of electrodes are used in EEG based studies to diagnose bipolar disorder.###Beck Depression Inventory (BDI) [74] is a common valuation tool used by healthcare specialists and researchers for pre-testing of depression and anxiety diagnosis.###The Young Mania Rating Scale (YMRS), Hamilton Depression Rating Scale (HDRS) and Structured Clinical Interview (SCID), are the few bipolar assessment tools that are widely used for participants selection [57].###The Diagnostic and Statistical Manual of Mental Disorders (DSM-IV) and Back Depression Inventory (BDI-II) are the two major self-reported psychometric tests that are used in bipolar studies [115, 116, 175, 174] for primary selection of the participants.###[122] 178(86D+92H) Depression [78] 24(12D+12H) 10F,14M 20-28 Major Depressive Disorder [123] 30(15D+15H) 16F,14M 20-50 Depression [124] 30D 20-50 Depression [125] 64(34D+30H) 26F,38M 15-38 Major Depressive Disorder [126] 25(13D+12H) 25F,0M (24.###Depression and bipolar anomalies usually indicate dysfunction in the human brain.###Back Depression Inventory (BDI) [74], which is first published in 1961, is another questionnaire-based approach commonly used as a valuation tool by healthcare specialists and scientists to diagnose depression and anxiety.###Clinical Depression or MDD assessment includes an extensive checkup of the patient,including examination of the mental state by discovering functional, relational, societal issues, and psychiatric history.###[55, 77, 78], used DSM-IV to monitor perinatal depression patients for an EEG study.###Hospital Anxiety and Depression Scale (HADS) is used in [53] to measure the different levels of depression.###Depression and Bipolar Disorder affects three portions of the brain: hippocampus (resides in the temporal lobe of the brain), prefrontal cortex (located at the front of the frontal lobe) and amygdala (the frontal portion of the temporal lobe) [61].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1934,,7f055de40052490feab747b3a3e835b6a5f6f985,Sex differences in perceptual processing of Chinese characters emerge in middle-grade primary school children,,,"###Their reading ability was assessed in a Chinese character recognition test (Wang & Tao, 1993), which has been widely used for measuring the vocabulary and reading experience of Chinese children (Shu et al., 2006; Zhao et al., 2019); it suggested that boys and girls had similar reading skills in both the junior (M = 631 vs.",impact-revealing,reporting findings on reading ability assessment
3988,5f06e5e591e0117f54657c19,f6d32ed0eee5fb3f6ac518f3aebc8ceff2aae397,NVAE: A Deep Hierarchical Variational Autoencoder,53e9b47db7602d9703f8321e,Information Maximization in Noisy Channels : A Variational Approach.,"First, VAEs maximize the mutual information between the input and latent variables [29, 30], requiring the networks to retain the information content of the input data as much as possible.",other,providing context on the function of VAEs
2665,58437722ac44360f1082f15c,cc16e43cce64b649da00892d1493425620c2d61c,Learning to Match Using Local and Distributed Representations of Text for Web Search.,5550489045ce0a409eb6f76a,A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval.,"Unlike some previous work [16, 36, 37] that train on clickthrough data with randomly sampled documents as negative examples, we train our model on human-judged labels.###[36] developed a convolutional version of the model.",other,highlighting a methodological difference in training data
1114,,c220b71ac300329637ca23bee115fb33c5826e3d,TwinDiffusion: Enhancing Coherence and Efficiency in Panoramic Image Generation with Diffusion Models,,,"###From DDPM [16] to DDIM [33] to LDM [27], diffusion models have paved the way for text-to-image generation, boosting AI-painting applications like Stable Diffusion [27] and DALLE2 [26].###Alongside the scheduler optimization [33, 9] and diffusion distillation [29], we refer to the trajectory stitching and interlaced sampling method [22] for our sampling acceleration request.",impact-revealing,highlighting the evolution and impact of diffusion models in text-to-image generation
1625,,0176f0a03b1495700221059959e3c01e6c9386d9,AVATAR : Fixing Semantic Bugs with Fix Patterns of Static Analysis Violations,,,"###In the framework, we leverage the Ochiai [52] ranking metric to actually compute the suspiciousness scores of statements that are likely to be the faulty code locations.###For evaluation purpose, we apply different fault localization schemes to the experiment of each RQ, while the default setting of A VATAR is to use the GZoltar framework with the Ochiai ranking metric for ordering suspicious statements.###We include in this category other APR systems whose authors do not explicitly describe the actual fault localization conﬁguration, but which still manage to ﬁx bugs that we could not localize with GZoltar/Ochiai.###To that end, we attempt to replicate two scenarios of fault localization used in APR assessments: the ﬁrst scenario assumes that the faulty method name is known [10] and thus focuses on ranking the inner-statements based on Ochiai suspiciousness scores; the second scenario makes no assumption on fault location and thus uses the default setting of A VATAR .###The usage of GZoltar and Ochiai reduces the comparison biases since both are widely used by APR systems in the literature.###In this experiment, we consider a group of APR systems, namely jGenProg [66], jKali [66], jMutRepair [57], Nopol [15], FixMiner [21] and LSRepair [60], which leverage a similar conﬁguration as A VATAR for fault localization: GZoltar/Ochiai.",impact-revealing,describing the framework and methods used for fault localization
1811,,8614cfb26ec2096420c16acd772d659d7c5744d2,Evidence‐Based Review of Therapeutic Plasma Exchange in Neurological Disorders,,,"###The committee also concluded that TPE is probably effective in milder disease and should be considered in such patients (3).###On the basis of evidence from RCTs, the AAN has concluded that TPE is effective in CIDP and should be offered as a short-term therapy in these patients (3).###On the basis of available literature, the AAN considers TPE to be probably effective as adjunctive treatment for relapsing forms of MS, possibly effective in cases of steroid-unresponsive acute fulminant CNS demyelination and ineffective in progressive forms of MS (3).###In their 2011 guidelines, AAN concluded that TPE is possibly effective in acute CNS demyelination (including MS, ADEM, NMO, and TM), which is not responsive to high-dose corticosteroids (3).###TheAANhas concluded that TPE is possibly effective in acute CNS demyelination (including MS, ADEM, NMO, and transversemyelitis [TM]) when there is a failure to respond to high-dose intravenous (IV) corticosteroid therapy (3).###In developing their updated practice guideline (3), AAN did not consider evidence from observational studies or case reports, which they categorized as class IV evidence; therefore, they concluded that the evidence was insufficient to support or refute the efficacy of TPE in the treatment of myasthenic crisis or MG prethymectomy.###The intent of this review is to discuss the role of TPE in neurological disorders, and their current categorization as indications for TPE by the American Society for Apheresis (ASFA) and the American Academy of Neurology (AAN) (2,3).###Thus, the AAN has concluded that TPE is effective in AIDP patients with severe disease (impaired ability to walk or requiring mechanical ventilation) and should be offered to these patients (3).###Recently emerging indications for TPE, concerning neuromyelitis optica (NMO) and natalizumab clearance, will be discussed in additional detail (2,3).###Similarly, AAN did not offer recommendations on this potential indication for TPE in their 2011 guidelines, but did acknowledge that initial data suggest a role for TPE in restoring leukocyte function by accelerating the clearance of natalizumab, although more studies are needed to determine whether this results in clinical benefit in patients with infectious complications (3).",impact-revealing,discussing the role and effectiveness of TPE in various neurological disorders
3903,5b3d98cc17c44a510f801acc,155b7782dbd713982a4133df3aee7adfd0b6b304,Unsupervised Feature Learning via Non-parametric Instance Discrimination,53e9b108b7602d9703b85b88,Distributed Representations of Words and Phrases and their   Compositionality,"Similar problems have been well addressed in the literature for learning word embeddings [25, 24], where the number of words can also scale to millions.###Popular techniques to reduce computation include hierarchical softmax [26], noise-contrastive estimation (NCE) [9], and negative sampling [24].",other,acknowledge existing techniques for learning word embeddings
298,5fb24ee191e01186d3f5decc,050fecf6e3e21faeeb7629f179ba839c4dfd1ead,EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints,5a9cb65d17c44a376ffb80ef,Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement,"Algorithm The reposition and insertion operations used in EDITOR are designed so that the Levenshtein edit distance algorithm (Levenshtein, 1966) can be used as the oracle.###To train EDITOR via imitation learning, the reposition operation is defined to preserve the ability to use the Levenshtein edit distance (Levenshtein, 1966) as an efficient oracle.",impact-revealing,describing the design and training of the EDITOR algorithm
3774,5dbebb7447c8f766462c2328,b0d941cfa0a3c43703e5222221addb1ad0f9e68d,Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks,599c7ce9601a182cd27e7834,Neural Collaborative Filtering.,"In recent years, the content-based RS [21] and the collaborative ﬁltering RS [7, 27] are two widely used methods because they can eﬀectively approximatethe similarity between items while being simple and eﬃcient.###Neural collaborativeﬁltering(NCF) [7] ﬁrst proposes to use the multi layer perceptron to approximate the matrix factorization process.",other,acknowledge widely used recommendation system methods
539,5da2f8a647c8f76646083cd9,b789abc47b7a92596050f6055a93c8fe1929db2a,Dynamic Multicontext Segmentation of Remote Sensing Images Based on Convolutional Networks,53e9b6d0b7602d970425d0f5,A Real-Time Algorithm for Signal Analysis with the Help of the Wavelet Transform,putation of wavelet transform [38] and employed in the deep,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
756,5aed147c17c44a4438153a60,5245d411b9dd97ffafe07320981d1282e8f32764,"dCat: dynamic cache management for efficient, performance-sensitive infrastructure-as-a-service",53e9ac42b7602d970360c87b,Gaining insights into multicore cache partitioning: Bridging the gap between simulation and real systems,OS-level page coloring Lin’s study [29] aims to offer cache partitioning by using OS-level page coloring which directly assigns LLC among threads in the real system.,impact-revealing,reporting prior findings on cache partitioning using OS-level page coloring
1256,,d2dcdff2fc263534eee87f0dcc987dac4b9c05e2,A comparative study of SQP-type algorithms for nonlinear and nonconvex mixed-integer optimization,,,"###A possible stabilization of the method of Exler et al. [21] is achieved by adding linear outer approximations as proposed by Fletcher and Leyffer [24] and Duran and Grossmann [17].###When applying an SQP algorithm at a node of the search tree, it is possible to apply early branching as described by Leyffer [38].###To motivate our method, we briefly present the theoretical background of the method of linear
outer approximation described by Fletcher and Leyffer [24] and Duran and Grossmann [17], see also Quesada and Grossmann [44] for an alternative approach.###The methods of Fletcher and Leyffer [24] and Duran and Grossmann [17] are based on the idea that (21) is equivalent to
minimize y∈T f(x(y), y) .###(31)
The methods of Fletcher and Leyffer [24] and Grossmann [28] require exact partial derivatives with respect to the continuous and integer variables in (31).###(27)
To be able to prove convergence, Fletcher and Leyffer [24] or Duran and Grossmann [17], e.g., assumed that the relaxed program (22) is convex, i.e., that f(x, y) is convex and that
g(x, y) is concave over X and YIR.###The idea is introduced by Duran and Grossmann [17] and is extended by Fletcher and Leyffer [24].",impact-revealing,providing theoretical background for method stabilization
3969,558c2b08e4b00c3c48e0a105,368e031ce85bee93ad5bda8c0970cda76c9cf140,the heterogeneous block architecture,53e99bc6b7602d970246fe01,Conservation Cores: Reducing The Energy Of Mature Computations,", [24, 33, 62]), HBA optimizes for a future in which energy and power (and not core area) are key performance limiters [14].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3118,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,57aa28de0a3ac518da9896d5,node2vec: Scalable Feature Learning for Networks,"Graph embeddings [23, 47, 58] can be thought of as (very restricted) unsupervised GNNs with an identity feature matrix, meaning each node learns its own positional representation [67].",other,providing context on graph embeddings and their relation to GNNs
2792,5de4e0b73a55ac2224ba53a6,a75649771901a4881b44c0ceafa469fcc6e6f968,how can we know what language models know?,5b1643998fbcbf6e5a9bc0e1,Style Transfer Through Back-Translation,"phrasing, we follow the simple method of using back-translation (Prabhumoye et al., 2018) to first translate the initial prompt into B candidates in another language, each of which is then back-translated into B candidates in the original language.",other,describing the method of back-translation for prompt generation
1115,,70252eb26d902e253dd2bf60595921995b7441ca,Prompt-Driven Feature Diffusion for Open-World Semi-Supervised Learning,,,"###Ongoing developments in DM have led to advancements such as higher-resolution image generation [ Ho et al. , 2020 ] , accelerated training processes [ Song et al. , 2021 ] , and reduced computational costs [ Rombach et al. , 2022 ] .###Additionally, Denoising Diffusion Implicit Models (DDIM) were introduced, building upon DDPMs by incorporating a non-Markovian diffusion process, resulting in an acceleration of the generative process [Song et al. , 2021].",impact-revealing,highlighting advancements in diffusion models and their impact on generative processes
3335,555048d345ce0a409eb71be1,eba36ac75bf22edf9a1bfd33244d459c75b98305,Recurrent convolutional neural networks for text classification,53e9a84eb7602d97031983a6,Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.,Socher et al. (2011b) use semi-supervised recursive autoen-coders to predict the sentiment of a sentence.###Socher et al. (2011a) proposed a method for paraphrase detection also with recurrent neural network.###Socher et al. (2011a; 2011b; 2013) proposed the Recursive Neural Network (RecursiveNN) that has been proven to be efﬁcient in terms of constructing sentence representations.,other,reporting prior findings on sentiment prediction and paraphrase detection
693,53e9ada5b7602d97037a48ea,bce4119d062a654433eb37df56688a2cb96ed70f,Comparing cache architectures and coherency protocols on x86-64 multicore SMP systems,53e9b94cb7602d9704536034,Investigating Cache Parameters of x86 Family Processors,"Babka and Tůma present their work in [4], focusing mainly on translation lookaside buffers and cache associativity.",impact-revealing,reporting prior findings on translation lookaside buffers and cache associativity
344,5efb0d5691e011063336d39c,0cee58946a13a5c2845647b4af8b9d2bf52a8b6b,BOND: BERT-Assisted Open-Domain Named Entity Recognition with Distant Supervision,5a9cb66717c44a376ffb8ac1,Deep contextualized word representations.,"To address the challenges in learning from distant supervision, our approach leverages the power of pre-trained language models (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLnet (Yang et al., 2019)) which are particularly attractive to this task due to the following merits: First , they are very large neural networks trained with huge amounts of unlabeled data in a completely unsupervised manner , which can be cheaply obtained; Second , due to their massive sizes (usually having hundreds of millions or billions of parameters), they have strong expressive power to capture general semantics and syntactic information e ﬀ ectively.###, ELMo (Peters et al., 2018), BERT (Devlin et al.###To address the challenges in learning from distant supervision, our approach leverages the power of pre-trained language models (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), XLnet (Yang et al., 2019)) which are particularly attractive to this task due to the following merits: First…",impact-revealing,highlighting the advantages of pre-trained language models for distant supervision
1611,,68ca093392aa729fd08b3d66a53e427008544216,Social Media User Satisfaction—Theory Development and Research Findings,,,"###…and Pearson 1983; Doll and Torkzadeh 1988; Ives et al. 1983), Doll and Torkzadeh’s EUCS instrument has been widely used as an EUCS measure that is relevant to specific software and information system applications, including web-based information systems (Rauniar et al. 2009; Somers et al. 2003).###Instruments that assess end-user computing satisfaction (EUCS; Doll and Torkzadeh 1988) have been widely used by past researchers (Gelderman 1998; Igbaria 1990; Somers, Nelson, and Karimi 2003; Rauniar et al. 2009) to explain the online behaviors of users.",impact-revealing,acknowledge the widespread use of EUCS instruments in research
1767,,7f3ed1cde01c1c8f3ef711809deacab5704efb52,Expression of Tcf/Lef and sFrp and localization of β-catenin in the developing mouse lung,,,"###Recent evidence that Wnts and other genes in the Wnt signaling pathway are expressed in embryonic and adult mouse lung suggests that this pathway is important for cell fate decisions and differentiation of lung cell types.###Anti-
sense and sense digoxigenin-labeled RNA probes were
generated for Tcf1 (1.8 kb), Tcf3 (1.8 kb), Tcf4 (0.5 kb),
Lef1 (1.0 kb) (Oosterwegel et al., 1993; Korinek et al.,
1998), sFrp1 (2.5 kb), sFrp2 (2.0 kb), sFrp3 (2.5 kb) and
sFrp4 (1.8 kb) (Rattner et al., 1997).###…and Wnt signaling related genes (Wnt2, Wnt11, Tcf1, Lef1, sFrp1, sFrp2 and
sFrp4) in the embryonic and adult mouse lung (Levay-
Young and Navre, 1992; Oosterwegel et al., 1993; Rattner
et al., 1997; Lako et al., 1998) did not address the temporal
and spatial localization of transcripts and…",impact-revealing,highlighting the significance of the Wnt signaling pathway in lung cell differentiation
406,5a73cbc317c44a0b3035eccf,7a0feeee49ac7692257fb21041e25511bc45192a,“Zero-Shot” Super-Resolution using Deep Internal Learning,5550472145ce0a409eb64ae3,Learning A Deep Convolutional Network For Image Super-Resolution,"Dataset Scale SRCNN [4] VDSR [9] EDSR+ [13] SRGAN [12] SelfExSR [7] ZSSR (ours)###Super-Resolution (SR) from a single image has recently received a huge boost in performance using Deep-Learning based methods [4, 10, 9, 12, 13].###In fact, ZSSR is signiﬁcantly better than the older SRCNN [3], and in some cases achieves comparable or better results than VDSR [8] (which was the SotA until a year ago).###In fact, ZSSR is significantly better than the older SRCNN [4], and in some cases achieves comparable or better results than VDSR [9] (which was the SotA until a year ago).###As done in previous CNN-based SR methods [10, 9, 4], we only learn the residual between the interpolated LR and its HR parent.",impact-revealing,highlighting the performance improvements in super-resolution methods
2897,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e998ceb7602d970210ab4d,Integrating Sample-Based Planning And Model-Based Reinforcement Learning,Relative pruning was found to increase the win rate of the Go program L IN G O against GNU G O 3.8 by approximately 3% [106].,other,reporting a specific finding in Go program performance
787,573697316e3b12023e6217ec,67cf1189c859d66bac309f9438df434fb651f97a,Cache Coherence Protocol and Memory Performance of the Intel Haswell-EP Architecture,558b4e19e4b037c0875c2b40,Memory Performance and Cache Coherency Effects on an Intel Nehalem Multiprocessor System,"In previous work [7], [8] we presented micro-benchmarks to measure the characteristics of distributed caches in NUMA systems with x86 64 processors.###It provides data from its appendent L3 slice or obtains a copy from another core's L1 or L2 cache as indicated by the core valid bits [7].",impact-revealing,reporting prior findings on micro-benchmarks in NUMA systems
3310,5c96086e3cb210d2716c4a08,bb76749fab841ccbe20ace74111a518f65d9870b,make the most out of last level cache in intel processors,53e9b5d4b7602d970411ff28,A Data Layout Optimization Framework For Nuca-Based Multicores,"Some other works focus on software-based strategies, such as data layout optimization [41, 87] or compiler optimization [7, 11, 30, 31], to exploit NUCA characteristics to improve performance.",other,acknowledge existing software-based strategies for performance improvement
3033,5b67b4b917c44aac1c867dbc,d18b48f77eb5c517a6d2c1fa434d2952a1b0a825,hierarchical graph representation learning with differentiable pooling,57a4e91dac44365e35c9830c,Learning Convolutional Neural Networks for Graphs,"network (GNN) models have been proposed in recent years, including methods inspired by convolutional neural networks [5, 8, 11, 16, 21, 24, 29, 36], recurrent neural networks [25], recursive neural networks [1, 30] and loopy belief propagation [7].###• PATCHYSAN [29] defines a receptive field (neighborhood) for each node, and using a canonical node ordering, applies convolutions on linear sequences of node embeddings.###Some recent approaches have also proposed applying CNN architectures to the concatenation of all the node embeddings [29, 39], but this requires a specifying (or learning) a canonical ordering over nodes, which is in general very difficult and equivalent to solving graph isomorphism.",other,acknowledge variations in GNN model approaches
54,5e3be3c33a55ac29c4ae7e18,6dbdc34000b034b75b8ff70872fc7c35549e273a,Interpretable & Time-Budget-Constrained Contextualization For Re-Ranking,599c7968601a182cd263a485,End-to-End Neural Ad-hoc Ranking with Kernel Pooling,"ears neural network based re-ranking models matured and a distinct trade-off emerged between a neural re-ranking model’s effectiveness and its efﬁciency. While IR-speciﬁc networks are reasonably fast [36, 5, 15], large Transformer based models [32], such as BERT [6], show substantially better effectiveness at the cost of orders of magnitude longer inference time [12, 20, 25]. Given the same amount of limited###ize query and document sequences independent from each other and distill the interactions between terms in a single interaction match matrix, followed by softhistogram scoring based on kernel-pooling [36]. This allows us to explain scoring reasons by probing the model at the point of the information bottleneck to analyze contextualized term representations and interaction patterns. We conduct experime### interactions with the histogram-based DRMM model. However, it suffered from the non-differentability of a hard histogram method and the resulting lack of ﬁne-tuned word representations. Xiong et al. [36] improve on the idea and propose the kernel-pooling technique as part of the KNRM model. Conceptually, it approximates a histogram with a set of Gaussian kernel functions for different similarity rang###^ 1:n together in a single match-matrix M 2 Rqlen dlen with pairwise cosine similarity as interaction extractor: M i;j = cos(^q i;d^ j) (4) Then, we transform each entry in Mwith a set of RBF-kernels [36]. Each kernel focuses on a speciﬁc similarity range with center  k. The size of all ranges is set by ˙. In contrast to Xiong et al. [36] we do not employ an exact match kernel – as contextualized rep###the other hand has a continuous stream of interactions in each layer and each attention head, making a focused analysis unfeasible. The differences of TK to previous kernel-pooling methods are: KNRM [36] uses only word embeddings, therefore a match does not have context or positional information. CONV-KNRM [5] uses a local-contextualization with limited positional information in the form of n-gram l",impact-revealing,highlighting the trade-off between effectiveness and efficiency in neural re-ranking models
2683,5f058d15dfae54570ec57ea1,fb93ca1e004cbdcb93c8ffc57357189fa4eb6770,Resnest: Split-attention networks,573696086e3b12023e51beec,Highway Networks,Highway network [50] introduces highway connections which makes the information ﬂow across several layers without attenuation and helps the network convergence.,other,providing context on highway network architecture
232,5e5e18a493d709897ce22b32,3345443925cec95381c2cf2f0b029c411945bfef,GraphSAINT: Graph Sampling Based Inductive Learning Method,5a9cb66717c44a376ffb8667,FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling.,"To mitigate such “neighbor explosion”, state-of-the-art methods [14, 6, 5, 33, 18] use various layer sampling techniques.###The work in [18] improves [6] by adding an additional sampling neural network.###Point (2) improves accuracy and robustness on sparse graphs and deep nets by avoiding the overly sparse minibatches of [6] (see Section 3.###In order to scale GCNs to large graphs, layer sampling techniques [14, 6, 33, 5, 9, 18] have been proposed for efficient minibatch training.###Point (3) reduces training time by decreasing the number of sampler invocations (compared with [6, 18]), and the cost of each invocation (compared with [18]).###Current works on Graph Convolutional Networks (GCNs) [14, 6, 9, 18, 5] mostly focus on shallow models (2 layers) on relatively small graphs.###Under the independent layer sampling assumption of [6], one would sample a connection ( u, v ) with probability p (l) u,v ∝ 1 deg(u) + 1 deg(v) .###This sampler is similar to the layer sampler in [6].###FastGCN [6] performs sampling from another perspective.###The works in [6, 18] further propose samplers that ensure a fixed number of samples in all layers, which potentially restricts the neighbor expansion factor to 1.",impact-revealing,highlighting advancements in layer sampling techniques for GCNs
1221,,1e320d7634f0a0280efaa8228c9c88244892d30b,"Diagnosis, evaluation, and treatment of childhood obesity in pediatric practice.",,,"###These characteristics are associated with a higher prevalence of obesity in children and a higher risk for comorbid diseases, in particular, diabetes mellitus.(4,5,7,17-26)###Evidence of treatment for overweight included documentation of any one of the following in the text of the index visit: (1) recommended diet modification (eg, “dietary changes recommended” or “↓ [decrease] snacks”), (2) increase in physical activity (eg, “discussed exercise” or “↑ activity”), (3) recommended follow-up visits specifically for a weight problem (eg, “RTC [return to clinic] 2 months for wt ”), (4) referral to a subspecialist for weight control (nutritionist, endocrinology clinic, weight loss program, or social worker), or (5) testing for comorbid disease (insulin levels, glucose testing, or lipid levels).###C HILDHOOD OBESITY CONfers a substantial risk of adult obesity, lifelong health risks, and considerable social and economic disadvantage.(1-10) Given the epidemic of childhood obesity, experts have emphasized the importance of identifying overweight youth, understanding correlates of obesity-related morbidity, and educating children and families regarding healthy diets and physical activity.",impact-revealing,Highlighting the significance of addressing childhood obesity and its associated risks
3049,5d5e6b9a3a55acfce79a16dd,6303bac53abd725c3b458190a6abe389a4a1e72d,Deep High-Resolution Representation Learning for Human Pose Estimation,5a260c8417c44a4ba8a31c12,Learning Feature Pyramids for Human Pose Estimation,"Following [14, 74, 60], a six-scale pyramid testing procedure is performed.###5 score, and outperforms the stacked hourglass approach [39] and its extensions [56, 14, 74, 30, 60].###Our network has two benefits in comparison to existing widely-used networks [39, 26, 74, 70] for pose estimation.###Hourglass and its follow-ups [39, 14, 74, 30] design the low-to-high process as a mirror of the high-to-low process.###Both the two processes are possibly repeated several times for boosting the performance [74, 39, 14].###Hourglass [39] and its extensions [74, 30] combine low-level features in the high-to-low process into the same-resolution high-level features in the low-to-high process progressively through skip connections.",other,acknowledge existing methods and their benefits in pose estimation
3207,5edf5dd891e011bc656deed4,ae0988c55095827384bf70e346e192a89216e75c,IterefinE: Iterative KG Refinement Embeddings using Symbolic Knowledge,5c2c7a9217c44a4e7cf3170a,Improved Knowledge Graph Embedding using Background Taxonomic   Information,"It is worth noting that embeddings, with a few recent exceptions [Guo et al., 2016, Minervini et al., 2017, 2018, Fatemi et al., 2019], do not make use of rich taxonomic/ontological rules when available.###Recently, SimplE [Fatemi et al., 2019] includes taxonomic information –i.###Recently, SimplE+ [Fatemi et al., 2019] includes taxonomic information –i.e., subtype and subproperty information– and also shows that state-of-the-art embeddings like ComplEx [Trouillon et al., 2016], SimplE [Kazemi and Poole, 2018], ConvE [Dettmers et al., 2018] cannot enforce subsumption.",other,highlighting limitations in existing embeddings and their use of taxonomic information
3151,5f576c1591e011f4c3d5dd7e,c36571ac50808c75fa8a5d37f1041af22e89e6ee,GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training,5ac1829d17c44a1fda917fdc,To understand deep learning we need to understand kernel learning,"Second, the majority of modern deep learning models are shown to be able to interpolate the data [5, 22, 41].###[5] Mikhail Belkin, Siyuan Ma, and Soumik Mandal.",other,acknowledging findings on deep learning model behavior
3944,5f0d85c69fced0a24be4f019,6817b5d48cf8f665b680de07d23b91814d2923b1,Bouquet of Instruction Pointers: Instruction Pointer Classifier-based Spatial Hardware Prefetching,5843772fac44360f10831725,SPAC: A Synergistic Prefetcher Aggressiveness Controller for Multi-Core Systems.,"Apart from ﬁlters, there are aggres-siveness controllers (throttlers) [16], [20], [30], [31], [39]–[41], [50] that control the prefetch degree and prefetch distance based on prefetch metrics like accuracy, coverage, LLC pollution, and DRAM bandwidth.",other,acknowledge existing methods for controlling prefetching
2470,5fae6dced4150a363cec41f7,71c97e7d4a529a21f32d98aad72fbbf32c9ee32e,Anomaly-Based Intrusion Detection From Network Flow Features Using Variational Autoencoder,599c7efe601a182cd28e0838,A stochastic data discrimination based autoencoder approach for network anomaly detection,"The NSL-KDD dataset [13], which is a revised version of KDDCUP99, is used for evaluating the methods proposed in the studies [3], [4], [6]–[8].",other,reporting the dataset used for evaluation
2658,5a260c8117c44a4ba8a30a57,908272f8e6340971600148d4e73f50e1e8843aaf,"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec",53e9ad72b7602d970375da90,"Networks, Crowds, and Markets: Reasoning About a Highly Connected World","conventional paradigm of mining and learning with networks usually starts from the explicit exploration of their structural properties [12, 30].",other,providing context on conventional mining and learning paradigms
2783,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,555043d345ce0a409eb49970,A Primer on Hardware Prefetching,"Nevertheless, several microservices under-utilize available bandwidth, and hence might beneﬁt from optimizations that trade bandwidth to improve latency, such as hardware prefetching [77].",other,highlighting potential optimizations for microservices
2642,599c7ea4601a182cd28b8342,103baca878b17a15d148a684c0b0152e78591be1,A Split Cache Hierarchy for Enabling Data-Oriented Optimizations,53e9b123b7602d9703ba1443,Distance Associativity For High-Performance Energy-Efficient Non-Uniform Cache Architectures,"Data classification has been used to steer NUCA placement [2], [3], [4], [5] and for filtering coherence activities [37] in broadcast protocols, among other things.###NUCA caches have been proposed to address the wire delays in LLCs [2], [3], [4], [5], [6], [7].###) • Data placement and replication to lower latency and reduce traffic by keeping data closer to where it is needed [2], [3], [4], [5], [6], [7].",other,reporting prior applications of data classification in NUCA placement
2974,5f0d85c69fced0a24be4f028,5d9073cfec34aea00247ec625fa94f6279ff580d,tailored page sizes,53e9b9cdb7602d97045c695c,Characterizing The Tlb Behavior Of Emerging Parallel Workloads On Chip Multiprocessors,"Prior work [8], [13], [23], [31], [32], [34] has demonstrated that some applications can spend up to 50% of their execution time servicing page table walks.###Prior work has proposed hardware PTE prefetchers [13], [33], [52].###Prior work has shown that excessive page walks may significantly degrade performance in applications suffering from limited TLB reach [8], [13], [23], [31], [32], [34].",other,highlighting the performance impact of page table walks in applications
2511,5bdc31b417c44a1f58a0b240,6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da,Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks,53e9b049b7602d9703aada9d,Marginalized Kernels Between Labeled Graphs,"Important early work in this area includes random-walk based kernels (G¨artner, Flach, and Wrobel 2003; Kashima, Tsuda, and Inokuchi 2003)) and kernels based on shortest paths (Borgwardt and Kriegel 2005).",other,acknowledging foundational work in kernel methods
2205,5c2c7a9217c44a4e7cf3161b,e6926981ef9c1d06d6c075cdae7b298d3dbf3a7d,Learning Latent Representations for Style Control and Transfer in End-to-end Speech Synthesis,57a4e91aac44365e35c9780b,Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations.,The encoder which deals with character inputs consists of three 1-D convolutional layers with 5 width and 512 channels followed by a bidirectional [15] LSTM [16] layer using zoneout [17] with probability 0.,other,describing the architecture of a character input encoder
3915,5aed14e217c44a4438159759,c097be22f1e87a846385047346b73610d91fea4e,GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs,58d82fc8d649053542fd59aa,Geometric deep learning on graphs and manifolds using mixture model CNNs,"Models like the adaptive forget gate strategy in Graph LSTM (Liang et al., 2016) and MoNet (Monti et al., 2017) employed pairwise sum ag-gregators with a single head or multiple heads.###Most existing graph aggregators are based on either pooling over neighborhoods (Kipf and Welling, 2017; Hamilton et al., 2017a) or computing a weighted sum of the neighboring features (Monti et al., 2017).###, 2016) and MoNet (Monti et al., 2017) employed pairwise sum aggregators with a single head or multiple heads.###, 2017a) or computing a weighted sum of the neighboring features (Monti et al., 2017).",other,acknowledge variations in existing graph aggregation methods
392,5e85c28491e0114016e821a6,21b11793b960a3e37c0eab7aae6127c28fd38e5c,Code Prediction by Feeding Trees to Transformers,5b8c9f4a17c44af36f8b6a50,code2seq: Generating Sequences from Structured Representations of Code.,"In Code2Seq, a decoder is then used to solve a code summarization task: given a method body, how well can Code2Seq generate the correct method name? The training proposed in [15] is not well suited for next token prediction.###[15] that embeds code snippets by embedding AST paths in a neural network.###Researchers have also investigated using the syntactic structure of code for prediction, as opposed to seeing code as text: both using probabilistic graphical models (probabilistic context-free grammars [11] and probabilistic higher-order grammars [12]–[14]), as well as using deep learning [15].###These representations have ranged from linear token sequence (as for code prediction [5], [7], [32]), to paths in an AST [15], [21], [52], and sometimes even ways to convey static analysis information to the neural network [24], [49], [51], [64].",impact-revealing,acknowledge existing methods and approaches in code summarization
2258,5f1022a091e01168a7d6fc4f,04e234c7e4f300b4e8a16370728d875f9f484b39,learning visual context by comparison,58d82fced649053542fd6bed,SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks   for Image Captioning,"In the visual recognition domain, recent self-attention mechanisms [15,34,7,22,38] generate dynamic attention maps for recalibration (e.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2687,5ce2d0feced107d4c63dd498,d07284a6811f1b2745d91bdb06b040b57f226882,Decoupled Weight Decay Regularization,5c75735bf56def97988902df,On the Convergence of Adam and Beyond,"…gradient methods, such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2014) and most recently AMSGrad (Reddi et al., 2018) have become a default method of choice for training feed-forward and recurrent neural networks (Xu et al., 2015; Radford et al.,…###, 2011), RMSProp (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2014) and most recently AMSGrad (Reddi et al., 2018) have become a default method of choice for training feed-forward and recurrent neural networks (Xu et al.###Adaptive gradient methods, such as AdaGrad (Duchi et al., 2011), RMSProp (Tieleman & Hinton, 2012), Adam (Kingma & Ba, 2014) and most recently AMSGrad (Reddi et al., 2018) have become a default method of choice for training feed-forward and recurrent neural networks (Xu et al., 2015; Radford et al., 2015).###While we focused our experimental analysis on Adam, we believe that similar results also hold for other adaptive gradient methods, such as AdaGrad (Duchi et al., 2011) and AMSGrad (Reddi et al., 2018).",other,acknowledging the prevalence and effectiveness of adaptive gradient methods in neural network training
401,5c8d57d74895d9cbc653a9e9,3afa826a594d90ee0f4fe062c988289bb213a114,Deep Back-Projection Networks For Super-Resolution,53e9a945b7602d970329639b,Improving resolution by image registration,"Inspired by [18], we construct an end-to-end trainable architecture based on the idea of iterative up- and downsampling: Deep Back-Projection Networks (DBPN).###Back-projection [18] is well known as the efficient iterative procedure to minimize the reconstruction error.###On the other hand, feedback connections were used effectively by one of the early SR algorithms, the iterative back-projection [18].",impact-revealing,describing the construction of a new architecture based on existing methods
2463,5bbacbad17c44aecc4eb007e,fdb87d305c59e9ab82a96198e34e46581b838c4e,one-shot relational learning for knowledge graphs,59ae3bf12bbe271c4c71c067,Meta-SGD: Learning to Learn Quickly for Few Shot Learning.,"ing the input example with a small labeled support set; (2) meta-learner based approaches (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Finn et al., 2017; Li et al., 2017), which aim to learn the optimization of model parameters (by either outputting the parameter updates or directly predicting the model parameters) given the gradients on few-shot examples.###…example with a small labeled support set; (2) meta-learner based approaches (Ravi and Larochelle, 2017; Munkhdalai and Yu, 2017; Finn et al., 2017; Li et al., 2017), which aim to learn the optimization of model parameters (by either out-putting the parameter updates or directly predicting the…",other,describing meta-learner based approaches in few-shot learning
2185,,bb0ca9b1a21e803148db98f8052c0e36e2739ef2,Revolution oder Evolution – Neue Trends in der Business Intelligence,,,"###Nowadays research institutions tend to build upon the NIST definition striving for standardization (Vaquero, Rodero-Merino, Caceres, & Lindner, 2008) (Vouk, 2008).",impact-revealing,acknowledging the trend towards standardization in research definitions
1000,,613173cc9bb6c2ee51ed4345761fec4b7584cc1d,"Including students with moderate and severe complexity of disability in kindergarten and first grade: Investigating the relationship between inclusive classroom quality indicators, level of inclusive education, and social competence",,,"###…David & Kuyini, 2012; Erwin & Guintini, 2000; Favazza & Odom, 1997; Foreman, Arthur-Kelly, Pascoe, & King, 2004; Gelzheiser, McLane, Meyers, & Pruzek, 1997; Hunt, Soto, Maier, Müller, & Goetz, 2002; Kwon, Elicker, & Kontos, 2011; Narian, 2011; Odom & Diamond, 1998; Ohtake, 2003; Sontag, 1997).###Odom and Diamond (1998) further support this finding by expounding that the manner in which adults respond to the questions of children in teachable moments, in conjunction with intentional activities, help to form children's ideas about, and interactions with, children with complex disabilities.###! ! 2! According to Reschly (2008), the nature of this shift comes from a variety of sources including continued problems with providing an effective system of support in general, remedial, and special education, current research producing better outcomes, changes in nationally recognized organizational policies, and the aforementioned federal laws coupled with supporting state laws.###…2012; Erwin & Guintini, 2000; Favazza & Odom, 1997; Forman, Arthur-Kelly, Pascoe, & King, 2004; Gelzheiser, McLane, Meyers, & Pruzek, 1997; Hunt, Soto, Maier, Müller, & Goetz, 2002; Kwon, Elicker, & Kontos, 2011; Mashburn et al., 2008; Narian, 2011; Odom & Diamond, 1998; Ohtake, 2003; Sontag,
!###Another strategy, the use of sensitivity training of non-disabled children to help them
understand the communication and behaviors of the included child/children with a disability, can also be incredibly helpful (Favazza & Odom, 1997; Odom & Diamond, 1998; Terpstra & Tamura, 2008).###Other researchers have
13
highlighted the importance of teachers in orchestrating opportunities for membership and social inclusion of children with disabilities within the regular education environment (David & Kuyini, 2012; Narian, 2011; Odom & Diamond, 1998; Ohtake, 2003).###…& King, 2004; Gelzheiser, McLane, Meyers, & Pruzek, 1997; Hunt, Soto, Maier, Müller, & Goetz, 2002; Kwon, Elicker, & Kontos, 2011; Narian, 2011; Odom & Diamond, 1998; Ohtake, 2003; Sontag, 1997), there has not been research to delineate which teacher behaviors may be more or less effective for…###…Elicker, & Kontos, 2011; Sontag, 1997), classroom memberships (David & Kuyini, 2012; Erwin & Guintini, 2000; Favazza & Odom, 1997; Narian, 2011; Odom & Diamond, 1998; Ohtake, 2003), and support for social communication (Cosbey & Johnson, 2006; Foreman, Arthur-Kelly, Pascoe, & King, 2004; Hunt,…###In spite of this logic, it is children with the most complex needs who continue to be segregated (Alquraini & Gut, 2012; Bently, 2008; Odom, 2000; Odom & Diamond, 1998).",impact-revealing,acknowledging the lack of research on effective teacher behaviors for inclusion
3084,558c2b08e4b00c3c48e0a105,368e031ce85bee93ad5bda8c0970cda76c9cf140,the heterogeneous block architecture,53e9a5f0b7602d9702f1c503,Mitigating Amdahl'S Law Through Epi Throttling,", [2, 3, 6, 10, 20, 23, 28, 33, 59, 61]), or combine an in-order and an out-of-order pipeline with a shared frontend in a single core [37].###Coarse-grained Heterogeneous Cores: Several works propose the use of statically heterogeneous cores [2, 3, 6, 10, 20, 23, 28, 33, 58, 59], dynamically heterogeneous cores [26, 30, 31, 60], one core with heterogeneous backends [37], or a core with variable parameters [4, 23] to adapt to the running application at a coarse granularity for better efficiency.###This is important for many algorithms and for any application with serialized code sections [1, 2, 15, 28, 29, 59].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
186,5d4d46fb3a55acff992fddf3,001ac7ef884ede59df7207a4cc894bb2d52ea94d,Automatic Grassland Degradation Estimation Using Deep Learning,573696056e3b12023e51921c,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,"Employing the Focal-Hinge loss, we further compare the semantic segmentation performance of our network with that of FCN-32s, FCN-16s, FCN-8s [Long et al., 2015], SegNet [Badrinarayanan et al., 2017], and DeepLab-v3 [Chen et al., 2018b].###In addition, we also showcase the competence of our network with comparison to other classic semantic segmentation networks – FCN [Long et al., 2015], SegNet [Badrinarayanan et al., 2017], and DeepLabV3 [Chen et al., 2018b].###, 2015], SegNet [Badrinarayanan et al., 2017], DeepLab-v3 [Chen et al.###, 2015], SegNet [Badrinarayanan et al., 2017], and DeepLabV3 [Chen et al.###, 2015], SegNet [Badrinarayanan et al., 2017], and DeepLab-v3 [Chen et al.###The proposed network is based on the classic encoderdecoder network architecture without the fully-connected layers [Badrinarayanan et al., 2017] and we improve it by adding the refined cross connections as shown in Figure 2.###Recently, there are many fabulous semantic segmentation models such as FCN [Long et al., 2015], SegNet [Badrinarayanan et al., 2017], DeepLab-v3 [Chen et al., 2018a], PSPNet [Zhao et al., 2017] and etc. Leong first proposed Fully Convolutional Networks (FCN) [Long et al., 2015] that is a…###Network Architecture The proposed network is based on the classic encoderdecoder network architecture without the fully-connected layers [Badrinarayanan et al., 2017] and we improve it by adding the refined cross connections as shown in Figure 2.",impact-revealing,comparing the performance of different semantic segmentation networks
688,5d1eb9b7da562961f0af38c9,c0f5e89cf9f1b4f3d9c76a2ae3b13315e691554b,Personalized Student Stress Prediction with Deep Multitask Network,5b16426b8fbcbf6e5a9b5b4a,"Predicting Tomorrow's Mood, Health, and Stress Level using Personalized Multitask Learning and Domain Adaptation.","To learn personalized models for each student, we follow (Jaques et al., 2017) and use a Multitask approach which comprises of a LSTM to model sequence of histograms followed by shared fully connected layers and a MLP for each student.",impact-revealing,describing the method for personalized model learning
3126,53e9a034b7602d970291ce1a,2b5bc598cd02362874245ccfd932f65f6bc571d9,local graph partitioning using pagerank vectors,53e9ba11b7602d9704616767,Spectral Partitioning Works: Planar Graphs And Finite Element Meshes,"Spectral partitioning can be applied recursively, with the resulting cuts combined in various ways, to solve more complicated problems; for example, recursive spectral algorithms have been used to find k-way partitions, spectral clusterings, and separators in planar graphs [2, 8, 13, 14].###Sweeps A sweep is an efficient technique for producing cuts from an embedding of a graph, and is often used in spectral partitioning [11, 14].",other,describing applications of spectral partitioning techniques
3600,5e5e18f793d709897ce40800,b21025dedb3dfcd80be001c7eff091d83579fddf,CopyMTL: Copy Mechanism for Joint Extraction of Entities and Relations with Multi-Task Learning,5ce2d0bbced107d4c63af699,Joint Extraction of Entities and Overlapping Relations using Position-Attentive Sequence Labeling,"To solve it, multi-pass tagging training, HRL 5 , has been purposed (Takanobu et al. 2018), based on the reinforcement learning framework and (Dai et al. 2019) leverage attention mechanism.###To solve this problem, the followers (Takanobu et al. 2018; Dai et al. 2019) run tagging on a sentence for multiple turns, which is akin to the table ﬁlling method together with the heavy computational burden.###2018), based on the reinforcement learning framework and (Dai et al. 2019) leverage attention mechanism.###To solve this problem, the followers (Takanobu et al. 2018; Dai et al. 2019) run tagging on a sentence for multiple turns, which is akin to the table filling method together with the heavy computational burden.",other,acknowledge existing methods for multi-pass tagging training
3947,5b1643ba8fbcbf6e5a9bc79b,b3f83e8416010e9c3a705a0b6390d268e5ddf5c0,Black-box adversarial attacks with limited queries and information,5843776cac44360f1083e1cb,Hidden Voice Commands,"Several papers have investigated practical black-box attacks on real-world systems such as speech recognition systems (Carlini et al., 2016), malware detectors (Hu & Tan, 2017; Xu et al., 2016), and face recognition systems (Sharif et al., 2017).###Several papers have investigated practical black-box attacks on real-world systems such as speech recognition systems (Carlini et al., 2016), malware detectors (Hu & Tan, 2017; Xu et al.",other,highlighting the investigation of black-box attacks on various real-world systems
3540,5ac1829d17c44a1fda9180a3,632d25ef7914ce962d258920460a9405b8c4553a,Sequence-Based Multi-Lingual Low Resource Speech Recognition,53e9a67cb7602d9702fb134b,Cross-Domain And Cross-Language Portability Of Acoustic Features Estimated By Multilayer Perceptrons,"With the on-set of deep learning the focus of the models shifted to learning features across languages which can be mapped to the same space [3, 11].###Multi-lingual or cross-lingual techniques allow transfer of models or features from well-trained scenarios to those where large amounts of training data may not be available, cannot be transcribed, or are otherwise hard to come by [3, 4].",other,highlighting the shift in focus towards multilingual and cross-lingual techniques in deep learning
2767,5c20b1fcda5629702063aff8,482f0510d8f64fe972ec7a05ae5989c25e98ea48,STRAIGHT: Hazardless Processor Architecture Without Register Renaming,53e99a04b7602d970224f3f6,Dynamic Cluster Assignment Mechanisms,Scalable ILP architectures such as clustered architectures are examples of the representative approaches [13] [19] [20].,other,providing examples of scalable ILP architectures
1695,,407e60682b5d168c44da90ede708f4c9cdba13af,A Social Identity Perspective of Personality Differences between Fan and Non-Fan Identities,,,"###Self-categorization theory builds upon social identity theory (Tajfel & Turner, 1979) and posits that one’s context influences the relative salience of his or her different identities, which, in turn, underlies his or her behavior (Turner, Hogg, Oakes, Reicher, & Wetherell, 1987).",impact-revealing,providing context for self-categorization theory
3267,5a4aef9e17c44a2190f7a4bd,1deb7f96fc92d5c9e04d2cbb277473fee878e144,Cascaded Pyramid Network for Multi-person Pose Estimation,556f622a2401b4b38c23635c,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,"Human detection approaches are mainly guided by the RCNN family [12, 11, 31], the upto-date detectors of which are [24, 15].",other,acknowledge existing human detection approaches
362,5fd0a7f691e01147f1d1e367,d4e9bf1d5fab52698a3aa48c65b098ea327deea6,CTRLsum: Towards Generic Controllable Text Summarization,5f8eab549e795e9e76f6f69e,Language Models are Unsupervised Multitask Learners,"In addition to comparing with the vanilla BART model, we also include the zero-shot performance from GPT2 language models (Radford et al., 2019) (without fine-tuning) as a reference point.###They have been utilized to perform multi-purpose text generation with a single unified model (Radford et al., 2019; Brown et al., 2020).###, 2016) and also to demonstrate the multi-task ability present in large pretrained models (McCann et al., 2018; Radford et al., 2019; Keskar et al., 2019; Brown et al., 2020).###In addition, in-091 spired by the multi-task ability of language models092 through prompting (Radford et al., 2019; Brown093 et al., 2020), we further study the possible combi-094 nation of keywords and prompts in CTRLSUM for095 more generic control purposes.096
We use pretrained BART (Lewis et…",impact-revealing,acknowledge the use of various language models for multi-task text generation
2637,5f896fa591e01149071e45df,1d16d4cdc3fcce26e2c2097d13896ec09683eee3,Self-training for Few-shot Transfer Across Extreme Task Differences,5dcd263a3a55ac58039516c5,Momentum Contrast for Unsupervised Visual Representation Learning,"Xie et al. (2020) found that training the student from scratch sometimes yields better results for ImageNet classiﬁcation.###Xie et al. (2020) posit that self-training introduces noise when training the student and thus yielding a more robust student.###The ﬁrst two terms are similar to those in prior self-training literature (Xie et al., 2020).###A more recent (and better performing) line of self-supervised learning is contrastive learning (Wu et al., 2018; Misra & Maaten, 2020; He et al., 2020; Chen et al., 2020) which aims to learn representations by considering each image together with its augmentations as a separate class.",other,highlighting findings on self-training and contrastive learning in image classification
3583,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,53e9a1c3b7602d9702abcf86,Aguri: An Aggregation-Based Traffic Profiler.,"proposed Aguri tree [22], an aggregationbased traffic profile that aggregates small volume flows with a fixed number of nodes in an IP tree for spatial measurement.",other,reporting a proposed method for traffic profiling
2818,5f2bd70491e011b36ba9ce89,902fc14117f67575067d8b2a1989ea900eaceb1c,TOAD-GAN: Coherent Style Level Generation from a Single Example,599c7b59601a182cd272c125,Wasserstein Generative Adversarial Networks.,"Many different extensions to the basic architecture were proposed to stabilize the training process, for example minimizing the Wasserstein distance (Arjovsky, Chintala, and Bottou 2017) and penalizing the norm of the gradients of the discriminator (Gulrajani et al. 2017).",other,acknowledge existing extensions to improve training stability
1294,,dea5278aded363d2ec4418f095bdf3df3fa78c98,Interactive Refinement of Cross-Lingual Word Embeddings,,,"###Since CLIME is designed for low-resource languages, we optimize an objective that reshapes the neighborhood more drastically than ATTRACT - REPEL .###The objective in ATTRACT - REPEL pulls synonyms closer to and pushes antonyms further away from their nearest neighbors.###The keyword ranking and the embedding reﬁnement modules build upon existing methods for interpreting neural networks (Li et al., 2016) and ﬁne-tuning word embeddings (Mrkši´c et al., 2017).###Mrkši´c et al. (2017) includes both monolingual and cross-lingual constraints to improve CLWE .###Our update equations are inspired by the attract-repel algorithm (Mrkšić et al., 2017), which fine-tunes word embeddings with a set of synonym and antonym constraints.###, 2016) and fine-tuning word embeddings (Mrkšić et al., 2017).###Prior post-processing methods emphasize regularization as a means of maintaining the topology, or properties that should be preserved under transformations, of the embedding space (Mrkšic et al., 2016; Mrkšić et al., 2017; Glavaš and Vulić, 2018).###Prior embedding post-processing methods emphasize regularization to maintain the topology— or properties that should be preserved under transformations—of the embedding space (Mrkši´c et al., 2016; Mrkši´c et al., 2017; Glavaš and Vuli´c, 2018).###Our update equations are inspired by ATTRACT - REPEL (Mrkši´c et al., 2017), which ﬁne-tunes word embeddings with synonym and antonym constraints.",impact-revealing,highlighting the optimization of objectives in low-resource language models
684,53e9aca8b7602d97036868cf,948b7d8d921dd709649c1efa53ebbf2af835d89e,dynamic branch prediction with perceptrons,53e9a87eb7602d97031ca626,The perceptron: a model for brain functioning. I,We can use a perceptron to learn correlations between particular branch outcomes in the global history and the behavior of the current branch.,impact-revealing,describing a method for learning correlations in branch outcomes
2198,5a4aef9e17c44a2190f7a6fb,af03709f0893a7ff1c2656b73249d60030bab996,NISP: Pruning Networks Using Neuron Importance Score Propagation,5a73cbcc17c44a0b3035f1ea,ReMotENet: Efficient Relevant Motion Event Detection for Large-scale   Home Surveillance Videos,"Besides the above work which focuses on network compression, other methods speedup deep network inference by refining the pipelines of certain tasks [33, 13, 46, 27].###Despite their impressive predictive power on a wide range of tasks [33, 40, 41, 13, 15, 48, 46, 24, 44, 42, 12, 45], the redundancy in the parameterization of deep learning models has been studied and demonstrated [6].",other,acknowledge methods for speeding up deep network inference
900,5d04e8d7da56295d08daef06,448b64df68335d3695a37c54770e7d5cd5f6fe68,Context Attentive Document Ranking and Query Suggestion,53e9a049b7602d970292deeb,Enhancing personalized search by mining and modeling task behavior,"nnnnnnn extracted from previous clicks or queries [4, 43, 51], or manually crafted rules are introduced to characterize the changes in a search sequence [13, 53].###[50, 51] develop a rich set of statistical features to quantify context information from users’ on-task search behavior.###[51] reported the use of users’ on-task behavior yielded promising gains in retrieval performance in the Microsoft Bing search engine.###, query reformulation and result clicks, often exhibit strong inter-dependency, which provides rich context information for systems to improve their retrieval performance [13, 26, 30, 51].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1133,,94286d66cd39f712678c56829dd367a0e5779ba5,Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling,,,"###Diffusion-based generative models Diffusion models are a family of generative models that are inspired by nonequilibrium thermodynamics [12, 32].",impact-revealing,providing context on diffusion-based generative models
3739,5eabf3cd91e011664efc496f,916595fc4e701ac6125725408912a1fac3d7a60b,CASINO Core Microarchitecture: Generating Out-of-Order Schedules Using Cascaded In-Order Scheduling Windows,5ce3cd34e1cd8e3f7932b8be,Fiforder Microarchitecture: Ready-Aware Instruction Scheduling For Ooo Processors,"Separate InO IQs in [7] reduce stalls caused by the intermingling of dependence chains.###For decades, researchers have tried to make an OoO core more energy efﬁcient by addressing the complexity of the scheduling logic [2], [4] or reducing the accesses to power-hungry structures [5], [6], [7], [8].###[7], [8] steer instructions that would not beneﬁt from OoO scheduling to the InO IQ(s), based on the availability and dependences of the source operands.",other,acknowledge existing research on energy efficiency in scheduling logic
160,5aed14e217c44a4438159868,d3707cf521e3596313af1f53acba6413d0d528a6,Training Tips for the Transformer Model,599c7987601a182cd2648373,Attention Is All You Need.,"We presented a broad range of basic experiments with the Transformer model (Vaswani et al., 2017) for English-to-Czech neural machine translation.###Abstract This article describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017).###In this article, we experiment with a relatively new NMT model, called Transformer (Vaswani et al., 2017) as implemented in the Tensor2Tensor1 (abbreviated T2T) toolkit, version 1.",impact-revealing,reporting on experiments with the Transformer model for neural machine translation
1649,,cc4c80ca59b69e3382d11fe34a6cca1211660872,Seeming Ethical Makes You Attractive: Unraveling How Ethical Perceptions of AI in Hiring Impacts Organizational Innovativeness and Attractiveness,,,"###This perspective builds on social identity theory (Tajfel & Turner, 1979) to suggest that individuals are attracted to an organization because, through personal affiliation, that organization’s symbolic features can in turn, communicate to others that those symbols also represent their personal…###In this paper, we leverage the P-O fit model (Kristof, 1996; Kristof-Brown et al., 2005), social identity theory (Tajfel & Turner, 1979), and the organizational attractiveness literature (Chapman et al., 2005) to explain why ethical perceptions about using AI in hiring can influence applicants’…###O) fit literature (Kristof, 1996) and social identity theory (Tajfel & Turner, 1979), which suggest that potential applicants seek out organizations whose (ethical) values fit well with their own, and that applicants consider the desirability of having such organizations become a part of their…###O fit model (Kristof, 1996) and social identity theory (Tajfel & Turner, 1979) to incorporate novel aspects about organizations, in this case the use of AI in hiring methods, and to integrate this novel aspect about organizations with the ethical perceptions people hold about using AI in hiring.###O fit literature (Kristof, 1996) and social identity theory (Tajfel & Turner, 1979) by investigating ethical perceptions of AI in hiring as a mechanism that determines organizational attractiveness and innovativeness.###Finally, we contribute to the P-O fit model (Kristof, 1996) and social identity theory (Tajfel & Turner, 1979) by showing that individuals who have higher ethical perceptions about using AI in hiring practices are likely to perceive that organizations that use AI in hiring share similar values to…",impact-revealing,Leveraging theories to explain ethical perceptions of AI in hiring
3423,59ec02da0cf22f5df7319dc3,c27db32efa8137cbf654902f8f728f338e55cd1c,mastering the game of go without human knowledge,58d82fd2d649053542fd7619,Learning to Act by Predicting the Future,"These systems have outperformed humans in computer games, such as Atari 6,7 and 3D virtual environments 8–10 .###However, very similar algorithms have subsequently proven highly effective in video games 6–8,10 , robotics 60 , industrial control 61–63 and online recommendation systems 64,65 .",other,highlighting the broad applicability and effectiveness of similar algorithms across various domains
1429,,e6a7d7c83ca792aad31b43ffc216d07a748041fb,Cal-QL: Calibrated Ofﬂine RL Pre-Training for Efﬁcient Online Fine-Tuning,,,"###To this end, Cal-QL builds on CQL [32] and then constrains the learned Q-function to produce Q-values larger than the Q-value of a reference policy µ per Deﬁnition 4.1.###Prior work [32, 6] shows that one can learn a good ofﬂine initialization by optimizing the policy against a conservative value function obtained from an ofﬂine dataset.###…challenges for a subset of ofﬂine RL methods, in Figure 2, we evaluate the ﬁne-tuning performance of a variety of prior ofﬂine RL methods (CQL [32], IQL [30], TD3+BC [11], AWAC [45]) on a particular diagnostic instance of a visual pick-and-place task with a distractor object and sparse binary…###In practice, Cal-QL can be implemented on top of conservative Q-learning [32], a prior ofﬂine RL method, without any additional hyperparameters.###These approaches typically employ ofﬂine RL methods based on policy constraints or pessimism [12, 49, 16, 15, 30, 51, 36] on the ofﬂine data, then continue training with the same method on a combination of ofﬂine and online data once ﬁne-tuning begins [43, 28, 62, 32, 4].###In the Antmaze domain, we used the dual version of CQL [32] and conducted ablations over the value of the threshold of the CQL regularizer R ( θ ) (target action gap) instead of α . the In the visual-manipulation domain which is not presented in original paper, we swept over the alpha values of α =…###Our approach will build on the conservative Q-learning (CQL) [32] algorithm.###This includes naïvely ﬁne-tuning ofﬂine RL methods such as CQL [32] and IQL [30], as well as ﬁne-tuning with AWAC [45], O3F [42] and online decision transformer (ODT) [65], methods speciﬁcally designed for ofﬂine RL followed by online ﬁne-tuning.",impact-revealing,describing the implementation and evaluation of offline reinforcement learning methods
1887,,d4dbe6554dee43fa35fed75b032ec59e41d84d67,Features of High-Quality Online Courses in Higher Education: A Scoping Review,,,"###Similarly, deNoyelles et al. (2014) promoted the Community of Inquiry framework in their article, which demonstrated the importance of a strong cognitive, teaching, and social presence in the classroom to nurture community and critical thought among virtual students (Garrison et al., 1999).###Contextually, features of high-quality online learning include specific frameworks that guide the creation and evaluation of online learning, such as the Community of Inquiry framework (Garrison et al., 1999), which defines quality teaching, social, and cognitive presences.",impact-revealing,highlighting the significance of the Community of Inquiry framework in online learning
2574,5fd0a7f691e01147f1d1e367,d4e9bf1d5fab52698a3aa48c65b098ea327deea6,CTRLsum: Towards Generic Controllable Text Summarization,5b1642d68fbcbf6e5a9b7f23,Guiding Generation for Abstractive Text Summarization Based on Key Information Guide Network.,"Keywords are used as extra input to improve unconstrained summarization or reduce hallucinations (Gehrmann et al., 2018; Li et al., 2018; Saito et al., 2020b; Elsahar et al., 2021; Dou et al., 2021).###While they mainly focus on improving the summary quality in traditional, unconstrained summarization tasks (Li et al., 2018; Elsahar et al., 2021; Saito et al., 2020b; Dou et al., 2021), or only study a specific con-trol aspect like length control (Saito et al., 2020a), we generalize keyword-guided…###We note that this is much simpler than recent keyword-based summarization systems (Li et al., 2018; Saito et al., 2020a; Dou et al., 2021) where they tweak the specific model architecture to incorporate the keywords signal.",other,highlighting the role of keywords in improving summarization tasks
2650,5ddcf7f53a55ac1c5e8cce13,1f2577071ca2aa1086f4b1c12cd911061aeea960,meta-learning of neural architectures for few-shot learning,53e99da4b7602d9702663540,Learning To Learn Using Gradient Descent,"This concept of learning from experience and related tasks is known as meta-learning or learning to learn [46, 50, 21].###Prior work [40, 16, 18, 19] often approaches few-shot learning via meta-learning or learning to learn [46, 50, 21, 22], where one aims at learning from a variety of learning tasks in order to learn new tasks much faster than otherwise possible [51].",other,highlighting the concept of meta-learning and its relation to few-shot learning
916,5ecc763e9e795e81e9307559,270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,mpnet: masked and permuted pre-training for language understanding,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"One of the most successful models is BERT [2], which mainly adopts masked language modeling (MLM) for pre-training1.###For language understanding, masked language modeling (MLM) in BERT [2] and permuted language modeling (PLM) in XLNet [5] are two representative objectives.###BERT (Devlin et al., 2019) 80.8 88.5 RoBERTa (Liu et al., 2019a) single model without any data augmentation for fair comparisons.###We assume all the three objectives mask and predict the same amount of tokens ( 15% ), following the common practice in BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) 5 .###Pre-training language models [1, 2, 3, 4, 5, 6, 7, 8] have greatly boosted the accuracy of NLP tasks in the past years.###On the dev set of GLUE tasks, MPNet outperforms BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019a) by 4.6, 3.2, 1.3 points on average.###The key of pre-training methods [1, 2, 4, 5, 10] is the design of self-supervised tasks/objectives for model training to exploit large language corpora for language understanding and generation.###Pre-training models (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019b; Song et al., 2019; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019a; Raffel et al., 2019a) have greatly boosted the accuracy of NLP tasks in the past years.###One of the most successful models is BERT (Devlin et al., 2019), which mainly adopts masked language modeling (MLM) for pre-training 2 .###For language understanding, masked language modeling (MLM) in BERT (Devlin et al., 2019) and permuted language modeling (PLM) in XLNet (Yang et al., 2019) are two representative objectives.###All of the listed re-sults are reported in BERT BASE setting and from MNLI QNLI QQP RTE SST MRPC CoLA STS Avg Single model on dev set BERT (Devlin et al., 2019) 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 83.1 XLNet (Yang et al., 2019) 86.8 91.7 91.4 74.0 94.7 88.2 60.2 89.5 84.5 RoBERTa (Liu et al.,…###1 Experimental Setup We conduct experiments under the BERT base setting (BERTBASE) [2], where the model consists of 12 transformer layers, with 768 hidden size, 12 attention heads, and 110M model parameters in total.###For the pre-training objective of MPNet, we randomly permute the sentence following PLM [5]5, choose the rightmost 15% tokens as the predicted tokens, and prepare mask tokens following the same 8:1:1 replacement strategy in BERT [2].###We conduct experiments under the BERT base setting (BERT BASE ) (Devlin et al., 2019), where the model consists of 12 transformer layers, with 768 hidden size, 12 attention heads as 12, and 110M model parameters in total.###We assume all the three objectives mask and predict the same amount of tokens (15%), following the common practice in BERT [2] and XLNet [5]3.###We use a sub-word dictionary with 30K BPE codes in BERT [2] to tokenize the sentences.###MLM in BERT BERT [2] is one of the most successful pre-training models for natural language understanding.###For the pre-training objective of MPNet, we randomly permute the sentence following PLM (Yang et al., 2019) 7 , choose the rightmost 15% tokens as the predicted tokens, and prepare mask tokens following the same 8:1:1 replacement strategy in BERT (Devlin et al., 2019).###For the non-predicted part (xz<=c ,Mz>c), we use bidirectional modeling [2] to extract the representations, which is illustrated as the light grey lines in Figure 2a.###The key of pre-training methods (Radford et al., 2018; Devlin et al., 2019; Song et al., 2019; Yang et al., 2019; Clark et al., 2020) is the design of self-supervised tasks/objectives for model training to exploit large language corpora for language understanding and generation.###MLM in BERT BERT (Devlin et al., 2019) is one of the most successful pre-training models for natural language understanding.",impact-revealing,highlighting the success and impact of pre-training models in NLP
1211,,0704d9419f7a9b3297ab6ea5f0fee4556c777dbb,Spatial dynamics of feedback and feedforward regulation in cell lineages,,,"###We note that the time scales of the agent-based portion of the model and the ODEs are the same, but that the processes occurring in these two parts of the model can occur at very different rates.###In contrast to previously published ODE models reviewed above [6,8,24], we here include the possibility of stem cell death for two reasons.###Examples include growth differentiating factor 11 (GDF11) and Activin βB (ACTβB), which negatively regulate self-renewal rates in progenitor and stem cells in the olfactory epithelium of mice [7,8]; transforming growth factor beta (TGF-β) [9], which is mutated in a variety of tumors [10–12]; the bone morphogenetic protein 4 pathway (BMP4) that is inactivated in glioblastomas [13]; and the adenomatous polyposis coli (APC) tumor suppressor gene that is inactivated in colorectal cancer, with concomitant activation of the Wnt cascade [14].###For each time step of the agent-based model, the ODEs in each patch were run for one time unit.###One These mathematical models of negative feedback regulation are typically based on ordinary differential equations (ODEs), which assume perfect mixing (mass action or meanfield assumption) of cells and molecules.###Denoting the populations of stem cells, differentiated cells, and feedback factors by S, D, and Z, respectively, the ODEs are given as follows. where negative feedback from differentiated cells onto the probability of self-renewal is given by p ¼ p 0 , and p’ is the self-renewal probability of stem cells in the absence of feedback.###Previous mathematical modeling approaches [6,8,24,27], based on the assumption that cells mix perfectly (mass action), suggested that negative feedback from differentiated cells on the self-renewal probability of stem cells is an important determinant of tissue homeostasis.###Examples include growth differentiating factor 11 (GDF11) and Activin β B (ACT β B), which negatively regulate self-renewal rates in progenitor and stem cells in the olfactory epithelium of mice [7,8]; transforming growth factor beta (TGF-β ) [9], which is mutated in a variety of tumors [10–12]; the bone morphogenetic protein 4 pathway (BMP4) that is inactivated in glioblastomas [13]; and the adenomatous pol-yposis coli (APC) tumor suppressor gene that is inactivated in colorectal cancer, with concomitant activation of the Wnt cascade [14].###For each time unit in the agent-based model, the ODEs describing the dynamics of the feedback factor are also advanced by one time unit (as both cell dynamics and factor diffusion happen concurrently in the system).###While the division events can in principle be formulated differently, the rules implemented here are meant to correspond to the previously published ODEs [6,8,24], with the addition of logistic growth rather than unlimited growth in the absence of feedback (see next section).###In addition, it is important to point out that this is a relatively simple model that was aimed to test how previously described homeostatic mechanisms [6,8,24] translate into spatial settings.###If we include the assumption that differentiated cells secrete negative feedback factors that influence stem cell division patterns, however, more realistic dynamics can be observed [6,8,24].###Fig 1A shows that this system of ODEs describes the average behavior of the agent-based model well for the limit of large cell migration and feedback diffusion rates.###For simplicity, we do not include feedback on the stem cell division rate, because this has been shown in the ODEs to not contribute to the existence of a stable equilibrium.###In addition, feedback loops have been proposed where differentiated cells reduce the rate of stem cell division [8,24], which can be expressed as r = r’/(1+f2D ), where r’ is the basic stem PLOS COMPUTATIONAL BIOLOGY Spatial dynamics of feedback regulation###Specifically, in the olfactory epithelium, there appears to be negative feedback from differentiated cells both on the probability of stem cell self-renewal and on the rate of cell division, mediated by GDF11 and Activin β [7,8].###If that spot is already occupied, the division event is aborted, introducing density dependence into the model (similar to logistic growth in ODEs).###For ODEs: r = 4.17x10 -2 , η = 0, α = 8.3x10 -3 , ξ = 8.33, β = 4.17, p’ = 0.7, f = 1.6, K = 100x100 and 200x200 for the small and large systems, respectively.###The amount of change that occurs during that one time unit can be very different for the agent-based portion and the ODEs, and is expressed in the rate constants and probabilities that determine the rate at which processes happen in the two parts of the model.###An ordinary differential equation model has been used to describe tissue hierarchy dynamics in a healthy tissue [8,15,24], and the models presented here build on these approaches.",impact-revealing,highlighting the integration of agent-based models and ODEs in understanding tissue dynamics
79,5edf5dd891e011bc656deed4,ae0988c55095827384bf70e346e192a89216e75c,IterefinE: Iterative KG Refinement Embeddings using Symbolic Knowledge,59ae3be32bbe271c4c71b8ba,Convolutional 2D knowledge graph embeddings,", 2016], SimplE [Kazemi and Poole, 2018], ConvE [Dettmers et al., 2018] cannot enforce subsumption.###FB15K-237: FB15K-237 [Dettmers et al., 2018], another popular benchmark does not have ontological and type label information.###We work with ComplEx [Trouillon et al., 2016] and ConvE [Dettmers et al., 2018] embeddings which have shown state of the art performance in many KG prediction tasks.###YAGO3-10: YAGO3-10 [Dettmers et al., 2018] is a subset of the YAGO3 [Suchanek et al., 2007] knowledge graph.###Specifically, we use Probabilistic Soft Logic (PSL) that can incorporate inference rules and ontologies, along with state-of-the-art KG embedding methods,viz., ConvE [Dettmers et al., 2018] and ComplEx [Trouillon et al., 2016], which do not make use of any ontological rules.###, ConvE [Dettmers et al., 2018] and ComplEx [Trouillon et al.###, 2016] and ConvE [Dettmers et al., 2018] embeddings which have shown state of the art performance in many KG prediction tasks.###We evaluate the performance of TypeE-X models in the KG refinement task, and compare them with ComplEx [Trouillon et al., 2016] and ConvE [Dettmers et al., 2018], two state-of-the-art KG embeddings methods,
4. https://www.w3.org/2006/03/wn/wn20/
and PSL-KGI.###On the other hand, neural and tensor-based embeddings have seen significant success in entity type and new fact predictions [Nickel et al., 2012, Trouillon et al., 2016, Dettmers et al., 2018].###Recently, SimplE+ [Fatemi et al., 2019] includes taxonomic information –i.e., subtype and subproperty information– and also shows that state-of-the-art embeddings like ComplEx [Trouillon et al., 2016], SimplE [Kazemi and Poole, 2018], ConvE [Dettmers et al., 2018] cannot enforce subsumption.###YAGO3-10: YAGO3-10 [Dettmers et al., 2018] is a subset of the YAGO3 [Suchanek et al.###, 2016] and ConvE [Dettmers et al., 2018], two state-of-the-art KG embeddings methods,",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2912,5fdb2e1691e0118a02c4f566,16913a534b1630d33770b392767bb316f4fdb11e,Clinical Temporal Relation Extraction with Probabilistic Soft Logic Regularization and Global Inference,53e99c74b7602d9702524918,Evaluating temporal relations in clinical text: 2012 i2b2 Challenge.,"The I2B2-2012 challenge corpus (Sun, Rumshisky, and Uzuner 2013) consists of 310 discharge summaries.###Clinical reports describe chronicle events, elucidating a chain of clinical observations and reasoning (Sun, Rumshisky, and Uzuner 2013; Chen, Podchiyska, and Altman 2016).###…early efforts to solve the clinical relation
extraction problem leverage conventional machine learning methods (Llorens, Saquete, and Navarro 2010; Sun, Rumshisky, and Uzuner 2013; Xu et al. 2013; Tang et al. 2013; Lee et al. 2016; Chikka 2016) such as SVMs, MaxEnt and CRFs, and neural network…###I2b2-2012 (Sun, Rumshisky, and Uzuner 2013) and Clinical TempEval (Bethard et al. 2015, 2016, 2017) are some great efforts of building clinical datasets with extensive annotations including labels of clinical events and temporal relations, the second of which was not tested in our paper due to lack…###For I2B2-2012, we leverage the TempEval evaluation metrics used by the official challenge (Sun, Rumshisky, and Uzuner 2013), which also calculates the Precision, Recall, and Micro-average F1 scores.",other,reporting on the I2B2-2012 challenge corpus and its significance in clinical relation extraction
1981,,9be6fcd7998e66704bbf604663c5fb183e896e7a,Efficient production of transgenic potato (S. tuberosum L. ssp. andigena) plants via Agrobacterium tumefaciens-mediated transformation,,,"###Although the Agrobacterium host strain LBA4404 has been commonly used for transformation of potato [5,6,8,11,26] previous work [29] showed that the Agrobacterium host strain GV2260 contributed to superior transformation efficiency compared to LBA4404.###There are numerous reports of Agrobacterium-mediated transformation with several commercial potato cultivars and species, including, for example, protocols for ‘Bintje’, ‘Desiree’, ‘Russet Burbank’ and ‘Superior’ [5,8,10,26,30,31].",impact-revealing,highlighting the comparative efficiency of different Agrobacterium strains in potato transformation
1591,,7b0a57a3de1693ee723cf3b52cdd82b15ffca4a3,Matrix compatibility of typical sol–gel solid-phase microextraction coatings in undiluted plasma and whole blood for the analysis of phthalic acid esters,,,"###It is non-invasive or less invasive toward a subject, and allows for real-time monitoring of samples including living systems in natural environments, and consequently can reflect more accurately the organismal physiological and biochemical processes under the nature condition [1].###Thereinto, the sampling rate calibration approach is widely used for in vivo sampling [1, 7].###In vivo sampling is currently an important research topic in the field of analytical chemistry [1, 2].###The existing calibration methods of SPME have been summarized and discussed in other document [1].###The sampling rate calibration method was used for quantitative calibration [1].",impact-revealing,highlighting the significance of in vivo sampling in analytical chemistry
796,5f0423a69e795e06bbe12b1e,21082bd98071f6948097df05cc9e4770fcd87de6,Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems,5f03f3b611dc830562231fce,Disentangled Self-Supervision in Sequential Recommenders,", discriminating whether two subsequences come from the same user’s behaviors [34].###Task u2u [34] adds an auxiliary loss where x and y are both sequences from the same user (before and after a sampled timestamp), which is co-trained with task u2i.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1014,,27f666bffb8c786eaedd421fd4861f692aa1b992,The role of psychological factors in chronic pain. I. A half century of study,,,###By the late 1950s it became increasingly evident that sensory explanations failed to account for certain puzzling pain phenomena (Wall 1979; Melzack and Wall 1986).,impact-revealing,highlighting the historical context of pain phenomena research
614,53e9a584b7602d9702eab396,45ce66a661eea0e5b5780ff8cfaf6b2085dd7a1e,Two level bulk preload branch prediction,557e28f46fee0fe990ca7812,Branch target buffer design and optimization,This paper describes the two level hierarchical branch predictor implemented in the IBM zEnterprise EC12 [13].,impact-revealing,reporting on a specific implementation in a technical context
3895,5c04967517c44a2c74709354,04c131293bf64c67972baa0053e85a510c4aa725,Intervention Harvesting for Context-Dependent Examination-Bias Estimation,53e99f5cb7602d970282bc61,Evaluating the accuracy of implicit feedback from clicks and query reformulations in Web search,"However, it is known that this type of data suffers from various biases due to both the system and the user, such as position bias [17], presentation bias [22] and trust bias [18].",other,highlighting biases in data collection methods
1338,,5fa6be74eb851878c67443c48c3c288a000079a9,Automated recognition of lung diseases in CT images based on the optimum-path forest classifier,,,"###Moreover, it performs quite similarly to the VIF index [19], which assesses differences between an image and its degraded version.###In this work, this approach is extended by combining Visual Information Fidelity (VIF) [19] with attributes obtained from the SIM method.###The VIF index [19] was also investigated, since it performs similarly to SIM-based descriptors.",impact-revealing,comparing performance of different indices in image assessment
2991,5b67b47917c44aac1c8639a8,957e4c03e7846a1f8670b250f5f9661c91dfcb75,Widar2.0: Passive Human Tracking with a Single Wi-Fi Link,573697f76e3b12023e6d14ca,Tonetrack: Leveraging Frequency-Agile Radios For Time-Based Indoor Wireless Localization,"However, as there are multiple links in typical indoor environments, it may group tracking results of these links and filter out outliers with NLoS conditions [40].###Generally, it requires objects to carry devices that transmits RF signals, and calculates signal parameters, e.g. AoA [9, 12, 23, 24, 39], ToF [29, 37, 39, 40], using fine-grained channel state information [43].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1308,,6ab3babea1b7bf16df149441adbf5a6cec959820,Face Concerns and Purchase Intentions: A Cross-Cultural Perspective,,,"###In general, social status correlates positively with face, such that the higher one’s social status, the more face one has (Ho, 1976).###Other researchers provide similar definitions (e.g., Brown & Levinson, 1987; Ho, 1976; Hwang, 1987; Stover, 1962; Ting-Toomey, 1988), but among these, Goffman’s (1955) definition is perhaps the most widely cited.###Other factors can contribute to face though (Ho, 1976).###People with more face concern focus on social roles and public perceptions as central to their identity (Ho, 1976).###—Lin Yutang, 1974, p. 200
Face, defined as self-image and/or status earned in a social network (Bolton, Keh, & Alba, 2010), is a cornerstone of collectivist cultures (Ho, 1976; Hwang, Francesco, & Kessler, 2003), with central importance for sociology research (Ho, 1976; Qi, 2011).###…expensive gifts brings honor to the gift giver by displaying his or her ability to
afford to give the gift
We expect that the belief that a high price can signal face is universal across cultures,
though in Western cultures this is typically not described or interpreted in such terms (Ho, 1976).###Still, no general consensus exists regarding the concept of face, because a precise definition has remained elusive (Ho, 1976).###The first stream
(e.g., Goffman, 1955; Ho, 1976; Stover, 1962) emphasizes face as the social interaction, such that face cannot be claimed unilaterally but can only be reinforced or diminished through interaction.",impact-revealing,acknowledging the complexity and varying definitions of the concept of face in social interactions
2435,5ac1829d17c44a1fda917eab,86aeec4d48d949190b3a0c2bf32c101fc23f13a3,Crepe: A Convolutional Representation for Pitch Estimation,5f0e23389fced0a24bee06f6,Comparison of pitch trackers for real-time guitar effects,"According to a few comparative studies, the state of the art is achieved by YIN-based methods [14, 15], with pYIN being the best performing method to date [13].",other,highlighting the best performing method in comparative studies
2594,573696026e3b12023e515eec,2c03df8b48bf3fa39054345bafabfeff15bfd11d,Deep residual learning for image recognition,573696086e3b12023e51beec,Highway Networks,"16) Highway [41, 42] 32 1.###39 Highway [41, 42] 19 2.###Concurrent with our work, “highway networks” [41, 42] present shortcut connections with gating functions [15].###Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as reported in [10, 41] and thoroughly verified by our experiments.###4, left) and on MNIST (see [41]), suggesting that such an optimization difficulty is a fundamental problem.###networks such as FitNet [34] and Highway [41] (Table 6), yet is among the state-of-the-art results (6.",other,discussing the challenges and findings related to highway networks in deep learning
1396,,21771b9df847c285baf320a4ae319c1086c5086a,Speed-accuracy optimization for skill learning,,,"###His hypothesis has been validated by many researchers [ 7 ], [8], and has been experimentally validated for a variety of skills [9].",impact-revealing,acknowledging validation of a hypothesis by prior research
2187,,d1363129580603adf22ddbc9567f95c75e34b000,Market Uncertainty Estimation of Desktop Cloud Services,,,"###The term cloud computing is quite recent, according to Vouk[48] the term was made popular by IBM and Google in 2007, but the underlying technology builds on virtualization techniques, research in distributed computing and network services, which are much older topics than cloud computing.###"" [48] SOA has a similar flexibility that also comes up with cloud computing.###According to Vouk [48], cloud computing builds on virtualization, distributed computing and SOA.",impact-revealing,providing historical context and development of cloud computing
1182,,ad324e5062fd5503b3f0e22c0252031fbfa62aa5,"Whether human-induced activities could change the gradient pattern of coastal land use along the sea-land direction: a case study in Manila Bay, Philippines",,,"###An improved density peaks clustering method was developed for fast cluster k search (Rodriguez and Laio, 2014; Xu et al., 2018) and has been widely used by relevant clustering research (Xu and Tian, 2015; Du et al., 2016; Chen et al., 2020).",impact-revealing,reporting prior findings and applications of clustering methods
1841,,aa613702d025ec050e68731608c41275c6dd2785,Petri nets in cryptographic protocols,,,"###The language we present was inspired by Paulson’s inductive approach to crypto-protocol analysis [11, 12].",impact-revealing,highlighting the influence of Paulson's work on the current research
3317,5b67b45517c44aac1c860884,9fa3e53b5937a0ec92499ed415e339ede6c92010,DeepInf: Social Influence Prediction with Deep Learning,58d82fcbd649053542fd5fc7,DeepCas: an End-to-end Predictor of Information Cascades,"[26] proposed an end-toend predictor for inferring cascade size by incorporating recurrent neural network (RNN) and representation learning.###, the DeepCas model [26] which formulate cascade prediction as a sequence problem and solve it with Recurrent Neural Network.###Indeed, extensive work has been done on social influence prediction in the literature [26, 32, 42, 43].",other,reporting prior findings on cascade prediction methods
1195,,7c91ee68ae4e27663c1d763d0649fd28a4cac3eb,The effects of using computer-based models on the learning of computer programming,,,"###Early research on programming focused on the features of the language and was motivated by the need to improve the efficiency of large scale software projects involving teams of programmers (Weinberg, 1971).",impact-revealing,providing historical context for programming research
2348,5def6ca63a55ac6095fe0607,2a6f2656c716082e34f9b86d9e589390842e0853,DistTC: High Performance Distributed Triangle Counting,53e9986eb7602d97020a592a,PATRIC: a parallel algorithm for counting triangles in massive networks.,"Triangle counting has been implemented on various platforms including shared-memory CPUs [7], clusters [8], [9], [10], [11] and GPUs [12], [13].",other,reporting implementation details of triangle counting
658,573698486e3b12023e711478,c2fd72cb2a77941e655b5d949d0d59b01e173c3b,grarep: learning graph representations with global structural information,53e9b253b7602d9703cf4028,DeepWalk: online learning of social representations,"For example, DeepWalk [20], one recent model, transforms a graph structure into a sample collection of linear sequences consisting of vertices using uniform sampling (which is also called truncated random walk ).###Following [20, 27], we use the LibLinear package [10] to train one-vs-rest logistic regression classiﬁers, and we run this process for 10 rounds and report the averaged Micro-F1 and Macro-F1 measures.###DeepWalk [20].###As mentioned in [20], for DeepWalk and E-SGNS, we set window size as 10, walk length as 40, walks per vertex as 80.###Perozzi et al. [20] presented an approach, which transformed graph structure into several linear vertex sequences by using a truncated random walk algorithm and generated vertex representations by using skip-gram model.",impact-revealing,reporting on the methodology and parameters used in DeepWalk
376,5c8ddce94895d9cbc6a97820,8fb5a6fe93a3d19ffee50677c0ae563e3377d2d4,Chinese NER Using Lattice LSTM,5736960c6e3b12023e51f0a0,End-To-End Sequence Labeling Via Bi-Directional Lstm-Cnns-Crf,"The current stateof-the-art for English NER has been achieved by using LSTM-CRF models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018) with character information being integrated into word representations.###Huang et al. (2015) uses hand-crafted spelling features; Ma and Hovy (2016) and Chiu and Nichols (2016) use a character CNN to represent spelling characteristics; Lample et al. (2016) use a character LSTM instead.###Denoting the embedding of character cj as ec(cj), the vector xci is given by:
xci = max t(i,1)≤j≤t(i,len(i)) (W>CNN
 e c(cj− ke−1 2 )
. . . ec(cj+ ke−1
2 ) + bCNN), (9)
where WCNN and bCNN are parameters, ke = 3 is the kernal size and max denotes max pooling.###We follow the best English NER model (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016), using LSTM-CRF as the main network structure.###A standard CNN (LeCun et al., 1989) structure is used on the character sequence of each word to obtain its character representation xci .###Finally, for each word wi, −→ hi and ←− hi are concatenated as its representation: Integrating character representations Both character CNN (Ma and Hovy, 2016) and LSTM (Lample et al.###Finally, for each word wi, −→ hwi and ←− hwi are concatenated as its representation: Integrating character representations Both character CNN (Ma and Hovy, 2016) and LSTM (Lample et al., 2016) have been used for representing the character sequence within a word.###Collobert et al. (2011) used a CNN-CRF structure, obtaining competitive results to the best statistical models. dos Santos et al. (2015) used character CNN to augment a CNN-CRF model.###• Word + char CNN.###A possible reason is that CNN inherently captures character n-gram information.###A CNN representation of character sequences gives a slightly higher F1-score compared to LSTM character representations.###On the other hand, further using character bigram information leads to increased F1-score over word+char LSTM, but decreased F1-score over word+char CNN.",impact-revealing,reporting on state-of-the-art methods for English NER
3526,5d1eb9b7da562961f0af38c9,c0f5e89cf9f1b4f3d9c76a2ae3b13315e691554b,Personalized Student Stress Prediction with Deep Multitask Network,53e9b12ab7602d9703ba893a,Acute stressors and cortisol responses: a theoretical integration and synthesis of laboratory research.,"Few use heart rate and heart rate variability (Vri-jkotte et al., 2000), cortisol levels (Dickerson & Kemenyr, 2004) and skin conductance (Setz et al., 2010).",other,acknowledge existing physiological measures used in research
2072,,cdb879eb472bd2b26a71eb635bd473d73223d0cb,SEEDS: a software engineer's energy-optimization decision support framework,,,"###Finally, researchers have begun to build on the accurate measurement work mentioned above to provide source codelevel feedback on energy consumption to developers [28].",impact-revealing,highlighting the development of energy consumption feedback for developers
1315,,c2f619adc1f98c308dcc3a74fd716b0219df4d05,"Hydrodynamics, resurgence, and transasymptotics",,,"###In [15] this phenomenon was referred to as an “attractor”.###And in what follows, for concrete numerical illustrations, we will also use the parameters that are associated with N = 4 super Yang-Mills theory [15, 27, 29]:###However, to facilitate direct comparison with previous results [15], we will not adopt this rescaling.###For the transport coefficients, we adopt the parameterization used in [15]###3 Note that our f differs from the f in [15] by: fours = ftheirs − 1.###This is motivated by an interesting recent paper by Heller and Spalinski concerning resummation of the gradient expansion in conformal hydrodynamics [15].###In d = 4 the resummation of this expansion is studied by Heller and Spalinski in [15].###Here we use the standard notation [15, 27, 29] to denote the symmetric, transverse projection of a tensor Aαβ 〈Aμν 〉 = 1 2 ∆∆ (Aαβ +Aβα)− 1 d− 1 ∆∆Aαβ (3)###Up to the simple shift, fours = (ftheirs − 1) noted above, (15) is the equation analyzed in [15].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2377,5e85c28491e0114016e821dc,2db6b9621862b4293c1433c91ee8fb45e394f376,DeepGS: Deep Representation Learning of Graphs and Sequences for Drug-Target Binding Affinity Prediction,5736986c6e3b12023e7308a0,Convolutional Networks on Graphs for Learning Molecular Fingerprints,"For example, graph convolutional network was used to model molecule based on the extraction of their circular fingerprint [6].###Meanwhile, traditional machine learning-based methods apply various features [32, 8] and descriptors [7, 13, 6], and simply depend on the similarities between drugtarget pairs [35, 40, 19].",other,acknowledge existing methods in molecular modeling
1910,,5df75d5492dd5e29fbf53fc96f1cf9fe9e675bc3,"The Effects of Formative and Summative Assessment on Student's Connectedness, Satisfaction, Learning and Academic Performance within an Online Healthcare Course",,,"###Questions were utilized from the CoI and CCS surveys primarily with the final
created survey possessing a length of 25 questions plus two qualitative questions, to gather additional reference data if needed at a later date.###A frequently cited theoretical framework to address many, if not all of the issues reported by Hart as well as other researchers is the Community of Inquiry (CoI) approach to delivery of distance education.###In an attempt to lessen attrition, online programs should encourage instructors to purposefully utilize the CoI framework.###When adopted by educators, the CoI framework has been shown to increase
student-student, student-instructor and student-course material interactions (Shea, Li & Pickett, 2006).###The variable of course satisfaction was explored by using five questions taken
from the CoI survey as created by Garrison et al. (2000): (21) The instructor clearly communicated important course topics; (22) The instructor clearly communicated important course goals; (23) The course was effectively organized; (24) I am satisfied with this course; (25) I would recommend this course to fellow students.###The variable of course satisfaction was explored by using five questions taken
from the CoI survey as created by Garrison et al. (2000): (21) The instructor clearly communicated important course topics; (22) The instructor clearly communicated important course goals; (23) The course was effectively…###Arbaugh et al. (2008) demonstrated that the CoI survey was a reliable tool with Cronbach’s alpha levels ranging from 0.91- 0.95 for the three presences identified.###The primary focus of the assessment tool development survey was the Online Student Connectedness Survey (OSCS), Community of Inquiry (CoI) survey and the Classroom Community Scale (CCS) (Appendix A).###The CoI instrument created by Garrison et al., (2000) utilizes 34 questions covering three main foci social presence, cognitive presence and teaching presence.###Questions were generated when possible, using two previously validated and
reliable surveys the Classroom Community Scale (CCS) created by Rovai, 2002 and the Community of Inquiry (CoI) created by Garrison et al., (2000).###Community of Inquiry
CoI consists of three basic components that include the concepts of social
presence, cognitive presence and teaching presence (Garrison & Arbaugh, 2007).###The instrument was further validated considering the majority of questions were taken either directly or were slight modifications of questions used in two previously validated studies the CCS (Rovai, 2002) and the CoI (Garrison et al., 2000).###Educators who actively create/delivery courses following the CoI framework, have been shown to increase the likelihood that many of the main contributing factors associated with student persistence and online learning effectiveness will be achieved (Abrami et al., 2011; Attri, 2012; Garrison & Arbaugh, 2007).###The CoI survey (Garrison et al., 2000) has been shown to identify “presence” as it relates to social, teaching and cognitive realms within distance higher education.###The CoI framework was created by Garrison, Anderson & Archer (2000) as a hypothesized solution to address the growing issue of lack of online student persistence and rising attrition rates (Garrison & Arbaugh, 2007).###The instrument was further validated considering the majority of questions were
taken either directly or were slight modifications of questions used in two previously validated studies the CCS (Rovai, 2002) and the CoI (Garrison et al., 2000).###The CoI has also been deemed as reliable and validated by subsequent studies by
other researchers efficiently showing social presence, cognitive presence and teaching presence (Arbaugh et al., 2008; Swan & Ice, 2010).###The CoI model is presented as a
16
way for educators to evaluate characteristics of a course of study to ensure online learning effectiveness, student satisfaction, community, interaction and consequently persistence.###The two previously mentioned facets of CoI are important components of creating an encouraging environment for interaction within an online course of study but it is the third component, teaching presence, which provides the structure for these interactions (Garrison & Arbaugh, 2007).",impact-revealing,highlighting the importance of the Community of Inquiry framework in enhancing online learning effectiveness and student satisfaction
2556,5fc4cfdf91e011abfa2faf94,633e2fbfc0b21e959a244100937c5853afca4853,score-based generative modeling through stochastic differential equations,5bdc31b817c44a1f58a0bcb4,FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models,"…et al., 2019) 3.45 - Glow (Kingma & Dhariwal, 2018) 3.35 - MintNet (Song et al., 2019b) 3.32 - Residual Flow (Chen et al., 2019) 3.28 46.37 FFJORD (Grathwohl et al., 2018) 3.40 - Flow++ (Ho et al., 2019) 3.29 - DDPM (L) (Ho et al., 2020) ď 3.70* 13.51 DDPM (Lsimple) (Ho et al., 2020) ď 3.75* 3.17…###The bits/dim values in Table 2 are computed with atol=1e-5 and rtol=1e-5, same as Grathwohl et al. (2018).###In many cases computing ∇  ̈ f̃θpx, tq is expensive, so we follow Grathwohl et al. (2018) to estimate it with the Skilling-Hutchinson trace estimator (Skilling, 1989; Hutchinson, 1990).###37 FFJORD (Grathwohl et al., 2018) 3.###Model NLL Test Ó FID Ó RealNVP (Dinh et al., 2016) 3.49 - iResNet (Behrmann et al., 2019) 3.45 - Glow (Kingma & Dhariwal, 2018) 3.35 - MintNet (Song et al., 2019b) 3.32 - Residual Flow (Chen et al., 2019) 3.28 46.37 FFJORD (Grathwohl et al., 2018) 3.40 - Flow++ (Ho et al., 2019) 3.29 - DDPM (L) (Ho et al., 2020) ď 3.70* 13.51 DDPM (Lsimple) (Ho et al., 2020) ď 3.75* 3.17 DDPM 3.28 3.37 DDPM cont.###In many cases computing ∇ ¨ f̃θpx, tq is expensive, so we follow Grathwohl et al. (2018) to estimate it with the Skilling-Hutchinson trace estimator (Skilling, 1989; Hutchinson, 1990).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3951,5f896fa591e01149071e45df,1d16d4cdc3fcce26e2c2097d13896ec09683eee3,Self-training for Few-shot Transfer Across Extreme Task Differences,5b3d98cc17c44a510f801acc,Unsupervised Feature Learning via Non-Parametric Instance Discrimination,"This induced notion of similarity is in contrast to current self-supervised techniques which often function by considering each image as its own class and dissimilar from every other image in the dataset (Wu et al., 2018; Chen et al., 2020).###A more recent (and better performing) line of self-supervised learning is contrastive learning (Wu et al., 2018; Misra & Maaten, 2020; He et al., 2020; Chen et al., 2020) which aims to learn representations by considering each image together with its augmentations as a separate class.",other,highlighting the contrast between traditional self-supervised techniques and contrastive learning
1292,,6c02cc17b38bb8066828c7123569e9c756f48191,Application of Compressed Sensing to Single Voxel J Resolved Magnetic Resonance Spectroscopy: Simulation and In Vitro Results,,,###The current gold standard in 1D MRS quantification is Linear Combination of Model Spectra (LCModel) which works in the frequency domain to estimate various parameters of a metabolite basis set in order to fit the measured data[11].###The approach is inspired by LCModel[11] for its incorporation of,impact-revealing,reporting on the current gold standard method in metabolite quantification
176,555045d745ce0a409eb59fe4,460d131e081486491af532604351a0691b52b11f,User-level psychological stress detection from social media using deep neural network,5550470045ce0a409eb63934,Psychological stress detection from cross-media microblog data using Deep Sparse Neural Network,"[17] design a cross-media learning method based on DNN, and leverage the model for detecting psychological states and corresponding categories from a single tweet.###Rather than summarizing the user’s state alone, we further incorporate the detail attributes with multiple modalities of every tweets by utilizing a recently proposed cross-media model, namely the Cross Autoencoders (CAE) [17].###To combine low-level content attributes with user-scope statistical attributes, we further design a convolutional neural network (CNN) with cross autoencoders to learn the latent high-level attributes on cross-modal units [17][18].###As for comparison with previous work, due to the different goal, our results are not comparable with [17].",impact-revealing,describing a cross-media learning method for psychological state detection
110,5ee8986891e011e66831c3bc,965652c0e426c5b42d7218d7429025be7ac542bf,DeeperGCN: All You Need to Train Deeper GCNs,5e09aa66df1a9c0c416bebf6,Deepgcns: Can Gcns Go As Deep As Cnns?,"Inspired by the beneﬁt of training deep CNN-based networks [He et al., 2016a, Huang et al., 2017, Yu and Koltun, 2016], DeepGCNs [Li et al., 2019b] propose to train very deep GCNs (56 layers) by adapting residual/dense connections (ResGCN/DenseGCN) and dilated convolutions to GCNs. DeepGCN variants…###Similar to Li et al. [2019b], we construct ResGCN by adding residual connections to PlainGCN following the ordering: GraphGonv → Normalization → ReLU → Addition.###DeepGCNs [Li et al., 2019b] show residual connections [He et al., 2016a] to be quite helpful in training very deep GCN architectures.###…social networks [Tang and Liu, 2009], modelling proteins for drug discovery [Zitnik and Leskovec, 2017, Wale et al., 2008], enhancing predictions of recommendation engines [Monti et al., 2017b, Ying et al., 2018], and efﬁciently segmenting large point clouds [Wang et al., 2018, Li et al., 2019b].###Inspired by the beneﬁt of training deep CNN-based networks [He et al., 2016a, Huang et al., 2017, Yu and Koltun, 2016], DeepGCNs [Li et al., 2019b] propose to train very deep GCNs (56 layers) by adapting residual/dense connections (ResGCN/DenseGCN) and dilated convolutions to GCNs. DeepGCN variants achieve state-of-the art results on S3DIS point cloud semantic segmentation [Armeni et al., 2017] and the PPI dataset.###Recent works have looked at frameworks to train deeper GCN architectures [Li et al., 2019b,a].",impact-revealing,highlighting the advancements in deep GCN architectures and their applications
1187,,dd40c51b41f905dfd4b9b5e4a07e30259e5a65f3,A Text Mining Based Approach for Mining Customer Attribute Data on Undefined Quality Problem,,,"###K-means clustering is a common clustering method based on centroids [21] that has a lower time complexity and a higher computational efficiency; however, the algorithm is not suitable for non-convex data, does not have robustness, is more sensitive to outliers, and can easily fall into a local…###Compared with other clustering methods, hierarchical clustering can be applied to arbitrary shapes and attribute data set types [21] ; however, the time complexity of the algorithm is relatively high and therefore not suitable for clustering massive amounts of text data [20] .###However, the spectral clustering time complexity and the number of clusters k needs to define in advance are high [21] .###Therefore, the clustering results are more susceptible to the influence of the number of predefined clusters [21] .",impact-revealing,comparing the strengths and weaknesses of different clustering methods
2057,,4ab1df9833bc1255ba5d5877688288d2c8f8c996,Diatoms exhibit dynamic chloroplast calcium signals in response to high light and oxidative stress,,,"###NH4Cl is commonly used as a tool to manipulate cytosolic pH, as NH3 readily crosses the plasma membrane and forms NH4 + in the cytosol, consuming a H (Boron, 2004).",impact-revealing,providing context for the use of NH4Cl in pH manipulation
2120,,bf775e51c95e1393c0006408e501fe05db917cff,Highway Traffic Control via Smart e-Mobility - Part I: Theory,,,"###In the literature, the most used model for traffic control is the CTM, see [19].###Remark 1 (r2s and s2r flows): The concepts of r2s and s2r flows are inspired by the “off-ramp” and “on-ramp” flows [19], respectively, and used to model the temporary stop of some",impact-revealing,acknowledge existing models in traffic control
3331,5fbe5cf091e011e6e11b3cf5,15a84047e5145891d0c7ee054c00a00f2f5d38a1,Boosting Contrastive Self-Supervised Learning with False Negative Cancellation,556f622a2401b4b38c23635c,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,"With the universal adaptability of deep neural networks, representation learning has become the backbone of most modern AI agents, in which good pretrained representations have proven essential to improving performance on downstream tasks [16, 20, 49, 27].",other,highlighting the importance of representation learning in modern AI
3934,5c234870da562935fc1d4da6,94bd59e507ba8496b36605be0f6740e5731e91d5,CounterMiner: Mining Big Performance Data from Hardware Counters,53e9b74fb7602d97042eecbb,Characterizing Data Analysis Workloads In Data Centers,Jia et al. use performance counters to characterize data analysis work-loads in datacenters [8].,other,reporting prior findings on performance counters in data analysis
2149,,06031c492dd056b9acc708eae81c21cd868fdfb5,Computer-Mediated Social Networks and Environmental Behavior,,,"###Understanding how the features of these types of computer-mediated information social structures, such as CMSNs, can influence individual environmental behavior is an important research focus (Watson et al. 2010).###Personal values are partly shaped by the individual’s interaction with society (Bandura 1986).###Further, Watson et al. (2010) highlighted the importance of social norms in changing “citizens’ behavior in an environmentally desirable direction” (p. 31).",impact-revealing,highlighting the significance of understanding social structures in influencing environmental behavior
2350,5db9295f47c8f766461f5135,2c1006c856fefdbd6cd710e840e57153f2d6cd04,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,599c7971601a182cd263ddc2,Adversarial Examples for Semantic Segmentation and Object Detection,"Adversarial examples have been shown to be ubiquitous beyond classification, ranging from object detection [64, 18] to speech recognition [11, 9].",other,highlighting the widespread impact of adversarial examples across various domains
3590,5736974d6e3b12023e6388bf,1d7e580740a3f96f1529e3d771915af4e996a259,Verbal and Nonverbal Clues for Real-life Deception Detection,53e9b290b7602d9703d38f5e,Warrants And Deception In Computer Mediated Communication,", 2012), forums (Warkentin et al., 2010; Joinson and Dietz-Uhler, 2002), social networks (Ho and Hollister, 2013), and consumer report websites (Ott et al.###…of deceptive content in a variety of domains, including online dating websites (Toma and Hancock, 2010; Guadagno et al., 2012), forums (Warkentin et al., 2010; Joinson and Dietz-Uhler, 2002), social networks (Ho and Hollister, 2013), and consumer report websites (Ott et al., 2011; Li…",other,acknowledge the presence of deceptive content across various online platforms
2142,,f95fdc38ee56cb366d9cf82e1cf1de6d14630bec,Vision–Language Navigation With Beam-Constrained Global Normalization,,,"###The model has been studied extensively with a number of improvements, including feature extraction [5]–[8], cross-modal co-grounding [9]–[13], regret module [3], imitation learning [2], [14], back translation [15]–[17], and BERT [18].###…and the input discrete words are converted into vectors by the embedding strategy as follows : where the superscript T refers to textual, and the pretrained word embedding can be used to initialize The textual encoding has been widely used as a standard setting for VLN [2], [9], [15], [29].###We follow the work [2] to use it as one additional inference mode.###3) Preexploration Mode: Xin et al. [2] proposed this mode using the results of beam-search mode in unseen environments as additional training instances to optimize the performance of the single-run mode by imitation learning.###They proposed a multimodal sequence-to-sequence model, which has been a widely adopted baseline in recent studies [2], [3], [18], [19], [29].###Furthermore, we compare the proposed method with several previous studies, including speaker–follower [19], self-monitoring [9], reinforced cross modal [2], environmental dropout [15], and VLN BERT [18].###2) Evaluation: We exploit four types of evaluation metrics for VLN, following previous studies [2], [9], as follows: path length (PL), NE, SR, and success rate weighted by inverse path length (SPL), where SR is adopted as the major metric; in particular, PL can reﬂect the time complexity of one…###6) Inference: There are three inference modes for the VLN task, which have been adopted in the existing work [2].",impact-revealing,highlighting the extensive study and improvements of the model in various aspects
205,5db80dc83a55acd5c14a2492,2fd519ba8408d4266ff81d1f934856badb08a370,Understanding and Quantifying Adversarial Examples Existence in Linear Classification,5bdc31b417c44a1f58a0b679,Are adversarial examples inevitable?,"Recently, Shafahi et al. (2019) shows that no classifier can achieve low misclassification rate and also be adversarialrobust for data distributions with bounded density on a compact region in a high-dimensional space.###Recently, Shafahi et al. (2019) showed that, for two classes of data distributed with bounded probability densities on a compact region of a high dimensional space, no classifier can both have low misclassification rate and be robust to adversarial examples attack.",impact-revealing,highlighting a significant finding regarding classifier performance and adversarial robustness
2929,5de7997c9e795e77580692f9,c919ae4366f5cc4901b854cc259101ccc13e6f3f,Constrained Reinforcement Learning Has Zero Duality Gap,5aed14d117c44a44381589f5,Simple random search provides a competitive approach to reinforcement learning,"In practice, the exact coefﬁcient is selected through a time consuming and a computationally intensive process of hyper-parameter tuning that often times are domain dependent, as showed in [7–9].",other,highlighting the challenges of hyper-parameter tuning in practice
499,57d063e8ac443673542950ad,0509facb30efa81c4730f98bfc28cdbd6178822e,"Image sentiment analysis using latent correlations among visual, textual, and sentiment views",558bc447e4b00c3c48de59ab,Large-scale image categorization with explicit data embedding,"In contrast, recent advances of explicit feature maps [12, 13] can convert nonlinear problems to linear problems, which can be solved by linear frameworks with a low computation cost [9,23].###To reduce the computation complexity, one can use explicit feature maps [12,13].###Speciﬁcally, to capture the non-linear relationship between features, we introduce explicit feature maps [12,13] to CCA.###All other histogram-based features were mapped using the exact Bhattacharyya kernel map-christmaspresentnewyorklight ping [13].",impact-revealing,highlighting the advantages of explicit feature maps in reducing computation complexity
522,5f7fdd328de39f0828397e22,21e33bd0ad95ee1f79d8b778e693fd316cbb72d4,Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"GCN [17] 7 7 7 GAT [36] 7 7 7 GCN-Cheby [7] 7 3 7 GraphSAGE [11] 3 7 7 MixHop [1] 7 3 7###5} – L2 Regularization Weight: {1e-5, 5e-4} • GCN [17]: – hidden1: 64 – early_stopping: {40, 100, 200} – epochs: 2000 • GCN-Cheby [17]: – hidden1: 64 – weight_decay: {1e-5, 5e-4} – max_degree: 2 – early_stopping: {40, 100, 200} – epochs: 2000 • GraphSAGE [11]: – hid_units: 64 – lr: {0.###Non-linear embedding transformations per round in H2GCN? GCN [17], GraphSAGE [11] and other GNN models embed the intermediate representations per round of feature propagation and aggregation.###GraphSAGE [11] generalizes the aggregation beyond averaging, and models the ego-features distinctly from the neighbor-features in its subsampled neighborhood.###While these designs have been utilized separately in some prior works [11, 7, 1, 38], we are the first to discuss their importance under heterophily by providing novel theoretical justifications and an extensive empirical analysis on a variety of datasets.###• GraphSAGE [11]: – hid_units: a ∈ {64, 128} – lr: b ∈ {0.###com/tkipf/gcn • GraphSAGE [11]: https://github.###[11] Will Hamilton, Zhitao Ying, and Jure Leskovec.###For heterophily, after aggregating the neighbors’ representations, the definition of COMBINE (akin to ‘skip connection’ between layers) is critical: a simple way to combine the ego- and the aggregated neighbor-embeddings without ‘mixing’ them is with concatenation as in GraphSAGE [11]—rather than averaging all of them as in the GCN model by Kipf and Welling [17].",impact-revealing,highlighting the novelty and significance of the proposed theoretical justifications and empirical analysis in GNN models
3323,5a260c0c17c44a4ba8a1e113,c751ab01aedc2888a7fe6e8b4f77ab1afa94072f,protein interface prediction using graph convolutional networks.,55a6cfb565ce054aad76c0d7,Progress and challenges in predicting protein interfaces.,"The prediction of those interactions, and the interfaces through which they occur, are important and challenging problems that have attracted much attention [10].###This calls for new methodologies or sources of information to be exploited"" [10].",other,highlighting the importance and challenge of predicting interactions
3391,5f0423a69e795e06bbe12b1e,21082bd98071f6948097df05cc9e4770fcd87de6,Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems,5bbacb6117c44aecc4ead3f5,Unbiased offline recommender evaluation for missing-not-at-random implicit feedback.,"The topic of reducing the bias in training and evaluating recommender systems has been explored before [35, 3, 40, 33, 9, 41].",other,acknowledge existing research on bias in recommender systems
3244,573696f46e3b12023e5f1198,7ffdbc358b63378f07311e883dddacc9faeeaf4b,Fast R-CNN,53e99b63b7602d970240a38e,Return of the Devil in the Details: Delving Deep into Convolutional Nets.,"” The second network is VGG CNN M 1024 from [3], which has the same depth as S, but is wider.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3839,5edf5dd891e011bc656deb7d,14b65a86c82e38fce0eb3506e0d4084ad5cdb583,deberta- decoding-enhanced bert with disentangled attention,5eabf33691e011664ffd248c,Adversarial Training for Large Neural Language Models,"Since 2018, we have seen the rise of a set of large-scale Transformer-based Pre-trained Language Models (PLMs), such as GPT [2, 3], BERT [4], RoBERTa [5], XLNet [6], UniLM [7], ELECTRA [8], T5 [9], ALUM [10], StructBERT [11] and ERINE [12] .",other,acknowledging the emergence of large-scale Transformer-based models
2928,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e9a74ab7602d97030836dc,An Analysis of UCT in Multi-player Games,"Sturtevant [207] shows that this variant of UCT converges to an###Other card games such as Hearts and Spades are also interesting to investigate in this area, although work to date has only applied MCTS to their perfect information versions [207].",other,acknowledge existing research on UCT and suggest further investigation in other card games
1006,,028dc91e80127b38d212758614878903a4e03d17,Interpersonal Processes of Couples’ Daily Support for Goal Pursuit: The Example of Physical Activity,,,"###The perceived availability of social support is widely recognized as having many benefits, including better mental and physical health (e.g., Holt-Lunstad, Smith, & Layton, 2010; Kawachi & Berkman, 2001).",impact-revealing,highlighting the benefits of perceived social support
2988,5f7d893591e011346ad27d16,ff8a988d88ebe5d5ca116340baa34ae00ce011e8,PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation,573696126e3b12023e524d29,A Diversity-Promoting Objective Function for Neural Conversation Models,"Despite the impressive progress made in many generation tasks, neural systems are known to produce low-quality content (Wiseman et al., 2017; Rohrbach et al., 2018), often with low relevance (Li et al., 2016) and poor discourse structure (Zhao et al., 2017; Xu et al., 2020).",other,highlighting the challenges of low-quality content generation in neural systems
463,5f842b5891e01129be18ffbd,7097137596f6755675f6aafcdd80969a747322ae,Contrastive Learning with Hard Negative Samples,5f7af09591e011983cc81efc,Hard Negative Mixing for Contrastive Learning,"Comparison with Kalantidis et al. (2020): Kalantidis et al. (2020) also consider ways to sample negatives, and propose a mixing strategy for hard negatives, called MoCHi.",impact-revealing,acknowledging related work on negative sampling strategies
1251,,0a50446f20b4bf36ed41c090bef74c005543b3a5,Design of qualitative surveys for persons with intellectual disorders,,,"###As the initially developed version is not suitable for smartphone usage, the design will be made available in responsive design [8].",impact-revealing,highlighting the adaptation of design for smartphone usage
2692,5cede0eada562983788c8fb4,940016df38b80c5f3fd9db411e722f58a9d7e227,Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation,573696ce6e3b12023e5ce95a,Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift,Batch normalization [25] is applied after each layer.,other,providing context for a method
4006,58d82fcbd649053542fd6178,515a21e90117941150923e559729c59f5fdade1c,the concrete distribution: a continuous relaxation of discrete random variables,573696ce6e3b12023e5cecfc,DRAW: A Recurrent Neural Network For Image Generation,"…works” when stochastic nodes1 can be reparameterized into deterministic functions of their parameters and a fixed noise distribution (Kingma & Welling, 2013; Rezende et al., 2014), has liberated researchers in the development of large complex stochastic architectures (e.g. Gregor et al., 2015).",other,highlighting the impact of reparameterization techniques on the development of complex stochastic architectures
3234,5d04eeba8607575390f83f3a,b67fd0612e3f72faee0fed9b1e930b69ed7ee98d,sparse reram engine: joint exploration of activation and weight sparsity in compressed neural networks,58d82fcbd649053542fd6091,The Power of Sparsity in Convolutional Neural Networks.,"The matrix-vector multiplication of OU2 and the LSB of the rest of values in the input sliding window [1, 1] is then performed at the second cycle to yield the output [3, 4].###Recent studies show that most neural network models have significant amounts of zeros in filter weights and input activations [3]; pruning zero weights and skipping zero activations can help to reduce resource consumption without accuracy loss.###the output neuron for the first input sliding window [1, 2, 3, 1], the data we feed into the input register after decomposition are [1, 0, 1,",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3320,5ed7796e91e011e6e91120f0,8ade0f3bbe8d8251c5de9ef39886a13055f463e5,TIMME: Twitter Ideology-detection via Multi-task Multi-relational Embedding,57aa28de0a3ac518da9896d5,node2vec: Scalable Feature Learning for Networks,"Random-walk-based approaches such as node2vec [12] with time complexity O(a2N ), where a is the average degree of the graph, suffer from the relatively-high degree in our dataset.",other,highlighting the limitations of random-walk-based approaches in the context of the dataset
1464,,eecc687738033043d332254b0a18f57eb7f9197e,Domain differences in the structure of artifactual and natural categories,,,"###Early research on category structure (e.g.,  Rosch, 1975 ) confounded typicality and category membership; in fact, typicality was regarded as a measure of membership.###Early research on category structure (e.g., Rosch, 1975) confounded typicality and category membership; in fact, typicality was regarded as a measure of membership.###In support of this resemblance-based model, Rosch and others demonstrated that the facility with which people learn (Mervis & Pani, 1980; Rosch, 1973; Rosch, Simpson, & Miller, 1976), categorize (Hampton, 1979, 1997; Rips, Shoben, & Smith, 1973; Rosch, 1973), and remember ( Rosch, 1975;  Rosch et al., 1976) objects is determined by the similarity between a given object and the category prototype.###…people learn (Mervis & Pani, 1980; Rosch, 1973; Rosch, Simpson, & Miller, 1976), categorize (Hampton, 1979, 1997; Rips, Shoben, & Smith, 1973; Rosch, 1973), and remember (Rosch, 1975; Rosch et al., 1976) objects is determined by the similarity between a given object and the category prototype.###Ironically, then, although resemblance theory specifically arose from a need to account for graded category structure (see, e.g., Rosch, 1975), it did account for absolute categorization but failed to account for graded categorization.###Thus, resemblance theory is generally accurate in predicting categorization decisions, and a host of current prototype models (e.g., Hampton, 1995) and exemplar models (e.g., Estes, 1994; Heit, 2001; Lamberts, 1995; Nosofsky, 1986) has emerged to replace the early models of resemblance theory (e.g., Medin & Schaffer, 1978;  Rosch, 1975 ).###…accurate in predicting categorization decisions, and a host of current prototype models (e.g., Hampton, 1995) and exemplar models (e.g., Estes, 1994; Heit, 2001;Lamberts, 1995;Nosofsky, 1986)has emerged to replace the early models of resemblance theory (e.g., Medin & Schaffer, 1978; Rosch, 1975).###Ironically, then, although resemblance theory specifically arose from a need to account for graded category structure (see, e.g.,  Rosch, 1975 ), it did account for absolute categorization but failed to account for graded categorization.",impact-revealing,highlighting the evolution and limitations of resemblance theory in categorization
470,5a260c8117c44a4ba8a30b08,79cfb51a51fc093f66aac8e858afe2e14d4a1f20,Focal Loss for Dense Object Detection,57a4e91aac44365e35c98023,Training Region-Based Object Detectors with Online Hard Example Mining,"In the second classification stage, sampling heuristics, such as a fixed foreground-to-background ratio (1:3), or online hard example mining (OHEM) [30], are performed to maintain a manageable balance between foreground and background.###This inefficiency is a classic problem in object detection that is typically addressed via techniques such as bootstrapping [32, 28] or hard example mining [36, 8, 30].###A common solution is to perform some form of hard negative mining [32, 36, 8, 30, 21] that samples hard examples during training or more complex sampling/reweighing schemes [2].###Online Hard Example Mining (OHEM): [30] proposed to improve training of two-stage detectors by constructing minibatches using high-loss examples.###(d) FL outperforms the best variants of online hard example mining (OHEM) [30, 21] by over 3 points AP.",impact-revealing,describing techniques for addressing inefficiencies in object detection
368,5fdc8e9d91e01104c91811a8,849b88ddc8f8cabc6d4246479b275a1ee65d0647,A Generalization of Transformer Networks to Graphs,5e20376c3a55ac61c1e188ad,Graph-Bert: Only Attention is Needed for Learning Graph Representations,"The WL-PE which are absolute structural roles of nodes in the original graph computed using WL algorithm (Zhang et al. 2020; Niepert, Ahmed, and Kutzkov 2016), are not variant to the subgraphs and can be easily used as a generic PE mechanism.###Since the original graph is not used directly in Graph-BERT and the subgraphs do not have edges between the nodes ( i.e. , linkless), the proposed combination of positional encodings attempts at retaining the original graph structure information in the nodes.###On that account, we swap Laplacian PE in our experiments for an ablation analysis and use WL-PE from Graph-BERT, see Table 3.###In addition to the reasons underscored in Sections 1.1 and 2.2, we demonstrate the usefulness of Laplacian eigenvectors as a suitable candidate PE for Graph Transformer in this section, by its comparison with different PE schemes applied in Graph-BERT (Zhang et al. 2020).###Graph-BERT uses 1) on each dataset against the GNN baselines (GCN (Kipf and Welling 2017), GAT (Veli ˇ ckovi ´ c et al. 2018), Gat-edGCN(Bresson and Laurent 2017)) of 500k model parameters.###Finally, we refer to Section 4.1 for a comparison of Laplacian PE with existing Graph-BERT PEs.###Graph-BERT employs a combination of several positional encoding schemes to capture absolute node structural and relative node positional information.###For example, the use of graph-speciﬁc positional features (Zhang et al. 2020), or node Laplacian position eigenvectors (Belkin and Niyogi 2003; Dwivedi et al. 2020), or relative learnable positional information (You, Ying, and Leskovec 2019), virtual nodes (Li et al. 2015), etc. Zhang et al. (2020)…###…we highlight the most recent research works which attempt to develop graph transformers (Li et al. 2019; Nguyen, Nguyen, and Phung 2019; Zhang et al. 2020) with few focused on specialized cases such as on heterogeneous graphs, temporal networks, generative modeling, etc. (Yun et al.…###As Laplacian PE capture better structural and positional information about the nodes, which essentially is the objective behind using the three Graph-BERT PEs, they outperform the WL-PE.###…Niyogi 2003; Dwivedi et al. 2020), or relative learnable positional information (You, Ying, and Leskovec 2019), virtual nodes (Li et al. 2015), etc. Zhang et al. (2020) propose Graph-BERT with an emphasis on pre-a training and parallelized learning using a subgraph batching scheme that creates…###In Graph-BERT, which operates on ﬁxed size sampled subgraphs, a node at-tends to every other node in a subgraph.###We perform detailed analysis of Graph-BERT positional encoding schemes, along with experimental comparison with the model we present in this paper in Section 4.1.###For example, the use of graph-speciﬁc positional features (Zhang et al. 2020), or node Laplacian position eigenvectors (Belkin and Niyogi 2003; Dwivedi et al. 2020), or relative learnable positional information (You, Ying, and Leskovec 2019), virtual nodes (Li et al. 2015), etc. Zhang et al. (2020) propose Graph-BERT with an emphasis on pre-a training and parallelized learning using a subgraph batching scheme that creates ﬁxed-size linkless subgraphs to be passed to the model instead of the original graph.",impact-revealing,discussing the structural roles of nodes in graph-based models and comparing positional encoding methods
3968,5f76f20a91e011f31b98056c,645bd6eadc247989abc5e0b0aa0be79ec8b11ea6,CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,5a73cb7117c44a0b303593e9,Social Bias in Elicited Natural Language Inferences.,"learn and use these biases (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2017; May et al., 2010; Zhao et al., 2018; Rudinger et al., 2017).###These language models, and embeddings extracted from them, have been shown to
∗Equal contribution.
learn and use these biases (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2017; May et al., 2010; Zhao et al., 2018; Rudinger et al., 2017).",other,acknowledge existing research on biases in language models
1922,,36e0fffbe6bee31b17ea2b8d4439997e7db2a126,Writer Verification Based on Three-dimensional Information using Kinect Sensor,,,"###Recently, a device, which receives location information on a two dimensional plane, such as a mouse or pen tablet, has been widely used as a tool for hand writing on computers [2][3][5][6].",impact-revealing,providing context about a device used for handwriting
777,53e9b54ab7602d97040825b6,e4fc3adca44206ecb5dc2c4960e578fe2d0994fe,Secure Untrusted Data Repository (SUNDR),53e9bab5b7602d97046e5ab2,Building secure file systems out of byzantine storage,"1 There remains the possibility of a malicious server entirely concealing some users’ actions from others, if neither collection of users expects anyone from the other to have accessed the ﬁle system.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
4015,5c8ddce94895d9cbc6a97820,8fb5a6fe93a3d19ffee50677c0ae563e3377d2d4,Chinese NER Using Lattice LSTM,5a260c8117c44a4ba8a30f1c,Lattice Long Short-Term Memory for Human Action Recognition,"They have been used to model motion dynamics (Sun et al., 2017), dependencydiscourse DAGs (Peng et al.###They have been used to model motion dynamics (Sun et al., 2017), dependencydiscourse DAGs (Peng et al., 2017), as well as speech tokenization lattice (Sperber et al., 2017) and multi-granularity segmentation outputs (Su et al., 2017) for NMT encoders.",other,acknowledge applications of models in various tasks
911,5c0f87a5da562944ac95a190,4d157bec76125b1121a999ab2b79730540ab7aad,"Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases",573696146e3b12023e527268,Pubchem Substance And Compound Databases,"Several data sets coming from PubChem, Tox21, MUV and DUDE were combined to achieve 38 million data points.###For example, PubChem contains the largest bioactivity data for compounds—mainly retrieved from HTS experiments— and the other databases generally import data from PubChem.###PubChem [1] 93 977 773 (C) 235 653 627 (S) 10 341 (P) 233 799 255 (I) 1 252 820 (E) https://pubchem.###However, one of the most significant differences of ChEMBL from the other large-scale sources is that the provided data are manually curated by experts from the literature in a comprehensive manner, making ChEMBL a more reliable resource, whereas the PubChem data are non-curated.###The main advantage of using PubChem over the other resources is its unmatched high volume (i.e. in terms of the number of bioassays, bioactivities, compounds and targets).###Most chemical databases (e.g. PubChem and ChEMBL) provide both line and graphical representations for the recorded compounds.###Employment of the large-scale bioactivity data from public bioassay databases (e.g. ChEMBL, PubChem and BindingDB) is an option that has already found applications in the literature.###One of the early studies employed multi-task feedforward DNNs for the prediction of activities of compounds against 19 target assays from the PubChem database [238].###Considering these bioactivity databases, PubChem, ChEMBL, Binding MOAD and BindingDB represent activity data with quantitative measurements such as the IC50, EC50, Ki and potency, while DrugBank, STITCH, KEGG, DCDB, HMDB and T3DB only provide the information regarding presence of an activity/interaction between the corresponding drug-target pairs.###Another benchmark data set designed for VS is maximum unbiased validation (MUV), which was generated from PubChem bioactivity data by topological optimization based on a refined nearest neighbour analysis.###In this sense, the prominent bioactivity and compound data resources can be listed as PubChem [1], ChEMBL [2], DrugBank [5], STITCH [167], BindingDB [168], BindingMoad [169], KEGG [170], SIDER [173], DCDB [171], HMDB [174] and T3DB [172].###With the increased volume of open access experimental data in repositories such as PubChem, ChEMBL and ZINC the data resources for VS studies has been significantly changed, compared with 10 years ago.###either approved or experimental drugs [1, 6].###In contrary to PubChem, ChEMBL and BindingDB, BindingMoad is a small-scale bioactivity database, which includes highresolution 3D structures of proteins and their ligand
annotations for related protein-ligand interactions.",impact-revealing,highlighting the significance and differences of various bioactivity databases
3104,5550411c45ce0a409eb3897f,fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5,Neural Machine Translation by Jointly Learning to Align and Translate,53e99de1b7602d97026a0026,Audio Chord Recognition with Recurrent Neural Networks.,", Graves, 2012; Boulanger-Lewandowski et al., 2013). Sutskever et al. (2014) used this approach to generate translations from their neural machine translation model.###Following the procedure described in Cho et al. (2014a), we reduce the size of the combined corpus to have 348M words using the data selection method by Axelrod et al.###Once a model is trained, we use a beam search to find a translation that approximately maximizes the conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al., 2013).###Once a model is trained, we use a beam search to ﬁnd a translation that approximately maximizes the conditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al. , 2013).",other,reporting prior findings on neural machine translation
155,5b67b45517c44aac1c86078b,e62ddf27659bc131968d2dcc3e2bd59de98c6917,"Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop.",58d82fcbd649053542fd6482,Variational Graph Auto-Encoders.,"In our implementation, we use a variational version of graph auto-encoder [14] by assuming Z is generated from a latent Gaussian distribution, hence, Equation (7) is extended as the sum of a reconstruction loss and the KL divergence between the learned latent distribution and the prior distribution.###We use an unsupervised autoencoder architecture [14, 26] to learn from the local linkage graph.",impact-revealing,describing the implementation of a variational graph auto-encoder
2944,5cede0fada562983788d93ae,3a4d79c5c2646051753ff0d98f57c08604299aa2,"TensorFlow Eager: A Multi-Stage, Python-Embedded DSL for Machine Learning",53e9b844b7602d970440513c,Imagenet: A Large-Scale Hierarchical Image Database,"The performance of training ResNet-50 on ImageNet (Deng et al., 2009) using TensorFlow Eager versus TensorFlow Eager with function is shown in Table 1.",other,comparing performance of different training methods on ResNet-50
1268,,ae51274c214889686f2b63196d4daa3cc5588b72,Multi-Scale and Multi-Modal Contrastive Learning Network for Biomedical Time Series,,,"###Methods RMSE TFC[8] 31.1 TS2Vec[3] 31.2 TimeMAE[12] 30.9 TST[14] 25.0 InceptionTime[5] 23.9 Ours 20.6###Deep learning (DL) models especially temporal convolution networks (TCN)[2][3] are widely used for MBTS data mining due to their good performance and low resource consumption, which uses dilated causal convolutions to capture long-term temporal dependency.###Besides, to learn useful representations from MBTS, contrastive learning[3][7][8] is also often used.###3(b), the regression results of TFC[8] and TS2Vec[3] on the test dataset appear as straight lines.###Therefore, the commonly used parameter-sharing encoder for all modalities[3][8] may limit the capability of the encoders to represent different modalities.###The two latest contrastive learning works, i.e. TFC[8]and TS2Vec[3], and one lat-est non-contrastive self-supervised learning work TimeMAE[12] are used as baseline models, and we also compare them with SOTA under the corresponding datasets.",impact-revealing,reporting performance comparisons of various models in MBTS data mining
4026,5c0f87a5da562944ac95a190,4d157bec76125b1121a999ab2b79730540ab7aad,"Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases",53e9b32bb7602d9703dfa1ed,The ChEMBL bioactivity database: an update.,"ChEMBL is also a large-scale compound and bioactivity database.###The system was trained on large-scale ChEMBL bioactivity data by generating training set sizes of 360 835 positive and 93 903 negative examples.###ChEMBL database was employed to obtain known compound–target pairs.###ChEMBL also categorizes targets as ‘Single Protein’, ‘Protein Family’ and ‘Protein Complex’ and assigns a confidence score to state the specificity of compound activity.###However, one of the most significant differences of ChEMBL from the other large-scale sources is that the provided data are manually curated by experts from the literature in a comprehensive manner, making ChEMBL a more reliable resource, whereas the PubChem data are non-curated.###ChEMBL database was used to create training data sets for seven different targets from diverse protein families and an individual prediction model was constructed for each target.###The training data set was generated using verified bioactivities in the ChEMBL database.###Most chemical databases (e.g. PubChem and ChEMBL) provide both line and graphical representations for the recorded compounds.###The chemical structures of drugs and compounds were retrieved from the ChEMBL database.###Three data sets (i.e. the DUD-E set and two generated data sets: a DUD-E like benchmark set composed of 78 904 active compounds, 2 367 120 inactive compounds and 290 targets and another data set with experimentally verified inactive molecules composed of 78 904 active compounds, 363 187 inactive compounds for 290 targets, both constructed using ChEMBL) were employed to train and validate their method.###Employment of the large-scale bioactivity data from public bioassay databases (e.g. ChEMBL, PubChem and BindingDB) is an option that has already found applications in the literature.###As an updated and enhanced version of DUD (DUD-E) with more diverse target classes such as GPCRs and ion channels (along with enzymes and nuclear receptors), DUD-E contains 22 886 ligands and their affinities against 102 targets retrieved from the ChEMBL database, together with property-matched decoys obtained from the ZINC database.###Considering these bioactivity databases, PubChem, ChEMBL, Binding MOAD and BindingDB represent activity data with quantitative measurements such as the IC50, EC50, Ki and potency, while DrugBank, STITCH, KEGG, DCDB, HMDB and T3DB only provide the information regarding presence of an activity/interaction between the corresponding drug-target pairs.###In this sense, the prominent bioactivity and compound data resources can be listed as PubChem [1], ChEMBL [2], DrugBank [5], STITCH [167], BindingDB [168], BindingMoad [169], KEGG [170], SIDER [173], DCDB [171], HMDB [174] and T3DB [172].###ChEMBL [2] 1 735 442 (C) 11 538 (P) 14 675 320 (I) 1 302 147 (E) https://www.###With the increased volume of open access experimental data in repositories such as PubChem, ChEMBL and ZINC the data resources for VS studies has been significantly changed, compared with 10 years ago.###ChEMBL database was used to obtain known compound– target interactions and the corresponding bioactivity values, which were discretized as active, weakly active, weakly inactive and inactive based on pre-defined bioactivity thresholds.###In contrary to PubChem, ChEMBL and BindingDB, BindingMoad is a small-scale bioactivity database, which includes highresolution 3D structures of proteins and their ligand
annotations for related protein-ligand interactions.",other,providing detailed information about the ChEMBL database and its significance in bioactivity data
1025,,f2e833db9c20f85fca7d928f5f4b0808076cf313,Relational Stressors and Depressive Symptoms in Late Adolescence: Rejection Sensitivity as a Vulnerability,,,"###Consistent with diathesisstress models of depression (e.g., Caspi et al. 2003; Metalsky and Joiner 1992), neither relational stressors nor rejection sensitivity may trigger depression in all individuals; however, the combination of the two substantially increased risk for developing depressive symptoms.###Such models propose that individuals at risk for depression have attributional biases and negative self and/or interpersonal schemas that make them vulnerable to interpreting life events negatively (Lewinsohn et al. 2001; Metalsky and Joiner 1992; Safran 1990).###Overall, findings build on previous diathesis stress models of depression (Abramson et al. 1989; Metalsky and Joiner 1992), by suggesting that rejection sensitivity represents a specific cognitive-affective diathesis that, when combined with relational stressors that inhibit the development of…###Consistent with diathesisstress models of depression (e.g., Caspi et al. 2003; Metalsky and Joiner 1992), neither relational stressors nor rejection sensitivity may trigger depression in all individuals; however, the combination of the two substantially increased risk for developing depressive…###Overall, findings build on previous diathesis stress models of depression (Abramson et al. 1989; Metalsky and Joiner 1992), by suggesting that rejection sensitivity represents a specific cognitive-affective diathesis that, when combined with relational stressors that inhibit the development of autonomy and relatedness within close relationships, creates a high level of risk for late adolescent depressive symptoms.",impact-revealing,building on previous diathesis-stress models of depression
3999,5fe4094e9e795e14f30e634a,94497472eecb7530a2b75c564548c540ebd61e9b,Learning to Pre-train Graph Neural Networks,5bdc31b417c44a1f58a0bb5e,Meta-Learning: A Survey.,"On another line, meta-learning intends to learn a form of general knowledge across similar learning tasks, so that the learned knowledge can be quickly adapted to new tasks (Vilalta and Drissi 2002; Vanschoren 2018; Peng 2020).",other,highlighting the purpose and potential of meta-learning
2473,5e15adca3a55ac47ab5b0729,97ebd482a78e6e6c1ba51da5e1b2f8e7640cc8b5,hyperbolic graph convolutional neural networks,53e9ac18b7602d97035d9131,Collective Classification In Network Data,"CORA [36] and PUBMED [27] are standard benchmarks describing citation networks where nodes represent scientific papers, edges are citations between them, and node labels are academic (sub)areas.",other,providing context on standard benchmarks in citation networks
3748,5ce3aebeced107d4c65ebaaf,7223fb56eeece5806f8d25718bbc78386e17f19f,SinGAN-GIF: Learning a Generative Video Model from a Single GIF,5bdc31b817c44a1f58a0c572,Large Scale GAN Training for High Fidelity Natural Image Synthesis.,"Recent work uses a BigGAN [2] like architecture along with temporal and spatial downsampling for the discriminators to achieve state of the art generation results on the Kinetics-600 dataset [6].###GANs can now generate high ﬁdelity images, particularly when constrained to a speciﬁc class like cars, faces, etc. [2, 17].",other,highlighting advancements in GAN architectures for image generation
2271,5f02f17c91e011ee5e0258c8,3da4626411d83c19c9919bb41dba94fff88da90e,Scaling Graph Neural Networks with Approximate PageRank,5bdc31b817c44a1f58a0c039,Adaptive Sampling Towards Fast Graph Representation Learning,"5.4) •How efficient is the proposed sparse inference scheme? (§ 5.5) 5.1 Large-Scale Datasets The majority of previous approaches are evaluated on a small set of publicly available benchmark datasets [2, 13, 14, 21, 25, 27, 41, 49]. The size of these datasets is relatively small, with the Reddit graph (233K nodes, 11.6M edges, 602 node features) [25] typically being the largest graph used for evaluation.4 Chiang et al. [16] rec###etection and graph classification [3, 15, 22, 37]. The success of GNNs on academic datasets has generated significant interest in scaling these methods to larger graphs for use in real-world problems [13, 14, 16, 21, 25, 27, 41, 54]. Unfortunately, there are few large graph baseline datasets available; apart from a handful of exceptions [16, 54], the scalability of most GNN methods has been demonstrated on graphs with fewer than###ically need to performarecursiveneighborhoodexpansiontocomputethehidden representations of a given node. While several approaches have been proposed to improve the efficiency of graph neural networks [13, 14, 16, 21, 25, 27, 41, 49, 54], the scalability of GNNs to massive (web-scale) graphs is still under-studied. As we discussed in § 1 the most prevalent approach to scalability is to sample a subset of the graph, e.g. based on diff###e, Chen et al. [13] directly sample the receptive field for each layer using importance sampling, while Chen et al. [14] use the historical activations of the nodes as a control variate. Huang et al. [27] propose an adaptive sampling strategy with a trainable sampler per layer, and Chiang et al. [16] sample a block of nodes corresponding to a dense subgraph identified by the clustering algorithm METIS",other,highlighting the need for larger datasets in graph neural networks
784,558bfbace4b00c3c48df9828,3c5b532f1b46013a3519f09a35fd1c8387ae59a7,Fast thread migration via cache working set prediction,53e99aecb7602d9702376517,The shared-thread multiprocessor,"The cores of our CMP feature hardware support for thread activation and deactivation, as found in prior studies of thread scheduling [3, 38].###Previous work [3, 38] describes support mechanisms for migrating register state in order to decrease the latency of thread activation and deactivation; however, performance subsequent to migration still suffers due to cold-cache effects.",impact-revealing,acknowledge prior studies on thread scheduling and their limitations
2757,5bdc318017c44a1f58a08780,5ab5658a1666e26c66f0319a469228dbe19598a2,An e-learning recommendation approach based on the self-organization of learning resource,53e9a416b7602d9702d33e2f,A Collaborative Filtering Recommendation Algorithm Based on User Clustering and Item Clustering.,"Considering the Markov chain approach has the random walking mechanism, we applied the Markov chain (MC) as one of the comparison strategies [35, 55].",other,providing context for the method comparison
3943,5d3c234c3a55acd386d4e112,a9ec03dbe702f6909acd1f1f14a3395d0141043b,generative models for graph-based protein design,57fa654c0cf2e8e03cec5d28,The coming of age of de novo protein design,"This ﬁeld has seen tremendous progess in the past two decades [1], including the design of novel 3D folds [2], enzymes [3], and complexes [4].###Protein design and interaction graphs For classical approaches to computational protein design, which are based on joint modeling of structure and sequence, we refer the reader to a review of both methods and accomplishments in [1].",other,acknowledging advancements and contributions in protein design
3648,5cf48a48da56291d582ab75a,c5f5f179d80a3bf9b4f29750283a87eaca42e91b,neural graph collaborative filtering,5550416845ce0a409eb3b00b,Collaborative Deep Learning for Recommender Systems,"information of items [30]; neural collaborative filtering models replace the MF interaction function of inner product with nonlinear neural networks [14]; and translation-based CF models instead use Euclidean distance metric as the interaction function [28], among others.###To enhance the embedding function, much effort has been devoted to incorporate side information like item content [2, 30], social relations [34], item relations [37], user reviews [3], and external knowledge graph [32, 35].",other,acknowledge various collaborative filtering models and their enhancements
3399,5b1642388fbcbf6e5a9b54be,b3dae9529f3caeeec9cc6872e94aa690418acb22,Reinforcement Learning for Relation Classification from Noisy Data,57d063b9ac4436735428eb01,Neural Relation Extraction With Selective Attention Over Instances,"…et al. 2014; dos Santos, Xiang, and Zhou 2015; Mooney and Bunescu 2005; Yang et al. 2016) including convolutional neural networks, recursive neural network (Ebrahimi and Dou 2015; Liu et al. 2015), and long short-term memory network (Miwa and Bansal 2016; Xu et al. 2015; Miwa and Bansal 2016).",other,acknowledge various neural network architectures used in prior research
3904,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,5f0e0a179fced0a24b978afa,Hiding the Microsecond-Scale Latency of Storage-Class Memories with Duplexity,"…overheads that arise from accesses to Flash [40], emerging memory technologies like 3D XPoint by Intel and Mi-cron [41–43], or 40-100 Gb/s Inﬁniband and Ethernet network interactions [44] can signiﬁcantly degrade the request latency of microsecond-scale microservices [45–48] like Cache1 or Cache2 .",other,highlighting the impact of emerging technologies on request latency
828,5f7fdd328de39f0828397fae,645054d31fa26b29bbfb0cf73b75f8906c359415,spectral temporal graph neural network for multivariate time-series forecasting,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,Current state-of-the-art models highly depend on Graph Convoluational Networks (GCNs) [13] originated from the theory of Graph Fourier Transform (GFT).###Spectral Graph Convolution The Spectral Graph Convolution [13] is composed of three steps.,impact-revealing,reporting on the dependency of state-of-the-art models on Graph Convolutional Networks
2101,,bad6fa34e9b00ba2409a997b61ce2a909197269a,Deep Latent Emotion Network for Multi-Task Learning,,,"###Furthermore, ESMM[4] puts forward a new perspective of multi-task modeling with sequential dependence to cope with the challenges of extreme data sparsity and sample selection bias, which however is not suitable for parallel multi-task learning.###Lately, MTL models have been found to be well suited for feed recommendation [4-8].",impact-revealing,highlighting the challenges and limitations of existing multi-task modeling approaches
3577,5fd0a7f691e01147f1d1e367,d4e9bf1d5fab52698a3aa48c65b098ea327deea6,CTRLsum: Towards Generic Controllable Text Summarization,5d7f5d2e3a55acb4692ce2fb,CTRL: A Conditional Transformer Language Model for Controllable  Generation,"…Dragic added 20 as the Miami Heat handed LeBron James another loss on his former home floor with a 106-92 victory over the Cleveland Cavaliers on Monday …… [ ignoring 60 tokens ] James scored 16 of his 26 points in the fourth quarter for Cleveland, which had its four-game winning streak snapped.###We detail g control next, but describe the automatic keyword extraction later in §2.3.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2128,,c9aef622e7bc20088c02fdec6a3d2f1432d08fd9,A kinematic wave theory of multi-commodity network traffic flow,,,"###However, CTM is in nature discrete simulation models and does not provide any analytical insights on congestion formation at a network intersection.###For example, in (Jin and Zhang, 2004), a multi-commodity CTM was developed for simulating traffic dynamics on general road networks, in which traffic flows through junctions with multiple upstream and downstream links are calculated based on the First-In-First-Out (FIFO) diverging principle (Papageorgiou, 1990; Daganzo, 1995b) and the fair merging principle (Jin and Zhang, 2003b).###But we incorporate physical merging and diverging rules used in the multi-commodity CTM developed by (Jin and Zhang, 2004) into our entropy conditions.###FIFO diverging rule (Papageorgiou, 1990; Daganzo, 1995b):
qa!b ¼ n0a!bqa;
which is equivalent to
qb ¼ Xm a¼1 n0a!bqa: ð20cÞ
4.###7 with the same initial and boundary conditions by using the multi-commodity CTM with fair merging and FIFO diverging rules (Jin and Zhang, 2004).###In (Jin et al., 2009), a new framework was proposed to solve the Riemann problem for an inhomogeneous link: the initial conditions were first mapped into supply-demand space, and then a boundary flux function used in CTM (Daganzo, 1995b; Lebacque, 1996), equal to the smaller one of upstream demand and downstream supply, was incorporated into entropy conditions.###As we know, in CTM fluxes through the junction shown in Fig.###Thus the new entropy condition in (20) is consistent with the multi-commodity CTM in (Jin and Zhang, 2004).###(iv) Also different from (Holden and Risebro, 1995), we use fair merging and FIFO diverging rules in the multi-commodity CTM (Jin and Zhang, 2004) as an additional entropy condition to prescribe boundary fluxes from interior states.###This is consistent with q = min{D1, S2} (Daganzo, 1995b; Lebacque, 1996).###Within the framework of CTM, various merging and diverging rules can be incorporated (Daganzo, 1995b; Lebacque, 1996).###In the other line, Daganzo (1995b) and Lebacque (1996) extended the Godunov discrete form of the LWR model to compute traffic flows through merging, diverging, and general junctions.###If we implement (41) in CTM, then numerical boundary fluxes are always the same as theoretical ones, but it is not the case with (20).###On a link in discrete CTM, an interior state, if different from the corresponding stationary state, only shows up in the cell right next to the junction.###In this study, we attempt to present a new kinematic wave theory of multi-commodity network traffic flow, which extends the analytical framework of (Holden and Risebro, 1995) but incorporates physical merging and diverging rules used in CTM into the entropy conditions.###In these Cell Transmission Models (CTMs), so-called traffic demand and supply functions are introduced, and boundary fluxes through various types of junctions can be written as functions of upstream demands and downstream supplies.###Such stationary states can be considered as fixed points of discrete equations, CTM in this case (Bultelle et al., 1998).###However, these models over-simplify traffic dynamics without describing traffic waves or bottlenecks correctly (Daganzo, 1995a).###But different from (Holden and Risebro, 1995; Coclite et al., 2005), the new Riemann solver is consistent with the discrete multi-commodity CTM.###2, we define the following demand and supply functions (Engquist and Osher, 1980; Daganzo, 1995b; Lebacque, 1996)
DaðqÞ ¼ Q aðminfq;qa;cgÞ; ð8aÞ
SaðqÞ ¼ Q aðmaxfq;qa;cgÞ; ð8bÞ
where Da 6 Ca is non-decreasing in q, Sa 6 Ca non-increasing, and
maxfDa; Sag ¼ Ca:
In addition Da = Sa = Ca iff traffic is…###But one can have different types of merging and diverging rules for the second type of entropy conditions (e.g. Holden and Risebro, 1995; Coclite et al., 2005; Daganzo, 1995b; Lebacque and Khoshyaran, 2005; Jin and Zhang, 2004).###CTM has been widely used as network loading models for solving dynamic traffic assignment and other problems (Buisson et al., 1996; Lo and Szeto, 2002; Bliemer, 2007; Durlin and Henn, 2008).###…traffic dynamics on general road networks, in which traffic flows through junctions with multiple upstream and downstream links are calculated based on the First-In-First-Out (FIFO) diverging principle (Papageorgiou, 1990; Daganzo, 1995b) and the fair merging principle (Jin and Zhang, 2003b).###…Riemann problem for an inhomogeneous link: the initial conditions were first mapped into supply-demand space, and then a boundary flux function used in CTM (Daganzo, 1995b; Lebacque, 1996), equal to the smaller one of upstream demand and downstream supply, was incorporated into entropy conditions.###In Section 5, we numerically demonstrate that analytical solutions of traffic dynamics at a general intersection are consistent with those of CTM.",impact-revealing,highlighting the limitations of existing traffic models and proposing a new approach
2017,,0b13b5986d2da8c31e9ed6b95b5d123887f63e24,Metabolic Signatures in Coronary Artery Disease: Results from the BioHEART-CT Study,,,"###Utilising a large cohort with CT coronary angiography characterisation of coronary atherosclerosis burden, we report novel associations between CAD plaque phenotypes and four candidate metabolites (TMAO, DMGV, glutamate, and phenylalanine) [15].###The metabolic associations we report in this study, enriched for metabolites with known biological roles in atherosclerosis, build upon findings from recent studies utilising metabolomics to identify signatures of incident cardiovascular disease that have identified: acylcarnitine, dicarboxylacylcarnitine, TMAO, amino acids such as phenylalanine, glutamate, and several lipid classes, as being associated with cardiovascular disease [15].###A recent systematic review of metabolomics studies attempting to identify metabolic signatures of incident cardiovascular disease identified 5 metabolites acylcarnitine, dicarboxylacylcarnitine, trimethylamine N-oxide (TMAO), phenylalanine, and glutamate [15].",impact-revealing,highlighting novel associations between CAD plaque phenotypes and metabolites in the context of cardiovascular disease
982,,469401168e5e260516bb9a0eaab4df5f95ff891f,Managing Procrastination on Social Networking Sites: The D-Crastinate Method,,,"###Numerous studies have demonstrated the importance of considering emotion as a trigger of procrastination, such as the work in [27,28].",impact-revealing,highlighting the significance of emotion in understanding procrastination
2203,5f842b5891e01129be18ffbd,7097137596f6755675f6aafcdd80969a747322ae,Contrastive Learning with Hard Negative Samples,53e9a645b7602d9702f7362e,Noise-contrastive estimation: A new estimation principle for unnormalized statistical models,"For each data point x ∼ p , the noise-contrastive estimation (NCE) objective (Gutmann & Hyvärinen, 2010) for learning the representation f uses a positive example x + with the same label as x , and negative examples { x − i } Ni =1 with (supposedly) different labels, h ( x − i ) (cid:54) = h ( x )…###The training objective, typically noise-contrastive estimation (Gutmann & Hyvärinen, 2010), guides the learned representation f to map positive pairs to nearby locations, and negative pairs farther apart; other objectives have also been considered (Chen et al., 2020a).",other,describing the noise-contrastive estimation training objective
936,5c87a964da56296d04a90b97,a20bc2ecd8eb9bd735e6b3ba3d1a88f1c872d8eb,Shinjuku: Preemptive Scheduling for μsecond-scale Tail Latency,5a260c2e17c44a4ba8a24046,ZygOS: Achieving Low Tail Latency for Microsecond-scale Networked Tasks.,"Shinjuku is a significant departure from the common pattern in IX [16] and ZygOS [46], which rely heavily on RSS to distribute in-###ZygOS improves on IX by using work stealing to approximate c-FCFS [46].###ZygOS [46] uses inter-processor interrupts for work stealing but does not implement preemptive scheduling.###We compare Shinjuku to IX [16] and ZygOS [46], two recent systems that use d-FCFS and approximate cFCFS respectively to improve tail latency.###A similar performance drop was also observed in the original ZygOS paper [46].###We compare Shinjuku with IX [16] and ZygOS [46], two state-of-the-art dataplane operating systems.###IX [16], Arrakis [45], MICA [39], Chronos [32], and ZygOS [46] fall in this category.###ZygOS [46] improved on d-FCFS by implementing low-overhead task stealing: threads that complete short requests steal work from threads tied up by longer ones.",impact-revealing,comparing performance and design differences among operating systems
1821,,579e9a55a7aa65de82b8a2cb98df2a0fa2cea215,Lived experience: Diverse perspectives on raising a child with autism,,,"###Current literature on ASD emphasizes the need for research on families from
diverse cultural backgrounds (Dyches et al., 2004; Magana & Smith, 2006).###Further, the study was limited to mothers of children with varied disabilities, not autism exclusively, and 81% of participants self-identified as Caucasian, thus indicating a need for additional research which examines autism (Dyches et al., 2004; Zionts & Zionts, 2003), lived experience, and culture (Dyches; Harry, 2008) from purely qualitative measures.###Current literature on ASD emphasizes the need for research on families from diverse cultural backgrounds (Dyches et al., 2004; Magana & Smith, 2006).###…varied disabilities, not autism exclusively, and 81% of participants self-identified as Caucasian, thus indicating a need for additional research which examines autism (Dyches et al., 2004; Zionts & Zionts, 2003), lived experience, and culture (Dyches; Harry, 2008) from purely qualitative measures.###Specifically, information addressing “familial appraisal of autism within a cultural context” (Dyches et al., 2004) is not readily available.###Subsequently, the intersection of race, family coping mechanisms, and autism has also been excluded (Dyches et al., 2004).",impact-revealing,highlighting the need for further research on autism in diverse cultural contexts
274,5c8dd94c4895d9cbc6a7d918,97f1d08c306040401112ff0564f37e6c6a312522,BiNE: Bipartite Network Embedding,59ae3c262bbe271c4c71f4a2,metapath2vec: Scalable Representation Learning for Heterogeneous Networks.,"In contrast to DeepWalk and other work [14] that apply a fixed length on the random walk, we allow the generated vertex sequences have a variable length, in order to have a close analogy to the variable-length sentences in natural languages.###While a recent work by Dong et al. [14] proposed metapath2vec++ for embedding heterogeneous networks which can also be applied to bipartite networks, we argue that a key limitation is that it treats the explicit and implicit relations as contributing equally to the learning.###Metapath2vec++ [14], HNE [27] and EOE [28] are representative vertex embedding methods for heterogeneous networks.",impact-revealing,highlighting the differences in approach to random walks in network embedding
2042,,cf775815b7332ecebc914a05b92275ac0682be52,Methods for automated analysis of macular OCT data,,,"###[63] and later extended by others for use in OCT [21,22,29,30].###finding an ordered set of surfaces [63, 83], or enforcing a nested structure for adjacent labels [84].###, X, it can be shown that the set of vertices on or below the boundary form a closed set in G, meaning there are no edges directed out of the set [63].###labeling by graph cut segmentation and requires a different graph construction [63].###by converting to polar/spherical coordinates to find closed surfaces) [63,83].###The second, and current state-of-the-art method, uses an optimal graph-based search algorithm [63] and is referred to as RF+GS.###and an optimal graph search method [63].###Also, we use only the basic graph algorithm in [63] and do not incorporate the spatially-varying smoothness, regional costs, and soft constraints that are used in more recent works, which can add computational complexity.###One powerful aspect of the minimum surface graph-based formulation is that multiple surfaces can be found simultaneously with a fixed ordering (which is necessary for retinal data) [63].###[63], but here, we include the spatially varying constraints ∆x(x, y), ∆ u x(x, y), ∆ l y(x, y), ∆ u y(x, y), δ l(x, y), and δu(x, y) introduced by Garvin et al.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
528,555048eb45ce0a409eb72996,791b65c65f8ae7e16c1ee9203cdc3ee59ffeb99f,Relation Classification via Convolutional Deep Neural Network.,53e99beab7602d970249335e,Natural Language Processing (almost) from Scratch,"Finally, the word dimension and learning rate are the same as in Collobert et al. (2011).###Similar to Collobert et al. (2011), we first process the output of Window Processing using a linear transformation.###Collobert et al. (2011) reported that word embeddings learned from significant amounts of unlabeled data are far more satisfactory than the randomly initialized embeddings.###Our work shares similar intuition with that of Collobert et al. (2011).###For example, Collobert et al. (2011) use a 50-dimensional vector to represent a word.###In (Collobert et al., 2011), all of the tasks are considered as the sequential labeling problems in which each word in the input sentence is given a tag.###The idea of extracting features for NLP using convolutional DNN was previously explored by Collobert et al. (2011), in the context of POS tagging, chunking (CHUNK), Named Entity Recognition (NER) and Semantic Role Labeling (SRL).",impact-revealing,acknowledge prior work and its relevance to current research
3007,57d063f6ac44367354296741,908f7931de8768786d9ef7d64f5a8156860709dd,Dynamic Pricing and Traffic Engineering for Timely Inter-Datacenter Transfers,53e9b7cdb7602d970437b0ca,How Many Tiers? Pricing In The Internet Transit Market,"[32] proposes destination-based tiered pricing for transit ISPs – based on both the traffic demand, as well the cost of carrying it; they use three or four tiers.",other,reporting prior findings on destination-based tiered pricing
1566,,b2ab86a1d1d1931848df79c289423496976719bc,Decision-Making under Uncertainty: A Heuristics Overview and the Analytic Network Process,,,"###The term “heuristics” implies an efficient cognitive process, conscious or unconscious, that ignores part of the information (Gigerenzer & Gaissmaier, 2011). The classical view on heuristics states that heuristic decisions imply greater errors than “rational” decisions do as defined by logic or statistical models. On the other hand, recent experiments show that in some cases a right heuristic can be more effective than an advanced approach (Mousavi & Gigerenzer, 2014). But could we apply heuristics without sufficiently reducing the complexity of a problem? Could heuristics be part of the analytical approach? Could mathematical theory describe the cognitive process? In this article we discuss heuristic and analytical approaches in decision-making under uncertainty. We provide an overview of known heuristic techniques as well as Analytical Network Process Methodology – a multicriteria decision-making method developed by T. L. Saaty. We assume that it is heuristics that is critical for decision-making with the Analytical Network Process. A Heuristics Overview Frank Knight (1921) was the first who argued that generating economic profit is related to making entrepreneurial decisions under uncertainty.###In fo rm at io n se ar ch is e
xp en
si ve
o r
ti m
ec on
su m
in g
A d
ri vi
ng fo
rc e
in c
ul tu
ra l
ev ol
ut io
n
Ta bl
e 1
(c on
ti nu
ed )
Anyway, the results of all research put heuristics on par with the standard statistical models of “rational” cognition (Gigerenzer, 2008).###The term “heuristics” implies an efficient cognitive process, conscious or unconscious, that ignores part of the information (Gigerenzer & Gaissmaier, 2011). The classical view on heuristics states that heuristic decisions imply greater errors than “rational” decisions do as defined by logic or statistical models. On the other hand, recent experiments show that in some cases a right heuristic can be more effective than an advanced approach (Mousavi & Gigerenzer, 2014). But could we apply heuristics without sufficiently reducing the complexity of a problem? Could heuristics be part of the analytical approach? Could mathematical theory describe the cognitive process? In this article we discuss heuristic and analytical approaches in decision-making under uncertainty. We provide an overview of known heuristic techniques as well as Analytical Network Process Methodology – a multicriteria decision-making method developed by T. L. Saaty. We assume that it is heuristics that is critical for decision-making with the Analytical Network Process. A Heuristics Overview Frank Knight (1921) was the first who argued that generating economic profit is related to making entrepreneurial decisions under uncertainty. However, situations of uncertainty occur in a wide range of decisions in everyday life. Mousavi and Gigerenzer emphasized that “each of these uncertain situations can be too unique to lend any useful data to statistical analysis and hence preclude not only explicit attainment of even near-perfect knowledge but also measurable probability” (Mousavi & Gigerenzer, 2014). H. A. Simon proposed an alternative concept of the mathematical modeling of decision-making. In Simon (1957) he introduced a theory of “bounded rationality” — the idea that when individuals make decisions, their rationality is limited by the tractability of the decision problem, the cognitive limitations of their minds, and the time available to make the decision.###As information search and computation is expensive in terms of time and effort, humans rely on heuristics that trade off some loss in accuracy for faster and more frugal decision (Gigerenzer & Gaissmaier, 2011). In contrast, Gerd Gigerenzer and colleagues pursued an approach that heuristics do not depend on a compromise between accuracy and effort — their simplicity is the reason they work well, by properly matching the heuristic approach with the environment (Mousavi & Gigerenzer, 2014). Heuristics are “fast and frugal” and deal with simple, task-specific decision strategies that can be used to make accurate and unbiased judgments. Gigerenzer conceptualized rational decisions in terms of the adaptive toolbox (the repertoire of heuristics an individual or institution has) and the ability to choose good heuristics for the task at hand. He used the term “ecological rationality” that “refers to functional matches between cognition and environment, and thus generates insight for engineering environments that are most conducive to achieving certain tasks” (Gigerenzer, Hertwig, & Pachur, 2011; Todd, Gigerenzer, & the ABC Research Group, 2012). Thus, fast and frugal heuristics are ecologically rational, which means that fast and frugal heuristics present a strategy that effectively corresponds to the structure of information in the environment According to Gigerenzer and colleagues (Mousavi & Gigerenzer, 2014), the effectiveness of this ecological match does not simplify the complex structure of the environment. Heuristic strategies in fact mimic the complexity of the environment. For example, in order to reduce the estimation error and effort people can ignore available information for estimating correlations from a sample. But “in an uncertain world, less often proves to be more” (Ibid.). This less-is-more effect Gigerenzer describes through the inverse-U-shaped relation between the level of accuracy and the amount of information, computation or time. The U-shaped relation implies that, at a certain point, more information does not improve the decision quality, but harms it (Todd et al., 2012). The reason why this less-is-more effect can produce positive outcomes is that heuristics are not randomly applied. They are adapted to suit the particular decision environment in which they are employed (Gigerenzer & Gaissmaier, 2011). Gigerenzer & Gaissmaier (2011) review four classes of fast and frugal heuristics: 1) recognition-based heuristics that exploit recognition memory: 2) one-reason heuristics that rely on one good reason only (and ignore all other reasons); 3) trade-off heuristics that weigh all cues; 4) social heuristics that rely on social information. Based on Schulkin (2012) and Gigerenzer, Gaissmaier (2011) we aggregate well-studied fast and frugal heuristics with evidence of use in the adaptive toolbox and examples of heuristics in each class (see Table 1).###As information search and computation is expensive in terms of time and effort, humans rely on heuristics that trade off some loss in accuracy for faster and more frugal decision (Gigerenzer & Gaissmaier, 2011). In contrast, Gerd Gigerenzer and colleagues pursued an approach that heuristics do not depend on a compromise between accuracy and effort — their simplicity is the reason they work well, by properly matching the heuristic approach with the environment (Mousavi & Gigerenzer, 2014). Heuristics are “fast and frugal” and deal with simple, task-specific decision strategies that can be used to make accurate and unbiased judgments. Gigerenzer conceptualized rational decisions in terms of the adaptive toolbox (the repertoire of heuristics an individual or institution has) and the ability to choose good heuristics for the task at hand. He used the term “ecological rationality” that “refers to functional matches between cognition and environment, and thus generates insight for engineering environments that are most conducive to achieving certain tasks” (Gigerenzer, Hertwig, & Pachur, 2011; Todd, Gigerenzer, & the ABC Research Group, 2012). Thus, fast and frugal heuristics are ecologically rational, which means that fast and frugal heuristics present a strategy that effectively corresponds to the structure of information in the environment According to Gigerenzer and colleagues (Mousavi & Gigerenzer, 2014), the effectiveness of this ecological match does not simplify the complex structure of the environment. Heuristic strategies in fact mimic the complexity of the environment. For example, in order to reduce the estimation error and effort people can ignore available information for estimating correlations from a sample. But “in an uncertain world, less often proves to be more” (Ibid.). This less-is-more effect Gigerenzer describes through the inverse-U-shaped relation between the level of accuracy and the amount of information, computation or time. The U-shaped relation implies that, at a certain point, more information does not improve the decision quality, but harms it (Todd et al., 2012). The reason why this less-is-more effect can produce positive outcomes is that heuristics are not randomly applied. They are adapted to suit the particular decision environment in which they are employed (Gigerenzer & Gaissmaier, 2011). Gigerenzer & Gaissmaier (2011) review four classes of fast and frugal heuristics: 1) recognition-based heuristics that exploit recognition memory: 2) one-reason heuristics that rely on one good reason only (and ignore all other reasons); 3) trade-off heuristics that weigh all cues; 4) social heuristics that rely on social information.###A Heuristics Overview and the Analytic Network Process 737 Anyway, the results of all research put heuristics on par with the standard statistical models of “rational” cognition (Gigerenzer, 2008).",impact-revealing,discussing the role and effectiveness of heuristics in decision-making under uncertainty
1253,,a1ce9ee2c37ee25830ccb2606bcb837532af02d7,Polymorphisms in the Methylenetetrahydrofolate Reductase Gene,,,###[94] Most of these studies did not consider the folate status of the study population and are therefore difficult to interpret.,impact-revealing,highlighting limitations in existing studies
807,5c04967517c44a2c7470927f,b9015d4f1e591eba7ea21c3566c919f80f7c2afe,attentive long short-term preference modeling for personalized product search,5c8bce1d4895d9cbc6adfafa,Learning a Hierarchical Embedding Model for Personalized Product Search,"[1] proposed a personalized product search model and took into consideration the users’ preference.###Compared to HEM, our method can achieve a larger improvement on the Clothing dataset (the absolute improvement of NDCG over Clothing is 0.025 (93% relative improvement) while on Phone is 0.013 (16%), Toys is 0.010 (12%), and Electronics is 0.008 (10%)), which is mainly because of the more frequent behaviors of purchasing clothes.###• On the Clothing dataset, the performance of both bag-of-words methods (QL and UQL) exceed the two representation learning methods (LSE and HEM), which is mainly because the average review length is much smaller than that of the other three datasets ( Clothing is 36, while on the other three are 135, 103, 161, respectively).###It is worth noting that the recently proposed HEM is the state-of-the-art method for personalized product search.###We compared the proposed ALSTP model with a logistic regression basedmethod and different retrieval approaches from two categories: (1) traditional methods based on bag-of-words representations, such as Query Likelihood Model [69] and Extended Query Likelihood with User Models [1] and (2) representation learning approaches based on latent space modeling, such as Latent Semantic Entity [59] and Hierarchical Embedding Model (HEM) [1].###The number of negative samples (i.e., N s ) for each positive training data is set to 5 for LSE, HEM and our model.###For simplicity, we set the word embedding and product embedding to the same size; and for the n -gram window size n , we tuned it exponentially Hierarchical Embedding Model (HEM).###[1] have presented a personalized product search method, which falls into the second category.###• For the logistic regression based method LRS, on the Toys dataset, it surpasses the QL and UQL with a large margin, and achieved better performance than LSE and HEM on the Clothing dataset.###This model (HEM) proposed in Reference [1] is the state-of-the-art approach for the personalized product search.###This model (HEM) proposed by [1] is the state-of-the-art approach for the personalized product search.###Extended Query Likelihood with User Models (UQL).###To analyze the effect of the embedding size on the baselines of LSE and HEM, as well as our proposed ALSTP model, we show the results of these methods with distinct embedding sizes over the four datasets.###Based on this observation and following the strategy of References [1, 59], for each product a user purchased, we extracted the corresponding search query from the categories to which the product belongs.###We compared the proposed ALSTP model with a logistic regression based method and different retrieval approaches from two categories: 1) traditional methods based on bag-of-words representations, such as Query Likelihood Model [69] and Extended Query Likelihood with User Models [1]; and 2) representation learning approaches based on latent space modeling, such as Latent Semantic Entity [59] and Hierarchical Embedding Model (HEM) [1].###Similar to UQL, HEM also uses a coefficient to control the weight between the query model q and the user model u by, HEM learns the distributed representations of queries, users and products by maximizing the likelihood of observed user-query-product triplets.###• For both the traditional bag-of-words (i.e., QL and UQL) and state-of-the-art representation learning (LSE and HEM) methods, personalized product search consistently outperforms the non-personalized ones.",impact-revealing,reporting on the performance comparison of personalized product search methods
3242,5f7fdd328de39f08283980ba,2fce1ef37391cd685fc5459e1cbfcb8490b85242,Graph information bottleneck,5736971f6e3b12023e612bee,Deep learning and the information bottleneck principle,"In particular, the Information Bottleneck (IB) [18, 19] provides a critical principle for representation learning: an optimal representation should contain the minimal sufficient information for the downstream prediction task.",other,highlighting the significance of the Information Bottleneck principle in representation learning
2863,53e9b9fbb7602d97045fabff,f89facea7ae5a3f51af96b549f04f3dbe8c884b3,SHIFT: Shared history instruction fetch for lean-core server processors,558c6c60e4b02b9f07a703a5,SLICC: Self-Assembly of Instruction Cache Collectives for OLTP Workloads,"A number of orthogonal studies mitigate instruction cache misses by exploiting code commonality across multiple threads [4, 11].",other,acknowledge existing studies on instruction cache optimization
3810,5d1eb9d5da562961f0b0fa03,037aeb767ab431eeebc74a0b85df0d2f5641c652,Pre-Training With Whole Word Masking for Chinese BERT,5cede0e2da562983788c1522,Reducing BERT Pre-Training Time from 3 Days to 76 Minutes.,"For smaller batch sizes, we adopt the original A DAM [25] with weight decay optimizer in BERT for optimization, and use LAMB optimizer [26] for better scalability in larger batch size.",other,describing optimization methods for different batch sizes
1419,,324e6d276151a87951f314cad78fa097fe9188e9,A Workflow for Offline Model-Free Robotic Reinforcement Learning,,,"###A large value for the CQL reg-ularizer, R ( θ ) , indicates an overestimation of Q-values relative to their true value [2] and thus, unlike the overﬁtting regime, we would not expect the average learned Q-value to decrease with more training.###We propose metrics and protocols to assist practitioners in selecting policy checkpoints, regularization parameters, and model architectures for conservative ofﬂine RL algorithms such as CQL [2] and BRAC [16].###…overﬁtting and underﬁtting generically for any conservative ofﬂine RL method, we consider an abstract optimization formulation for such methods [2]: J D ( π ) denotes the average return of policy π in the empirical MDP induced by the transitions in the ofﬂine dataset D , and D ( π, π β )…###For example, Equation 1 translates to utilizing D CQL ( p, q ) := x p ( x )( p ( x ) /q ( x ) − 1) in Equation 2 (see Theorem 3.5 in Kumar et al. [2] for a proof).###To prove this result, we build on the analysis in Kumar et al. [2] and ﬁnd that the Q-function at iteration k + 1 can be written as follows: where π β ( a s ) denotes the behavior policy action-conditioned on state marginal in the dataset D .###Algorithms for ofﬂine deep RL [37, 15] can be divided into three categories: those that constrain the policy to the dataset [38, 39, 16, 40, 41, 42], those that prevent overestimation via critic regularization [2, 43, 44] and those that train dynamics models and apply a reward penalty [45, 46].###Overﬁtting and underﬁt-ting in CQL. Conservative ofﬂine RL algorithms [2, 43] like CQL can be sensitive to design choices, including number of gradient steps for training [56, 57] and network capacity.###While ofﬂine RL algorithms have improved significantly [1, 2, 3, 4, 5], applying such methods to real-world robotic control problems presents a number of major challenges.###…k : ∀ s , a , π k ( a s ) = δ [ a = arg max a (cid:48) Q k ( s , a (cid:48) )] . actor-critic Arguably, this is closer to how CQL (and other actor-critic The Q-learning variant of CQL [2] actually performs exact policy improvement for each step, which is exactly what is shown in Equation 5.###Q-function We focus on conservative ofﬂine RL algorithms that modify the Q-function to penalize distributional shift, with most experiments on CQL [2], though we also adapt our workﬂow to BRAC [16] in Appendix E.1.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
156,5a260c8117c44a4ba8a30a57,908272f8e6340971600148d4e73f50e1e8843aaf,"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec",555048f245ce0a409eb72cfc,Neural Word Embedding as Implicit Matrix Factorization.,"Most recently, an excellent work written by Levy and Goldberg [21] proves that SGNS is actually conducting implicit matrix factorization, which provides us a tool to analyze the above network embedding models.###Inspired by an approach from skip-gram model named Shifted PPMI (SPPMI) [21], we define M such that M 0 i , j = max ( M i , j , 1 (Line 3).###Besides the analysis in Levy and Goldberg [21], we also utilize the conclusions from random walk on graph [23], spectral graph theory [10] and matrix analysis [16, 39] to help understand network embedding models.###We note that the skip-gram with negative-sampling for word embedding has been shown to be an implicit factorization of certain word-context matrix [21].###…the words come from a textual corpus of words w 1 , · · · w L and the contexts for word w i are words surrounding it in an T -sized window i − T · · · i − i · · · i T Following the work by Levy and Goldberg [21], the SGNS is actually factorizing where b is the number of negative samples.###To be consistent with the discussion in [21], we also use term # ( w , c ) to denote the number of times the vertex-context pair ( w , c appears in corpus D , # ( w ) = c # w , c ) and # ( c ) = w # w 0 , c .",impact-revealing,highlighting the significance of recent findings in network embedding models
3073,5e15adca3a55ac47ab5b0729,97ebd482a78e6e6c1ba51da5e1b2f8e7640cc8b5,hyperbolic graph convolutional neural networks,5b67b47917c44aac1c8637f9,Representation Tradeoffs for Hyperbolic Embeddings.,"However, the volume of balls in Euclidean space only grows polynomially with respect to the radius, which leads to high distortion embeddings [34, 35], while in hyperbolic space, this volume grows exponentially.",other,highlighting the difference in volume growth between Euclidean and hyperbolic space
2153,,67293c55dfeb2f7bb08063b3e39c1691c69d6f1a,A categorical model for the geometry of interaction,,,"###Any partially additive category (following Manes and Arbib [25]) forms a traced UDC, with trace given by the formula in Proposition 6.###Such categories are inspired from early categorical analyses of programming languages by Elgot, Arbib and Manes, et. al.###These are motivated by the partially additive categories (PAC) of Manes and Arbib [25] and models in GoI.###In the case where the tensor is coproduct and Σ -monoids satisfy an additional condition, such categories were studied in computer science in the early categorical analyses of ﬂow charts and programming languages by Bainbridge, Elgot, Arbib and Manes, et. al.###We end with a list of other models, which are partially additive categories in the sense of Manes and Arbib [25].",impact-revealing,acknowledge foundational concepts in category theory and their relevance to programming languages
1450,,4c63be29af1fc476c8169258428535aa7e4c4715,Elevated risk of attention deficit hyperactivity disorder (ADHD) in Japanese children with higher genetic susceptibility to ADHD with a birth weight under 2000 g,,,"###While previous studies have highlighted the importance of gene-environmental interactions in the etiology of ADHD [39, 40], the evidence of this relationship remains limited [40–42].",impact-revealing,acknowledging the limited evidence of gene-environmental interactions in ADHD
2980,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,5ed090749e795e34136f6e07,"On the Behavior of UCT in Synthetic Search Spaces,_x0094_","[164], MCTS approaches to games such as Chess are not as successful as for games such as Go.###search spaces (Section III-E) is continued elsewhere [164].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3649,5c0f8548da562944ac906a71,5a3da29970d0c3c75ef4cb372b336fc8b10381d7,CNN-Based Real-Time Dense Face Reconstruction with Inverse-Rendered Photo-Realistic Face Images,573696966e3b12023e5a0170,Real-time high-fidelity facial performance capture,"Most of them use a 3D Morphable Model [53], [18], [22] or a multi-linear face model [9], [8], [48], [7], [45] as a prior.###For example, [8] and [7] learn facial geometry while not recovering facial appearance property, such as albedo.###Recently, several approaches have been proposed for RGB video based facial performance captureing [8], [7], [53], [18], [45], [22].###[9], [8] adopt a learning-based regression model to fit a generic identity and expression model to a RGB face video in realtime and [7] extends this approach by also regressing fine-scale face wrinkles.",other,acknowledge existing methods in facial performance capture
2114,,6f6fffd3ea2ee672a5bb44d451004c59819db428,BOOT-TS : A Scalable Bootstrap for Massive Time-Series Data,,,"###The core of our approach relies on the Bag of Little Bootstraps (BLB) [5] for bootstrap scalability, on the Stationary Bootstrap (SB) [9] for time-series resampling support and on Hadoop4 for computational scalability.###This naı̈ve approach is inefficient because it sends the entire dataset to all machines, which can still be impractical for large datasets, but authors in [5] address this concern for iid data by splitting the sample into chunks and processing each chunk in parallel thus avoiding sending the whole dataset to all nodes.###The limitations of the solution proposed in [5] is that (i) it is only applicable to iid data and (ii) it treats all block samples as equal.###Note that s is dependent on b and at convergence can be as low as 1-5 for b = n or 10-20 for b = n which is consistent with results for iid data found in [5].###In our implementation, r is not fixed, and each node can converge independently which was a limitation in [5].###We build on previous work of bootstrap [6, 5, 4] and time-series theory [1, 9] to provide a scalable approach for assessing the quality of large time-series data samples.###The difficulty in supporting fast and accurate estimator assessment based on a sample has long been recognized as one of the major obstacles in interactive big data processing [5, 3].###This means that setting a fixed r, as is done in [5], may lead to inefficiencies.###The key difference of BOOT-TS from [9] and from [5] is that we focus on scalability when extending the bootstrap to time-series.###Borrowing notation from [5] we define Qn{P} ∈ Q as the true distribution of θ̂n.",impact-revealing,highlighting the limitations and inefficiencies of existing bootstrap methods for time-series data
2585,5d3ed25a275ded87f97deb36,37f7eaf57b88ef03efd4196f2f57ae0b4c657a79,Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommendation,599c797d601a182cd2643e8a,Modeling Relational Data with Graph Convolutional Networks.,"Researchers also propose using GNNs to model KGs [17], but not for the purpose of recommendation.",other,acknowledge existing research on GNNs and KGs
3402,573695fd6e3b12023e511373,e49ff72d420c8d72e62a9353e3abc053445e59bd,Deep convolutional networks on graph-structured data,53e9bb08b7602d9704744ccc,Learning the 2-D Topology of Images,"While correlations are typically sufﬁcient to reveal the intrinsic geometrical structure of images [16], the effects of higher-order statistics might be non-negligible in other contexts, especially in presence of sparsity.",other,highlighting the importance of higher-order statistics in image analysis
3412,5a4aef9e17c44a2190f7a8b8,ff772950f66ac6a57f4201ce1f02f0013ccdc1bb,Receptive Field Block Net for Accurate and Fast Object Detection,555042f445ce0a409eb45605,HSOG: A Novel Local Image Descriptor Based on Histograms of the Second-Order Gradients.,"lps to highlight the importance of the region nearer to the center and elevate the insensitivity to small spatial shifts. A few shallow descriptors coincidentally make use of this mechanism to design [34,14,37] or learn [1,38,29] their pooling schemes, and show good performance in matching image patches. Regarding current deep learning models, they commonly set RFs at the same size with a regular sampling g",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1235,,4d017e0d497cfb8d2529490bf890b697b5be31ee,Machine Learning Methods for Predicting HLA–Peptide Binding Activity,,,"###They also benchmarked 16 publicly available tools such as MHCPRED, 50 MULTIPRED, 38,39 NetMHC, 40 and SVMHC. 56,57 Five Class I HLAs were evaluated but only two of them had been predicted by all these methods.###SVM has been used by a few servers for HLA–peptide binding prediction including MHC2PRED, 54 MULTIPRED, 55 SVMHC, 56,57 T a b l e 1 . a n o v e r v i e w o f m a j o r m a c h i ne l ea r n i ng t o o l s f o r p r ed i c t i ng h l a – pep t i de b i nd i ng s o r t ed b y c a t ego r y and m e t hod . t he t oo l s w e r e d i v i ded i n t o t w o c a t ego r i e s , qua li t a t i v e o r quan t i t a t i v e , de pen d i n g on t he ou t pu t s . t he unde r l y i ng m e t hod , de sc r i p t o r s , pe r f o r m an c e , and Ur l w e r e ha r v e s t ed f r o m t he o r i g i na l p a pe r s . t he s uppo r t ed nu m be r and c l a ss o f h l a s and c o rr e s pond i ng l eng t h o f pe p t i de s w e r e ha r v e s t ed f r o m e i t he r t he o r i g i na l pape r s o r t he i r w eb s i t e s . s o m e t oo l s u t ili z e e x t r a p r o c e ss t o dea l w i t h pep t i de s w i t h v a r i ou s l eng t h s , w h i c h a r e li s t ed i n t he “ e x t r a p r o c e ss ” c o l u m n . fixed lengths of peptides; therefore, separate models have to be developed for peptides with different lengths. is problematic for Class II HLA binders since peptides partially interact with Class II HLAs.###Sparse encoding is simple but widely used by servers such as ANNPred/nHLAPred, 37 MHC2PRED, 54 NetMHC/NetMHCII, 40,41 NetMHC-pan/NetMHCIIpan, 42,43 SVMHC, 56,57 and SVRMHC. 51 The concept of sparse encoding is similar to dummy variables in the machine learning field.###SVM has been used by a few servers for HLA–peptide binding prediction including MHC2PRED, 54 MULTIPRED, 55 SVMHC, 56,57 T a b l e 1 . a n o v e r v i e w o f m a j o r m a c h i ne l ea r n i ng t o o l s f o r p r ed i c t i ng h l a – pep t i de b i nd i ng s o r t ed b y c a t ego r y and m e t…",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3321,53e9ad3bb7602d970372109a,8686368908956c506f6e3bbcaa2810adfda14914,NoC-sprinting: Interconnect for fine-grained sprinting in the dark silicon era,53e99858b7602d970208fc30,The gem5 simulator.,We simulate CMP systems using gem5 [3] and observe the performance speedup when varying the core count.###We use the gem5 [3] full system simulator to setup a sprinting-based multicore architecture with 16 ALPHA CPUs.,other,describing the simulation setup and performance observation
2957,5aed14d617c44a4438159123,921196c32213a229245a9705ee4768bc941e7a26,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,53e99967b7602d97021a9c82,Supervised Sequence Labelling With Recurrent Neural Networks,"Recurrent networks are dedicated sequence models that maintain a vector of hidden activations that are propagated through time (Elman, 1990; Werbos, 1990; Graves, 2012).",other,providing context on recurrent networks as sequence models
977,,77ed203c8339a2f127d3d8249e60e0727f28772f,Students Taking Social Action: Critical Literacy Practices Through School-As-Museum Learning,,,"###It is here that we connect with third-generation
activity theory, also known as Cultural Historical Activity Theory (CHAT; Cole, 1996; Engeström, 2009; Stetsenko, 2016).###Thus, the narratives—created through both linguistic and visual texts— and meanings made by visitors were collaborative practices that contributed to what Stetsenko (2016) refers to as the continual chain of “acting-being-doing” (p. 36).###Stetsenko (2012, 2016) builds on this research tradition and theorizes a transformative activist stance.###Our analysis was theoretically informed by CHAT (e.g., Cole, 1996; Engeström, 2009; Stetsenko, 2016) and, in particular, emphasized collaborative transformative practice, social action, and expansive learning.###Stetsenko (2016) writes:
What is at stake here is the unique phenomenological richness of each and every human deed, of each and every act of being, knowing, and doing.###From here, we called on critical forms of discourse analysis (Gee, 2005; Rogers & Mosley Wetzel, 2013) combined with CHAT (e.g., Engeström, 2009; Stetsenko, 2016) to look more carefully at how ideas and stances were multimodally constructed in the activity systems that comprised the exhibit (e.g.,…",impact-revealing,connecting research to third-generation activity theory and its implications
2386,5aed146117c44a4438152803,5f0da3cedda449b72fe36fa78798651a038f515c,MAERI: Enabling Flexible Dataflow Mapping over DNN Accelerators via Reconfigurable Interconnects,5a260c4017c44a4ba8a26419,Rethinking NoCs for spatial neural network accelerators,"Without loss of generality, we assume a DNN accelerator implementation comprising of an on-chip memory buffer (that we call a prefetch buffer (PB)) to prefetch inputs/weights/intermediate values from DRAM), and a multitude of PEs.###This work also opens up exciting opportunities in compiler designs that can take arbitrary DNNs and map them efficiently over a Maeri-like fabric.",other,highlighting opportunities for efficient DNN mapping and compiler designs
546,5d1eb9ddda562961f0b17476,c0aaee2337e5af680e5dca1bfc349a737dfec573,Fixing the train-test resolution discrepancy,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"Repeated augmentations already improve the default PyTorch ResNet-50 from 76.2% top-1 accuracy to 77.0%.###We measure the distribution of activation values after the average pooling in a ResNet-50 in fig.###We run our experiments on machines with 8 Tesla V100 GPUs and 80 CPU cores to train and fine-tune our ResNet-50.###E.g., the accuracy of a ResNet-50 trained at resolution 224 increases from 77.0 to 78.4 top-1 accuracy, an improvement of 1.4 percentage points.###In this paper, we focus instead on the ResNet-50 architecture [10] due to its good accuracy/cost tradeoff (25.6M parameters) and its popularity.###We use standard state-of-the-art neural network architectures with no modifications, We consider in particular ResNet-50 [10].###In this paper, we focus on the ResNet-50 architecture [13] due to its good accuracy/cost tradeoff (25.###For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128×128 images, and 78% with our multi-resolution classification.###Another performance-boosting strategy is to classify an image by feeding it at multiple resolutions [13, 33, 35], again averaging the predictions.###For example, [13, 20, 35] used ten crops (one central, and one for each corner of the image and their mirrored versions).###We train ResNet-50 with SGD with a learning rate of 0.1 × B/256, where B is the batch size, as in [12].###By simply fine-tuning the classifier (the fully connected layers of ResNet-50) with test-time augmentation, we reach 78.9% in Top-1 accuracy with the classic ResNet-50
initially trained at resolution 224.###Our ResNet-50 CutMix outperforms others ResNet-50.###We use standard state-of-the-art neural network architectures with no modifications, We consider in particular ResNet-50 [13].###The difference with the ResNet-50 fine-tuning is that we modify the last three cells, in one epoch and with a learning rate of 0.0008.###%) with the classic ResNet-50 trained at Ktrain = 224.###Improvement of our approach on a ResNet-50.###Our ResNet-50 is slightly worse than ResNet50-D and MultiGrain, but these do not have exactly the same architecture.###For instance, the accuracy of ResNet-50 on the ImageNet validation set as Ktest is changed (see section 5) are:
Ktest 64 128 224 256 288 320 384 448 accuracy 29.4 65.4 77.0 78.0 78.4 78.3 77.7 76.6
Thus for Ktest = 288 the accuracy is 78.4%, which is greater than 77.0% obtained for the native crop size Ktest = Ktrain = 224 used in training.",impact-revealing,reporting performance metrics and experimental setup for ResNet-50
2047,,f24df4983ccfe16f8c66dc170acfb2b8170d65c0,Automated multilayer segmentation and characterization in 3D spectral-domain optical coherence tomography images,,,###The graph search approach used in this study was inspired by the strategy previously described by Li et al.(5) Multiple surfaces segmentation could be considered as an optimization problem with the goal being to find a set of surfaces with the minimum cost such that the found surface set was feasible.###reported a graph search framework for the automated multiple layer segmentation of mutually interacting surfaces in 3-D volumetric images.(5) It was later on adapted for the multiple retinal layer segmentation in SD-OCT volumes and had demonstrated a great suitability in several applications.,impact-revealing,drawing inspiration from previous graph search strategies for segmentation
2038,,4be3c6b43b304b7a86d86c2e8a62b96f8f4fec35,Belief in numbers: When and why women disbelieve tailored breast cancer risk statistics.,,,"###diet, exercise) may have had some merit behind their reasoning, because obesity can indeed raise the risk of breast cancer and the BCRAT model does not account for this factor either [28,29].",impact-revealing,highlighting a limitation in the BCRAT model regarding obesity and breast cancer risk
1029,,69b1308c52638b8580df579eeab128e19c5ed193,The Distinct Influence of Cognitive Busyness and Need for Closure on Cultural Differences in Socially Desirable Responding,,,"###Conversely, impression management is a conscious, active, and deliberate attempt to fake good behavior in front of a real or imagined audience (Leary and Kowalski 1990; Mick 1996; Paulhus and John 1998).###Some research findings suggest that self-enhancement is a spontaneous and unconscious response that requires little cognitive deliberation (Paulhus 1991; Paulhus and John 1998).###Self-enhancement is the tendency to describe oneself in an inflated yet honestly held manner and is motivated by the desire to see oneself in a positive, overconfident light (Paulhus 1991; Paulhus and John 1998).",impact-revealing,distinguishing between impression management and self-enhancement
2045,,46cc69e3be461ed816953a7bcd07f5bc11028586,3D intrathoracic region definition and its application to PET-CT analysis,,,"###Recent work on intrathoracic definition relied on a graph-based global energy-minimization method.(4,5) Unfortunately, the method is computationally intensive, is not suitable for high-resolution 3D image data, and does not provide an efficient means to refine segmentation results.",impact-revealing,highlighting limitations of existing methods in intrathoracic definition
275,5aed14d617c44a4438159040,ebc96892b9bcbf007be9a1d7844e4b09fde9d961,YOLOv3: An Incremental Improvement,58d82fc8d649053542fd5862,Feature Pyramid Networks for Object Detection,Our sys-tem extracts features from those scales using a similar concept to feature pyramid networks [8].,impact-revealing,describing the feature extraction method used
737,5fc75d8591e0114897921043,b62edbf6e619eeed886c63e51fdff2c3d94f998f,graph convolutions that can finally model local structure,5e427c903a55acbff4c40b1d,Can graph neural networks count substructures?,", 2019; 2018) or relational pooling (Murphy et al., 2019; Chen et al., 2020).###Although such network could in principle have a strong discriminative power with enough layers, it was shown that practical networks can have trouble solving even basic structure related tasks, such as detecting small cycles(Loukas, 2019; Chen et al., 2020).###A few works are fundamentally diverging from the standard convolution based framework, such as invariant graph networks (Maron et al., 2019; 2018) or relational pooling (Murphy et al., 2019; Chen et al., 2020).###This problem has been investigated by many recent works (Chen et al., 2020; Nikolentzos et al., 2020; Abu-El-Haija et al., 2019; Loukas, 2019; Fey et al., 2020).###Despite quick progress in the last few years, recent studies have shown that modern graph neural networks can still fail at simple tasks, such as detecting small cycles (Loukas, 2019; Chen et al., 2020).",impact-revealing,highlighting challenges faced by modern graph neural networks in basic tasks
2828,5c8ddce94895d9cbc6a97820,8fb5a6fe93a3d19ffee50677c0ae563e3377d2d4,Chinese NER Using Lattice LSTM,5c8a87a74895d9cbc631d733,Cross-Sentence N-ary Relation Extraction with Graph LSTMs,", 2017), dependencydiscourse DAGs (Peng et al., 2017), as well as speech tokenization lattice (Sperber et al.###Lattice structured RNNs can be viewed as a natural extension of tree-structured RNNs (Tai et al., 2015) to DAGs.###They have been used to model motion dynamics (Sun et al., 2017), dependencydiscourse DAGs (Peng et al., 2017), as well as speech tokenization lattice (Sperber et al., 2017) and multi-granularity segmentation outputs (Su et al., 2017) for NMT encoders.",other,acknowledge various applications of lattice structured RNNs
2506,5feb068b91e011f5d3420813,e339c5d31ffc7029c1f72d567ac07b4606701c72,ALP-KD: Attention-Based Layer Projection for Knowledge Distillation,573697556e3b12023e63e99f,The Fifth PASCAL Recognizing Textual Entailment Challenge.,"(Dagan, Glickman, and Magnini 2005; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3431,5e09a76bdf1a9c0c41677a7b,71bd6b3a2bfa54ee4ea8499be0c4ff478fd735f6,POLAR++: Active One-Shot Personalized Article Recommendation,53e9bd55b7602d97049e9ec9,Audio Visual Person Authentication By Multiple Nearest Neighbor Classifiers,"For example, in the following text [59]: Example 1.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1918,,9b85fd2438731d76660b0583411461c51e792abe,Principles of antiracist social emotional justice learning,,,"###Antiracist social emotional justice learning (ASEJL)
As previously mentioned, SEL wasn’t developed in a vacuum, but rather in response to the “social and cultural gap between home and school” (Comer, 1988, p. 43).###Below we build on the beginnings of SEL (Comer, 1988) to expand toward antiracist social###More specifically, Comer stated that social and emotional learning, in this way, allowed for school bonding to occur which ultimately fosters positive interactions among educational stakeholders and promoted greater academic success (Comer, 1988).###promoted greater academic success (Comer, 1988).###Below we build on the beginnings of SEL (Comer, 1988) to expand toward antiracist social emotional justice learning (ASEJL) through the discussion of foundational principles that are critical in antiracist work.###…on fostering positive interaction between parents and school staff, which he did through the creation of a School Advisory Council (SAC), composed of a representative governance and management team that included parents, administrators, teachers, and mental-health specialists (Comer, 1985, 1988).###Principle 3: Student & family voice
One of the main principles Comer (1988) reinforced through his SEL approach to education was how the attitudes, values, and behavior of the family affected the development of the child.",impact-revealing,building on foundational principles of social and emotional learning to expand toward antiracist education
561,5550443b45ce0a409eb4c3b9,0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f,Towards End-To-End Speech Recognition with Recurrent Neural Networks,53e99d81b7602d970263e20c,From Speech To Letters - Using A Novel Neural Network Architecture For Grapheme Based Asr,"The combination of bidirectional LSTM and CTC has been applied to character-level speech recognition before (Eyben et al., 2009), however the relatively shallow architecture used in that work did not deliver compelling results (the best character error rate was almost 20%).",impact-revealing,highlighting limitations in previous character-level speech recognition approaches
3898,5e3a93a93a55ac06c6119df5,cad9e682ddec3b1dd532cb8301737109d9eda7d7,Collaborative Distillation for Top-N Recommendation,5f817807c6c3b86a50617c2e,Binarized Neural Networks,"First, [30], [31] proposed the binary encoding of model parameters.",other,reporting prior findings on binary encoding of model parameters
1752,,9cfc07744ed54cd5114422922a82c41e470e5bca,Optimized Spatial Architecture Mapping Flow for Transformer Accelerators,,,"###Both traditional fixed dataflow accelerators (like ShiDianNao [15], NVDLA [2], Eyeriss [10], and TPU [17]) and flexible dataflow accelerators (like MAERI [24] and GAMMA [18]) did not consider operator fusion opportunities.",impact-revealing,highlighting a gap in existing dataflow accelerator designs
1754,,0e950f528b318a42d714cac2d9bf57a75ac74aba,Row-wise Accelerator for Vision Transformer,,,"###However, existing deep learning hardware accelerators [7]–[9] are optimized for CNN based models, which is not suitable for vision transformer models due to the significant difference in the favored architecture structure and parameters.###They exploit the high parallelism of CNN computation with hundreds of PEs for massive computation, and reuse input/weights/activation in different ways to reduce the external memory bandwidth requirement [7], [8], [11], [12].",impact-revealing,highlighting the limitations of existing hardware accelerators for vision transformer models
3635,5ce3aebeced107d4c65ebaaf,7223fb56eeece5806f8d25718bbc78386e17f19f,SinGAN-GIF: Learning a Generative Video Model from a Single GIF,5b3d98d617c44a510f8023a9,Non-Stationary Texture Synthesis by Adversarial Expansion.,"[29, 40] successfully trained such models for speciﬁc tasks like super-resolution, texture expansion, etc. InGAN [28] used a generative model to learn the internal distribution of image patches using a multi-scale hierarchical discriminator, but focused mainly on changing aspect ratio.",other,acknowledge existing methods and their specific applications
2693,5dcbd5da3a55ac789b0dbbdd,d6b414487787d0b6efd735a3236a690ad13aae70,TENER: Adapting Transformer Encoder for Named Entity Recognition,5736960c6e3b12023e51f0a0,End-To-End Sequence Labeling Via Bi-Directional Lstm-Cnns-Crf,"To alleviate the problems of data sparsity and outof-vocabulary (OOV), most NER models adopted the CNN character encoder (Ma and Hovy, 2016; Ye and Ling, 2018; Chen et al., 2019) to represent words.###Owing to BiLSTM’s high power to learn the contextual representation of words, it has been adopted by the majority of NER models as the encoder (Ma and Hovy, 2016; Lample et al., 2016; Zhang et al., 2018; Gui et al., 2019b).###The previous work has proved that character encoder is necessary to capture the character-level features and alleviate the out-of-vocabulary (OOV) problem (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Xin et al., 2018).###, 2019), it still lags behind the BiLSTM-based models (Ma and Hovy, 2016).###Since then, the BiLSTM has been extensively used in the field of NER (Chiu and Nichols, 2016; Dong et al., 2016; Yang et al., 2018; Ma and Hovy, 2016).###, 2011), various neural models have been introduced to avoid hand-crafted features (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016).",other,acknowledge the necessity of character encoders and the dominance of BiLSTM in NER models
929,5e85c28491e0114016e821dc,2db6b9621862b4293c1433c91ee8fb45e394f376,DeepGS: Deep Representation Learning of Graphs and Sequences for Drug-Target Binding Affinity Prediction,53e9b755b7602d97042f928b,Genome scale enzyme-metabolite and drug-target interaction predictions using the signature molecular descriptor.,"Meanwhile, traditional machine learning-based methods apply various features [32, 8] and descriptors [7, 13, 6], and simply depend on the similarities between drugtarget pairs [35, 40, 19].",impact-revealing,acknowledging existing methods in traditional machine learning
1652,,fed42d5486523cbe31e4ae871e9cf97fccea1d93,Which side are you on? Insider-Outsider classification in conspiracy-theoretic social media,,,"###different categories is inspired by social categorization, identification and comparison in the wellestablished Social Identity Theory (SIT) (Tajfel et al., 1979; Tajfel, 1974) and rests on established perspectives from Narrative Theory (Dundes, 1962; Labov and Waletzky, 1967; Nicolaisen, 1987).###…group. different categories is inspired by social categorization, identiﬁcation and comparison in the well-established Social Identity Theory (SIT) (Tajfel et al., 1979; Tajfel, 1974) and rests on established perspectives from Narrative Theory (Dundes, 1962; Labov and Waletzky, 1967; Nicolaisen,…###For our purposes, these agents need not have full agency : Diseases and natural disasters, for example, would be universal outsiders, and any man-made object/policy that works against the Insider s would be included in this group. different categories is inspired by social categorization, identiﬁcation and comparison in the well-established Social Identity Theory (SIT) (Tajfel et al., 1979; Tajfel, 1974) and rests on established perspectives from Narrative Theory (Dundes, 1962; Labov and Waletzky, 1967; Nicolaisen, 1987).",impact-revealing,drawing on established theories to support categorization approach
3133,5aed147c17c44a4438153ea5,665c0dde22c2f8598869d690d59c9b6d84b07c01,domino temporal data prefetcher,53e9b5edb7602d9704140027,Temporal Streaming of Shared Memory,"While it has been shown that simple prefetching techniques, such as stride prefetching [3], [4], [5], are ineffective for server workloads [1], [6], more advanced data prefetchers may eliminate or reduce the negative effect of data misses.###One of the promising prefetching techniques is temporal prefetching [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17].",other,highlighting the effectiveness of advanced prefetching techniques
3327,5ecbc8889fced0a24b51eb0e,57c38661af2d1ac5ac79cc51a443f5f1cca4b03b,single shot video object detector,5aed14d617c44a44381591ff,Object Detection in Video with Spatiotemporal Sampling Networks,"One common solution for video object detection is box-level tracking [15]–[18] and another branch is feature-level aggregation [19]– [24].###STSN leverages spatio-temporal sampling instead of motion calibration for feature aggregation, which achieves comparable performance with FGFA under the same backbone of Deformable Convolution Network [61] (DCN).###Compared to [19] that adopts four stacked deformable convolution###The latter three baselines ( i.e. , FGFA [24], MANet [21], and STSN [19]) further boost the performance for video object detection by enhancing per-frame feature.###The research on object detection in videos has proceeded along two different directions: box-level tracking [15]–[17] and feature-level aggregation [19], [22], [24].###, FGFA [24], MANet [21], and STSN [19]) further boost the performance for video object detection by enhancing per-frame###Instead of estimating the motion across frames for warping the feature map, STSN [19] per-###R-FCN[8] ResNet-101 73.6 DorT[43] ResNet-101 73.9 Faster R-CNN[13] ResNet-101 75.4 D(& Tloss)[15] ResNet-101 75.8 FGFA[24] ResNet-101 76.3 LWDN[64] ResNet-101 76.3 PSLA[65] ResNet-101 77.1 MANet[21] ResNet-101 78.1 THP[23] ResNet-101+DCN 78.6 STSN[19] ResNet-101+DCN 78.9 in each frame unexploited.###Instead of estimating the motion across frames for warping the feature map, STSN [19] performs object detection in a frame by learning to spatially sample features from adjacent frames for aggregation.###TPN+LSTM [17] GoogLeNet 68.4 TCNN[41] DeepID+Craft 73.8 FGFA (cid:52) [24] ResNet-101 78.4 D&T( τ = 1 ) [15] ResNet-101 79.8 STSN (cid:52) [19] ResNet-101+DCN 80.4 STMN [22] ResNet-101 80.5 HQ-link[44] ResNet-101 80. consistently demonstrate that our proposed SSVD by integrating two-stream feature aggregation into one-stage detection paradigm exhibits better performance than all the six two-stage end-to-end models.###Technically, both of FGFA and STSN adopt Seq-NMS as post-processing.",other,describing different approaches in video object detection
674,5f86cae991e011dbc7eba2fa,5e9bb0f3e74be4d8f75cca6ceb3ec87b3e04d7cc,piuma: programmable integrated unified memory architecture,57a4e91aac44365e35c97b34,"Novel graph processor architecture, prototype system, and results","[11] propose a graph processor based on sparse matrix algebra, building on the observation that many graph applications can be represented as operations on sparse matrices.",impact-revealing,reporting prior findings on graph processing methods
1844,,d0492f5958c1c42e08322c172c539a60af78cd64,Models for name-passing processes: interleaving and causal,,,###[44]) systems are described by disjunctions of predicates specifying when a given trace can be extended by a particular label.,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2709,5d0b003a8607575390fb4f6a,43d74cd04fb22bbe61d650861766528e369e08cc,An Encoding Strategy Based Word-Character LSTM for Chinese NER,53e99f4fb7602d9702822089,Named Entity Recognition using Support Vector Machine: A Language Independent Approach,"Various methods have been proposed to tackle this problem, including Hidden Markov Models(HMMs) (Saito and Nagata, 2003), Maximum Entropy Models(ME) (Chieu and Ng, 2003), Support Vector Machines(SVM) (Ekbal and Bandyopadhyay, 2010) and Conditional Random Fields(CRF) (Feng et al., 2006).",other,acknowledge various methods proposed for the problem
2484,5cf48a48da56291d582ab75a,c5f5f179d80a3bf9b4f29750283a87eaca42e91b,neural graph collaborative filtering,5bbacb6117c44aecc4ead3d3,HOP-rec: high-order proximity for implicit recommendation.,"Lastly, it is worth mentioning that although the high-order connectivity information has been considered in a very recent method named HOP-Rec [40], it is only exploited to enrich the training data.###To evaluate the effectiveness of top-K recommendation and preference ranking, we adopt two widely-used evaluation protocols [14, 40]: recall@K and ndcg@K3.###• HOP-Rec [40]: This is a state-of-the-art graph-based model,###The recently proposed method HOP-Rec [40] alleviates the problem by combining graph-based with embedding-based method.###Another line of research [12, 24, 40] exploits the user-item interaction graph to infer user preference.###We conduct extensive experiments on three public benchmarks, demonstrating significant improvements over several state-of-the-art models like HOPRec [40] and Collaborative Memory Network [5].",other,highlighting the effectiveness of the HOP-Rec method and its improvements over existing models
2229,5b67b46f17c44aac1c86329e,ef5cb5d49716bcd754b971a9a7303d7da2cd1036,From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks,57d063b4ac4436735428dc08,Evaluating Search Result Diversity using Intent Hierarchies.,"See also [2, 4, 7, 9– 11, 22, 29] Machine learning techniques have been applied to construct diverse ranking, also adopting the greedy sequential decision making as the basic framework.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3633,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,53e9ab4fb7602d97034e363c,Manifold learning visualization of network traffic data.,"2004 [86] Network Aggregate based on port, chart and graph Security event detection 2005 [30] Network IP aggregation of traffic bursts, chart & graph Worm detection and backbone monitoring 2005 [103] Network Manifold learning, chart Monitoring, detection 2006 [100] Individual Extended the quad-tree,3D navigation and playback Internet traffic",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1888,,3d5fd246ce3f1e9d6d04036a42a9e967e48cc50f,The Use of Community of Inquiry Framework-Informed Facebook Discussion Activities on Student Speaking Performances in a Blended EFL Class,,,"###The Community of Inquiry (CoI) framework (Garrison et al., 2000) is widely used by researchers and instructors to understand the online learning environment (Garrison et al., 2010), yet only a few studies have investigated the effectiveness of using the CoI framework to guide the use of Facebook…###Cognitive presence is operationalized through the Practical Inquiry model, which has four phases: triggering event (e.g., sense of puzzlement), exploration (e.g., information exchange), integration (e.g., connecting ideas), and resolution (e.g., apply new ideas) (Garrison et al., 2000).###…of three elements: how instructors or/and instructional designers design the learning environment, how instructors and/or students facilitate the learning activities in that environment, and how instructors and/or students provide direct instruction in that environment (Garrison et al., 2000).",impact-revealing,acknowledge the use and effectiveness of the Community of Inquiry framework in online learning
2677,5cede10fda562983788ef75c,2a69ddbafb23c63e5e22401664bea229daaeb7d6,Res2Net: A New Multi-Scale Backbone Architecture,5cede104da562983788e4508,Deep High-Resolution Representation Learning For Human Pose Estimation,"One common operation in [5], [9], [11], [48], [49] is that they all use pooling or up-sample to re-size the feature map to 2 times of the original scale to save the computational budget while maintaining or even improving performance.###Other than the low-resolution representations in current works, the HRNet [48], [49] introduces high-resolution representations in the network and repeatedly performs multiscale fusions to strengthen high-resolution representations.",other,acknowledge existing methods and their operations
1157,,6ac2b7c9a52e5190108e1749fc3c498285e3999b,Model misspecification and bias for inverse probability weighting estimators of average causal effects,,,"###The design is inspired by the simulation study of Funk et al. (2011). We generate the same covariates (X1, X2, X3, X4) where X1 ∼ Normal(0,1), X2 ∼ Normal(0,1), X3 ∼ Uniform(0.",impact-revealing,drawing inspiration from a previous simulation study
1496,,3db908df1f1cb5ef4f949e3074dad65e10388624,Semantic-Preserving Image Coding Based on Conditional Diffusion Models,,,"###As stated before, the idea builds on the very foundation of DDPM and, more specifically, on the concept of SR Diffusion Models [20], properly modified to guarantee the dual conditioning used in our strategy.",impact-revealing,providing context on the foundation of the proposed idea
1112,,7d926460e07342e0a4578f95c256610513decf65,Model-Based Diffusion for Trajectory Optimization,,,"###Diffusion models have been widely adopted as generative models for high-dimensional data, such as image [41], audio [9], and text [4] through iterative refinement processes [40, 20].",impact-revealing,highlighting the application of diffusion models across various data types
2367,5c6a37d03a69b1c9e12a9fc4,81b6d24e8f313fd88b0fe5ff6c21dd154fbe32d2,SimGNN: A Neural Network Approach to Fast Graph Similarity Computation,5843777eac44360f10841775,Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement.,"An analogy is that, in Natural Language Processing, the performance of sentence matching based on one embedding per sentence can be further enhanced through using fine-grained word-level information [19, 21].",other,providing context for enhancing sentence matching performance
559,5ce3af9aced107d4c65f6b25,f2bb7e2f5a1afad5370159c15760c44df93c0438,Very Deep Self-Attention Networks for End-to-End Speech Recognition,573696046e3b12023e517573,Deep Networks With Stochastic Depth,"Motivated by the previous work of [10], we propose to apply stochastic residual layers into our Transform-ers.###• As suggested by [10], the lower layers of the networks handle raw-level acoustic features on the encoder side, and the character embeddings on the decoder side.###Second, in order to facilitate training of very deep conﬁgurations, we propose a variation of stochastic depth for the Transformer inspired by the Stochastic Residual Network for image classiﬁcation [10].",impact-revealing,proposing a new method inspired by previous work
1959,,949286f7bf66bafdd3f1e65b4960ea80b7f50154,Iridium oxide reference electrodes for neurochemical sensing with MEMS microelectrode arrays,,,"###IrOx electrodes are especially attractive for use on neurochemical sensing probes, as IrOx is commonly used as the electrode material for stimulation on microfabricated neural probes due to its high charge injection capabilities [10]–[15].",impact-revealing,highlighting the advantages of IrOx electrodes in neurochemical sensing
171,5cf48a2cda56291d5828e868,c42816f497d663c681df20d48a6e66a5632600d8,Mixmatch: A holistic approach to semi-supervised learning,58437722ac44360f1082f13a,Temporal Ensembling for Semi-Supervised Learning.,"Unlike past work using MixUp for SSL [43, 44, 18], we “mix” labeled examples with unlabeled examples and vice versa which we ﬁnd results in improved performance (section 4.2.3).###As with CIFAR-10, we evaluate the performance of each SSL method on SVHN with a varying number of labels from 250 to 4000 .###To set the stage for MixMatch , we ﬁrst introduce existing methods for SSL.###Semi-supervised learning [6] (SSL) seeks to largely alleviate the need for labeled data by allowing a model to leverage unlabeled data.###As is typical in many SSL methods, we use data augmentation both on labeled and unlabeled data.###STL-10 is a dataset speciﬁcally designed for SSL, with 5,000 labeled images and 100,000 unlabeled images which are drawn from a slightly different distribution than the labeled data.###MixMatch is a “holistic” approach which incorporates ideas and components from the dominant paradigms for SSL discussed in section 2.###We do not propagate gradients through computing the guessed labels, as is standard [25, 44, 31, 35]###For this reason, it is often used as the unlabeled data loss in SSL [25, 44] as well as a measure of predictive uncertainty [26].###In this paper, we introduce MixMatch , an SSL algorithm which introduces a single loss that gracefully uniﬁes these dominant approaches to semi-supervised learning.###In the simplest case, for unlabeled points x, prior work [25, 40] adds the loss term ‖pmodel(y | Augment(x); θ)− pmodel(y | Augment(x); θ)‖ 2 2.###MixUp is designed as a regularizer for supervised learning, so we modify it for SSL by applying it both to labeled examples (mixing pairs ( x 1 , p 1 ) and ( x 1 , p 2 ) from X ) and unlabeled examples (mixing pairs of ( u 1 , p model ( y | u 1 , θ )) and ( u 2 , p model ( y | u 2 , θ )) and using the result as a guessed label).###Using data augmentation to obtain an artificial target for an unlabeled example is common in consistency regularization methods [25, 40, 44].###As baselines, we consider the four methods considered in [35] (Π-Model [25, 40], Mean Teacher [44], Virtual Adversarial Training [31], and Pseudo-Label [28]) which are described in section 2.###We focus mainly on those which are currently state-of-the-art and that MixMatch builds on; there is a wide literature on SSL techniques that we do not discuss here (e.g., “transductive” models [13, 22, 21], graph-based methods [48, 4], generative modeling [3, 27, 39, 9, 16, 23, 36, 32, 40], etc.).",impact-revealing,introducing a new semi-supervised learning algorithm and its improvements over existing methods
1879,,44efa92c217847e852040bcaeacc3be4355767a4,Therapeutic Use of the Ketogenic Diet in Refractory Epilepsy: What We Know and What Still Needs to Be Learned,,,"###Although the brain, unlike any other organ in the body, has an absolute minimum requirement of glucose, when ketone bodies concentration in the blood attain the range of 2–4 mM, these can satisfy as much as 60% of the brain’s energy requirements [13].###Ketosis appears to be a natural adaptive mechanism which has been instrumental in the biological survival of humanity and, perhaps, essential for the evolution of the human brain [13,18].",impact-revealing,highlighting the significance of ketone bodies in brain energy requirements and evolution
1224,,f4b495f3dbde88dff9412fa4a2b8e5a7ce2c45ab,levels in,,,"###Child and adolescent obesity is a signiﬁcant health problem worldwide (Flodmark et al., 2004; Gomes et al., 2014) reaching epidemic proportions both in developed (Ogden et al., 2002; Gomes et al., 2014) and less developed countries (Ebbeling et al., 2002).###Although BMI has been commonly used as a sensible indicator of overall adiposity in children (Ogden et al., 2002), it seems that BMI has limitations in assessing the prevalence of age-and sex-speciﬁed obesity as well as body composition and fat distribution (Savva et al., 2000).###To deﬁne underweight, overweight and obese according to BMI, the 5th, 85th and 95th BMI for age, the Centers for Disease Control and Prevention (CDC) reference percentiles were used (Ogden et al., 2002; Goncalves et al., 2015).",impact-revealing,highlighting the significance of child and adolescent obesity as a global health issue and discussing BMI limitations
1924,,eaf10ff2145de95e3272d4a57f2ddb3b24b033b6,Body shape-based biometric recognition using millimeter wave images,,,"###In the biometric field, it was first used for signature verification [8], [6].###This is inspired by previous works, which show that recognition through the shape and boundary of traits such as hand or signature are fairly reliable [5], [6].",impact-revealing,highlighting the application of biometric methods in signature verification
3653,5ecb57199e795ec6f2ba59cc,512f34906ddaefe885af2e5eec9b2b3b50ffd377,deep entity matching with pre-trained language models,5736974d6e3b12023e63866b,A Neural Attention Model for Abstractive Sentence Summarization,"There are many ways to perform summarization [25, 33, 35].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1384,,66595e2119a7f12d0010e41d17b2959cae0d237c,Rotated test problems for assessing the performance of multi-objective optimization algorithms,,,"###Previous work in [12] demonstrated the importance of evaluating single objective Evolutionary Algorithms on rotated problems in order to test their rotationally invariant behaviour, the contention being that evaluations of EAs should be independent of any coordinate system.###In [12] a strong case is presented for all EA evaluations to be independent of a particular coordinate system, and our contention is that this should equally be true for the evaluation of EMO algorithms.###On these types of problems, the small mutation rates frequently used in Genetic Algorithms are known to be even less efficient than a random search [12].###Problem R4 is based on the Schwefel function [12], where a local front is located far from the global minimum.",impact-revealing,highlighting the importance of evaluating algorithms independently of coordinate systems
1425,,f204041dd567025217adc8070ca292e89cc80488,COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning,,,"###We instead utilize the conservative Q-learning (CQL) [3] algorithm that additionally penalizes Q-values on out-of-distribution actions during training.###There has been a significant amount of recent interest in devising methods for offline RL [1, 2, 3, 4, 5, 6], which can leverage previously collected datasets without environment interaction.###These works largely focuses on developing more effective offline deep RL algorithms by correcting for distribution shift [41, 3, 4, 38, 39, 40] which renders standard off-policy RL algorithms inadmissible in purely offline settings [4, 1].###Our method builds on a recently proposed model-free offline RL method called conservative Q-learning (CQL) [3].###While offline (or “batch”) RL is a well-studied area [35, 36, 37, 1], there has been a significant amount of recent interest in offline deep RL, where deep RL agents are trained on a static, previously collected dataset without any environment interaction [1, 3, 2, 4, 5, 38, 39, 40].",impact-revealing,highlighting the significance of conservative Q-learning in offline reinforcement learning
1175,,d064f7beca4108dab106521812aa28d1b9ec9f40,Coverage Path Planning for Surveying Disjoint Areas,,,"###Several variations of the BCD have been proposed.###Choset and Pignon addressed the problem of covering polynomial wolds [6].###The average saving with respect to the BCD approach is 5.6% and with respect to the GA approach it is 4.4%.###The compared methods are the following:
• Boustrophedon cellular decomposition (BCD) [6].###In this paper, we perform a comparison against the popular approach of BCD. Huang’s method for coverage path planning [2] was motivated by a robot performing demining operations.###We can observe that the BCD approach computes the worst paths given that the orientation of the back and forth patterns are fixed.###Xu et al. [9] reuse the BCD but optimizes the path by solving a Chinese postman problem instead of a TSP.
Torres et al. [3] propose a method based on the same coverage optimality for a polygon defined by Huang [2] but optimizes the path between target regions.###• Boustrophedon cellular decomposition (BCD) [6].###Their method, called Boustrophedon cellular decomposition (BCD), performs a line sweep decomposition of the world; such decomposition divides the world into cells.###On the other hand, see in table II that the computation time for the TSPP strategy it is almost the same that the BCD (in all cases it is less than a second) and it is quite smaller than the GA approach.###mx with previous problems where the vehicle can only move to the adjacent areas [6].###However, in all the BCD approaches the movement is always done in the same direction despite of the area shape, this could lead to an inefficient path by increasing the number of turns.",impact-revealing,acknowledge variations in BCD approaches and their comparisons
3189,599c7968601a182cd2639c4c,1a0912bb76777469295bb2c059faee907e7f3258,Mask R-CNN,5736986c6e3b12023e73037b,Spatial transformer networks,"We use bilinear interpolation [22] to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and aggregate the result (using max or average).###So even though RoIWarp also adopts bilinear resampling motivated by [22], it performs on par with RoIPool as shown by experiments (more details in Table 2c), demonstrating the crucial role of alignment.",other,providing context on bilinear interpolation and its application in RoI bin processing
1990,,424f0c1382c59ce2b8960d0948ea46bf8ffe0146,Floods control the influence of environmental gradients on the diversity of zooplankton communities in a neotropical floodplain,,,"###In addition, recent human-induced processes of environmental change have raised concerns about the future of biodiversity (Smith and May 1993; Pimm et al. 1995; Luck and Daily 2003; Sax 2003), especially if this change affects the natural patterns of environmental variation (Palmer et al. 1997;…",impact-revealing,highlighting concerns about biodiversity due to human-induced environmental changes
2296,5b1642388fbcbf6e5a9b5740,3913d2e0a51657a5fe11305b1bcc8bf3624471c0,learning structured representation for text classification via reinforcement learning,599c7987601a182cd2648444,A Structured Self-attentive Sentence Embedding.,"• Self-Attentive: Structured Self-Attentive model, a selfattention mechanism and a special regularization term are used to construct sentence embedding (Lin et al. 2017).###Attention-based methods (Yang et al. 2016; Zhou, Wan, and Xiao 2016; Lin et al. 2017) use attention mechanisms to build representations by scoring input words or sentences differentially.",other,providing context on self-attentive models and their applications
447,5ac1829d17c44a1fda9180a3,632d25ef7914ce962d258920460a9405b8c4553a,Sequence-Based Multi-Lingual Low Resource Speech Recognition,558be27ee4b02b9f07a3d209,Multilingual training of deep neural networks,"With the on-set of deep learning the focus of the models shifted to learning features across languages which can be mapped to the same space [3, 11].",impact-revealing,highlighting the shift in focus of models with the advent of deep learning
586,5dd6604a3a55ac78684acf68,af54c890ffce96bae303310182be2ca301f2f97e,On Using SpecAugment for End-to-End Speech Translation,573697016e3b12023e5fb275,Audio Augmentation For Speech Recognition,"Inspired by the success of augmentation methods in ASR [16, 17], as a remedy to avoid overfitting while using lowresource translated speech data, we study the use of spectrogram augmentation (SpecAugment) for direct ST model.",impact-revealing,motivating the use of augmentation methods in speech translation
2608,5db6c73a3a55acec0731cd68,73a5605ce482bd639078ebbb19baac7b903017e2,A Unified MRC Framework for Named Entity Recognition,5a260c2817c44a4ba8a2370a,Fast and Accurate Entity Recognition with Iterated Dilated Convolutions,"To separate the influence from large-scale BERT pretraining, we compare the LSTM-CRF tagging model (Strubell et al., 2017) with other MRC based models such as QAnet (Yu et al.###To separate the inﬂuence from large-scale BERT pretraining, we compare the LSTM-CRF tagging model (Strubell et al., 2017) with other MRC based models such as QAnet (Yu et al., 2018) and BiDAF (Seo et al., 2017), which do not rely on large-scale pretraining.",other,comparing the influence of BERT pretraining on different models
2644,5db929e947c8f766461fd005,11ead744b2edbdabe174258a8dfe4023be8e501d,Hierarchical Attention Prototypical Networks for Few-Shot Text Classification,5c8e26614895d9cbc6ca426a,Diverse Few-Shot Text Classification with Multiple Metrics.,"Recently, Yu et al. (2018) propose an adaptive metric learning approach that automatically determines the best weighted combination from a set of metrics obtained from meta-training tasks for a newly seen few-shot task such as intention classiﬁcation, Han et al. (2018) present a relation…",other,reporting prior findings on adaptive metric learning approaches
189,573696f46e3b12023e5f1198,7ffdbc358b63378f07311e883dddacc9faeeaf4b,Fast R-CNN,556f622a2401b4b38c23635c,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,"Yet it achieves a mAP of 66.9%, which is slightly higher than the 66.0% reported for traditional R-CNN [8], even though R-CNN uses “inﬁnite” scales in the sense that each object proposal is warped to a canonical size.",impact-revealing,comparing performance metrics of different models
1893,,f5a947d290383dd9d9157fce1e223ba17747ca89,A Construct Revalidation of the Community of Inquiry Survey: Empirical Evidence for a General Factor Under a Bifactor Structure,,,"###Here, H = .778 for TP, .819 for SP, .478 for CP, and .988 for the general factor.###Swan et al. (2008) and Garrison et al. (2010) both conducted a PCA of the CoI responses (𝑛𝑛 = 287 master’s and doctoral students and 𝑛𝑛 = 205 master’s students, respectively) with an oblimin rotation; the results supported a three-factor solution congruent with Garrison et al. (2000).###Such an inadequacy was unfortunate because the literature has repeatedly emphasized the importance of the interaction of all three presences which represents an online learning or educational experience (Caskurlu, 2018; Diaz et al., 2010; Garrison et al., 2000; Garrison & Arbaugh, 2007; Garrison et al., 2010; Kozan, 2016; Kozan & Richardson, 2014; Olpak & Kiliç Çakmak, 2016; Swan & Ice, 2010).###When operationalizing CP, Garrison et al. (2000) used the practical inquiry (PI) model reflecting the critical thinking process for creating CP (Olpak & Kiliç Çakmak, 2016).###24 There has been a growing recognition of the importance of TP for successful online teaching and learning, especially when critical thinking and discourse were required (Garrison et al., 2000).###The factor shed light on the general factor in the bifactor model which documented a general CoI construct measuring students’ online educational experience (Garrison et al., 2000; Reise, 2012).###It referred to the extent to which students in a community of inquiry were able to construct meaning through sustained communication and reflected the process of inquiry and learning (Bangert, 2009; Garrison et al., 2000).###Finally, each presence was further conceptualized to be multidimensional with multiple categories represented by multiple indicators (Caskurlu, 2018; Garrison et al., 2000; Olpak & Kiliç Çakmak, 2016).###Here, the subscale 𝜔𝜔′𝑠𝑠 were .984 for TP, .966 for SP, and .984 for CP, suggesting that 96.6% to 98.4% of the subscale total score variance was due to the general factor plus the domain factor.###The community of inquiry (CoI) theoretical framework was first laid out by Garrison et al. (2000).###SP played an important role in creating an online learning environment that encouraged critical thinking (Bangert, 2009; Garrison et al., 2000).###There has been a growing recognition of the importance of TP for successful online teaching and learning, especially when critical thinking and discourse were required (Garrison et al., 2000).###…of the interaction of all three presences which represents an online learning or educational experience (Caskurlu, 2018; Diaz et al., 2010; Garrison et al., 2000; Garrison & Arbaugh, 2007; Garrison et al., 2010; Kozan, 2016; Kozan & Richardson, 2014; Olpak & Kiliç Çakmak, 2016; Swan &…###A primary limitation has been that the correlated-factor model most studies have universally relied on cannot fully describe the CoI framework by Garrison et al. (2000).",impact-revealing,highlighting the importance of the community of inquiry framework in online education
917,5ecc763e9e795e81e9307559,270f3bea8ca801870a6cc56b4d36f7f2019c9ed0,mpnet: masked and permuted pre-training for language understanding,5d1eb9d4da562961f0b0e960,XLNet: Generalized Autoregressive Pretraining for Language Understanding.,"…listed re-sults are reported in BERT BASE setting and from MNLI QNLI QQP RTE SST MRPC CoLA STS Avg Single model on dev set BERT (Devlin et al., 2019) 84.5 91.7 91.3 68.6 93.2 87.3 58.9 89.5 83.1 XLNet (Yang et al., 2019) 86.8 91.7 91.4 74.0 94.7 88.2 60.2 89.5 84.5 RoBERTa (Liu et al., 2019a) 87###To improve BERT, XLNet (Yang et al., 2019) introduces permuted language modeling (PLM) for pre-training to capture the dependency among the predicted tokens.###For language understanding, masked language modeling (MLM) in BERT [2] and permuted language modeling (PLM) in XLNet [5] are two representative objectives.###For example, when predicting token x z 5 = x 6 , the query stream in the original two-stream attention (Yang et al., 2019) takes mask token M z 5 = [ M ] and position p z 5 = p 6 as the attention query, and can only see previous to-kens x z   5 = ( x 1 , x 3 , x 5 , x 4 ) and positions p z < 5 = (…###We assume all the three objectives mask and predict the same amount of tokens ( 15% ), following the common practice in BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) 5 .###Pre-training language models [1, 2, 3, 4, 5, 6, 7, 8] have greatly boosted the accuracy of NLP tasks in the past years.###On the dev set of GLUE tasks, MPNet outperforms BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019a) by 4.6, 3.2, 1.3 points on average.###In practice, only a part of last tokens xz>c (usually c = 85% ∗ n) are chosen to predict and the remaining tokens are used as condition in order to reduce the optimization difficulty [5].###PLM in XLNet Permuted language model (PLM) is proposed in XLNet [5] to retain the benefits of autoregressive modeling and also allow models to capture bidirectional context.###MLM pre-trains the model θ by maximizing the following objective PLM in XLNet Permuted language model (PLM) is proposed in XLNet (Yang et al., 2019) to retain the beneﬁts of autoregressive modeling and also allow models to capture bidirectional context.###We pre-train MPNet on a large-scale text corpora (over 160GB data) following the practice in [5, 7], and fine-tune on a variety of down-streaming benchmark tasks, including GLUE, SQuAD, RACE and IMDB.###The key of pre-training methods [1, 2, 4, 5, 10] is the design of self-supervised tasks/objectives for model training to exploit large language corpora for language understanding and generation.###Pre-training models (Radford et al., 2018; Devlin et al., 2019; Radford et al., 2019b; Song et al., 2019; Yang et al., 2019; Dong et al., 2019; Liu et al., 2019a; Raffel et al., 2019a) have greatly boosted the accuracy of NLP tasks in the past years.###To this end, we follow PLM to adopt two-stream self-attention (Yang et al., 2019) to autoregressively predict the tokens, which is illustrated in Figure 3.###For language understanding, masked language modeling (MLM) in BERT (Devlin et al., 2019) and permuted language modeling (PLM) in XLNet (Yang et al., 2019) are two representative objectives.###• Output Dependency : As shown in Equation 1, MLM assumes the masked tokens are independent with each other and predicts them separately, which is not sufﬁcient to model the complicated context dependency in natural language (Yang et al., 2019).###Additionally, we also apply whole word mask (Cui et al., 2019) and relative positional embedding (Shaw et al., 2018) 8 in our model pre-training since these tricks have been successfully validated in previous works (Yang et al., 2019; Raf-fel et al., 2019b).###We pre-train our model for 500K steps to be comparable with state-of-the-art models like XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019a) and ELECTRA (Clark et al., 2020).###In practice, only a part of last tokens x z >c (usually c = 85% ∗ n ) are chosen to predict and the remaining tokens are used as condition in order to reduce the optimization difﬁculty (Yang et al., 2019).###Under this unified view, we propose a new pre-training method, masked and permuted language modeling (MPNet for short), which addresses the issues in both MLM and PLM while inherits their advantages: 1) It takes the dependency among the predicted tokens into consideration through permuted language modeling and thus avoids the (1)We do not consider next sentence prediction here since previous works [5, 7, 9] have achieved good results without next sentence prediction.###To improve BERT, XLNet [5] introduces permuted language modeling (PLM) for pre-training to capture the dependency among the predicted tokens.###For more details about two-stream self-attention, please refer to Yang et al. (2019).###We pre-train MPNet on a large-scale text corpora (over 160GB data) following the practice in Yang et al. (2019); Liu et al. (2019a), and ﬁne-tune on a variety of down-streaming benchmark tasks, including GLUE, SQuAD, RACE and IMDB.###• Output Dependency: As shown in Equation 1, MLM assumes the masked tokens are independent with each other and predicts them separately, which is not sufficient to model the complicated context dependency in natural language [5].###For the pre-training objective of MPNet, we randomly permute the sentence following PLM (Yang et al., 2019) 7 , choose the rightmost 15% tokens as the predicted tokens, and prepare mask tokens following the same 8:1:1 replacement strategy in BERT (Devlin et al., 2019).###MLM leverages bidirectional context of masked tokens efficiently, but ignores the dependency among the masked (and to be predicted) tokens [5].###During ﬁne-tuning, we do not use query stream in two-stream self-attention and use the original hiddens to extract context representations following Yang et al. (2019).###The key of pre-training methods (Radford et al., 2018; Devlin et al., 2019; Song et al., 2019; Yang et al., 2019; Clark et al., 2020) is the design of self-supervised tasks/objectives for model training to exploit large language corpora for language understanding and generation.###MLM leverages bidirectional context of masked tokens efﬁciently, but ignores the dependency among the masked (and to be predicted) tokens (Yang et al., 2019).",impact-revealing,describing the development and advantages of a new pre-training method for language models
2845,5e5e189993d709897ce1ddbc,055fd6a9f7293269f1b22c1470e63bd02d8d9500,Reformer: The Efficient Transformer,57a4e921ac44365e35c9927c,One-shot Learning with Memory-Augmented Neural Networks.,The requirement that the memory be ﬁxed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016).,other,acknowledging advancements in memory requirements in neural networks
2254,599c797a601a182cd2641eda,d65ce2b8300541414bfe51d03906fca72e93523c,on calibration of modern neural network,573696d96e3b12023e5d8d36,Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks,"On SST, we train TreeLSTMs (Long Short Term Memory) (Tai et al., 2015).###As described in (Tai et al., 2015), the training/validation/test sets contain 6920/872/1821 documents for binary, and 544/1101/2210 for fine-grained.",other,reporting training and dataset details for TreeLSTMs
1998,,1eb9cce36f1fdd672f2e2f1938e1108d594f810b,Cancelable Biometrics Based on Quaternion Adaptive Filter Optimization for Security Applications,,,"###In their study, Paul et al. (2014) intro duced a multi-order cancelable biometric transform that incor porates a random cross-over technique, random projection, and multi.modal cancelable template selection, including facial and ear biometrics [22].###In their study, Chin et al. (2014) introduced an approach that integrates fingerprint and palmprint templates.###In their study, Chin et al. (2014) introduced a novel approach that integrates fingerprint and palmprint templates.###In their study, Paul et al. (2014) introduced a multi-order cancelable transform that in corporates a random cross-over technique, random projection, and multimodal cancelable template selection, including facial and ear biometrics [22].",impact-revealing,reporting existing findings on biometric transforms and integration methods
319,5f1ff7ea91e011d50a621ab3,39b45b0d60807296db97d9eb37b1fb99d244e2ed,MACU-Net for Semantic Segmentation of,5de0e36ddf1a9c0c415ca22b,Acnet: Strengthening The Kernel Skeletons For Powerful Cnn Via Asymmetric Convolution Blocks,"The importance of positions on the central skeleton of a square convolution kernel outweighs those on the corners [14, 15].###We modify the asymmetric convolutions proposed in [14] and design an asymmetric convolution block (ACB) to capture features from different receptive fields as shown in Fig.###The effectiveness of ACB has been demonstrated in other domains, such as image classification [14], image demoireing [16, 17], and medical image segmentation [18].###without an increase in computational complexity [14].###As reported in [14], a square convolution kernel captures features with uneven proportions.",impact-revealing,highlighting the significance of the asymmetric convolution block in various domains
890,5b67b45517c44aac1c8607aa,d5aefe86b1ba8c773a6bd0e84812ace161b8c0db,large-scale learnable graph convolutional networks,57a4e91dac44365e35c9830c,Learning convolutional neural networks for graphs,"egular convolutional operations to fit them for generic graph data, we instead propose to transform graphs into grid-like data to enable the use of CNNs directly. This idea was previously explored in [20]. However, the transformation in [20] is implemented in the preprocessing process while our method includes the transformation in the networks. Additionally, we introduce a sub-graph training method i",impact-revealing,highlighting a novel approach to graph data processing
1118,,e51384e11dd82bf56aa2044ff2e04437b838c9a9,DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face Reenactment,,,"###In this work, we propose a novel neural face reenact-ment framework, called DiffusionAct, which incorporates the generative power of a pre-trained Denoising Diffusion Implicit Model (DDIM) [53] and the remarkable reconstruction performance of a Diffusion AutoEncoder (Dif-fAE) [46].###Never-theless, as shown in Table 7 in this document, our method, which builds on a DDIM-based method [46, 53], exhibits slower inference time compared to GAN-and StyleGAN2-based methods, as expected.###We then continue training the reenactment encoder E r (main training stage) by incorporating the DDIM sampler [53] as a generative module to synthesize the reenacted images (as illustrated in Fig.###Diffusion Probabilistic Models (DPMs) : DPMs [12, 21, 53] are generative models that learn a data distribution through a denoising process that iteratively transforms noisy samples into the target distribution.###The advent of Generative Adversarial Networks (GANs) [18, 27, 28] and Diffusion Probabilistic Models (DPMs) [21, 49, 53] has led to impressive performance in the tasks of image synthesis and manipulation.###DiffAE utilizes a conditional DDIM [53] that learns to iteratively transform noisy samples into images that resemble a target distribution.",impact-revealing,introducing a novel neural face reenactment framework and comparing it to existing methods
2890,5e5e18de93d709897ce37796,0a6a9e6d4e3efd7c69357769305b70097281655f,DropEdge: Towards Deep Graph Convolutional Networks on Node Classification,53e9a5beb7602d9702ee8434,Minimizing Effective Resistance of a Graph.,", 2008); (2) predicting which community different posts belong to in the Reddit social network (Hamilton et al., 2017). Note that the tasks in Cora, Citeseer and Pubmed are transductive underlying all node features are accessible during training, while the task in Reddit is inductive meaning the testing nodes are unseen for training. We apply the full-supervised training fashion used in Huang et al. (2018) and Chen et al.",other,acknowledge differences in task settings for various datasets
3873,5d1eb9ddda562961f0b17476,c0aaee2337e5af680e5dca1bfc349a737dfec573,Fixing the train-test resolution discrepancy,58437725ac44360f10830286,End-to-End Learning of Deep Visual Representations for Image Retrieval,"Furthermore, advances in image classification translate to improved results on many other tasks [12, 18].",other,highlighting the broader impact of advances in image classification
2096,,51950bb46fbf09384d7d83fe289c53b69f72a36b,Recruits Comparison in a Category-Learning Task,,,"###Explanation can help people detect (Rozenblit & Keil, 2002) and fill gaps in their knowledge (Chi, 2000), as well as resolve inconsistencies (Johnson-Laird, Girotto, & Legrenzi, 2004).",impact-revealing,highlighting the role of explanation in knowledge detection and resolution
3761,5bdc315017c44a1f58a05ce0,4930de1aff4b1948157a15ac9cdb02364bee97bb,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,5d9edc8b47c8f76646043fb7,End-To-End Relation Extraction Using Lstms On Sequences And Tree Structures,"One common approach to leverage dependency information is to perform bottom-up or top-down computation along the parse tree or the subtree below the lowest common ancestor (LCA) of the entities (Miwa and Bansal, 2016).###Previous studies (Xu et al., 2015b; Miwa and Bansal, 2016) have shown that removing tokens outside this scope helps relation extraction by eliminating irrelevant information from the sentence.",other,acknowledge existing methods for leveraging dependency information
375,5c8ddce94895d9cbc6a97820,8fb5a6fe93a3d19ffee50677c0ae563e3377d2d4,Chinese NER Using Lattice LSTM,573695fe6e3b12023e511e25,Bidirectional LSTM-CRF Models for Sequence Tagging,"Huang et al. (2015) uses hand-crafted spelling features; Ma and Hovy (2016) and Chiu and Nichols (2016) use a character CNN to represent spelling characteristics; Lample et al. (2016) use a character LSTM instead.###In particular, lexicon features have been widely used (Collobert et al., 2011; Passos et al., 2014; Huang et al., 2015; Luo et al., 2015).###We follow the best English NER model (Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016), using LSTM-CRF as the main network structure.",impact-revealing,reporting prior findings on spelling features and NER models
3762,5d64ff713a55acf547f20de0,26d3f8db3a4275225d355a9ce8e132b8c19c225b,Once-for-All: Train One Network and Specialize it for Efficient Deployment on Diverse Hardware Platforms,5c8f69874895d9cbc647064f,MnasNet: Platform-Aware Neural Architecture Search for Mobile,"It is impossible for previous NAS methods (Tan et al., 2019; Cai et al., 2019) due to the prohibitive training cost.",other,highlighting limitations of previous NAS methods
907,53e99998b7602d97021da64b,638ac2a3b560f93a541678d98f9710381ba7cce9,Taming hardware event samples for FDO compilation,53e9aa16b7602d97033835d1,ProfileMe: hardware support for instruction-level profiling on out-of-order processors,"AMD processors, on the other hand, provide instructionbased sampling (IBS) which is similar to the ProfileMe approach [7].###at the head of the instruction queue 6-cycles after the one that triggered the overflow [7].###ProfileMe was proposed hardware support to allow accurate instruction-level sampling [7] for Alpha processors.",impact-revealing,comparing instruction-based sampling methods
3443,599c7ea4601a182cd28b81a7,f323036340ad0ec30cafacd6b09a3e28379e24f1,maximizing cache performance under uncertainty,53e99cf4b7602d97025a841c,Improving Cache Management Policies Using Dynamic Reuse Distances,"Instead, replacement policies for processor caches are designed empirically, using heuristics based on observations of common-case access patterns [11,14,16,17,19,20,21,30,35,39].###The eviction priority array is a versatile mechanism that can implement many policies, specifically any ranking function of ages [9], such as random, LRU, PDP [14], IRGD [35], etc.###47% for SHiP [39], 41% for DRRIP [17], and 42% for PDP [14].###PDP [14] protects lines from eviction for a fixed number of accesses.###And PDP [14] and IRGD [35] use auxiliary monitors to profile the access pattern and periodically recompute their policy.###In contrast, no “protecting distance” in PDP [14] can do so.###Aging: We use per-set, coarsened ages [14].###Similarly, a few policies that do not assume recency still base their policy on when a candidate was last referenced: PDP [14] protects candidates until a certain age; and IRGD [35] uses a heuristic function of ages.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2720,5aed14e217c44a4438159759,c097be22f1e87a846385047346b73610d91fea4e,GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs,573696ce6e3b12023e5ceb9b,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.","For activation functions, we denote h ( · ) to be the LeakyReLU activation (Xu et al., 2015a) with negative slope equals to 0.1 and σ ( · ) to be the sigmoid activation.###Neural attention mechanism Neural attention mechanism is widely adopted in deep learning literature and many variants have been proposed (Chorowski et al., 2014; Xu et al., 2015b; Seo et al., 2017; Vaswani et al., 2017).",other,providing context on activation functions and neural attention mechanisms
4045,5f7fdd328de39f0828397e7f,9fa283d4f9c2ed991383c0434ef6043bee0dc8e2,Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"With ResNets and DenseNets, GFNet reduces the number of required Multiply-Adds for the given test accuracy by approximately 2− 3× times.###4.2 Visualization
We show the image patches found by a ResNet-50 based GFNet on some of test samples in Figure 7.###The GFNet is implemented on the basis of several state-of-the-art CNNs, including MobileNet-V3 [16], RegNet-Y [40], EfficientNet [50], ResNet [14] and DenseNet [22].###For ResNets, GFNet generally requires 2−3× times lower latency to achieve the same performance as baselines.###For example, when the MobileNets-V3 and ResNets are used as the backbone network, GFNet has up to 1.4× and 3× less Multiply-Add operations compared to the original models when achieving the same level of accuracy, respectively.###One can observe that GFNet effectively accelerates the inference of both MobileNets-V3 and ResNets.###We adopt the same data augmentation and pre-processing configurations as [22, 14, 57].###For example, state-of-the-art CNNs have achieved super-human-level performance on the competitive ILSVRC [9] competition with 224×224 or 320×320 images [47, 14, 61, 18, 22].",other,reporting performance improvements of GFNet over existing models
516,57d063d3ac44367354292258,db645d8a227d024121dc95a1c0eeec1b15c7fe53,A multi-player Markov stopping game for delay-tolerant and opportunistic resource sharing networks,53e9bdd4b7602d9704a81d07,Knowing when to act: an optimal stopping method for smart grid demand response,"In [12], the dynamism of electricity price and the deferrability of some home applications are exploited to design an optimal electricity utilization rule that can balance the electricity expense and waiting time.",impact-revealing,reporting on the design of an optimal electricity utilization rule
2300,5d9edc7547c8f766460401fb,f797fd44b9ddd5845611eb7a705ca9464a8819d1,very deep convolutional networks for text classification,573696ce6e3b12023e5ce95a,Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift,"Each convolutional block (see Figure 2) is a sequence of two convolutional layers, each one followed by a temporal BatchNorm (Ioffe and Szegedy, 2015) layer and an ReLU activation.###So, for a mini-batch of size m and feature maps of temporal size s , the sum and the standard deviations re-lated to the BatchNorm algorithm are taken over |B| = m · s terms.",other,providing context for convolutional block architecture
3587,5b67b4b917c44aac1c867dbc,d18b48f77eb5c517a6d2c1fa434d2952a1b0a825,hierarchical graph representation learning with differentiable pooling,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"For the GRAPHSAGE and SET2SET baselines, we use the base implementation and hyperparameter sweeps as in our DIFFPOOL approach.###We use the “mean” variant of GRAPHSAGE [16] and apply a DIFFPOOL layer after every two GRAPHSAGE layers in our architecture.###These results provide positive answers to our motivating questions Q1 and Q2: We observe that our DIFFPOOL approach obtains the highest average performance among all pooling approaches for GNNs, improves upon the base GRAPHSAGE architecture by an average of 6.27%, and achieves stateof-the-art results on 4 out of 5 benchmarks.###In recent years there has been a surge of interest in developing graph neural networks (GNNs)—general deep learning architectures that can operate over graph structured data, such as social network data [16, 21, 36] or graph-based representations of molecules [7, 11, 15].###network (GNN) models have been proposed in recent years, including methods inspired by convolutional neural networks [5, 8, 11, 16, 21, 24, 29, 36], recurrent neural networks [25], recursive neural networks [1, 30] and loopy belief propagation [7].###DIFFPOOL can be applied to other GNN architectures besides GRAPHSAGE to capture hierarchical structure in the graph data.###Concretely, we found that GRAPHSAGE with DIFFPOOL was 12× faster than the GRAPHSAGE model with SET2SET pooling, while still achieving significantly higher accuracy on all benchmarks.###, for node classification [16] or link prediction [32], and the whole model can be trained in an endto-end fashion.###The embedding matrix and the assignment matrix are computed by two separate GRAPHSAGE models respectively.###In our experiments, the GNN model used for DIFFPOOL is built on top of the GRAPHSAGE architecture, as we found this architecture to have superior performance compared to the standard GCN approach as introduced in [22].###• GRAPHSAGE with global mean-pooling [16].###The general approach with GNNs is to view the underlying graph as a computation graph and learn neural network primitives that generate individual node embeddings by passing, transforming, and aggregating node feature information across the graph [15, 16].###Batch normalization [18] is applied after every layer of GRAPHSAGE.###There are many possible implementations of the propagation function M [15, 16].###GNNs have been applied to a wide variety of tasks, including node classification [16, 21], link prediction [31], graph classification [7, 11, 39], and chemoinformatics [14, 19, 27, 28, 32].###When applying DIFFPOOL to GRAPHSAGE, the input adjacency matrices are not normalized, and bias terms are added to every graph convolutional layers.###In both variants, S2V model is used to compute the embedding matrix, while GRAPHSAGE model is used to compute the assignment matrix.",other,highlighting the performance improvements and significance of the DIFFPOOL approach in graph neural networks
1329,,a2904e885c3b2fe90246d7a24788291378af6c31,Cervical cancer screening in medically underserved California Latina and non-Latina women: effect of age and regularity of Pap testing.,,,"###Studies [25] have found suboptimal rates of cervical cancer screening among young Hispanic females, even though our study emphasized the importance of all medically disadvantaged young women who appear to understand the serious nature of cervical cancer, their susceptibility to the disease, and the benefits of Pap tests.",impact-revealing,highlighting the importance of cervical cancer screening among medically disadvantaged young women
1039,,e156f0245fd5ee5d0aca868e1a5df1e2fd6ce5e6,Compressed Indexing for Consecutive Occurrences,,,"###For a prime p and an r ∈ F ∗ p , the Karp–Rabin ﬁngerprint [24] of a string X is deﬁned as a tuple ( r | X |− 1 mod p, r −| X | +1 mod p, ϕ p,r ( X )) , We use the following result of Christiansen et al. [8], which builds on Belazzougui et al. [3] and Gagie et al. [16, 17].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
654,5550488245ce0a409eb6ef7f,12d8ead802196d498877ffe92ff4c42ca1ce7694,Sandbox Prefetching: Safe run-time evaluation of aggressive prefetchers,53e9992ab7602d9702165fe1,Access Map Pattern Matching for High Performance Data Cache Prefetch.,"On the other hand, because immediate prefetchers work on the granularity of individual cache lines, and not streams, a next-line prefetcher would be able to perfectly prefetch the second cache line of these linked list nodes.",impact-revealing,providing context on prefetching techniques
1073,,aae2902a352c5b0369278d0b37ace17bdb378d02,Modeling the Non-Substitutability of Multiword Expressions with Distributional Semantics and a Log-Linear Model,,,"###…that have been widely used as a means of identifying collocations: Pointwise Mutual Information ( PMI ) (Church and Hanks, 1990; Evert, 2005; Bouma, 2009; Pecina, 2010), t-score (Manning and Sch¨utze, 1999; Church et al., 1991; Evert, 2005; Pecina, 2010), and Log-likelihood Ratio ( LL r )…",impact-revealing,reporting methods used for identifying collocations
3956,5e3940c73a55ace46ed436d2,845b4941d8c016aa5f8967da2f86d38ef6c18fa3,a survey on knowledge graphs representation acquisition and applications,5b1642388fbcbf6e5a9b55de,Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM.,"Through the evaluation of simple KG-QA with and without neural networks, Mohammed et al. [198] found that sophisticated deep models such as LSTM and GRU with heuristics achieve state of the art, and non-neural models also gain reasonably well performance.###KagNet [201] performs concept recognition to build a schema graph from ConceptNet and learns path-based relational representation via GCN, LSTM, and hierarchical path-based attention.###While other works consider the relational path and structure of knowledge graphs, KPRN [206] regards the interaction between users and items as an entity-relation path in the knowledge graph and conducts preference inference over the path with LSTM to capture the sequential dependency.###Recent work applies sequence-to-sequence neural architectures, for example, LSTM-CNN [110] for learning character-level and word-level features and encoding partial lexicon matches.###BRCNN [144] combines RNN for capturing sequential dependency with CNN for representing local semantics using two-channel bidirectional LSTM and CNN. 2) Attention Mechanism: Many variants of attention mechanisms are combined with CNNs, including word-level attention to capture semantic information of words [145] and selective attention over multiple instances to alleviate the impact of noisy instances [146].###Inspired by the diachronic word embedding, Goel et al. [182] CNN + max pooling position embedding Multi CNN [137] Multi-window convolution + max pooling position embedding PCNN [138] CNN + piecewise max pooling position embedding MIMLCNN [139] CNN + piecewise and cross-sentence max pooling position embedding Ye et al. [140] CNN/PCNN + pairwise ranking position embedding, class ties Zeng et al. [141] CNN + max pooling position embedding, relation path RNNs SDP-LSTM [142] Multichannel LSTM + dropout dependency tree, POS, GR, hypernyms LSTM-RNN [143] Bi-LSTM + Bi-TreeLSTM POS, dependency tree BRCNN [144] Two-channel LSTM + CNN + max pooling dependency tree, POS, NER Attention Attention-CNN [145] CNN + word-level attention + max pooling POS, position embedding Lin et al. [146] CNN/PCNN + selective attention + max pooling position embedding Att-BLSTM [80] Bi-LSTM + word-level attention position indicator APCNN [147] PCNN + sentence-level attention entity descriptions HATT [148] CNN/PCNN + hierarchical attention position embedding, relation hierarchy GCNs C-GCN [150] LSTM + GCN + path-centric pruning dependency tree KATT [152] Pre-training + GCN + CNN + attention position embedding, relation hierarchy AGGCN [151] GCN + multi-head attention + dense layers dependency tree Adversarial Wu et al. [153] AT + PCNN/RNN + selective attention indicator encoding DSGAN [154] GAN + PCNN/CNN + attention position embedding RL Qin et al. [155] Policy gradient + CNN + performance change reward position embedding Zeng et al. [156] Policy gradient + CNN + +1/-1 bag-result reward position embedding Feng et al. [157] Policy gradient + CNN + predictive probability reward position embedding HRL [158] Hierarchical policy learning + Bi-LSTM + MLP relation indicator took an entity and timestamp as the input of entity embedding function to preserve the temporal-aware characteristics of entities at any time point.###To enable sentiment-related information filtering, Sentic LSTM [185] injects knowledge concepts to the vanilla LSTM, and designs a knowledge output gate for concept-level output as a complement to the token level.###Lample et al. [111] proposed stacked neural architectures by stacking LSTM layers and CRF layers, i.e., LSTM-CRF (in Fig.###Rather than CNN-based sentence encoders, Att-BLSTM [80] proposes word-level attention with BiLSTM.###It encodes one-hop neighbors to capture the structural information with R-GCN and then takes the structural entity embedding for multi-step matching guided by long short-term memory (LSTM) networks to calculate the similarity scores.###Garc´ıa-Dur´an et al. [178] concatenated predicate token sequence and temporal token sequence, and used LSTM to encode the concatenated time-aware predicate sequences.###To enable sentiment-related information ﬁltering, Sentic LSTM [220] injects knowledge concepts to the vanilla LSTM and designs a knowledge output gate for concept-level output as a complement to the token level.###The last hidden state of LSTM is taken as temporal-aware relational embedding r temp .###8a) and Stack-LSTM.###Katiyar and Cardie [167] proposed a joint extraction framework with an attention-based LSTM network.###RNNs are also introduced, for example, SDP-LSTM [142] adopts multi-channel LSTM while utilizing the shortest dependency path between entity pair, and Miwa et al. [143] stacks sequential and tree-structure LSTMs based on dependency tree.",other,acknowledge various approaches and models in knowledge graph question answering
2418,53e9bb37b7602d97047778cc,1406543a4e2930a7c6a8e312e91e64fc90e10521,A survey of network flow applications.,53e9a3b9b7602d9702cc8ce4,A Distributed Real-Time Tool For Ip-Flow Measurement,[66] proposed a real-time system with a bitpattern based flow definition and round-robin mechanism to balance packet steams.,other,reporting prior findings on a real-time system
1358,,92a1f02d65bf1cfe3b48a5c411870fa237182633,Peptide substrate for caspase-3 with 2-aminoacridone as reporting group,,,###Coumarin and rhodamine dyes are the most frequently used reporter groups (Cai et al. 2001; Gray and Sullivan 1989; Gurtu et al. 1997; Liu et al. 1999; Smith et al. 1980; Wang et al. 2005).###Peptide substrates containing appropriately situated fluorophores have been widely used for the determination of caspase activity in cell culture based assays (Garcia-Calvo et al. 1999; Gurtu et al. 1997; Thornberry et al. 1997).,impact-revealing,reporting common reporter groups and their applications in assays
572,5a73cb3517c44a0b303556bf,eac48f406c46527f5ca821de7fe8d62d6db56a27,"Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer",5c8c0de54895d9cbc6c5d4cf,A Comparison Of Sequence-To-Sequence Models For Speech Recognition,"A deﬁ-ciency of end-to-end systems appears to be in their language modeling capacity [15] which may be because large text-only data are not utilized in end-to-end systems.###…in Figure 1, consists of an encoder (referred to as the transcription network in [13]), a prediction network and a joint network; as described in [15], the RNN-T model can be compared to other encoder-decoder architectures such as “listen, attend, and spell” [7], if we view the combination of…###For example, in our previous work [15] we evaluated a number of end-to-end models including attention-based models [7] and RNN-T [13, 14] trained on ∼ 12,500 hours of transcribed training data; although end-to-end approaches were found to be comparable to a state-of-the-art context-dependent…",impact-revealing,highlighting a deficiency in end-to-end systems' language modeling capacity
1678,,40a07b63aeb404bf34dfdc0304660a66b340f4e1,Integrating Online-Offline Interactions to Explain Societal Challenges,,,###Early work (Tajfel and Turner 1979) has highlighted the importance of social identification for selfesteem.,impact-revealing,acknowledging foundational research on social identification
2012,,52196fd164a7fcb2f9b5143628a584061a2cd25a,The effects of lumbar spine manipulation on the flexion-relaxation response in chronic low-back pain participants,,,"###While support has been found for this model (Farina, Arendt-Nielsen, & Graven-Nielsen, 2005; Stohler, Zhang, & Lund, 1996; van Dieën et al., 2003), it remains incomplete and fails to consider how pain can affect various intensities of contraction, from relaxation through to maximum voluntary…",impact-revealing,highlighting limitations in the existing model
2582,5ee7495191e01198a507f7ae,09bda461aa4911d0513e8e46dd39a4113947e450,Ansor : Generating High-Performance Tensor Programs for Deep Learning,5d29afd3275ded87f954b943,Learning to optimize halide with tree search and random programs,"We adopt a learned cost model similar to related works [2, 12] with newly designed program features.###Given the importance of DNNs’ performance, researchers and industry practitioners have turned to search-based compilation [2, 11, 32, 49, 59] for automated generation of tensor programs, i.###Halide has three versions of auto-scheduler based on different techniques [2, 31, 36].###, Halide auto-scheduler [2]), which prevents them from covering a comprehensive search space (§2).###5) [39], Halide autoscheduler (commit: 1f875b0) [2], FlexTensor (commit: 7ac302c) [59], and AutoTVM (commit: 69313a7) [12] as baselines.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1053,,7586852c4e954efec96147349d602814fb879c51,Distributed multi-query optimization of continuous clustering queries,,,"###Gigascope [5] is a d ata stream management system which is designed for processing traffic monitoring queries over IP networks.###3 Multi-ACQs optimization Gigascope [5] is a d ata stream management system which is designed for processing traffic monitoring queries over IP networks.###[5] Cranor, C., Johnson, T., Spataschek, O., and Shkapenyuk, V. Gigascope: a s tream database for network applications.###Listing 3: A set of ACQs with varying grouping attributes
/*Q1*/ SELECT A, COUNT(*) FROM R [RANGE 1 Min, SLIDE 1 Sec] GROUP BY A
/*Q2*/ SELECT B, COUNT(*) FROM R [RANGE 1 Min, SLIDE 1 Sec] GROUP BY B
/*Q3*/ SELECT C, COUNT(*) FROM R [RANGE 1 Min, SLIDE 1 Sec] GROUP BY C
Listing 2: simple Gigascope ACQ SELECT win_time, source_IP, COUNT(*) FROM IP_header_stream [RANGE 1 Min, SLIDE 1 Sec] GROUP BY win_time, source_IP
Gigascope does not support dynamic query optimization, and does not consider selection predicates P* in multi-ACQ optimization.###An example ACQ in Gigascope is given in Listing 2.###Guirguis et al. [14] [13] improve the single pipeline approach in [11] by incorporating the optimizations in Gigascope on sharing grouped aggregates, but the non-scalable selection predicate sharing problem remains, as does the problem of parallelized or distributed execution.###Figure 3 s hows the two main execution strategies in Gigascope investigated in [10]: naïve (none shared) (Fig.",impact-revealing,describing the functionality of Gigascope and its limitations
700,5736982b6e3b12023e6fd332,d0b8c5464b582e1a582a4affb270e8b02f8414a2,a scalable architecture for ordered parallelism,53e9b6b9b7602d970423e9b1,Data Speculation Support for a Chip Multiprocessor,"It may seem that thread-level speculation (TLS) [28,60,66, 68], which speculatively parallelizes sequential programs, could exploit this parallelism.###Commit serialization: Prior TLS schemes enforce inorder commits by passing a token among ready-to-commit tasks [28,61,66,68].###By contrast, most TLS systems use lazy versioning (buffering speculative data in caches) or more expensive multiversioning [11, 25, 28, 29, 56, 60, 61, 66, 68, 69] to limit the cost of aborts.###Much prior work has investigated thread-level speculation (TLS) schemes to parallelize sequential programs [25, 28, 60, 61, 66, 69].###Thus, upon detecting mispeculation, most TLS schemes abort the task that caused the violation and all later speculative tasks [25, 28,61,66,68].",impact-revealing,acknowledge existing research on thread-level speculation schemes
1481,,e7e0ef9935137d0f7ca14f0464cedb5362d58b47,RMAF: Relu-Memristor-Like Activation Function for Deep Learning,,,"###The function is similar to ReLU in terms of identity functions for non-negative inputs Clevert et al. [25].###It has been noted from several research that the network can beneﬁt from the negative representations to achieve better performance [25], [27].###Thiswork was inspired by Clevert et al. [25] ﬁnding a novel window function to solve boundary lock phenomenon.",impact-revealing,highlighting the significance of a novel window function in improving network performance
3197,5d0b00ed8607575390fedd4e,493d5f344eea1468260946b29a80dc81b2be409c,HopSkipJumpAttack: A Query-Efficient Decision-Based Attack,5a260c2817c44a4ba8a2332e,ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models,"Zeroth-order algorithms have been applied to the generation of adversarial examples under the score-based threat model [8–10].###Chen et al. [8] and Ilyas et al. [9, 10] introduced score-based methods using zeroth-order gradient estimation to craft adversarial examples.",other,reporting prior findings on adversarial example generation methods
912,53e9b11db7602d9703b99bc4,41f128798ca097aec531ff7804aafe1043c019a1,Stream chaining: exploiting multiple levels of correlation in data prefetching,53e9bc6eb7602d97048eba78,Data Cache Prefetching Using a Global History Buffer,"Based on the mostly orthogonal stages of localization and correlation, a taxonomy to classify prefetching algorithms was proposed in [12].###We evaluate the proposed prefetcher with benchmarks from the SPEC 2006 and BioBench suites and compare it against both a state-of-the-art PC based localization prefetcher – PC/DC [12] – and its non-localized counterpart – G/DC [9, 12].###For instance, common stride prefetchers [2, 4] and the predictor in [12] separate the stream of misses according to the PC of the missing memory access instruction, which we call PC localization and is an instance of execution context localization.###We show that this scheme captures well common application behavior and we show that it can be easily implemented on a simple extension to the Global History Buffer (GHB) [12], which is a convenient structure to implement various prefetching algorithms.###Unfortunately, to increase coverage and amortize long memory access times, these prefetchers often resort to large degrees of prefetching [12].###Schemes based on localizing streams according to the PC of memory access instructions that generate the misses can potentially lead to higher performance gains than those that treat all misses as a single global stream [12, 15], although this varies according to individual applications.###Popular options for miss stream localization include grouping misses according to the PC of the instruction that generated them [2, 4, 12], or to the region of memory they reference [13, 14, 19], or to some period of time in which the misses occur [22], or even to previously executed branch instructions [21].###For instance, a simple stride prefetcher with PC based localization and constant stride correlation would be referred to as PC/CS, and the best prefetcher in [12] uses PC based localization and address delta correlation and is then referred to as PC/DC.###Many (hardware) data structures can be used to implement different prefetching algorithms, but [12] has proposed a common data structure that is flexible enough to allow efficient implementation of a number of prefetching algorithms.",impact-revealing,acknowledge existing prefetching algorithms and their evaluation
3738,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,573698586e3b12023e71ee03,Rethinking Design Metrics for Datacenter DRAM.,"Many works tune individual server knobs, such as selective voltage boosting [98–100], exploiting multicore heterogeneity [101–103], trading memory latency/bandwidth [104–107], or reducing front-end stalls [70, 96, 108].",other,acknowledge various tuning strategies in server optimization
1269,,59c54214ab307c7a31318af450a991388eb99cec,Self-Distilled Representation Learning for Time Series,,,"###Our specific architecture is inspired by the TS2Vec encoder [32], which proposes a cascade of residual convolutional blocks.###Ours TS2Vec [32] Ti-MAE [22] T-Loss [14] TNC [30] TS-TCC [13] TST [35] UCR For a more detailed introduction to data2vec and an in-depth analysis of its design choices, we refer to the original paper [2] as well as its successor data2vec 2.0 [1].###For time-series classification, we follow the experimental setup of [32], which is based on the standard SVM implementation of scikit-learn , specified to an RBF kernel and a one-vs-rest strategy for multiclass classification; each evaluation step involves a simple grid search cross-validation to…###Following [32], we benchmark our approach on the UCR archive [10] and UEA archive [3], which consist of 128 (univariate) and 30 (multivariate) datasets, respectively.###This protocol is closely aligned with the one of TS2Vec [32], which will serve as our primary reference point for comparison with state-of-the-art SSL methods; 4 see also [24] for an independent benchmarking study.###At the same time, a suitable padding scheme ensures consistent feature dimensionality from layer to layer, which is key to the hierarchical contrastive loss function developed in [32].###Our selection is adopted from [32], which is also the origin of the scores reported in this work (except for our method and Ti-MAE [22]).###We refer to [32] for reproduction details of each method as well as a more extensive discussion of conceptual differences between them.###Adopting the experimental setup of [32] again, we consider three versions of the ETT datasets [37] as well as the Electricity dataset [16], both in the uni-and multivariate setting.###Few prominent examples are TS2Vec [32], T-Loss [14], TS-TCC [13], and TNC [30]; see [24, 36] for recent surveys of the field.",impact-revealing,providing context on the architecture and experimental setup
709,53e99e61b7602d97027252c8,e3369553f399fb63b9f24261247afe64fb6c3f65,Virtual Program Counter (VPC) Prediction: Very Low Cost Indirect Branch Prediction Using Conditional Branch Prediction Hardware,53e9aeaab7602d97038cba71,Profile-guided receiver class prediction,"Devirtualization is the substitution of an indirect method call with direct method calls in object-oriented languages [11], [24], [21], [6], [28].###The VPC prediction algorithm is inspired by a compiler optimization, called receiver class prediction optimization (RCPO) [11], [24], [21], [6] or devirtualization [28].###However, even though there could be many different shapes in the program, if the types of shapes are mostly either an instance of the Rectangle class or the Circle class at runtime, the compiler can convert the indirect call to multiple guarded direct calls [21], [18], [6], as shown in Fig.",impact-revealing,providing context on devirtualization and its optimization
751,58d82fcbd649053542fd6178,515a21e90117941150923e559729c59f5fdade1c,the concrete distribution: a continuous relaxation of discrete random variables,5736986c6e3b12023e73077e,Gradient Estimation Using Stochastic Computation Graphs,"While these can be made to work with AD, they involve special casing and defining surrogate objectives [39], and even then they can have high variance.###Stochastic computation graphs (SCGs) provide a formalism for specifying, potentially stochastic, input-output mappings with learnable parameters using directed acyclic graphs (see [39] for a review).###While these can be made to work with AD, they involve special casing and defining surrogate objectives (Schulman et al., 2015), and even then they can have high variance.###Stochastic computation graphs (SCGs) provide a formalism for specifying input-output mappings, potentially stochastic, with learnable parameters using directed acyclic graphs (see Schulman et al. (2015) for a review).",impact-revealing,providing context on stochastic computation graphs and their relation to automatic differentiation
2931,5a73cbcc17c44a0b3035f34a,04957e40d47ca89d38653e97f728883c0ad26e5d,Cascade R-CNN: Delving Into High Quality Object Detection,5736960d6e3b12023e5204f1,LocNet: Improving Localization Accuracy for Object Detection,"[36, 12, 11] used a multi-stage procedure to generate accurate proposals, and forwarded them to an accurate model (e.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2859,5aed14d117c44a4438158a8d,36db98d8604df17e7ab68952c6f7a6cb70cff2e7,a deep learning approach for multimodal deception detection,53e99fe4b7602d97028bf016,A Convolutional Neural Network For Modelling Sentences,"We use Convolutional Neural Networks (CNN) [18, 19] to extract features from the transcript of a video, v .",other,reporting method used for feature extraction
3294,5bdc31b417c44a1f58a0b240,6ea57a2aea08ce0628c93f77bdc24c2f3e9cc6da,Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks,57a4e91dac44365e35c9830c,Learning Convolutional Neural Networks for Graphs,"To compare GNNs to kernels we used the basic 1 -GNN layer of Equation (5), DCNN (Wang et al. 2018), PatchySan (Niepert, Ahmed, and Kutzkov 2016), DGCNN (Zhang et al. 2018).",other,comparing different models in the context of GNNs
3361,5f0d8b6891e011047aff993b,1c53d27c742fb4658fa03085c7c2ca014a122385,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,5f6a38c39fced0a24ba81ddb,Language modelling for biological sequences _x0096_ curated datasets and baselines,"Special thanks to Konstantin Schütze for helping with grant writing and providing early results for the structure prediction task.###The leap of NLP through advanced LMs has been successfully generalized toward understanding the language of life through advanced LMs trained on proteins [31], [32], [33], [34], [35], [36], [37], [38], [39].",other,acknowledging contributions and advancements in NLP through advanced language models
786,5c757586f56def9798a004b1,99b722f4ff8614b3e2a541a59a9ee6e1ed322f93,temporal point processes and the conditional intensity function,53e9b844b7602d9704408c52,An Introduction to the Theory of Point Processes,"On the other hand, measure theory is not assumed; for a much more thorough treatment with all the measure theoretical details, see Daley and Vere-Jones (2003) and Daley and Vere-Jones (2008).",impact-revealing,providing context for measure theory
3476,5550443b45ce0a409eb4c3b9,0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f,Towards End-To-End Speech Recognition with Recurrent Neural Networks,53e9b30fb7602d9703dd432b,Learning A Better Representation Of Speech Soundwaves Using Restricted Boltzmann Machines,"Although it is possible to directly transcribe raw speech waveforms with RNNs (Graves, 2012, Chapter 9) or features learned with restricted Boltzmann machines (Jaitly & Hinton, 2011), the computational cost is high and performance tends to be worse than conventional preprocessing.",other,highlighting the challenges and limitations of direct transcription methods
649,5bdc315017c44a1f58a05e13,1717255b6aea01fe956cef998abbc3c399b5d7cf,AMC: AutoML for Model Compression and Acceleration on Mobile Devices,573696106e3b12023e52307c,Learning both Weights and Connections for Efficient Neural Networks,"For channel pruning, we use max response selection (pruning the weights according to the magnitude [20]), and preserve Batch Normalization [25] layers during pruning instead of merging them into convolutional layers.###Extensive works [20, 19, 34, 12, 18, 17] have been done on accelerating neural networks by compression.###However, it requires iterative prune & fine-tune procedure to achieve decent performance [20], and single-shot pruning without retraining will greatly hurt the prediction accuracy###We observe a correlation between the pre-fine-tune accuracy and the post fine-tuning accuracy [20, 22].",impact-revealing,highlighting the challenges and considerations in channel pruning for neural networks
1941,,64d0b4d9b5b479d03d95e7cba0a2a4462f0aa775,Reading with meaning: the contributions of meaning-related variables at the word and subword levels to early Chinese reading comprehension,,,"###Although our primary focus in the present study was on the importance of different levels of meaning for reading comprehension, we also included phonological awareness and rapid automatized naming, both of which have been tested in relation to reading comprehension in previous studies of Chinese children (Shu et al., 2006; Tong et al., 2009).###The present study built upon previous research highlighting the role of morphological awareness to literacy development (Shu et al., 2006; Tong et al., 2009) at the lexical level and has extended the recognized importance of meaning-building from beyond the word level to the Table 7 Hierarchical regression explaining Time 1 reading comprehension from Time 1 vocabulary, morphological awareness, and orthography-semantic awareness without Time 1 Chinese character recognition controlled###Previous work (Shu et al., 2006; Tong et al., 2009) has already demonstrated the importance of morphological awareness for reading comprehension in Chinese children.###Given the importance of word reading for overall reading comprehension (Gough & Tunmer, 1986; Hoover & Gough, 1990), the inclusion of word reading is important particularly because Chinese word reading has been linked to morphological awareness in several previous studies of children (McBride-Chang, Shu, Zhou, Wat, & Wagner, 2003; McBride-Chang et al., 2005; Shu et al., 2006).###nized as a potentially important factor in Chinese character acquisition (McBrideChang et al., 2003, 2005; Shu et al., 2006).",impact-revealing,highlighting the role of morphological awareness in reading comprehension
3201,573697316e3b12023e6217ec,67cf1189c859d66bac309f9438df434fb651f97a,Cache Coherence Protocol and Memory Performance of the Intel Haswell-EP Architecture,53e9ada5b7602d97037a48ea,Comparing cache architectures and coherency protocols on x86-64 multicore SMP systems.,"In previous work [7], [8] we presented micro-benchmarks to measure the characteristics of distributed caches in NUMA systems with x86 64 processors.",other,reporting prior findings on micro-benchmarks in distributed caches
276,5aed14d617c44a4438159040,ebc96892b9bcbf007be9a1d7844e4b09fde9d961,YOLOv3: An Incremental Improvement,58d82fcbd649053542fd6729,"YOLO9000: Better, Faster, Stronger","Following YOLO9000 our system predicts bounding boxes using dimension clusters as anchor boxes [15].###This ﬁgure blatantly self-plagiarized from [15]. is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction, following [17].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
243,5736974d6e3b12023e6388bf,1d7e580740a3f96f1529e3d771915af4e996a259,Verbal and Nonverbal Clues for Real-life Deception Detection,53e9b79fb7602d9704343fa0,"The MUMIN coding scheme for the annotation of feedback, turn management and sequencing phenomena.","The gesture annotation is performed using the MUMIN coding scheme (Allwood et al., 2007).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1519,,029e8fb1c0e77f354175067937fc99cbaac92c3b,Pulmonary neuroendocrine tumors: What (little) do we know?,,,"###Concomitant chemotherapy and thoracic radiation are commonly used for curative intent in limited stage SCLC. Randomized studies of chemotherapy and radiation have shown that radiation administered concurrently with chemotherapy improves survival.64,65 Turrisi et al.66 reported that twice-daily thoracic radiation was superior to once-daily treatment.###Turrisi et al.(66) reported that twice-daily thoracic radiation was superior to once-daily treatment.",impact-revealing,highlighting the effectiveness of concurrent chemotherapy and radiation in improving survival rates
757,558c9cd8e4b0cfb70a1eab9c,beb69e174c75a19299c446a427425edfd55209f2,sampling dead block prediction for last-level caches,53e9afd3b7602d9703a24a4b,Dead-block prediction & dead-block correlating prefetchers,"In keeping with the methodology of recent cache papers [12], [13], [20], [19], [15], [9], [11], [7], [8], we choose a memory-intensive subset of the benchmarks.###From the last reference until the block is evicted the block is dead [13].###Previous work introduced several dead block predictors and applied them to problems such as prefetching and block replacement [13], [15], [11], [5], [1].",impact-revealing,acknowledge methodology in recent cache papers
1174,,8c288ecbad71d39f9a16c43318961f09054913ca,Efficient Coverage Path Planning for Mobile Disinfecting Robots Using Graph-Based Representation of Environment,,,"###Under classical CPP approaches (e.g., Choset and Pignon, 1998; Choset et al., 2000), an adjacency graph was built based on the topology of the decomposition, where the nodes of the graph represented the cells and the edges of the graph connected the nodes with adjacent corresponding cells in the…###In the Boustrophedon decomposition, as the most commonly-used exact cellular decomposition approach, the free space was divided into smaller regions (cells) by sweeping a line through the whole target area in one direction (Choset and Pignon, 1998; Choset et al., 2000).###Most of the previous works on CPP (e.g., Choset and Pignon, 1998; Jimenez et al., 2007; Xu et al., 2014) did not consider the current position of the robot in the current cell to choose the next cell in the coverage sequence.###Similar to some other suggested CPP-based techniques cited in the literature (e.g., Choset and Pignon, 1998; Mannadiar and Rekleitis, 2010; Hameed et al., 2013; Chen et al., 2019), the proposed CPP technique in general is not limited to a particular robot and is implementable on most of the mobile…###Critical points are the vertices of the environment boundaries where the sweeping slice connectivity changes (Choset and Pignon, 1998).",impact-revealing,providing context on classical coverage path planning approaches
1734,,5126012e679710a175fea3f4522579eff99d23bb,Detectability of hemodynamic oscillations in cerebral cortex through functional near-infrared spectroscopy: a simulation study,,,"###…represents an alternative to the short-SDD regression method used in CW fNIRS, mitigating the challenges associated with the use of multiple SSDs. 21 To evaluate the potentiality of the time-windowing approach, the signals obtained in cases B_UP, B_DW, and B_UPDW were analyzed by slicing the…###21,37 However, this assumption is challenged by the results of our study, in which the CW fNIRS signal recorded at 1 cm is affected by perturbations imposed in the DW layer of the medium.###20,21 In our study, we address these limitations by conducting numerical simulations to evaluate the effect of key operational parameters on the performance of both TD and CW fNIRS in detecting periodical oscillations in the probed tissue hemodynamics, aiming at assisting researchers in defining…###This approach does not consider the variation in fNIRS signal sensitivity to the different layers of the probed medium when the SDD is varied, 21 and the conventional assumption underlying the multi-distance approach is that the penetration depth of photons detected at a given SDD in CW modality is…###For this reason, it represents an alternative to the short-SDD regression method used in CW fNIRS, mitigating the challenges associated with the use of multiple SSDs. 21 To evaluate the potentiality of the time-windowing approach, the signals obtained in cases B_UP, B_DW, and B_UPDW were analyzed by slicing the DTOF using 10 gates, each with a width of 500 ps, covering the time window between 0 and 5 ns following the IRF temporal position.###21 Another remarkable result obtained with these simulations is the high sensitivity of the TD fNIRS signal to periodical perturbations in the optical properties of the probed medium.",impact-revealing,addressing limitations and evaluating performance of fNIRS methods
3288,5fd8964891e0119b22c1f219,80bcfee1766e56a01e6feeaa3cb47d3291acdcdf,Pre-Training Graph Neural Networks for Cold-Start Users and Items Representation,5cfa5b985ced2477cb3c5108,Social Influence-Based Group Representation Learning for Group Recommendation,"On one hand, existing recommender systems incorporate the side information such as spatial information [40, 44], social trust path [10, 36, 41, 42] and knowledge graphs [35, 37] to enhance the representations of the cold-start users/items.",other,acknowledge existing approaches in recommender systems
314,5b67b45517c44aac1c860885,fd3f489fc0438e500c0473af40dfebe4705df6d2,STAMP: Short-Term Attention Memory Priority Model for Session-based Recommendation,5c8c8ddc4895d9cbc60e60c3,Neural Attentive Session-based Recommendation.,"Inspired by recent advances in natural language processing area [16], some deep learning based solutions have been developed and some of which represent the state-of-the-art in SRS research field [2, 5, 6, 10].###Li et al.[10] propose an RNN based encoder-decoder model (NARM), which takes the last hidden state from the RNN as the sequential behavior, and uses the hidden states of previous clicks for attention computation to capture the main purpose(general interests) in a given session.###However, STAMP explicitly emphasizes the current interest reflected by the last click to capture the hybrid features of current and general interests from previous clicks, thus explicitly introducing the importance of last click into the recommender system while NARM only captures the general interests.###Alternatively, NARM combines main purpose and sequential behavior to get the session representation which treats them as equally important complementary features.###In order to verify the performance of our proposed STAMP model and the recent state-of-the-art NARM model in real production environment, where recommendation systems can only suggest a few items at once, the relevant item should be amongst the first few items in the recommendation list[12].###We first present experimental results varying the length of sessions on STMP, STAMP and NARM as shown by Figure 3(a).###We also record the runtime of the recurrent neural model NARM and the proposed STAMP approach.###Following [5] and [10], we filter out sessions of length 1 and items that appear less than 5 times in both of the datasets.###We can observe that when the length of sessions is above 20 the performance of NARM quickly decreases in contrast with STMP and STAMP.###• NARM[10]: An RNN based state-of-the-art model which employs attentionmechanism to capture main purpose from the hidden states and combines it with the sequential behavior as final representation to generate recommendations.###Differences:Ourmodel has significant differenceswith SWIWO and NARM.###It outperforms STMO in all three experiments and performs comparably with GRU4Rec+ but a little inferior to NARM.###NARM achieves the best performances among the baselines, because it not only models the sequential behavior using RNN with GRU units but also uses attention mechanism to capture main purpose, which indicates the importance of main purpose information in recommendations.###We argue that this is because the NARM model contains a lot of complex operations in each GRU unit, and our proposed model is simpler and faster as it introduces a simplified neural model to save the cost of recurrent calculations in dealing with sequential inputs.###Our model performs consistently better than NARM and shows obvious advantages in three experiments which demonstrates the effectiveness of considering both general interests and short-term interests, and the validity of the learned item embeddings.###The training time of each epoch on three datasets is given in Table 4,which illustrates that STAMP is more efficient than NARM.###We can observe that STAMP performs well on this mission and much more competitively than NARM when evaluated under stricter rules in an simulated production environment.###This suggests that short-term interests priority based models may be more powerful in handling long sessions than NARM.",impact-revealing,highlighting the performance comparison between STAMP and NARM in recommendation systems
1242,,b8112a4459f259a177522fd78689961050661631,Wearable static posturography solution using a novel pressure sensor sole,,,"###Static posturography is an important balance control tool and is frequently used for patients with postural instability or movement disorders like Parkinson’s disease [1], somatosensory vertigo [2] or other diseases affecting the balance control.",impact-revealing,highlighting the significance of static posturography in balance control
2269,5ec49a639fced0a24b4de8d2,5c5751d45e298cea054f32b392c12c61027d2fe7,S2ORC: The Semantic Scholar Open Research Corpus,5843778eac44360f10844192,Identifying Meaningful Citations.,", 2019), identifying citation sentiment (Athar and Teufel, 2012), identifying meaningful citations (Valenzuela et al., 2015), extracting key phrases (Caragea et al.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3271,5eede0b091e0116a23aafbd3,91fb815361fdbf80ff15ce4d783a41846bd99232,GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training,57aa28de0a3ac518da9896d5,node2vec: Scalable Feature Learning for Networks,"res include Jaccard similarity (counting common neighbors), RWR similarity [36] and SimRank [21], etc. Most recently developed network embedding algorithms, such as LINE [48], DeepWalk [39], node2vec [14], also follow the neighborhood similarity assumption. Structural similarity Different from neighborhood similarity which measures similarity by connectivity, structural similarity doesn’t even assume ###e problem setting in graph neural network research. In addition, the goal is to pre-train a structural representation model and apply it to unseen graphs, differing from traditional network embedding [14, 39, 48] and recent attempts on pre-training graph neural networks with attributed graphs as input and applying them within a specific domain [19]. 3.2 Graph Contrastive Coding Overview To pre-train structura### Graph Pre-training Skip-gram based model Early attempts to pre-train graph representationsareskip-grambasednetworkembeddingmodelsinspired by Word2vec [30], such as LINE [48], DeepWalk [39], node2vec [14], and metapath2vec [11]. Most of them follow the neighborhood similarity assumption, as discussed in section 2.1. The representations learned by the above methods are tied up with graphs used to train",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2094,,fc58aeb9450063cbfa791ed6ae5c086e3d1da37c,"I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI",,,"###In this paper, we argue that because of this illusion of explanatory depth (IOED) [46], XAI explanations (especially in the form of additive local explanations for individual predictions) may be misleading for non-technical XAI users.###Rozenblit and Keil coined this type of overconfidence bias as the illusion of explanatory depth (IOED) [46].###Rozenblit and Keil confronted their participants after the diagnostic questions with an expert statement [46].###The IOED is more pronounced for explanatory knowledge , i.e., knowledge that involves complex causal patterns, than it is for descriptive knowledge , i.e., knowledge about facts (names of capitals), procedures (baking), or narratives (movie plots) [28, 46].###Our procedure was inspired by the study designs of the initial IOED studies [46] but adjusted to the XAI context.###Psychological research has demonstrated in many contexts that humans have a robust bias of overconfidence regarding their understanding of how complex concepts work [46].###According to the theory-based approach, people form theories about all their concepts, not just for those that they use regularly [46].###The IOED has first been demonstrated for people’s understanding of causally complex systems in mechanical (bicycles, crossbows) [28, 33, 46] and natural (tides, rainbows) [46] domains.###Four factors are believed to influence the emergence of an IOED [46]: (i) Representation/recovery confusion : We overestimate our abilities to remember what we have observed.",impact-revealing,highlighting the potential misleading nature of XAI explanations due to overconfidence bias
351,5e79da4491e0115bb1157b77,8e74106415626a21bb909ff489ef310625c769e1,Pairwise Multi-Class Document Classification for Semantic Relations between Wikipedia Articles,53e9b976b7602d970456470f,Distributed Representations of Sentences and Documents.,"Next, they encode the segments with GloVe [31] and Paragraph Vectors [23] and compute their similarity to determine whether papers are similar with respect to those segments.###Paragraph Vectors [23] (also known as Doc2vec), extends word2vec to learn embeddings for word sequences of arbitrary length.###Second, we implement six different models using word-based document embeddings from GloVe [31] and Paragraph Vectors [23] (as Doc2vec implementation [33]), and deep contextual language models from BERT [15] and XLNet [41] in a vanilla and Siamese architecture [9].",impact-revealing,describing the methodology for segment encoding and similarity computation
1189,,544095f1edb05e30b8dc2f77823cd084d88604ff,Multi-Scale Hybrid Network for Polyp Detection in Wireless Capsule Endoscopy and Colonoscopy Images,,,"###Inspired by the feature fusion module, the proposal adopts the FSSD network structure [25].###This was inspired by the feature fusion SSD (FSSD) [25].###Based on experiments conducted in the work of [25], a feature map of resolution smaller than 10 px × 10 px may not contain much information to reuse.###Inspired by the feature fusion single-shot multibox detector (FSSD) [25] and the GoogLeNet inception v4 [26] Szegedy et al.",impact-revealing,providing context for the proposed network structure
240,56d86073dabfae2eee7807ea,6928b1bf7c54a4aa8d976317c506e5e5f3eae085,Deception detection using real-life trial data,53e9a611b7602d9702f42fa8,Automatic deception detection in Italian court cases,"The work closest to ours is presented by Fornaciari and Poesio [14], which targets the identiﬁcation of deception in statements issued by witnesses and defendants using a corpus collected from hearings in Italian courts.###While there is research work that has used court trial transcripts to identify deceptive statements [14], we are not aware of any previous work that took into consideration modalities other than text for deception detection on trial court data.",impact-revealing,highlighting the uniqueness of the current research in deception detection
3406,5b67b45517c44aac1c86084b,af8a8dcb74561d52d904f7bc4afcc747e079b702,modeling task relationships in multi-task learning with multi-gate mixture-of-experts,55323d0c45cec66b6f9dd66e,Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization.,"The tuning algorithm is a Gaussian Process model similar to Spearmint as introduced in [14, 32].",other,describing a tuning algorithm
1982,,b36ba6f245138d538d79a06ef64c2398692b000d,Machine Health Indicator Construction Framework for Failure Diagnostics and Prognostics,,,"###In [27], the authors presented a novel method to enhance fault diagnostics and prognostics of gearbox based on data fusion using Support Vector Data Description (SVDD) technique.",impact-revealing,reporting a novel method for fault diagnostics and prognostics
861,5c7572b7f56def97988385ce,c342c71cb23199f112d0bc644fcce56a7306bf94,active learning for convolutional neural networks: a core-set approach,53e9b92ab7602d97045168b7,Analysis of a greedy active learning strategy,"On the theoretical side, it is shown that greedy active learning is not possible in algorithm and data agnostic case (Dasgupta, 2005).",impact-revealing,reporting theoretical findings in active learning
1360,,059bb375a4cb592c68a45f8447fb3c5c8fa2b9bf,A study into the usability and security implications of text and image based challenge questions in the context of online examination,,,"###Many studies (Just and Aspinall 2009a; Schechter et al. 2009; Ullah et al. 2014a; Renaud and Just 2010) reported memorability as one of the key issues with the use of text-based challenge questions.###The data of the relaxed algorithm was compiled using a substring and distance algorithms (Schechter et al. 2009).###For example AOL utilizes favourite and personal questions, Google uses a small set of personal questions and Microsoft and Yahoo implement a combination of personal and favourite questions (Schechter et al. 2009).###Schechter et al. (2009) investigated
text-based questions widely used by corporate email services includingMicrosoft, AOL, Google, and Yahoo.###Challenge question authentication is a knowledge-based feature, which is widely seen as a credential recovery technique (Just and Aspinall 2009a; Schechter et al. 2009).###We implemented an string-tostring comparison algorithm for authentication purposes (Schechter et al. 2009; Ullah et al. 2014a).###These issues were identified in the text-based questions as discussed in the literature review above (Just and Aspinall 2009a, c; Schechter et al. 2009).",impact-revealing,highlighting key issues with text-based challenge questions in authentication
3429,59ae3c262bbe271c4c71f4a2,610cff0a09c76c43739be1a6e5b0ed7a1a24ee60,metapath2vec: Scalable Representation Learning for Heterogeneous Networks,573696076e3b12023e51a3c9,Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label   Embedding,"In addition, several other methods have been proposed for learning representations in networks [4, 5, 11, 20, 23].",other,acknowledge existing methods for learning representations in networks
299,5fb24ee191e01186d3f5decc,050fecf6e3e21faeeb7629f179ba839c4dfd1ead,EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints,5d04e8feda56295d08dd0e05,Levenshtein Transformer,"Figure 4 shows an example for creating the roll-in sequences: we first create the initial sequence y0 by applying random word dropping (Gu et al., 2019) and random word shuffle (Lample et al.###Knowledge Distillation We apply sequencelevel knowledge distillation from autoregressive teacher models as widely used in nonautoregressive generation (Gu et al., 2018; Lee et al., 2018; Gu et al., 2019).###Replacing dualpath roll-in with the simpler roll-in policy used in Gu et al. (2019), the model’s translation quality drops significantly (by 0.9–1.3 on BLEU and 0.6– 1.9 on RIBES) with fewer constraints preserved and slower decoding.###Edit operations such as substitution (Ghazvininejad et al., 2019) and
insertion-deletion (Gu et al., 2019) have reduced the quality gap between non-autoregressive and autoregressive models.###Extensive experiments showed that EDITOR exploits soft lexical constraints more effectively than the Levenshtein Transformer (Gu et al., 2019) while speeding up decoding dramatically compared to constrained beam search (Post and Vilar, 2018).###, 2018) or multi-pass decoding (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019).###We stop refining if 1) the output sequences from two consecutive iterations are the same (Gu et al., 2019), or 2) the maximum number of decoding steps is reached (Lee et al., 2018; Ghazvininejad et al., 2019).5
Incorporating Soft Constraints Although EDITOR is trained without lexical constraints,…###BLEU differences are small (∆   1.1) as in prior work (Gu et al., 2019).###Evaluation We evaluate translation quality via case-sensitive tokenized BLEU (as in Gu et al. (2019))9 and RIBES (Isozaki et al., 2010), which is more sensitive to word order differences.###…the softmax function captures the similarity between the hidden state hi and each input embedding ej or the deletion vector b.
Insertion Following Gu et al. (2019), the insertion operation consists of two phases: (1) placeholder insertion: given an input sequence y1...n, the placeholder…###…English-Japanese MT show that EDITOR achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer (Gu et al., 2019) on the standard MT tasks and exploit soft lexical constraints better: it achieves significantly better translation quality and…###While Gu et al. (2019) define roll-in using only the model’s insertion policy, we call our approach dual-path because roll-in creates two distinct intermediate sequences using the model’s reposition or insertion policy.###These issues have been addressed via partially parallel decoding (Wang et al., 2018; Stern et al., 2018) or multi-pass decoding (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al., 2019).###Figure 4 shows an example for creating the roll-in sequences: we first create the initial sequence y0 by applying random word dropping (Gu et al., 2019) and random word shuffle (Lample et al., 2018) with probability of 0.5 and maximum shuffle distance of 3 to the reference sequence y∗, and produce…###1) as in prior work (Gu et al., 2019).###We follow the same preprocessing steps in Gu et al. (2019): we apply normalization, tokenization, true-casing, and BPE (Sennrich et al., 2016b) with 37k and 40k operations for En-De and Ro-En.###Dataset Following Gu et al. (2019), we experiment on three language pairs spanning different language families and data conditions (Table 1): Romanian-English (Ro-En) from WMT16 (Bojar et al., 2016), English-German (En-De) from WMT14 (Bojar et al., 2014), and EnglishJapanese (En-Ja) from WAT2017…###Empirically, EDITOR uses soft lexical constraints more effectively than the Levenshtein Transformer (Gu et al., 2019) while speeding up decoding dramatically compared to constrained beam search (Post and Vilar, 2018).###Building on recent models for non-autoregressive sequence generation (Gu et al., 2019), EDITOR generates new sequences by iteratively editing hypotheses.###Experiments on Romanian-English, EnglishGerman, and English-Japanese MT show that EDITOR achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer (Gu et al., 2019) on the standard MT tasks and exploit soft lexical constraints better: it achieves significantly better translation quality and matches more constraints with faster decoding speed than the Levenshtein Transformer.###1 Specifically, the Levenshtein Transformer (Gu et al., 2019) showed that iteratively refining output sequences via insertions and deletions yields a fast and flexible generation process for MT and automatic post-editing tasks.###…progress on non-autoregressive sequence generation (Lee et al., 2018; Ghazvininejad et al., 2019).1 Specifically, the Levenshtein Transformer (Gu et al., 2019) showed that iteratively refining output sequences via insertions and deletions yields a fast and flexible generation process for MT…###We stop refining if 1) the output sequences from two consecutive iterations are the same (Gu et al., 2019), or 2) the maximum number of decoding steps is reached (Lee et al.",impact-revealing,acknowledge existing methods and findings in non-autoregressive sequence generation
2000,,f0f2f0293008b5688d3a6b4ea65537179c16293b,A 2μW Three-Axis MEMS-based Accelerometer,,,"###For example, single axis MEMS accelerometers are widely used for pacing rate adjustment in implantable pacemakers based on a patient's activity level [8].",impact-revealing,providing context for the application of MEMS accelerometers in pacemakers
1469,,9420e95dd6bfb333b9c6cca5c340a908847042db,Theory and research in social education 12/04,,,"###A considerable amount of psychological research supports Wittgenstein's views (e.g ., Posner & Keele, 1968 ; Reed, 1972 ; Franks & Bransford, 1971 ; Rosch, 1973, 1975 ; Rosch & Mervis, 1975 ; Rosch et al., 1976; and Hampton, 1979, 1981) .###Given that subjects can readily and consistently rate exemplars of a concept on how typical they are of the concept (e.g ., Rosch, 1975), Rosch and Mervis (1975) investigated the psychological baisis for these judgements .###Consequently, such research has been criticized for its apparent lack of concern for the complex structure of natural concepts (Rosch, 1973, 1975, 1976) .",impact-revealing,acknowledging prior psychological research supporting Wittgenstein's views
1832,,be8cd02f8f711caa97fc4e962afbc5aee34f3f24,"Discovering, quantifying, and displaying attacks",,,"###cessful strand of literature in protocol verication, where a protocol is translated into a set of rst-order Horn clauses and resolution-based theorem proving is used to establish security properties [31, 45], and by the ow logic approach to static analysis [34]. In particular, the translation from processes to propositional formulae is inspired by ProVerif [9] translation of protocols into rst-order Horn",impact-revealing,acknowledge existing literature in protocol verification
3324,599c7988601a182cd2648a09,6b7d6e6416343b2a122f8416e69059ce919026ef,Inductive Representation Learning on Large Graphs.,53e9b917b7602d97044ff484,Scikit-learn: Machine Learning in Python,"For the feature only model and to make predictions on the embeddings output from the unsupervised models, we used the logistic SGDClassiﬁer from the scikit-learn Python package [24], with all default settings.",other,reporting method used for predictions
1999,,615adc4b044ba168ec37105ef591a2e10c9550a1,Evaluación del impacto en la calidad de vida de pacientes adultos rehabilitados con nuevas prótesis removibles totales,,,"###Reconociendo como una debilidad de este trabajo no haber utilizado el OHIP-EDENT, un estudio comparativo que señala que el OHIP-14 le da mayor énfasis a los factores sicosociales que el GOHAI motivó la selección del mismo (32) .###Two of the indicators or instruments used most frequently to measure the OHRQL are the “Geriatric Oral Health Assessment Index” (GOHAI), currently renamed “General Oral Health Assessment Index” 8 and the “Oral Health Impact Profile” (OHIP).###Reconociendo como una debilidad de este trabajo no haber utilizado el OHIP-EDENT, un estudio comparativo que señala que el OHIP-14 le da mayor énfasis a los factores sicosociales que el GOHAI motivó la selección del mismo((32)).###Acknowledging as a weakness of this work that the OHIP-EDENT was not used, the choice of the OHIP-14 was motivated by a comparative study that indicates that the OHIP-14 places more emphasis on psychosocial factors than the GOHAI (32) .###Acknowledging as a weakness of this work that the OHIP-EDENT was not used, the choice of the OHIP-14 was motivated by a comparative study that indicates that the OHIP-14 places more emphasis on psychosocial factors than the GOHAI((32)).###Dos de los indicadores o instrumentos más frecuentemente utilizados para medir la CVRSO son el “Índice de Salud Oral Geriátrico” (GOHAI, del inglés: “Geriatric Oral Health Assesment Index”) actualmente renombrado “Indice de Salud Oral General” (8) y el “Perfil de Impacto de Salud Oral (OHIP, del inglés: “Oral Health Impact Profile).",impact-revealing,acknowledging a weakness in the study regarding the choice of measurement instruments
1196,,1cf4abbd052c94e63557b7922f7a5fc7e22c6e3f,Multimodal Similarity-Preserving Hashing,,,"###In the ﬁrst experiment, we reproduced the multimodal shape retrieval experiment of Bronstein et al. (2010) using the ShapeGoogle dataset Bronstein et al. (2011), containing 583 geometric shapes of 12 different classes subjected to synthetic transformations as well as 456 unrelated shapes…###For CM-SSH, we used the code with settings provided by Bronstein et al. (2010).###Furthermore, setting α X = α Y = 0 , we obtain the particular setting of cross-modal loss, whose relaxed version is minimized by the CM-SSH algorithm of Bronstein et al. (2010).###A simplified setting of the multimodal hashing problem used in [6] is cross-modality similarity-preserving hashing, in which only the inter-modal dissimilarity dXY is taken into consideration and dX , dY are ignored.###The appealing property of similarity-preserving hashing methods like the CM-SSH [6] is the compactness of the representation and the low complexity involved in distance computation.###…down to ﬁnd two embeddings ξ : minimizing the aggregate of false positive and false negative rates, (1) Cross-modality similarity sensitive hashing Bronstein et al. (2010) studied the particular case of cross-modal similarity hashing (without incorporating intra-modality similarity), with…###We compared three algorithms: our coupled siamese framework in the full multimodal setting (MM-NN) and its reduced version (CM-NN), as well as CM-SSH [6] .###This paper is motivated by the work of [6] on multimodal similarity-preserving hashing.###Furthermore, setting αX = αY = 0, we obtain the particular setting of cross-modal loss, whose relaxed version is minimized by the CM-SSH algorithm of [6].###In the first experiment, we reproduced the multimodal shape retrieval experiment of [6] using the ShapeGoogle dataset [5], containing 583 geometric shapes of 12 different classes subjected to synthetic transformations (deformations, topological noise, subsampling, etc) as well as 456 unrelated shapes (“distractors”).###[6] studied the particular case of cross-modal similarity hashing (without incorporating intra-modality similarity), with linear embeddings of the form ξ(x) = sign(Px + a) and η(y) = sign(Qy + b).###For additional details on the dataset and the descriptors, see[6], [5].###Bronstein et al. (2010) used a supervised learning algorithm based on boosting to construct hash functions of data belonging to different modalities in a way that makes them comparable using the Hamming metric.###Contributions This paper is motivated by the work of Bronstein et al. (2010) on multimodal similarity-preserving hashing.###[6] used a supervised learning algorithm based on boosting to construct hash functions of data belonging to different modalities in a way that makes them comparable using the Hamming metric.###The appealing property of similarity-preserving hashing methods like the CM-SSH Bronstein et al. (2010) is the compactness of the representation and the low complexity involved in distance computation.###For CM-SSH, we used the code with settings provided by [6].###A simpliﬁed setting of the multimodal hashing problem used in Bronstein et al. (2010) is cross-modality similarity-preserving hashing , in which only the inter-modal dissimilarity d XY is taken into consideration and d X , d Y are ignored.",impact-revealing,drawing inspiration from prior work on multimodal similarity-preserving hashing
1103,,5ee2ef1f9f4806087b7b95bfe15e6371cdbe7ed5,Semi-supervised Generative Adversarial Hashing for Image Retrieval,,,"###Network in Network Hashing (NINH) [9] straightly learns feature representation and hash functions for binary codes which preserve relative similarity of raw samples in an end-to-end manner.###Deep hashing methods (CNNH [30], NINH [9], DPSH [12], DHN [35], DSDH[11]) simultaneously learn feature representation and hash functions based on deep networks and usually are trained with supervised information.###However, class labels are more difficult and expensive to obtain than triplet-wise labels [9].###Following [30, 9], we randomly sample 100 images per class to construct query set, and the others are as the base set.###The semi-supervised methods (BGDH, SSDH, DSH-GANs) outperforms the supervised ones (NINH, CNNH), which demonstrates that unlabeled data indeed improves the performance of binary codes.###For NUS-WIDE, we follow [9] to use the images associated with the 21 most frequent concepts, where each of these concepts associates with at least 5,000 images.###Based on the evaluation protocol, we compare our SSGAH with nine stateof-the-art hashing methods, including four traditional hashing methods LSH [4], SH [29], ITQ [10], SDH [24], two supervised deep hashing methods CNNH [30], NINH [9], and three semi-supervised deep hashing methods DSH-GANs [21], SSDH [34] and BGDH [31].###In contrast, the most easily available labels are triplet-wise labels, followed by pair-wise ones and class ones [9].###Following the settings in [9], hand-crafted features for traditional hashing methods are presented by 512-dimensional GIST [20] features in the CIFAR-10 dataset and by 500-dimensional bag-of-words features in the NUS-WIDE dataset.###Supervised Ranking Term For most existing hashing methods, class labels [32], pairwise labels [30], and triplet-wise labels [9] are most frequently used as supervised information.",impact-revealing,describing the method and its comparison with existing hashing methods
859,5dc5488edf1a9c0c41511e7e,59ce117f1c290075ee7bb67b8928344b37f788cc,the impact of cache inclusion policies on cache management techniques,53e9a45cb7602d9702d79e76,Performance evaluation of exclusive cache hierarchies,"Ten years later, Zheng et al. evaluated the performance of exclusive caches with respect to inclusive [40].",impact-revealing,reporting prior findings on cache performance evaluation
3160,5f7fdd328de39f0828397e7f,9fa283d4f9c2ed991383c0434ef6043bee0dc8e2,Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification,5d9ed44b47c8f76646fab87c,Convolutional Networks with Adaptive Inference Graphs,"Some other works propose to dynamically skip unnecessary layers [52, 55, 60] or channels [33].",other,acknowledge existing methods for optimizing model layers
1438,,25d24a635cd16601eb1d758d8554d88b54f6a311,Mobile Robot Indoor Autonomous Navigation with Position Estimation Using RF Signal Triangulation,,,###It is one of the methods more widely used for sensorial fusing in mobile robotics applications [13].,impact-revealing,acknowledge a widely used method in mobile robotics
3243,5f576c1591e011f4c3d5dd7e,c36571ac50808c75fa8a5d37f1041af22e89e6ee,GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training,57a4e921ac44365e35c98ea1,Instance Normalization: The Missing Ingredient for Fast Stylization.,", 2020); Instance normalization (InstanceNorm) has been found effective for style transfer tasks (Ulyanov et al., 2016) .###…is a standard component in computer vision (Ioffe & Szegedy, 2015); Layer normalization (LayerNorm) is popular in natural language processing (Ba et al., 2016; Xiong et al., 2020); Instance normalization (InstanceNorm) has been found effective for style transfer tasks (Ulyanov et al., 2016) .###…methods shift and scale the hidden representations and are shown to help the optimization for deep neural networks (Ioffe & Szegedy, 2015; Ulyanov et al., 2016; Ba et al., 2016; Salimans & Kingma, 2016; Xiong et al., 2020; Salimans et al., 2016; Miyato et al., 2018; Wu & He, 2018;…###Closely related to our work, InstanceNorm (Ulyanov et al., 2016) is originally proposed for real-time image generation.###Normalization methods shift and scale the hidden representations and are shown to help the optimization for deep neural networks (Ioffe & Szegedy, 2015; Ulyanov et al., 2016; Ba et al., 2016; Salimans & Kingma, 2016; Xiong et al., 2020; Salimans et al., 2016; Miyato et al., 2018; Wu & He, 2018; Santurkar et al., 2018).",other,acknowledge the effectiveness of normalization methods in deep learning
1520,,f0efeb74d5cbccf9e7ac4c88f02e03558c7956e2,omparison of survival outcomes among standard radiotherapy egimens in limited-stage small cell lung cancer patients receiving oncurrent chemoradiation harles,,,"###Available trial data has employed a wide range of doses and fractionation schedules, yielding similar median survival durations of approximately 23 months in patients receiving 45 Gy in 30 fractions delivered twice daily (BID) or 70 Gy in 35 fractions delivered on a daily basis [4,6].###5 Gy, and showed a benefit to twice daily reatment.[4] As a result, the BID fractionation schema became a tandard regimen, although the trial has been criticized for the se of an inferior standard arm.",impact-revealing,highlighting the outcomes and implications of different treatment regimens in clinical trials
2422,5d04eeba8607575390f83f3a,b67fd0612e3f72faee0fed9b1e930b69ed7ee98d,sparse reram engine: joint exploration of activation and weight sparsity in compressed neural networks,5550417d45ce0a409eb3bc08,Going Deeper with Convolutions,"The neural networks used in our evaluation are LeNet [27] on MNIST, a CNN with three convolution layers and two fully-connected layers on CIFAR-10, and four large-scale CNNs (CaffeNet [26], VGG-16 [41], GoogLeNet [43], and ResNet50 [19]) on ImageNet.",other,describing the neural network architectures used in the evaluation
116,5ea16b2b91e011fa08b8f6e3,787c17aa8c3d4788e8144e0717d296754a02c244,Single-Step Adversarial Training With Dropout Scheduling,5ca600ae6558b90bfa4d76e9,Towards Deep Learning Models Resistant to Adversarial Attacks.,"[22] demonstrated that adversarially trained model can be made robust against white-box attacks, if perturbation crafted while training maximizes the loss.###In this direction, adversarial training method [22], shows promising results for learning robust deep learning models.###Method (MI-FGSM) [10] and Projected Gradient Descent (PGD) [22].###, [13, 29, 23]), in this direction Adversarial Training (AT) procedure [13, 35, 22, 40] shows promising results.###We obtain the plot of R versus iteration for models trained using single-step adversarial training method [13] and multi-step adversarial training method [22].###[22] demonstrated that models trained using adversarial samples that maximize the training loss are robust against single-step and multi-step at-###[22] demonstrated that it is possible to learn robust models using adversarial training method, if adversarial perturbations (l∞ norm bounded) crafted while training maximizes the model’s loss.###[1] showed that obfuscated gradients give a false sense of robustness, and broke seven out of nine defense papers [6, 21, 14, 38, 32, 31, 22, 21, 9] accepted to ICLR 2018.###[22] solves the maximization step by generating adversarial samples using an iterative method named Projected Gradient Descent (PGD).###Perturbation size: For l∞ based attacks, we set perturbation size ( ) to the values described in [22] i.",impact-revealing,reporting findings on adversarial training methods
2237,5f0ed12691e011ead96652e9,3611f04c1861b6e956597d56afafdefc71c6af6a,TCGM: An Information-Theoretic Framework for Semi-Supervised Multi-Modality Learning,53e9b2c6b7602d9703d6f058,Co-Adaptation of audio-visual speech and gesture classifiers.,"[17,3,12,21,11] use weak classifiers trained by the labeled data from each modality to bootstrap each other by generating labels for the unlabeled data.",other,reporting prior findings on weak classifiers and their training methods
3156,5bdc315017c44a1f58a05e7e,7a71941e60894ae7e1f5af8e79c37cec6cd6c6ad,Multi-scale Residual Network for Image Super-Resolution,5d9edbf747c8f7664602da0d,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,"After that, many SR models have been proposed, including DRCN [5], DRNN [7], LapSRN [6], SRResNet [8], and EDSR [9].###It based on SRResNet [8] while enhanced the network by removing the normalization layers as well as using deeper and wider network structures.###In this work, we have reconstructed some classic SR models, such as SRCNN [1], EDSR [9] and SRResNet [8].",other,reporting advancements in super-resolution models
3103,5b67b46b17c44aac1c861fc4,f6d06993e003fa6fec5bf630efded9e4fd90a030,fluency boost learning and inference for neural grammatical error correction,53e99b43b7602d97023e4c70,Mining Revision Log of Language Learning SNS for Automated Japanese Error Correction of Second Language Learners.,"As previous studies (Ji et al., 2017), we use the public Lang-8 Corpus (Mizumoto et al., 2011; Tajiri et al., 2012), Cambridge Learner Corpus (CLC) (Nicholls, 2003) and NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) as our original error-corrected training data.",other,reporting data sources used for training
2696,53e9a232b7602d9702b3a1a9,327722247ffc70a0d51f5c2246bc9a53c0e7daa3,Accurate branch prediction for short threads,53e9ac19b7602d97035db49b,A scalable approach to thread-level speculation,"Those proposed include models that work best when threads are largely independent [44, 7] and those that work even in the presence of dependences [24, 32].",other,acknowledging variations in proposed models
2454,5f0bde8e9e795ea206ff8ef5,0feea94f89d395436bf41bd10c797447eecbc128,Unsupervised data augmentation for consistency training,5736986b6e3b12023e72fb73,Semi-supervised Sequence Learning,"More recently, Dai and Le [10], Peters et al. [49], Radford et al. [50], Howard and Ruder [26], Devlin et al. [13] have shown that pre-training using language modeling and denoising auto-encoding leads to signiﬁcant improvements on many tasks in the language domain.",other,highlighting significant advancements in pre-training methods for language tasks
2397,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",59ae3c0c2bbe271c4c71d6ac,On the Graph Fourier Transform for Directed Graphs.,"As an example, the work in [27] uses an optimization procedure to construct explicitly an orthogonal basis set that minimizes a quantity related to the cut size.",other,reporting prior findings on optimization procedures
11,599c7988601a182cd2648a09,6b7d6e6416343b2a122f8416e69059ce919026ef,Inductive Representation Learning on Large Graphs.,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,"Since, the “convolutional” variant of GraphSAGE is an extended, inductive version of Kipf et al’s semi-supervised GCN [17], we term this variant GraphSAGE-GCN.###A simple variant of our algorithm can be viewed as an extension of the GCN framework to the setting of inductive, unsupervised learning, a point which we revisit in Section 3.3.###The proof of Theorem 1 relies on some properties of the pooling aggregator, which also provides insight into why GraphSAGE-pool outperforms the GCN and mean-based aggregators.###The mean aggregator is nearly equivalent to the convolutional propagation rule used in the transductive GCN framework [15].###So far, graph convolutional networks (GCNs) have only been applied in the transductive setting with fixed graphs [17, 18].###It is worth noting, however, that Kipf et al’s “featureless” GCN approach has parameter dimension O ( |V| ) , so this requirement is not entirely unreasonable [15, 16].###However, the performance GraphSAGE-GCN was not so robust, which makes intuitive sense given that the Lemmas 1, 2, and 3 rely directly on the universal expressive capability of the pooling aggregator.###The original GCN algorithm [17] is designed for semi-supervised learning in a transductive setting, and the exact algorithm requires that the full graph Laplacian is known during training.###(4)Note that this differs from Kipf et al’s exact equation by a minor normalization constant [17].###4% on average) compared to an aggregator inspired by graph convolutional networks [17].###In this work we both extend GCNs to the task of inductive unsupervised learning and propose a framework that generalizes the GCN approach to use trainable aggregation functions (beyond simple convolutions).###(That said, Kipf et al [15][16] found that GCN-based approach consistently outperformed DeepWalk, even in the transductive setting on link prediction, a task that theoretically favors DeepWalk.)###However, our approach is closely related to the graph convolutional network (GCN), introduced by Kipf et al. [15, 16].###Again we see that GraphSAGE signiﬁcantly outperforms the baseline approaches, with the LSTM and max-pooling based aggregators providing substantial gains over the mean and GCN-based aggregators.###The mean aggregator is nearly equivalent to the convolutional propagation rule used in the transductive GCN framework [17].###Since, the “convo-lutional” variant of GraphSAGE is an extended, inductive version of Kipf et al’s semi-supervised GCN [15], we term this variant GraphSAGE-GCN.###We ﬁnd that GraphSAGE outperforms all the baselines by a signiﬁcant margin, and the trainable, neural network aggregators provide signiﬁcant gains compared to the GCN approach.###There are also recent approaches to learning over graph structures using convolution operators that offer promise as an embedding methodology [17].###In particular, we can derive an inductive variant of the GCN approach by replacing lines 4 and 5 in Algorithm 1 with the following: We call this modiﬁed mean-based aggregator convolutional since it is a rough, linear approximation of a localized spectral convolution [15].###So far, graph convolutional networks (GCNs) have only been applied in the transductive setting with ﬁxed graphs [15, 16].###We call this modified mean-based aggregator convolutional since it is a rough, linear approximation of a localized spectral convolution [17].###The original GCN algorithm [15] is designed for semi-supervised learning in a transductive setting, and the exact algorithm requires that the full graph Laplacian is known during training.",impact-revealing,highlighting the extension and performance of GraphSAGE-GCN compared to existing methods
1702,,18e291c9c08036996a58b255609dcfe2ab69a89d,Recategorization into the In-group,,,"###According to social categorization theory (Tajfel and Turner, 1979; Turner, 1987), people tend to view individuals as belonging to distinct social categories based on their salient attributes, such as gender, ethnicity, and age.###Developed by Gaertner and colleagues (1989), recategorization theory builds on but is distinct from social categorization theory, which focuses on the occurrence of out-group categorization and its negative consequences (Tajfel and Turner, 1979; Turner, 1987).",impact-revealing,providing context on social categorization and recategorization theories
167,5ec49a639fced0a24b4de7d4,9eda533cf0badf8dbed5c8240bb828b622328183,Fine-grained Fact Verification with Kernel Graph Attention Network,5cf48a40da56291d582a2b85,Deeper Text Understanding for IR with Contextual Neural Language Modeling,"The kernel extracts matching patterns which provide a variety of relevance match signals and shows strong performance in various ad-hoc retrieval dataset (Dai and Callan, 2019).",impact-revealing,highlighting the effectiveness of the kernel in retrieval tasks
2049,,b001451e49befbba00fe58eaaf8cd458e9087798,Efficient optimization for labeling problems with prior information: applications to natural and medical images,,,"###Graph-search (also known as LOGISMOS [110, 64]) is widely used for medical image segmentation [52, 34, 87].###While the interacting surfaces with known topologies are segmented via graph searching ([52]).###For the high-order terms with multiple possible variable values, the multilabeling problem is first transformed to an equivalent binary high-order labeling problem using the “column” technique in graph-search [52, 106].###Graph-searching ([52]) is an effective approach for segmenting terrain-like surfaces.###The method integrates a graph-cut subgraph for the object segmentation with a graph-searching subgraph ([52]) for the adjacent surface segmentation.###Our approach results in a novel integration of the traditional graph-cut method [12] and the graph searching method ([52]).###The graph-searching method ([52]) is used to add arcs within each subgraph to encode the energy in Eq.###graph-cut [52] and graph-search [15] are reviewed.###We develop a new segmentation approach that would for the first time integrate graph cut ([12]) and graph searching ([52]) to combine the strengths of both methods while overcoming their individual drawbacks.###This ‘column’ structures enables elegant incorporation of various priors like minimum/maximum surface distance constraints when applied to the multi-surface segmentation problem [52], which proves to be very beneficial for medical image segmentation where such prior informations are usually vital to obtain good segmentation [87, 33, 34, 51, 80].",impact-revealing,highlighting the novel integration of graph-cut and graph-search methods for improved medical image segmentation
2953,5f7fdd328de39f08283980ba,2fce1ef37391cd685fc5459e1cbfcb8490b85242,Graph information bottleneck,5a9cb66717c44a376ffb8667,FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling,"Graph Neural Networks (GNNs) [1, 3, 5–7] have demonstrated impressive performance, by learning to fuse information from both the node features and the graph structure [8].",other,highlighting the performance of Graph Neural Networks in learning from node features and graph structure
417,59ae3c262bbe271c4c71e9e8,718e1b453fe9dce79458e0db035091db603775fb,Deep Pyramid Convolutional Neural Networks for Text Categorization,58437725ac44360f1082ff5b,Convolutional Neural Networks for Text Categorization: Shallow Word-level vs. Deep Character-level.,"Note that ShallowCNN enhanced with unsupervised embeddings (row 2) was originally proposed in (Johnson and Zhang, 2015b) as a semi-supervised extension of (Johnson and Zhang, 2015a), and then it was tested on the large datasets in (Johnson and Zhang, 2016).###For example, as also mentioned in (Johnson and Zhang, 2016), the complete vocabulary of the Ama.p training set contains 1.3M words.###To facilitate comparison with ShallowCNN, we matched our unsupervised embedding setting exactly with that of (Johnson and Zhang, 2016).###Enhancing region embedding with unsupervised embeddings In (Johnson and Zhang, 2015b, 2016), it was shown that accuracy was substantially improved by extending ShallowCNN with unsupervised embeddings obtained by tv-embedding training (‘tv’ stands for two views ).###However, in (Johnson and Zhang, 2016), very shallow 1-layer word-level CNNs were shown to be more accurate and much faster than the very deep character-level CNNs of (Conneau et al., 2016).###Data pre-processing was done as in (Johnson and Zhang, 2016).###Moreover, DPCNN can be regarded as a deep extension of ShallowCNN, which we proposed in (Johnson and Zhang, 2015b) and later tested with large datasets in (Johnson and Zhang, 2016).###For comparison, the Shal-lowCNN results (green ‘x’) from (Johnson and Zhang, 2016) are also shown.",impact-revealing,acknowledge the development and testing of ShallowCNN with unsupervised embeddings
2875,5736982b6e3b12023e6fd332,d0b8c5464b582e1a582a4affb270e8b02f8414a2,a scalable architecture for ordered parallelism,53e9bd6fb7602d9704a0cd33,Pin: Building Customized Program Analysis Tools With Dynamic Instrumentation,"[34] M. A. Hassaan, D. Nguyen, and K. Pingali, “Kinetic Dependence Graphs,” in ASPLOS-XX, 2015.###[55] K. Pingali, D. Nguyen, M. Kulkarni et al., “The tao of parallelism in algorithms,” in PLDI, 2011.###[46] C. Luk, R. Cohn, R. Muth et al., “Pin: building customized program analysis tools with dynamic instrumentation,” in PLDI, 2005.###[32] M. A. Hassaan, D. Nguyen, and K. Pingali, “Brief announcement: Parallelization of asynchronous variational integrators for shared memory architectures,” in SPAA, 2014.###Modeled system: We use an in-house microarchitectural, event-driven, sequential simulator based on Pin [46] to model a 64-core CMP with a 3-level cache hierarchy.###[33] M. A. Hassaan, M. Burtscher, and K. Pingali, “Ordered vs. unordered: a comparison of parallelism and work-efficiency in irregular algorithms,” in PPoPP, 2011.###Analysis tool: We developed a pintool [46] to analyze these programs in x86-64.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2829,5aed148b17c44a4438154fae,fa54b47df8641dff1579b5e8e0f18f057de68e73,DRN: A Deep Reinforcement Learning Framework for News Recommendation,53e9bb44b7602d9704785e5b,SCENE: a scalable two-stage personalized news recommendation system.,"Although there are some online recommendation methods [11, 24] that can capture the dynamic change of news features and user preference through online model updates, they only try to optimize the current reward (e.###The current methods can be generally categorized as content based methods [19, 22, 33], collaborative filtering based methods [11, 28, 34], and hybrid methods [12, 24, 25].###To combine the advantages of the former two groups of methods, hybrid methods [12, 24, 25] are further proposed to improve the user profile modeling.",other,acknowledge existing online recommendation methods and their categorization
1931,,cbb6b148033d070fbfb325d976d8bbd10ac4787f,Cytotoxic effects of chitosan against oral cancer cell lines is molecular-weight-dependent and cell-type-specific,,,"###It has been widely used as a multipurpose biomaterial because it is biocompatible, biodegradable, not toxic and adsorptive (Singla and Chawla, 2001; Suh and Matthew, 2000).",impact-revealing,highlighting the properties and applications of a biomaterial
1198,,70db90eb6fd4c0ff32228bf7e0aa49cbb3918cc6,On the utilization of Simultaneous Localization and Mapping(SLAM) along with vehicle dynamics in Mobile Road Mapping Systems,,,###The use of the EKF was motivated by the initial work of [22] and [77] which established a statistical basis for describing relationships between landmarks and manipulating geometric uncertainty.,impact-revealing,highlighting the foundational work that motivated the use of EKF
2214,5edf5dd891e011bc656deb7d,14b65a86c82e38fce0eb3506e0d4084ad5cdb583,deberta- decoding-enhanced bert with disentangled attention,5b8c9f4a17c44af36f8b6977,SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense   Inference,"‚ SWAG is a large-scale adversarial dataset for the task of grounded commonsense inference, which unifies natural language inference and physically grounded reasoning [35].###0 [32], RACE [33], ReCoRD [34] and SWAG [35]; (2) Natural Language Inference: MNLI [36]; and (3) NER: CoNLL-2003.###SWAG consists of 113k multiple choice questions about grounded situations.",other,highlighting the significance of the SWAG dataset in commonsense inference
1428,,70ed12638e3ac76597dfcf268e831798d697d8ab,Multi-Task Ofﬂine Reinforcement Learning with Conservative Data Sharing,,,"###A number of ofﬂine RL algorithms use some kind of regularization on either the policy [36, 20, 80, 29, 66, 55] or on the learned Q-function [38, 35] to ensure that the learned policy does not deviate too far from the behavior policy.###In this paper, utilizing the tools from the safe policy improvement framework [40, 56, 38, 89], we show that data sharing can be harmful or brittle in the ofﬂine setting because it can exacerbate the distribution shift between the policy represented in the data and the policy being learned.###Fortunately, we can optimize a lower-bound approximation to Equation 4 that uses the dataset state distribution for the policy up-date in Equation 4 similar to modern actor-critic meth-ods [12, 44, 21, 26, 38] which only introduces an additional D ( π, π β ) term in the objective.###To formally characterize the effects of data-sharing in multi-task ofﬂine RL, we appeal to safe policy improvement bounds [40, 38, 89] and discuss cases when data-sharing between tasks i and j can degrade the amount of worst-case guaranteed improvement over the behavior policy.###First note that for any given policy π , and a dataset D eﬀ i with effective behavior policy π β ( a | s ) , the following bound holds [38]: where the O ( · ) notation hides constants depending upon the concentration properties of the MDP [40] and 1 − δ , the probability with which the statement…###We build on the analysis of safe-policy improvement guarantees of conventional ofﬂine RL algorithms [40, 38] and show that data sharing using CDS attains better guarantees in the worst case.###…induced by the transitions in the dataset, and D ( π, π β ) denotes a divergence measure (e.g., KL-divergence [29, 80], MMD distance [36] or D CQL [38]) between the learned policy π and the behavior policy π β , where D CQL ( p, q ) denote the following distance between two distributions p ( x )…###For our analysis in this work, we will abstract these algorithms into a generic policy optimization problem [38]: J D ( π ) denotes the average return of policy π in the empirical MDP induced by the transitions in the dataset, and D ( π, π β ) denotes a divergence measure (e.g., KL-divergence [29,…###Prior work [38] has shown that the generic ofﬂine RL algorithm in Equation 1 enjoys the following guarantees of policy improvement on the actual MDP, beyond the behavior policy: In the above equation, ( J D ( π ) − J D ( π β )) denotes the improvement of π over π β in the empirical MDP induced by…###Prior works [38] have shown that the optimal policy π ∗ i that optimizes Equation 1 attains a high probability safe-policy improvement guarantee, i.e., J ( π ∗ i ) ≥ J ( π β ) − ζ i , where ζ i is: The ﬁrst term in Equation 6 corresponds to the decrease in performance due to sampling error and this…",impact-revealing,highlighting the significance of data-sharing in multi-task offline reinforcement learning
2150,,48f7199fc1c36c9aca6d1f845608e90e2e5b07f3,Symmetry group factorization reveals the structure-function relation in the neural connectome of Caenorhabditis elegans,,,"###Circulant matrices are well-known in the field of digital signal processing, recurrent and feedforward neural networks [4] and cryptography, and are widely used as efficient linear filters to solve a variety of tasks in digital image processing, most notably as edge-detection and signal compression [4, 30], but also in tracking [31], voice recognition, and computer vision [32].###[30] Mitra, A.###For instance a widely used filter in signal processing is the edge-detector [4, 30] which employs a circulant matrix defined by M = circ(0, 1,−1, 0, · · · , 0) to compute a ‘derivative’ of the spatial signal and detect sharp edges [4].",impact-revealing,highlighting the applications and significance of circulant matrices in various fields
2328,5ec49a639fced0a24b4de82e,724d182e2b0217ce10ff05e3ad0b2f548fb39b9f,Iterative Edit-Based Unsupervised Sentence Simplification,56d842b0dabfae2eee950b60,An evaluation of syntactic simplification rules for people with autism,"For instance, it can bene-ﬁt people with autism (Evans et al., 2014), dyslexia (Rello et al., 2013), and low-literacy skills (Watan-abe et al., 2009).",other,highlighting the potential benefits of the approach for specific groups
919,53e9afa5b7602d97039f09be,140145d91bf2e2d0039d7986a70b656c077b5e43,Static and dynamic co-optimizations for blocks mapping in hybrid caches,558c261ae4b02b9f07a55c62,High-endurance and performance-efficient design of hybrid cache architectures through adaptive line replacement,"Another set of work migrates the write-intensive cache blocks to other cache lines in the same/different cache set or in the SRAM in order to reduce the average write frequency of the STT-RAM cache lines [4].###The lifetime is measured from the start of the simulation until the first STT-RAM line becomes defective, which is similar to the estimation methodology proposed in [4] and [5].###In this work, we assume that the L2 cache is a hybrid cache architecture with 4-way SRAM and 12-way STT-RAM, which is similar to the setting described in [4][7].###We use the dynamic migration scheme similar to [4], which is briefly described as follows.###Note that life time of the STT-RAM mainly depends on the peak write count of all the cells [4][5].###As an example, Table 1 shows the STT-RAM cell write count distribution of the segmentation application [8] for both of the pure static [10] and the pure dynamic [4] scheme.###In this work, we use a 1MB 16-way hybrid cache including a 4way SRAM data array and a 12-way STT-RAM data array similar to the configurations used in [2][4][7].###Compared to PRAM, STT-RAM has significantly higher endurance (10(9) versus 10(12) write cycles) and shorter write latency [1] and is much more promising in the last-level cache design [2] [3][4][5][6][7].###Pure dynamic optimization (dynamic): We use the dynamic migration scheme proposed in [4].###Moreover, due to the intensive writes of caches, hybrid caches consisting of both SRAM and STT-RAM are investigated [2][3][4][7], where the SRAM can accommodate write-intensive data and the STT-RAM can accommodate other data with its dense capacity.",impact-revealing,reporting on cache architecture and write optimization methods
3099,5db92a1a47c8f76646200974,8330c7c98c6c3e338ca578e0ab47f41bc18d0019,HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs,58d82fd2d649053542fd75d8,Geometric deep learning: going beyond Euclidean data.,"Deep learning on graphs: Geometric deep learning [5] is an umbrella phrase for emerging techniques attempting to generalise (structured) deep neural network models to non-Euclidean domains such as graphs and manifolds.###The reader is referred to a comprehensive literature review [5] and extensive surveys [20, 4] on this topic of deep learning on graphs.###The detailed derivation from the convolution theorem uses existing tools from graph signal processing [41, 21, 5] and is provided in the supplementary material.",other,providing context and references for geometric deep learning
728,5c2c7a9217c44a4e7cf314f8,b5e6fb219ca76f0175bd9f28e1460280fa11d5e1,making classification competitive for deep metric learning,573696966e3b12023e59fda6,Learning visual similarity for product design with convolutional neural networks,Standard deep neural network metric learning approaches learn image representations through the local relationships between images in the form of pairs [3] [1] or triplets [8] [19].,impact-revealing,providing context on standard deep neural network metric learning approaches
3378,5ede0553e06a4c1b26a83f63,1d81e7f428fea2b2e15ee3a96fe843ca603acc4c,Simple and Deep Graph Convolutional Networks,5bdc31b817c44a1f58a0c14d,GAMENet: Graph Augmented MEmory Networks for Recommending Medication   Combination,"…range of applications, including social analysis (Qiu et al., 2018; Li & Goldwasser, 2019), traffic prediction (Guo et al., 2019; Li et al., 2019), biology (Fout et al., 2017; Shang et al., 2019), recommender systems (Ying et al., 2018), and computer vision (Zhao et al., 2019; Ma et al., 2019).",other,acknowledge diverse applications of the research
588,5c04966a17c44a2c74708959,6b98bef930182a848c027dece1bfb58ca706449d,Improving End-to-end Speech Recognition with Pronunciation-assisted Sub-word Modeling,5a73cbc317c44a0b3035edb5,No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models,"On the other hand, linguists have developed very sophisticated pronunciation dictionaries of high quality for most languages, which can potentially improve the performance of end-to-end systems [7].###[10] use the phone-level alignment to generate a probabilistic lexicon and proposed a worddependent silence model to improve ASR accuracy; for use in end-to-end ASR models, [7] investigated the value of a lexicon in end-to-end ASR.",impact-revealing,acknowledging advancements in pronunciation dictionaries and their potential impact on ASR systems
3694,53e9b11db7602d9703b99bc4,41f128798ca097aec531ff7804aafe1043c019a1,Stream chaining: exploiting multiple levels of correlation in data prefetching,53e9aa79b7602d97033f191f,Branch History Guided Instruction Prefetching,"The work in [21] proposed using the PC of branches to localize streams.###Popular options for miss stream localization include grouping misses according to the PC of the instruction that generated them [2, 4, 12], or to the region of memory they reference [13, 14, 19], or to some period of time in which the misses occur [22], or even to previously executed branch instructions [21].",other,reporting prior findings on stream localization methods
1728,,96ded74141a2113e0add275a37541f1355aa13a0,Epidemia: Variable Consistency for Transactional Cloud Databases,,,"###In general, cloud database systems have put aside traditional replication techniques [Daudjee & Salem(2006), Wiesmann & Schiper(2005)] to overcome their scalability limitations [Gray et al.(1996)].###However, the throughput of update operations in primarybackup solutions is limited by the capacity of the primary [Gray et al.(1996)].###[Gray et al.(1996)].",impact-revealing,acknowledging limitations in traditional replication techniques for cloud databases
3025,5f993ec291e011a3fbe2fb5c,f1e5e65941617604923225cc4bf464e370fcae67,Combining Label Propagation and Simple Models Out-performs Graph Neural Networks,5992a3b85ba2006b76482e09,Local Higher-Order Graph Clustering,"Finally, we use an email dataset of a European research institute, where classes are department membership and there are no features (Leskovec et al., 2007; Yin et al., 2017).",other,reporting dataset details
762,53e9ac5bb7602d970362bd46,9e7cfbf2083432ea013685eeeea2670c87ea4c55,comparing program phase detection techniques,558ac999e4b031bae1f9acc4,Dynamic microarchitecture adaptation via co-designed virtual machines,"In addition to instruction working set based techniques [10][11], we evaluate branch and procedure working set based techniques.###Consequently, algorithms such as the ones proposed in [10][11] do not perform tuning while in unstable regions.###A working set signature is a lossy-compressed representation of the complete working set [10][11].###In previous work [10][11], we proposed a hardware structure called the working set signature, which is a compact representation of the working set.###Recently, several researchers have proposed hardware techniques aimed specifically at detecting phase changes, identifying phases and predicting phases [2][10] [11][12][19].###In previous work [10][11], we defined a similarity metric called the relative working set distance, to compare working sets.###explicitly detect program phase changes [2][10][11][12] [19][20][21].###We propose the use of virtual machine software to compute the relative signature distance [10][11].###In this work, we focus on three of these proposed schemes – the first based on working set signatures [10][11], the second based on basic block vectors [19], and the third based on a conditional branch counter [2].###Program phase detection techniques have been used in power/performance optimization algorithms [2][10][11] [12][19] and to reduce simulation time of benchmarks by identifying sections of code whose performance is representative of the entire benchmark [20][21].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3921,5d3ed25a275ded87f97deae1,f5252075bb34666863cd01cc82c2d941d4ffe6c6,robust graph convolutional networks against adversarial attacks,5d0b00548607575390fbd152,Disentangled Graph Convolutional Networks,"Further improvements include adding an attention mechanism to assign different weights in aggregating node neighborhoods [28, 30, 31, 34], adding residual and jumping connections [32], sampling to improve efficiency [4, 5, 14, 33] considering edge attributes [15, 23, 26], disentangling node representations [20] and automatically selecting hyper-parameters [29].",other,acknowledge various improvements in graph-based models
1117,,817721d43f5792f1db88015a0b330f4facf115fb,Frame by Familiar Frame: Understanding Replication in Video Diffusion Models,,,"###Diffusion Probabilistic Models (DPMs) are a sub-set of deep generative models known for their ability to incrementally introduce noise into data points [14,38].###Recent advancements in diffusion-based image generative models have paved the way for video generation [14, 32, 38].",impact-revealing,highlighting advancements in diffusion probabilistic models and their applications
1258,,687e6af89c6ece57974e1c22ced728f7a9ee5961,Recent advances in the use of optical coherence tomography in neuro‐ophthalmology: A review,,,"###42 Similar to acute anterior optic neuritis, the value of the pRNFL in assessing axonal loss in the acute stages of non-arteritic anterior ischemic optic neuropathy (NA-AION) is limited by optic nerve head oedema and therefore in the early stages, mGCIPL thinning is a better indicator of axonal…",impact-revealing,highlighting the limitations of pRNFL in assessing axonal loss
396,5b3d98b017c44a510f7ffede,bf230a74d62b98d6c16c06161f89f89626f12f12,intelligent video surveillance for real-time detection of suicide attempts,557c5aaf6feeaa8086d9b331,Detection of a Suicide by Hanging Based on a 3-D Image Analysis,"Unlike in (Lee et al., 2014), we performed our experiments on a large dataset captured by an RGB-D camera, where 21 persons are asked to simulate different scenarios for unsuspected behavior and suicide attempts.###In this sense, (Lee et al., 2014) presented a method for automatically analyzing depth images captured by an Asus Xtion Pro camera, and detect suicidal behavior.",impact-revealing,highlighting differences in experimental setups and datasets
3591,5f0d8b6891e011047aff993b,1c53d27c742fb4658fa03085c7c2ca014a122385,ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Learning,5dfb4a97df1a9c0c4164fbf3,Modeling aspects of the language of life through transfer-learning protein sequences,"Using embeddings as exclusive input to relatively small-size CNN/FNN models without much optimization yielded methods that appeared competitive in predicting secondary structure, localization and in classifying proteins into membrane/other.###The leap of NLP through advanced LMs has been successfully generalized toward understanding the language of life through advanced LMs trained on proteins [31], [32], [33], [34], [35], [36], [37], [38], [39].###Special thanks to Konstantin Schütze for helping with grant writing and providing early results for the structure prediction task.###Throughout, we extracted the embeddings from the last hidden state of the pre-trained LMs as described in detail elsewhere [32].###This could save immense expenses when routinely applying embedding-based protein predictions to large data sets, but it also opens a path toward protein-speciﬁc rather than family-averaged predictions.###…or pre-training was measured. mmseqs2 was run on an Intel Skylake Gold 6248 processor with 40 threads, SSD and 377GB main memory, while protein LMs were run on a single Nvidia Quadro RTX 8000 with 48GB memory using half precision and dynamic batch size depending on sequence length (blue bar).",other,highlighting the potential of embedding-based methods in protein predictions
270,5feb068b91e011f5d3420813,e339c5d31ffc7029c1f72d567ac07b4606701c72,ALP-KD: Attention-Based Layer Projection for Knowledge Distillation,5d89e9483a55acd95282ff4c,TinyBERT: Distilling BERT for Natural Language Understanding,"Other models such as TinyBERT (Jiao et al. 2019) and MobileBERT (Sun et al. 2020) also found it crucial for training competitive student models.###Other models such as TinyBERT (Jiao et al. 2020) and MobileBERT (Sun et al.###A common heuristic to deviseA is to divide teacher layers into m buckets with approximately the same sizes and pick only one layer from each (Jiao et al. 2020; Sun et al. 2019).###Different alternatives have been proposed to this end, which compare networks’ internal layers in addition to final predictions (Jiao et al. 2020; Sun et al. 2020, 2019), but they suffer from other types of problems.",impact-revealing,acknowledge existing models and their importance
1859,,eea7caef1699d9689502ea58620e0f2b7f98895b,Sulfone Metabolite of Fipronil Blocks γ-Aminobutyric Acid- and Glutamate-Activated Chloride Channels in Mammalian and Insect Neurons,,,"###Under normal use conditions, fipronil can be degraded mainly to fipronil sulfone via biotic/abiotic oxidation and to a desulfinyl photoproduct via photolysis (Environmental Protection Agency, 1996; Bobe et al., 1998; Ngim et al., 2000; Fenet et al., 2001; Tingle et al., 2003).###Consistent with this notion are the observations that the toxicity of fipronil sulfone to freshwater invertebrates, freshwater fish, and birds was indeed higher than that of the parent compound (Environmental Protection Agency, 1996; Schlenk et al., 2001; Tingle et al., 2003).###Fipronil, a highly effective phenylpyrazole insecticide, has become widely used for control of a wide range of crop, public hygiene, amenity, and veterinary pests since its introduction to the market in 1993 (Moffat, 1993; Tingle et al., 2003).",impact-revealing,highlighting the environmental impact and degradation of fipronil
2638,5cede0f6da562983788d5a5f,31c343d741b31daeab7cce6ddb768767523d185e,Relational Graph Attention Networks.,5736986c6e3b12023e7308a0,Convolutional Networks on Graphs for Learning Molecular Fingerprints,"Spatial approaches are limited by an absence of shift invariance and lack of coordinate system (Duvenaud et al., 2015; Atwood and Towsley, 2016; Monti et al., 2017).",other,highlighting limitations in spatial approaches
2505,5ecb57199e795ec6f2ba59cc,512f34906ddaefe885af2e5eec9b2b3b50ffd377,deep entity matching with pre-trained language models,5cda9eabced107d4c67d513d,The WDC Training Dataset and Gold Standard for Large-Scale Product Matching,"When reporting DeepMatcher’s F1 scores, we use the numbers in [27] for the ER-Magellan datasets and numbers in [31] for the WDC datasets.###The numbers for DeepMatcher (DM) are taken from [31].###• Weevaluated the effectiveness ofDitto on three benchmark datasets: the Entity Resolution benchmark [21], the Magellan dataset [20], and the WDC product matching dataset [31] of various sizes and domains.###Meanwhile, DeepMatcher is allowed to use any subsets of attributes to determine the best attribute set as in [31].###We present the experiment results on benchmark datasets for EM: the ER Benchmark datasets [21], the Magellan datasets [20] and the WDC product data corpus [31].###The WDC product data corpus [31] contains 26 million product offers and descriptions collected from e-commerce websites [47].",other,reporting evaluation metrics and datasets used in experiments
3776,57d063e0ac44367354294777,9c62532d78d6eb31f7b0489b6bcb4c2baf517860,CASH: Supporting IaaS Customers with a Sub-core Configurable Architecture,558af9e484ae84d265c0c0cf,TILE64 - Processor: A 64-Core SoC with Mesh Interconnect,Highly configurable processors have unique benefits for the IaaS Cloud as resources can be moved between customers.###The move to IaaS systems has spawned new architectures optimized for the data center and the Cloud.###This feature enables IaaS Clouds to spatially allocate computation resources.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
602,558aed5ce4b0b32fcb39b5f6,ebe508e6a460e35e5c7a66d03c041d97e9145607,high-performance routing at the nanometer scale,53e9a931b7602d9703286b08,PathFinder: a negotiation-based performance-driven router for FPGAs,"FGR s core algorithms are 
directly relevant to detail routing of ASICs and FPGAs, while its constraint-driven nature makes it amenable 
to the handling of com­plex design rules.###NCR builds upon RRR by gradually mak­ing routing edges that are consistently 
congested more expensive, en­couraging the maze router to choose alternative routes when they are available.###We note that negotiated-congestion routing (NCR) was originally developed for global routing of FPGAs [21], but in most recent work [29] has been extended with A*-search and applied to hard instances of detail routing for FPGAs.###The ordering of nets during rip-up-andre-route is the same for each iteration, but can be chosen arbitrarily, according to the authors of [21], because the gradual cost increase in congested areas removes ties that require sophisticated net ordering techniques in traditional RRR implementations.###is a function of the intrinsic cost (be), added cost reflecting congestion history (he), and penalty for current congestion (pe) [21].###All analysis and improvements to NCR reported in our work are directly applicable 
in that context.###NCR seeks to minimize 
.e ce.###[20] D. McGrath, Routing Technology Came from Within Cadence, ex­ecs say, EE Times, Sept. 8, 2006. 
http://www.eetimes.com/ showArticle.jhtml?articleID=192700243 [21] L. McMurchie and C. Ebeling, PathFinder: 
A Negotiation-based Performance-driven Router for FPGAs, In Proc.###Negotiated-congestion 
Routing (NCR) [21] was introduced in the mid-1990s for global routing in FPGAs, but has not seen much 
use in the ASIC literature.###APPLICATIONS 
We now discuss applications of proposed techniques for high­performance routing to congestion and delay 
estimation, timing­driven routing, as well as detail routing of ASICs and FPGAs.###Major differences between various implementations [4,8,10,21,23,24] include which nets are ripped up and rerouted at each iteration, the order in which to rip up nets and reroute them, if nets are allowed to be rerouted through areas that are already congested, and the costs associated with routing through a particular routing edge given its current congestion.###Additionally, MCF techniques offer less .exibility in terms 
of objective functions and constraints than the RRR and NCR frameworks.###We note that negotiated-congestion routing (NCR) was originally developed for global routing 
of FPGAs [21], but in most recent work [29] has been extended with A*-search and applied to hard instances 
of detail routing for FPGAs.###which is different than Formula 1 [21], but also makes more sense since it preserves the base cost.###Therefore FGR uses this Discrete Lagrange Multiplier (DLM) formulation 
instead of NCR which was used in FGR s ISPD 07 contest submission.###Moreover, the constraint-driven nature of DLM provides a generic way to handle new constraints, 
including those endemic to FPGAs and ASICs.###Detail Routing for ASICs and FPGAs.###However, since Lagrange multipliers 
remain continuous, the same update rule can be adopted.3 Interpreting Formula 6 for a given net i in 
terms of NCR yields ce =be +he · pe (8) which is different than Formula 1 [21], but also makes more sense 
since it preserves the base cost.###Negotiated-congestion Routing (NCR) [21] was introduced in the mid-1990s for global routing in FPGAs, but has not seen much use in the ASIC literature.###Instead of routing all nets by their short­est 
paths to .nd an initial routing solution, which is common in NCR, FGR uses be +pe as the weight for edges 
to create an initial solution.###In this work we develop a high-performance routing technique based on Discrete Lagrange Multipliers (DLM), while pointing out inaccuracies, limitations and pitfalls of the related technique known as negotiated-congestion routing [21].",impact-revealing,highlighting the limitations and differences of routing techniques in ASICs and FPGAs
3986,5dce788a3a55ac9580a162f8,56cafbac34f2bb3f6a9828cd228ff281b810d6bb,KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation,5a260c2817c44a4ba8a236bf,Position-aware Attention and Supervised Data Improve Slot Filling.,"To incorporate world knowledge into our pre-trained language representation models (PLMs), we design a multi-task loss as shown in Figure 1 and Equation 1, where L KE represents knowledge embedding loss and L LM represents language model loss.###Since our PLMs are involved in both tasks, jointly optimizing the two objectives could implicitly integrate knowledge from external graphs with text encoders, while keeping the strong abilities of PLMs for syntactic and semantic understanding.###(3) Their sophisticated mechanisms to retrieve and use entity embeddings lead to additional inference overhead compared with vanilla PLMs.###Pre-trained language representation models (PLMs) such as ELMo (Peters et al., 2018a), BERT (Devlin et al., 2019a) and XLNet (Yang et al., 2019) learn effective language representations from large-scale nonstructural and unlabelled Work in progress. corpora and achieve great performance on various NLP tasks.###Here we follow the relation extraction ﬁne-tuning procedure from Zhang et al. (2019), where four special tokens are added before and after entity mentions in the sentence to highlight where the entities are.###Different from conventional knowledge embedding methods, for entity embeddings h and t , instead of looking up in embedding tables, we use PLMs as our text encoders to extract entity representations from their descriptions.###Our KEPLER enjoys the following advantages: (1) We integrate world knowledge into PLMs with the supervision of the KE objective, which is more ﬂexible for the PLMs, and encode the entity and text into the same space, which avoids the gap be-tween the language representations and ﬁxed entity embeddings.###Recent works (Zhang et al., 2019; Peters et al., 2017; Liu et al., 2019a) utilize entity embeddings of large-scale knowledge bases to provide exter-nal knowledge for PLMs and improve their performance on various NLP tasks.###In our pre-training procedure, we only use the English Wikipedia corpus to save time and also for a fair comparison with previous knowledge-enhanced PLMs (Zhang et al., 2019; Peters et al., 2019###For this task, we evaluate all the models on OpenEn-tity (Choi et al., 2018) following the setting from Zhang et al. (2019), which focuses on nine general entity types.###During inference, our KEPLER is exactly the same as standard PLMs, which can be adopted in a wide range of NLP applications.###In this paper, we propose to learn knowledge embedding and language representation with a uniﬁed model and encode them into the same semantic space, which can not only better integrate knowledge into PLMs but also help to learn more informative knowledge embeddings with the effec-a tive language representations.",other,describing the integration of world knowledge into pre-trained language models
2999,5e8d92a09fced0a24b628d13,abed09d4722b3259b68cb42141d532126076dab2,DiffNet++: A Neural Influence and Interest Diffusion Network for Social Recommendation,5550427b45ce0a409eb429c8,Personalized Recommendation Combining User Interest and Social Circle,"Social recommendation has also been extended with social circles [29], temporal context [33], rich contextual information [36], and efﬁcient training models without negative sampling [6].",other,acknowledge advancements in social recommendation
3686,5a73cb3517c44a0b303556bf,eac48f406c46527f5ca821de7fe8d62d6db56a27,"Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer",599c7972601a182cd263e384,Optimizing Expected Word Error Rate Via Sampling For Speech Recognition,"The acoustic model is a CTC trained LSTM that predicts context-dependent (CD) phonemes first fine-tuned with sequence discriminative training as described in [5] and further improved with word-level edit-based minimum Bayes risk (EMBR) proposed recently by Shannon [23].###The acoustic model is a CTC trained LSTM that predicts context-dependent (CD) phonemes first fine-tuned with sequence discriminative training as described in [5] and further improved with word-level edit-based minimum Bayes risk (EMBR) proposed recently by Shannon [22].###[23] M. Shannon, “Optimizing expected word error rate via
sampling for speech recognition,” in Proc. of Interspeech, 2017.###[12] H. Sak, M. Shannon, K. Rao, and F. Beaufays, “Re-
current neural aligner: An encoder-decoder neural network model for sequence-to-sequence mapping,” in Interspeech, 2017.",other,describing the architecture and training of the acoustic model
1398,,a9b73f604a5c6bcbcdb4516ec605a4c6280d9ea2,Symptoms of dementia among adults with Down's syndrome: a qualitative study.,,,"###Holland et al. (2000) and Ball et al. (2006) highlighted the importance of carer reporting of symptoms of dementia among adults with DS. In a 18-month follow-up study, Holland et al. (2000) found that in the initial assessment carers reported changes predominantly in behaviour and personality among adults with DS who later developed dementia. The frontal-like dementia was more prevalent particularly among the younger groups. On the basis of their findings, the authors hypothesized that the frontal lobe functions are first to be compromised with the progressive development of Alzheimer-like neuropathology in people with DS. In another 4-year follow-up study, Oliver et al. (1998) found that those adults with DS who developed dementia at the follow-up showed evidence of impaired orientation and visuospatial memory at the beginning of the study, but language function and praxis deteriorated later.###Holland et al. (1998) reported features similar to those associated with fronto-temporal###Holland et al. (2000) and Ball et al. (2006) highlighted the importance of carer reporting of symptoms of dementia among adults with DS. In a 18-month follow-up study, Holland et al. (2000) found that in the initial assessment carers reported changes predominantly in behaviour and personality among adults with DS who later developed dementia.###Holland et al. (2000) and Ball et al. (2006) highlighted the importance of carer reporting of symptoms of dementia among adults with DS.###However, Cosgrave et al. (2000) reported memory loss as early clinical feature of dementia among 85.",impact-revealing,highlighting the significance of carer reporting in dementia symptoms among adults with Down syndrome
1299,,be2b2bb19c05d734458f9ad361bbae00b931aa5b,EFFECTS OF DIFFERENT DOSES OF BONE MORPHOGENETIC PROTEIN 4 ON THE VIABILITY AND PROLIFERATION OF CCE MOUSE EMBRYONIC STEM CELLS,,,"###In the mouse, the roles of BMPs in the formation of skeletal system (11-14), heart, nervous system, urogenital system, mesoderm induction ( 15-19 ), as well as formation and early proliferation of primordial germ cells (PGCs) (20-22) have confirmed with targeted or spontaneous mutations in various BMPs, their receptors and Smads.###This finding reinforced by other researcher's studies that observed the mutations in the BMP type I receptors (Bmpr1a and Acvr1) ( 15 , 16, 24), the BMP type II receptor (Bmpr2) (18), and signaling component, Smads, (Smad1, Smad5,",impact-revealing,highlighting the roles of BMPs in various biological systems and confirming findings through mutations
959,,7a515630311d26de6d69a98de16c85ad618fb155,Using Panel Data to Understand the Dynamics of Human Behavior in Response to Flooding,,,"###The review of Galatzer-Levy et al. (2018) shows that only seven studies addressed or included natural hazards.###, 2009), such as “resilience” or “delayed onset” (Galatzer-Levy et al., 2018).###Therefore, second, based on the results of the repeatedmeasures ANOVA and guided by prior empirical findings (Galatzer-Levy et al., 2018; Weyrich et al., 2020), latent class growth analyses (LCGA) were carried out for assessing heterogeneous trajectories of recovery (Section 2.3.2) and adaptation…###Guided by empirical evidence (Galatzer-Levy et al., 2018) (see Fig.###LCGA was mainly developed and extended by Nagin and colleagues and is specifically designed and used to analyze longitudinal data (Galatzer-Levy et al., 2018; Jones & Nagin, 2013; Jones, Nagin, & Roeder, 2001; Nagin, 1999).###Based on theory (Galatzer-Levy et al., 2018; see Fig.###Therefore, second, based on the results of the repeatedmeasures ANOVA and guided by prior empirical findings (Galatzer-Levy et al., 2018; Weyrich et al., 2020), latent class growth analyses (LCGA) were carried out for assessing heterogeneous trajectories of recovery (Section 2.###…using the Stata extension Traj (Jones & Nagin, 2013) in a Stata/SE15 environment.1 The optimal number of trajectories was identified in an iterative process on the basis of model fit indices, interpretability, and theory (Galatzer-Levy et al., 2018; Nagin & Odgers, 2010; Self-Brown et al., 2013).###” This is in line with the literature on other potentially traumatic events, which identified this trajectory as the least common (Galatzer-Levy et al., 2018).###This is in line with the literature on other potentially traumatic events, which identified this trajectory as the least common (Galatzer-Levy et al., 2018).###The only group out of the four prototypical trajectories reported by Galatzer-Levy et al. (2018) that is not represented in the best-fitting model is “delayed onset.”###The heterogeneity in response trajectories within the sample is summarized by a finite set of unique polynomial functions, each representing a distinct subgroup (Andruff et al., 2009), such as “resilience” or “delayed onset” (Galatzer-Levy et al.,
2018).###While these patterns have been consistently identified across a wide range of potentially traumatic events (Galatzer-Levy et al., 2018), insights into the response and recovery of individuals to floods is largely lacking.###, 2012; Kienzler, Pech, Kreibich, Muller, & Thieken, 2015), may indicate that heterogeneous trajectories also exist in terms of adaptation behavior, as found in relation to mental coping (Galatzer-Levy et al., 2018).###The three trajectories identified in the best-fitting model, which are “resilient,” “slight recovery,” and “chronic,” are well in line with the theory on prototypical trajectories of individuals in response to potentially traumatic events (Galatzer-Levy et al., 2018).###1 The optimal number of trajectories was identified in an iterative process on the basis of model fit indices, interpretability, and theory (Galatzer-Levy et al., 2018; Nagin & Odgers, 2010; Self-Brown et al., 2013).###…multiple risk-reducing measures immediately after the event (Bubeck et al., 2012; Kienzler, Pech, Kreibich, Muller, & Thieken, 2015), may indicate that heterogeneous trajectories also exist in terms of adaptation behavior, as found in relation to mental coping (Galatzer-Levy et al., 2018).###The review and statistical analysis of 54 panel studies of Galatzer-Levy et al. (2018) identified four prototypical trajectories of human response following potential trauma, which are schematically depicted in Fig.",impact-revealing,highlighting the need for further understanding of response trajectories to floods based on prior findings
3102,5c2c7a9217c44a4e7cf317b4,61946177ea5d44ad18ff09de7929556d7b34cd8b,simd-x: programming and processing of graph algorithms on gpus.,573697bf6e3b12023e6a05f5,GridGraph: Large-Scale Graph Processing on a Single Machine Using 2-Level Hierarchical Partitioning.," the updates on the vertex. This model has been adopted by a number of existing projects, e.g., Pregel [37], GraphLab [33], PowerGraph [14], GraphChi [28], FlashGraph [75], Mosaic [35], and GridGraph [77], as well as GPU-based implementation such as CuSha [25] and Gunrock [63]. On the other hand, the edge-centric model is initially introduced by the external-memory graph engine X-stream [49] to improv###instance, the thread count increases by 1.2 and 5.1 on K40 and P100 than on K20 for BFS. 8Related Work Recent advance in graph computing falls in algorithm innovation [39, 72], framework developments [37, 14, 33, 28, 30, 75, 77, 18, 53, 51, 49, 19, 42, 48, 61, 6, 66, 68, 52, 73, 62, 43, 74, 70, 69, 3, 64, 40, 17, 8] and accelerator optimizations [63, 29, 38, 25, 71, 47]. This section covers relevant work from three aspects: programming model, task management and kernel fusion. Besides edge and vertex centric mod",other,acknowledge existing projects and models in graph computing
412,599c7b59601a182cd272bf53,08a426042c9926419198ee22c9bf80e6e4b5791b,Soft-DTW: a Differentiable Loss Function for Time-Series,53e99a20b7602d9702278e3f,Fast Global Alignment Kernels.,"Because soft-DTW can be used with kernel machines, one typically observes an increase in performance when using soft-DTW over DTW (Cuturi, 2011) for classiﬁcation.",impact-revealing,highlighting the performance benefits of soft-DTW over DTW
3568,5e09aa66df1a9c0c416bebf6,4e7dd1e79f0f13650b2612325e6ba8d206dc04fb,DeepGCNs: Can GCNs Go As Deep As CNNs?,5aed14d617c44a4438159010,Image Generation from Scene Graphs,"Scene graphs also facilitate the inverse process, where an image is reconstructed given a graph representation of the scene [17].",other,providing context on scene graph applications
2079,,53cedb7c3fd8843c9ba04670f4143b3557ad28d5,Nonstationary-state hidden Markov model representation of speech signals for speech enhancement,,,"###It is an extension of the framework with use of stationary-state HMMs published in [8].###For the standard HMM-based enhancement systems, the models used for clean speech and noise are both AR-HMMs of diLerent AR orders.###The algorithm is motivated by and is an extension of the segmental k-means algorithm developed in the past for training conventional HMMs [13].###For the MMSE enhancement, the noise HMMs we have implemented contained 3 states and 3 mixture components.###NS-HMMs with 4 states and 4 mixture components and of orders 0, 1, and 2 were trained with the utterance.###(28), we generalize a result of [8] from stationary-state HMMs to NS-HMMs.###In Section 2 we 6rst address the issue of speech feature selection, which is tightly associated with use of the NS-HMM as the speech model, and then provide a detailed description of the MMSE formulation for speech enhancement in which NS-HMMs are employed.###By using diagonal covariance matrices for the NS-HMMs, the problem of high computation cost for the inversion of the covariance matrix for calculation of the output likelihoods is also avoided.###Then, the likelihood for each pre-trained noise HMM is calculated and compared with likelihoods for the other noise HMMs and the model associated with the highest likelihood determines the selected noise model.###This implies independence 5 of vectors yt for diLerent time frame t (given the model parameters y), as assumed in all types of HMMs including our NS-HMM.",impact-revealing,providing context for an extension of a framework
2129,,f3206d0dafc0ff0aed0ad1ff1b977e7ce7ada6d1,Macrocosmic analysis method for dynamic traffic flow based on hypernetworks of celluar automata,,,###Note that the spatial structure of intersection does not consider the faulty hypothesis that vehicles fly over intersections freely [6].,impact-revealing,providing context on spatial structure in relation to vehicle movement
2168,,b3c0ec45e49d923a64002a5a9df5613702498d08,Ten Years of Hoare's Logic: A Survey Part II: Nondeterminism,,,"###The issue of fairness is discussed in several papers (see, for example, [19]).###In the case of concurrent programs various other important properties and hypotheses (see, e.g., [19]) cannot be naturally expressed and axiomatized in Hoare-like logics either.###More appropriate framework for such an analysis seems to be temporal logic (see, e.g., [19]) which explicitly deals with the properties of sequences of states and not states only.###RuLE 7 is inspired by the discussion of clean behaviour of programs in [ 19].",impact-revealing,acknowledging prior discussions on fairness and program properties
1836,,4e6535d7b667828fbd9ca1ef4560cfeedcad4732,Integrating Automated and Interactive Protocol Verification,,,"###Such a typed model which is standard in protocol verification, even in interactive verification with Isabelle [23,3], considerably simplifies the verification task.###Despite some differences, our model is similar to the one of Paulson [23].###The second novel aspect is that the proof is entirely based on a standard protocol model without over-approximation close to the model employed for instance in [23].###The model is inspired by the formalization of several security protocols in Isabelle by Paulson and others in [23,3] and is close to the persistent IF model in [20].###Isabelle has been successfully used for the interactive verification in various areas, including protocol verification [23,3].",impact-revealing,highlighting the simplification of verification tasks through a typed model
3039,53e997ddb7602d9701fd5474,7aca628a75775530a5b946900af827890a4208de,"A PPM-like, Tag-based Predictor",53e9a93eb7602d9703291f25,Idealized Piecewise Linear Branch Prediction,"Some of the CBP-1 predictors (e.g., [8, 9]) dynamically adjust the history lengths according to the application.",other,acknowledge variations in existing predictors
1207,,1c54e537d8c139edd679c8ffefdf2a088442cfd6,"Decomposition Strategies for Nonconvex Problems, a Parametric Approach",,,"###However, our local analysis of Algorithm 1 brings up important concepts, such as strong regularity [137], which is a cornerstone of Chapter 3.###The first step to achieve this fundamental property is the following Theorem proven in [137].###Hence, by the necessary and sufficient conditions of Section 4 in [137], we obtain that the generalised equation (2.###2) with respect to parameter variations, namely Robinson’s strong regularity [137].###3 (Strong regularity of a generalised equation [137]).###For our specific setting, strong regularity [137] is needed.###Theoretical properties regarding the sensitivity of the solution of a Nonlinear Program (NLP) to small changes in the parameters have been established by means of the fixed point theorem for smooth as well as non-smooth problems [52, 137].###Its proof, which is given in [137], relies on a fixed point argument, like the implicit function theorem.###This is captured by the strong regularity concept [137], which has already been introduced in Chapter 1.###1 in [137], as the radii δA and rA are assumed not to depend on the parameter sk ∈ S.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3884,5b67b45517c44aac1c8607aa,d5aefe86b1ba8c773a6bd0e84812ace161b8c0db,large-scale learnable graph convolutional networks,53e9ba85b7602d97046aa61d,Understanding the difficulty of training deep feedforward neural networks," over-fitting, theL2 regularization with λ= 0.0005 is applied. For training, the Adam optimizer [14] with a learning rate of 0.1 is used. Weights in LGCNs are initialized by the Glorot initialization [7]. We employ the early stopping strategy based on the validation accuracy and train 1,000 epochs at most. 4.3 Analysis of Results The experimental results are summarized in Tables 2 and 3 for transduct",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3362,5d04eeba8607575390f83f53,404da55d10913915f1283ed518681679d8a9c7b0,SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale,53e9abbeb7602d970356e9e5,Design Tradeoffs for SSD Performance.,"For example, microsecondscale overheads that arise from accesses to Flash [40], emerging memory technologies like 3D XPoint by Intel and Micron [41–43], or 40-100 Gb/s Infiniband and Ethernet network interactions [44] can significantly degrade the request latency of microsecond-scale microservices [45–48] like Cache1 or Cache2.",other,highlighting the impact of emerging technologies on request latency
2467,5e524da993d709897cb69fde,09e48adf1a3f012ec56fbb4f92acae975ee43d87,a deep learning approach to antibiotic discovery,56a388860cf24167b2a84745,Antibacterial drug discovery in the resistance era,"Moreover, the decreasing development of new antibiotics in the private sector that has resulted from a lack of economic incentives is exacerbating this already dire problem (Brown and Wright, 2016; PEW Trusts, 2019).###698 Cell 180, 688–702, February 20, 2020 ity in clinically implementing new antibiotics due to the high risk of early discovery and low return on investment is exacerbating this problem (Brown and Wright, 2016).###Concurrently, the steadily declining productivity in clinically implementing new antibiotics due to the high risk of early discovery and low return on investment is exacer-bating this problem (Brown and Wright, 2016).",other,highlighting the significant issue of antibiotic development and economic incentives
3851,5d245bb6da56295a28fcd54f,25fd9e491c748995e94719f52d896a41299b5b75,geometric scattering for graph data analysis,573695fe6e3b12023e512507,Wavelet Scattering On The Pitch Spiral,"We note that even in the classical Euclidean case, while the stability of scattering transforms to deformations can be established analytically (Mallat, 2012), their capacity is typically examined by empirical evidence when applied to machine learning tasks (e.g., Bruna & Mallat, 2011; Sifre & Mallat, 2012; Andén & Mallat, 2014). Similarly, in the graph processing settings, we examine the capacity of our proposed geometric scattering features via their discriminative power in graph data analysis tasks, which are described in detail in Sec. 4. We show that geometric scattering enables graph embedding in a relatively low dimensional Euclidean space, while preserving insightful properties in the data. Beyond establishing the capacity of our specific construction, these results also indicate the viability of graph scattering transforms as universal feature extractors on graph data, and complement the stability results established in Zou & Lerman (2018) and Gama et al.###A geometric wavelet scattering transform follows a similar construction as the (Euclidean) wavelet scattering transform of Mallat (2012), but leverages a graph wavelet transform.###Indeed, scattering features have been shown effective in several audio (e.g., Bruna & Mallat, 2013a; Andén & Mallat, 2014; Lostanlen & Mallat, 2015; Andén et al., 2018) and image (e.g., Bruna & Mallat, 2013b; Sifre & Mallat, 2014; Oyallon & Mallat, 2015; Angles & Mallat, 2018) processing…",other,highlighting the effectiveness of geometric scattering transforms in graph data analysis
2211,5f7fdd328de39f0828397afd,c841c9704bf35873a051f228a15f67b30d650c2f,Scalable Graph Neural Networks via Bidirectional Propagation,5f02f17c91e011ee5e0258c8,Scaling Graph Neural Networks with Approximate PageRank,"PPRGo [4] calculates approximate the Personalized PageRank (PPR) matrix ∑∞ `=0 α(1 − α)Ã by forward push algorithm [2] and then applies the PPR matrix to the feature matrix X to derive the propagation matrix.###We also use one state-ofthe-art scalable GNN from each of the three categories: LADIES (layer sampling) [40], GraphSAINT (graph sampling) [37], SGC and PPRGo (linear model) [30, 4].###PPRGo [4] uses Personalized PageRank to capture multi-hop neighborhood information and uses a forward push algorithm [2] to accelerate computation.###PPRGo [4] calculates approximate the Personalized PageRank (PPR) matrix ∑∞ `=0 α(1 − α)`Ã` by forward push algorithm [2] and then applies the PPR matrix to the feature matrix X to derive the propagation matrix.###A major drawback of PPRGo is that it takes O( n ε ) space to store the PPR matrix, rendering it infeasible on billion-scale graphs.###For PPRGo, it has a longer running time than other methods because of its expensive feature propagation per epoch.###However, we will mainly focus on two setups in this paper: 1) w` = α(1− α)` for some constant decay factor α ∈ (0, 1), in which case P becomes the Personalized PageRank used in APPNP and PPRGo [16, 17, 4]; 2) w` = 0 for ` = 0, . . . , L− 1 and wL = 1, in which case P degenerates to the L-th transition probability matrix in SGC [30].###However, we will mainly focus on two setups in this paper: 1) w` = α(1− α) for some constant decay factor α ∈ (0, 1), in which case P becomes the Personalized PageRank used in APPNP and PPRGo [16, 17, 4]; 2) w` = 0 for ` = 0, .###We also point out that PPRGo starts to converge and achieves an F1-score of 0.15 in 4500 seconds when the feature dimension is increased to 10000.###We first observe that both GBP and SGC can capture the structural information with random features, while PPRGo and GBP(PPR) fail to converge.",other,describing the PPRGo method and its limitations
1037,,eda79f06d16df72f921177ff91c03a9567743f65,Enacted social support in sport : the effects of support type and support visibility,,,"###This is an important finding given the failure of social support research to uncover psychological mechanisms for links between enacted support and outcomes (Uchino et al., 2012), and further emphasizes the need for studies to be sensitive to the specific context in which support is provided (Thoits.###Though this may be surprising, Uchino and colleagues (2012) have noted the difficulty to uncover the psychological mechanisms responsible for the links between social support and outcomes of interest (Uchino et al., 2012).###This finding is extremely important with decades of social support research failing to uncover psychological mechanisms for links between enacted support and outcomes (Uchino et al., 2012) and implies that players’ attentional focus might be a central mechanism in the effects of different types of enacted support in pressuring performance situations.###…important with decades of social support research failing to uncover psychological mechanisms for links between enacted support and outcomes (Uchino et al., 2012) and implies that players’ attentional focus might be a central mechanism in the effects of different types of enacted support in…###An important avenue for future research remains the study of mediating processes underpinning the effects of social support on outcomes in a carefully structured and systematic manner (Uchino et al., 2012).###…is an important finding given the failure of social support research to uncover psychological mechanisms for links between enacted support and outcomes (Uchino et al., 2012), and further emphasizes the need for studies to be sensitive to the specific context in which support is provided (Thoits.###Still, knowing the mechanisms by which social support influences performance remains important to enhance theoretical understanding of how social support operates, and to better structure support interventions (Cohen, Gottlieb, & Underwood, 2000; Uchino et al., 2012).###An important avenue for future research remains the study of mediating
processes underpinning the effects of social support on outcomes in a carefully structured and systematic manner (Uchino et al., 2012).",impact-revealing,highlighting the significance of findings in social support research
2894,5d84a3433a55acc20782ce9e,554d300f00fc14c2e4f48a740019496137d060c1,self-training for end-to-end speech recognition,53e9b3c7b7602d9703eb2c4b,Unsupervised Multilingual Sentence Boundary Detection,"First, we detect sentence boundaries using the “punkt” tokenizer [12] implemented in NLTK [13].",other,describing a method for sentence boundary detection
1540,,8f94c3ffa902fb5246406c9a2b618c0e8935e512,An RNA interference screen identifies new avenues for nephroprotection,,,"###Importantly, these cells tend to accumulate toxic by-products of incomplete fatty acid oxygenation, and remain hypoxic for an extended period of time, well after reperfusion.(16) Our effort was inspired by the observations that the death of hypoxic kidney cells is an active process, both in the sense of metabolic maladaptations that inflict the initial biochemical",impact-revealing,highlighting the significance of hypoxic kidney cell death in metabolic processes
241,56d86073dabfae2eee7807ea,6928b1bf7c54a4aa8d976317c506e5e5f3eae085,Deception detection using real-life trial data,53e9b79fb7602d9704343fa0,"The MUMIN coding scheme for the annotation of feedback, turn management and sequencing phenomena.","The gesture annotation is performed using the MUMIN coding scheme [3], which is a standard multi-modal annotation scheme for interpersonal interactions.",impact-revealing,providing context for a standard multi-modal annotation scheme
3217,5ee8986891e011e66831c452,a9872078cc6dabd2428750543862b45f4a482dfc,Graph Meta Learning via Local Subgraphs,5992a1c65ba2006b76482de3,Weisfeiler-Lehman Neural Machine for Link Prediction,"local subgraphs, as is evidenced by the connection to the Weisfeiler-Lehman test [54, 60].###Besides, G-META’s construction of local subgraphs gives local structural representations that enable direct structural similarity comparison using GNNs based on its connection to Weisfeiler-Lehman test [54, 60].",other,providing context on local subgraphs and their significance
996,,f2da12961b1417bc6dd17ab529161b0eeead5169,"Longitudinal Adult Practical Problem-Solving: 1983, 1993, 2003",,,"###In the quality point scoring criteria developed by Denney and Palmer (1981), more quality points were awarded for independent solutions (those that did not require another person to take charge).###The 1983 findings differ from those of Cornelius and Caspi (1987) who reported that performance is not related to education level. Education accounted for a larger proportion of variance in the number of solutions offered in 1983 than did age. Only age, however, accounted for the observed variance on the number of 1993 and 2003 solutions. Similar patterns were observed with the other dependent measure (quality points). Certainly, education affects experience which influences the aging process. Level of education has been reported to partially explain cognitive performance, and this was observed in the initial administration time of the present study (1983). Technological changes over the past 20 years directly impacted the number of solutions offered by participants of all ages to the everyday practical problems.###Older adults did not show a decline in everyday problem-solving in research conducted by Cornelius and Caspi (1987).###The 1983 findings differ from those of Cornelius and Caspi (1987) who reported that performance is not related to education level.###The 1983 findings differ from those of Cornelius and Caspi (1987) who reported that performance is not related to education level. Education accounted for a larger proportion of variance in the number of solutions offered in 1983 than did age. Only age, however, accounted for the observed variance on the number of 1993 and 2003 solutions. Similar patterns were observed with the other dependent measure (quality points). Certainly, education affects experience which influences the aging process. Level of education has been reported to partially explain cognitive performance, and this was observed in the initial administration time of the present study (1983). Technological changes over the past 20 years directly impacted the number of solutions offered by participants of all ages to the everyday practical problems. The quality point scoring criteria developed by Denney and Palmer (1981) resulted in lower quality point scores because of fewer solutions.###Researchers have examined adult practical problem-solving by developing problems geared toward all age groups (Denney, Pearce, & Palmer, 1982), developing problems typical of older adults (Denney & Pearce, 1989), comparing different problem types and scoring methods (Camp, Doherty, Moody-Thomas, & Denney, 1989), and examining the role of instructions on performance (Denney, Tozier, & Schlotthauer, 1992). Willis and Schaie (1986) noted that traditional tasks, such as mental ability tests, were developed to correspond with early development and may not be a valid measure of practical intelligence.###Older adults did not show a decline in everyday problem-solving in research conducted by Cornelius and Caspi (1987). An Everyday Problem-Solving Inventory (EPSI) was developed and administered to 126 adults between the ages of 20 and 78.###Older adults did not show a decline in everyday problem-solving in research conducted by Cornelius and Caspi (1987). An Everyday Problem-Solving Inventory (EPSI) was developed and administered to 126 adults between the ages of 20 and 78. Performance on the EPSI and verbal ability scores increased with age, while performance on a traditional abstract-problem-solving test declined after middle age. Their findings were similar to previous results (e.g., Denney, 1984; Denney & Palmer, 1981) in showing an increase in practical abilities from early adulthood through middle-age. Education levels of their participants were not related to everyday problem-solving performance. Individual differences in 111 older adults’ performance on everyday problem-solving tasks using three instruments—including Denney & Pearce’s (1989) practical problems—were investigated by Marsiske and Willis (1995). Age did not account for a large proportion of the variance in any of the problem-solving factors, and no subscale from the practical-problems measure was significantly related to age.###Older adults did not show a decline in everyday problem-solving in research conducted by Cornelius and Caspi (1987). An Everyday Problem-Solving Inventory (EPSI) was developed and administered to 126 adults between the ages of 20 and 78. Performance on the EPSI and verbal ability scores increased with age, while performance on a traditional abstract-problem-solving test declined after middle age. Their findings were similar to previous results (e.g., Denney, 1984; Denney & Palmer, 1981) in showing an increase in practical abilities from early adulthood through middle-age. Education levels of their participants were not related to everyday problem-solving performance. Individual differences in 111 older adults’ performance on everyday problem-solving tasks using three instruments—including Denney & Pearce’s (1989) practical problems—were investigated by Marsiske and Willis (1995). Age did not account for a large proportion of the variance in any of the problem-solving factors, and no subscale from the practical-problems measure was significantly related to age. These researchers concluded that everyday problem-solving is a multidimensional construct with components such as cognitive demands, task novelty, and problem domain. They suggested moving away from use of the global construct label of ‘‘everyday problemsolving.’’ Few age-related longitudinal studies in adult practical problem-solving have been conducted. Willis, Jay, Diehl, and Marsiske (1992) examined longitudinal change in everyday-task competence in 102 older adults over 7 years.###To assess practical and social competence components of intelligence, Cornelius and Caspi (1987) constructed the Everyday Problem-Solving Inventory (EPSI).###Older adults did not show a decline in everyday problem-solving in research conducted by Cornelius and Caspi (1987). An Everyday Problem-Solving Inventory (EPSI) was developed and administered to 126 adults between the ages of 20 and 78. Performance on the EPSI and verbal ability scores increased with age, while performance on a traditional abstract-problem-solving test declined after middle age. Their findings were similar to previous results (e.g., Denney, 1984; Denney & Palmer, 1981) in showing an increase in practical abilities from early adulthood through middle-age. Education levels of their participants were not related to everyday problem-solving performance. Individual differences in 111 older adults’ performance on everyday problem-solving tasks using three instruments—including Denney & Pearce’s (1989) practical problems—were investigated by Marsiske and Willis (1995).###Researchers have examined adult practical problem-solving by developing problems geared toward all age groups (Denney, Pearce, & Palmer, 1982), developing problems typical of older adults (Denney & Pearce, 1989), comparing different problem types and scoring methods (Camp, Doherty, Moody-Thomas, & Denney, 1989), and examining the role of instructions on performance (Denney, Tozier, & Schlotthauer, 1992). Willis and Schaie (1986) noted that traditional tasks, such as mental ability tests, were developed to correspond with early development and may not be a valid measure of practical intelligence. Several excellent reviews of the status of adult cognition have been published, and important theoretical and methodological issues have been considered (Poon, Rubin, & Wilson, 1989; Sinnott, 1989). More recently, BlanchardFields and Hess (1996) explored the multidimensional changes in adult cognition and the related implications for adult life experience.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
640,5da1a6d447c8f766460688bc,9570147aee646e15affffc30a6b18be0568cfa82,SWQUE: A Mode Switching Issue Queue with Priority-Correcting Circular Queue,53e999ffb7602d970224d68c,Complexity-effective superscalar processors,"The most widely known circuit used as select logic is a tree arbiter [21].###The path composed of these three operations is the critical path of the IQ, and is a critical path of the processor [21].###,дrantIW −1 correspond to the instructions in descending order in terms of the priority [12, 21]; that is, дrant0 is the grant signal of the instruction with the highest priority, дrant1 is that of the instruction with the second-highest priority, and so on.###For an IW -issue IQ, IW tree arbiters are stacked and the grants are determined from the highest priority in order from the bottom of the stacked arbiters [12, 21].",impact-revealing,providing context on select logic circuits and their critical paths
2948,5bdc315017c44a1f58a05e13,1717255b6aea01fe956cef998abbc3c399b5d7cf,AMC: AutoML for Model Compression and Acceleration on Mobile Devices,59ae3be32bbe271c4c71b77b,Reinforcement Learning for Architecture Search by Network Transformation.,"During the update, the baseline reward b is subtracted to reduce the variance of gradient estimation, which is an exponential moving average of the previous rewards [56, 6]:###[6] proposed to speed up the exploration via network transformation [8].###Comparisons of reinforcement learning approaches for models searching (NAS: Neural Architecture Search [57], NT: Network Transformation [6], N2N: Network to Network [2], and AMC: AutoML for Model Compression.",other,describing reinforcement learning approaches and their comparisons
1341,,19695725b203bd649cccec6cd3eb71108b299fa0,Adaptive high-frequency clipping for improved image quality assessment,,,"###MS-SSIM [2], IFC [3], VIF [4], IW-PSNR/SSIM [5] and MIS-SSIM [6], as inspired by the multi-resolution property of the HVS, achieved some of the best performance.###In the current IQA research, the so called multi-scale type of methods, e.g. MS-SSIM [2], IFC [3], VIF [4], IW-PSNR/SSIM [5] and MIS-SSIM [6], as inspired by the multi-resolution property of the HVS, achieved some of the best performance.",impact-revealing,reporting performance of multi-scale methods in IQA research
4004,5fae6dced4150a363cec41f7,71c97e7d4a529a21f32d98aad72fbbf32c9ee32e,Anomaly-Based Intrusion Detection From Network Flow Features Using Variational Autoencoder,53e99a2bb7602d9702284699,Estimating the support of a high-dimensional distribution.,The SVM method is modiﬁed into a One-Class SVM (OCSVM) as explained in [46].###The formulation of OCSVM can be carried out as follows [46]: The working logic of the algorithm is as follows.,other,describing the modification of the SVM method
625,5ea013159fced0a24b9cf180,351140fe9b4186a1ea17984397be022046f39946,AccelTCP: Accelerating Network Applications with Stateful TCP Offloading,53e9bbeab7602d970483ebad,Server network scalability and TCP offload,"There have been a large number of works and debates on NIC offloading of TCP features [35, 47, 50, 57].",impact-revealing,acknowledge existing works and debates on NIC offloading
2579,5db9295f47c8f766461f5135,2c1006c856fefdbd6cd710e840e57153f2d6cd04,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,5a260c8417c44a4ba8a31511,mixup: Beyond Empirical Risk Minimization.,"Mixup [70] generates training examples by linear interpolation between pairs of natural examples, thus introducing an linear inductive bias in the vicinity of training samples.",other,providing context for a data augmentation method
2906,5e7232fe93d709897cfa3461,e39ec42bc0b393fd2d21e4fe9ae55d361e2a752b,A New Method of Fuzzy Support Vector Machine Algorithm for Intrusion Detection,5736954a6e3b12023e477186,Malware Detection in Cloud Computing Infrastructures.,[55] detected features of the system in the hypervisor.,other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3279,57d063e8ac443673542950ad,0509facb30efa81c4730f98bfc28cdbd6178822e,"Image sentiment analysis using latent correlations among visual, textual, and sentiment views",53e9af40b7602d970397fa2f,Affective image classification using features inspired by psychology and art theory.,"The idea of associating low-level visual features with sentiments has been investigated based on psychology and art theory using relatively small and controlled datasets [14,15], while recent works have started to analyze the sentiments of unconstrained real-world images on social media [4–7].",other,highlighting the evolution of sentiment analysis in visual features
3233,599c7ea4601a182cd28b81a7,f323036340ad0ec30cafacd6b09a3e28379e24f1,maximizing cache performance under uncertainty,53e9a539b7602d9702e5e0b3,Reuse-based online models for caches.,"More recently, a large body of work [1, 9, 15, 18, 33] has studied cache behavior under more complex memory reference models.",other,acknowledge recent studies on cache behavior
1501,,868f052bf099692d4d188675488b21dce35fdd07,Decoder Denoising Pretraining for Semantic Segmentation,,,"###We then propose several modiﬁcations of the standard formulation that are motivated by the recent success of diﬀusion models in image generation (Ho et al., 2020; Nichol & Dhariwal, 2021; Saharia et al., 2021b).###…Diﬀusion Models (DDPMs) have recently been applied to conditional generation tasks such as super-resolution, colorization, and inpainting (Li et al., 2021; Saharia et al., 2021b; Song et al., 2021; Saharia et al., 2021a), suggesting these models may be able to learn useful image representations.###This approach has yielded impressive results in image and audio synthesis (Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021; Saharia et al., 2021b; Ho et al., 2021; Chen et al., 2021b), outperforming strong GAN and autoregressive baselines in sample quality scores. is particularly eﬀective when a…",impact-revealing,highlighting the recent success and potential of diffusion models in various generation tasks
2117,,2fce05e168bdb6393de99f0b770717996a9ac436,Exploring Cultural Impact on Long-Term Utilization of Enterprise Systems,,,"###Hofstede dimensions provide a good starting point to study culture and its impact on information systems, this model has been criticized for being too simplified to describe all important aspects of national cultures [6,  39 ].",impact-revealing,highlighting the limitations of Hofstede dimensions in cultural studies
767,5d1b2f5a3a55ac071793c55c,83b56c3c7a61767bd88d85796aa5dbc4976912c3,gpt-based generation for classical chinese poetry,5550456245ce0a409eb55c88,Chinese Poetry Generation with Recurrent Neural Networks.,Recurrent Neural Network (RNN) [11] was recently introduced as it has been proved to be eﬀective in generation tasks such as machine translation and dialog generation.###Recurrent Neural Network (RNN) [11] was recently introduced as it has been proved to be effective in generation tasks such as machine translation and dialog generation.,impact-revealing,acknowledge the effectiveness of RNN in generation tasks
18,5db9299b47c8f766461f993e,156d217b0a911af97fa1b5a71dc909ccef7a8028,SciBERT: A Pretrained Language Model for Scientific Text,5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"3617 Casing We follow Devlin et al. (2019) in using the cased models for NER and the uncased models for all other tasks.###, 2018) and BERT (Devlin et al., 2019), unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks.###BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.###Background The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al.###We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of highquality, large-scale labeled scientific data.###We mostly follow the same architecture, optimization, and hyperparameter choices used in Devlin et al. (2019). For text classification (i.###We finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5e-6, 1e5, 2e-5, or 5e-5 with a slanted triangular schedule (Howard and Ruder, 2018) which is equivalent to the linear warmup followed by linear decay (Devlin et al., 2019).",impact-revealing,acknowledge the foundational work and improvements in language models
627,5b1642d68fbcbf6e5a9b7e77,0be19fd9896e5d40222c690cc3ff553adc7c0e27,Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods,599c794e601a182cd262eb67,Counterfactual Fairness.,"Kusner et al. (2017) propose the method based on causal inference to achieve the model fairness where they do the data augmentation under speciﬁc cases, however, to the best of our knowledge, we are the ﬁrst to propose data augmentation based on gender swapping in order to reduce gender bias.",impact-revealing,highlighting a novel approach to data augmentation for reducing gender bias
78,5edf5dd891e011bc656deed4,ae0988c55095827384bf70e346e192a89216e75c,IterefinE: Iterative KG Refinement Embeddings using Symbolic Knowledge,57a4e91aac44365e35c97887,Complex embeddings for simple link prediction,"We work with ComplEx [Trouillon et al., 2016] and ConvE [Dettmers et al., 2018] embeddings which have shown state of the art performance in many KG prediction tasks.###With this dataset containing both positive and negative samples, training can be done for the refinement task with a negative log-likelihood loss function as follows [Trouillon et al., 2016].###KG embedding methods define a scoring function f to score the plausibility of a triple1 and learn embeddings in such a way as to maximise the plausibility of the triples that are already present in the KG [Nickel et al., 2011, Socher et al., 2013, Trouillon et al., 2016].###Specifically, we use Probabilistic Soft Logic (PSL) that can incorporate inference rules and ontologies, along with state-of-the-art KG embedding methods,viz., ConvE [Dettmers et al., 2018] and ComplEx [Trouillon et al., 2016], which do not make use of any ontological rules.###We work with ComplEx [Trouillon et al., 2016] and ConvE [Dettmers et al.###, subtype and subproperty information– and also shows that state-of-the-art embeddings like ComplEx [Trouillon et al., 2016], SimplE [Kazemi and Poole, 2018], ConvE [Dettmers et al.###We evaluate the performance of TypeE-X models in the KG refinement task, and compare them with ComplEx [Trouillon et al., 2016] and ConvE [Dettmers et al., 2018], two state-of-the-art KG embeddings methods,
4. https://www.w3.org/2006/03/wn/wn20/
and PSL-KGI.###, 2018] and ComplEx [Trouillon et al., 2016], which do not make use of any ontological rules.###On the other hand, neural and tensor-based embeddings have seen significant success in entity type and new fact predictions [Nickel et al., 2012, Trouillon et al., 2016, Dettmers et al., 2018].###We evaluate the performance of TypeE-X models in the KG refinement task, and compare them with ComplEx [Trouillon et al., 2016] and ConvE [Dettmers et al.###Recently, SimplE+ [Fatemi et al., 2019] includes taxonomic information –i.e., subtype and subproperty information– and also shows that state-of-the-art embeddings like ComplEx [Trouillon et al., 2016], SimplE [Kazemi and Poole, 2018], ConvE [Dettmers et al., 2018] cannot enforce subsumption.",impact-revealing,reporting on the use of state-of-the-art knowledge graph embedding methods
1951,,a080866fb9b2e2717cd2d8fc37cae5d6f4b4bb32,Swallow: Joint Online Scheduling and Coflow Compression in Datacenter Networks,,,"###5) From Flow to Coflow: Shifting the scenario from flow to coflow, the minimum CCT (ΓC) of a coflow (C) is determined by the slowest flow (f ) [14] and can be defined as:###33× in terms of minimizing average flow completion time (FCT) [9, 14], (2) Swallow speeds up coflow completion time (CCT) [14, 16] and job completion time (JCT) [5] by up to 1.###This model has been widely adopted by existing works [14, 19].###Existing works [14, 33, 34] have shown that network flows in datacenters follow a heavy-tailed distribution, in terms of flow amounts and traffic bytes.###To conquer this challenge, many network traffic scheduling techniques have been proposed [8–13], but they have achieved only limited success due to the ignorance of job-specific traffic patterns [14].###an effective coflow scheduling algorithm [14], with average CCT of 4.###66× on average, respectively, over the Smallest Effective Bottleneck First (SEBF) algorithm [14] in Varys, respectively, and (3) Swallow reduces data traffic by up to 48.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2670,5aed148b17c44a4438154efb,2de0f24f91d86725a26a983776ed9f27c370fa4c,higher-order network representation learning,573697296e3b12023e61afcd,Efficient Graphlet Counting for Large Networks,"The termmotif is used generally and may refer to graphlets or orbits (graphlet automorphisms) [1, 6].",other,providing context about the term 'motif'
2746,5aed14d617c44a4438158cff,b77b179522ac01b6903c2719d9b5d29c1efa652e,Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba,57aa28de0a3ac518da9896d6,Structural Deep Network Embedding,"These methods could be categorized into three broad categories: (1) Factorization methods such as LINE [1] try to approximately factorize the adjacencymatrix and preserve both first order and second proximities, (2) Deep learning methods [4, 21, 22] enhance the model’s ability of capturing non-linearity in graph, and (3) Random walk based techniques [8, 9, 16] use random walks on graphs to obtain node representations which are extraordinary efficient and thus could be used in extremely large-scale network.",other,providing an overview of different methods for node representation in graphs
3864,5f1022a091e01168a7d6fc4f,04e234c7e4f300b4e8a16370728d875f9f484b39,learning visual context by comparison,5d0b00688607575390fc32b9,DualCheXNet: dual asymmetric feature learning for thoracic disease classification in chest X-rays,"It is used as a benchmark dataset in previous studies [41,12,29,6,25,23,32,10].",other,reporting the use of a benchmark dataset in prior studies
736,5cede0e5da562983788c40d8,e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b,Unsupervised Embedding Learning via Invariant and Spreading Instance Feature,5b3d98cc17c44a510f801acc,Unsupervised Feature Learning via Non-parametric Instance Discrimination,"We follow the experimental settings in [46] to conduct the experiments on CIFAR-10 [23] and STL-10 [4] datasets, where training and testing set share the same categories.###Given a test sample, we retrieve its top-k ( k = 200 ) nearest neighbors based on cosine similarity, then apply weighted voting to predict its label [46] plar CNN [8], NPSoftmax [46], NCE [46] and Triplet loss with and without hard mining.###Existing works can be roughly categorized into three categories [3]: 1) generative models , this approach aims at learning a parameterized mapping between images and predeﬁned noise signals, which constrains the distribution between raw data and noises [46].###However, the learned feature representation may not preserve visual similarity and its performance drops dramatically for similarity based tasks, e.g . nearest neighbor search [46, 48, 50].###Existing soft-max embedding is usually built on classiﬁer weights [8] or memorized features [46], which has limited efﬁciency and discriminability.###Following [46], we adopt weighted k NN classiﬁer to evaluate the performance.###[46] propose to set up a memory bank to store the instance features f i calculated in the previous step.###To address above issues, we propose a softmax embedding variant for unsupervised embedding learning, which directly optimizes the real instance feature rather than clas-siﬁer weights [8] or memory bank [46].###Methods kNN RandomCNN 32.1 DeepCluster (10) [3] 44.4 DeepCluster (1000) [3] 67.6 Exemplar [8] 74.5 NPSoftmax [46] 80.8 NCE [46] 80.",impact-revealing,describing experimental settings and methods used
3449,5f50ba4291e01182e69239cb,20454697ea082975db2503a52418efb8f65b8ae6,clocs: camera-lidar object candidates fusion for 3d object detection,56d90a73dabfae2eee136349,Bayesian optimization for learning gaits under uncertainty,"Compared to 2D object detection, which has been well-studied [1], [2], [3], [4], 3D object detection is more challenging with more output parameters needed to specify 3D oriented bounding boxes around targets.",other,highlighting the challenges in 3D object detection compared to 2D
1273,,364c73c0d005ad7c454e40b4d8909de2b710424a,Maximal stress-induced transcription from the human HSP70 promoter requires interactions with the basal promoter elements independent of rotational alignment,,,"###Present also is a sequence between -95 and -88 closely related to a metal regulatory element (10, 16).###Second, mutation of the 6 of 7 match to the MRE (16) to 1 of 7 in the LSN -94/-84 construct did not result in a selective loss of metal-induced transcription.###The MRE-like sequences in the HSP70 promoter are related to the GC-like element (16, 43) which by itself is insufficient for metal responsiveness (10).",impact-revealing,discussing findings related to metal regulatory elements and their transcription effects
1944,,536f786cb03b38a33aca6ce07b6b27ab0a1f6196,Social Media as an Employee Recruitment Tool,,,"###Methodologically, this study will build on instruments used by Allen, Mahto, and Otondo (2007).",impact-revealing,acknowledge methodological foundations from prior work
3875,5c8dd94c4895d9cbc6a7d918,97f1d08c306040401112ff0564f37e6c6a312522,BiNE: Bipartite Network Embedding,57d063b4ac4436735428dc15,Learning Query And Document Relevance From A Web-Scale Click Graph,"For example, in search engines, queries and webpages form a bipartite network, where the edges can indicate users’ click behaviors that provide valuable relevance signal [1, 2]; in another application of recommender systems, users and items form a bipartite network, where the edges can encode users’ rating behaviors that contain rich collaborative filtering patterns [3].###ns number of negative samples [1, 2, 4, 6, 8, 10] ws size of window [1, 3, 5, 7, 9] p walk stopping probability [0.",other,providing examples of bipartite networks in different applications
1314,,55a267f4ca206e97c4a60269d188ffc1d2ac8922,Diapause by seed predators and parasitoids in Chionochloa mast seeding communities,,,"###, all the individuals in the population will flower in large episodes in the same years (Kelly, 1994).###…subtropical areas such as Chile, China, Costa-Rica, India east through Burma, Jamaica, Japan, Madagascar and Thailand (Janzen, 1976) and in temperate grasslands, such as New Zealand (Webb & Kelly, 1993; Kelly, 1994; Kelly et al., 2000; Kelly et al., 2001), see also Figure 1c in Kelly & Sork (2002).###A large number of plant species that exhibit mast seeding use wind pollination for reproduction (Janzen, 1971, , 1976; Silvertown, 1980; Kelly, 1994).###Nevertheless, it is a valuable measurement for seed output in the population level and indeed many studies used it when measuring mast seeding intensity (Silvertown, 1980; Webb & Kelly, 1993; Kelly, 1994; Herrera et al., 1998; Kelly et al., 2000; Schauber et al., 2002; Koenig et al., 2003).###Hence, during low flowering years, populations of seed predators would starve (Kelly, 1994).###The evolutionary benefit of mast seeding is possibly explained by at least eight theories; of these, three have attracted most attention: predator satiation, wind pollination and environmental prediction (Kelly, 1994).###Nonetheless, one of the most general definitions and the most frequently used for mast seeding is ‘synchronous highly variable seed production among years by a population of plants’ (Kelly, 1994).###The predator satiation theory is the most commonly invoked to explain mast seeding behaviour in plants (Kelly, 1994) and there are many studies which support it empirically, e.g., Shibata et al., (1998) studied mast seeding in four species of Carpinus trees in Japan and found clear evidence for…###Other terms for the same phenomenon of prolific flowering can be found in the literature including gregarious flowering (Janzen, 1976), mass seeding, mass fruiting, periodic flowering, supra-annual flowering and sporadic seasonal synchrony (Kelly, 1994).###The best way of measuring the intensity of mast seeding is by the coefficient of variation (CV = standard deviation/mean) of the crop size (Silvertown, 1980; Kelly, 1994).###Various possible factors can select for masting in a plant species (Kelly, 1994), but in Chionochloa it has been shown that the primary benefit is predator satiation (Kelly & Sullivan, 1997; Sullivan & Kelly, 2000; Kelly et al., 2001).###The output of seed crops during years of significant flowering is variable and changes from high to low (Kelly, 1994).###in temperate forest trees (Silvertown, 1980, Norton & Kelly, 1988), in tropical and subtropical areas such as Chile, China, Costa-Rica, India east through Burma, Jamaica, Japan, Madagascar and Thailand (Janzen, 1976) and in temperate grasslands, such as New Zealand (Webb & Kelly, 1993; Kelly, 1994; Kelly et al., 2000; Kelly et al., 2001), see also Figure 1c in Kelly & Sork (2002).###The term ‘mast seeding’ comes from the German word for fattening livestock on abundant seed crops such as those of beech trees (Janzen, 1976; Silvertown, 1980; Kelly, 1994; Kelly & Sork, 2002).###…intensity among years; i.e., in some years, a certain population of perennial plants will produce large seed crop while in other years seed crop will be rather small and; (2) synchrony i.e., all the individuals in the population will flower in large episodes in the same years (Kelly, 1994).",impact-revealing,discussing the concept of mast seeding and its evolutionary benefits
2566,59ae3c262bbe271c4c71e9e8,718e1b453fe9dce79458e0db035091db603775fb,Deep Pyramid Convolutional Neural Networks for Text Categorization,58437725ac44360f1082ff5b,Convolutional Neural Networks for Text Categorization: Shallow Word-level vs. Deep Character-level.,"Note that ShallowCNN enhanced with unsupervised embeddings (row 2) was originally proposed in (Johnson and Zhang, 2015b) as a semi-supervised extension of (Johnson and Zhang, 2015a), and then it was tested on the large datasets in (Johnson and Zhang, 2016).###For example, as also mentioned in (Johnson and Zhang, 2016), the complete vocabulary of the Ama.p training set contains 1.3M words.###To facilitate comparison with ShallowCNN, we matched our unsupervised embedding setting exactly with that of (Johnson and Zhang, 2016).###Enhancing region embedding with unsupervised embeddings In (Johnson and Zhang, 2015b, 2016), it was shown that accuracy was substantially improved by extending ShallowCNN with unsupervised embeddings obtained by tv-embedding training (‘tv’ stands for two views ).###However, in (Johnson and Zhang, 2016), very shallow 1-layer word-level CNNs were shown to be more accurate and much faster than the very deep character-level CNNs of (Conneau et al., 2016).###Data pre-processing was done as in (Johnson and Zhang, 2016).###Moreover, DPCNN can be regarded as a deep extension of ShallowCNN, which we proposed in (Johnson and Zhang, 2015b) and later tested with large datasets in (Johnson and Zhang, 2016).###For comparison, the Shal-lowCNN results (green ‘x’) from (Johnson and Zhang, 2016) are also shown.",other,acknowledging prior work and comparisons in CNN methods
196,5dcd263a3a55ac58039516c5,add2f205338d70e10ce5e686df4a690e2851bdfc,Momentum contrast for unsupervised visual representation learning,5b3d98cc17c44a510f801acc,Unsupervised Feature Learning via Non-parametric Instance Discrimination,"As the focus of this paper is not on designing a new pretext task, we use a simple one mainly following the instance discrimination task in [61], to which some recent works [63, 2] are related.###Contrastive loss functions can also be based on other forms [29, 59, 61, 36], such as margin-based losses and variants of NCE losses.###Several recent studies [61, 46, 36, 66, 35, 56, 2] present promising results on unsupervised visual representation learning using approaches related to the contrastive loss [29].###Contrastive learning is at the core of several recent works on unsupervised learning [61, 46, 36, 66, 35, 56, 2], which we elaborate on later in context (Sec.###The instance discrimination method [61] is related to the exemplar-based task [17] and NCE [28].###In this paper, we follow a simple instance discrimination task [61, 63, 2]: a query matches a key if they are encoded views ( e.g ., different crops) of the same image.###Another mechanism is the memory bank approach proposed by [61] (Figure 2b).###The input x q and x k can be images [29, 61, 63], patches [46], or context consisting a set of patches [46].###A momentum update is adopted on the memory bank in [61].###With similarity measured by dot product, a form of a contrastive loss function, called InfoNCE [46], is considered in this paper: where τ is a temperature hyper-parameter per [61].",impact-revealing,describing the use of a specific pretext task in the context of contrastive learning
3850,573695fd6e3b12023e510ff5,06c06885fd53b2cbd407704cf14f658842ed48e5,deeply-recursive convolutional network for image super-resolution,5550417845ce0a409eb3b963,A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.,"For recursive convolutions, we set all weights to zero except self-connections (connection to the same neuron in the next layer) [26, 14].",other,providing context for recursive convolutions
1553,,a033ed2bfec61ab68a4ae081759deedc6fad4732,Inactivation of a Thiol-Dependent Enzyme by Urate Hydroperoxide,,,"###The presence of ascorbate or thiols tripled urate’s antioxidant capacity (74) Urate’s antioxidant activity therefore coexists with ascorbate, forming an antioxidant cycle (9, 10, 74).###Urate’s antioxidant activity with lipid peroxyl radicals is prolonged and replenished by the presence of ascorbate (9, 10).###inability to scavenge lipid environments (9, 10, 71).###This antioxidant scavenges a number of oxidant species, thus protecting other biomolecules from damage (3, 9, 10).###Urate's antioxidant activity with peroxyl radicals is debated in the literature (9, 10, 71, 72).###(9, 10) found that urate and ascorbate were depleted sequentially by peroxyl radicals, indicating urate is the first line of antioxidant defence.###Previous studies (9, 10, 98, 99) added urate prior to peroxidation initiation, hence did not account for the effect of oxidative stress on urate.###Ascorbate, a serum antioxidant, reduces urate back to its antioxidant state hence promoting its activity (9, 10).###Ascorbate reduced the urate radical back to urate to form an antioxidant cycle (9, 10).",impact-revealing,highlighting the interactions and roles of urate and ascorbate in antioxidant activity
3474,5c8c52bc4895d9cbc6ddad8d,76e4d56d712d64ec2f77fd5b2fcb504888c07eab,Island loss for learning discriminative features in facial expression recognition,5550468245ce0a409eb5f05d,Decision Level Fusion of Domain Specific Regions for Facial Action Recognition,"Gabor wavelets [4], Scale Invariant Feature Transform (SIFT) features [54], [6], histogram of Oriented Gradients (HOG) [2], histograms of Local Binary Patterns (LBP) [45], [17], histograms of Local Phase Quantization (LPQ) [13], histograms of Local Gabor Binary Patterns (LGBP) [31] have been demonstrated to be the most successful human designed features.###Gabor wavelets [4], Scale Invariant Feature Transform (SIFT) features [49], histogram of Oriented Gradients (HOG) [2], histograms of Local Binary Patterns (LBP) [41], [15], histograms of Local Phase Quantization (LPQ) [11], histograms of Local Gabor Binary Patterns (LGBP) [29] have been demonstrated to be the most successful human designed features.",other,reporting successful human-designed features
1077,,83d03f2f64471c90b0fd562c086147e1746525bf,"From User Models to the Cyber-I Model: Approaches, Progresses and Issues",,,"###In the early age of computer, some individual attributes, e.g., gender, age, and the settings of the computer were collected as user models, to help computers improving the HCI dynamically.###However, the label representation is not suitable for knowledge, plan, or goal that can’t be represented simply by a value, such as a plan of HCI or a social relationship.###Such as the preference setting of HCI from users’ setting, the personality perception from users’ prosodic features of the input text [44], and the behavior habits from the past detected behaviors.
b) Dynamic User Modeling (DUM) Different to the SUM, the dynamic user modeling is to build up and update the user model dynamically from continuous user data.###The Goals, Operators, Methods, and Selection rules (GOMS) was applied to the users’ behavior simulation in HCI [25].###Different to the attribute, the knowledge is mainly obtained during the Human-Computer Interaction (HCI) [10].###User models are widely applied to many different applications (APPs) to improve the user experience (UE) of human-computer interaction (HCI).",impact-revealing,providing context on user modeling in HCI
2235,5e2ac357df1a9c0c41e7fa48,73bc990757d54f2161eceacd330e0aeb189393bf,Building attention and edge message passing neural networks for bioactivity and physical–chemical property prediction,53e9baf6b7602d97047286b3,Maximum Unbiased Validation (MUV) Data Sets for Virtual Screening Based on PubChem Bioactivity Data.,The MUV dataset [54] contains PubChem bio-activity data specially selected and arranged by refined nearest-neighbour analysis for benchmarking virtual screening approaches.,other,reporting on a specific dataset used for benchmarking
3408,58d82fced649053542fd729f,fddc32f3880688238847077fd927ab3025db7a6a,EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis,573695fd6e3b12023e510ff5,Deeply-Recursive Convolutional Network for Image Super-Resolution,"Figure 5 gives an overview of different approaches including the current state of the art by PSNR [25, 26] on the zebra image from Set14 which is particularly well-suited for a visual comparison since it contains both smooth and sharp edges, textured regions as well as repeating patterns.###…include dictionary-based methods [33, 40, 52, 59, 61, 64] that learn a sparse representation of image patches as a combination of dictionary atoms, as well as neural network-based approaches [4, 8, 9, 24, 25, 26, 47, 48, 62] which apply convolutional neural networks (CNNs) to the task of SISR.###Later works successfully apply deeper networks and the current state of the art in SISR measured by PSNR is based on deep CNNs [25, 26].###Bicubic DRCN [26] PSyCo [40] ENet-E ENet-EA ENet-PA ENet-PAT Baseline Top-1###Glasner [17] Kim [27] SCSR [60] SelfEx [22] SRCNN [8] PSyCo [40] VDSR [25] DRCN [26] ENet-E ENet-PAT I HR Figure 5.###In our comparison, some of the results roughly coincide with the PSNR scores, with bicubic interpolation resulting in the worst performance followed by DRCN [26] and PSyCo [40] which yield visually comparable images and hence similar scores as our ENet-E network.",other,providing an overview of approaches in super-resolution image processing
2294,5b8c9f5317c44af36f8b778e,16ba65426ed5e1e3367eff5bd507dcf6d99bd7c2,Wide Activation for Efficient and Accurate Image Super-Resolution,5a9cb65d17c44a376ffb834b,Generative Image Inpainting with Contextual Attention,"It has many applications in security, surveillance, satellite, medical imaging [24, 34] and can serve as a built-in module for other image restoration or recognition tasks [6, 21, 37, 40, 41].",other,highlighting the diverse applications of a technology in various fields
464,5efdaf7b91e01191d3d28242,6ff1eb9cdf64a464bf43b54d852456e9ddf55b28,Debiased Contrastive Learning,5e4672c93a55ac14f595d8b5,A Simple Framework for Contrastive Learning of Visual Representations,"Cifar 10 and STL 10 We adopt PyTorch to implement SimCLR [ 2 ] with Resnet-50 [ 15 ] as the encoder architecture and use the Adam optimizer [ 19 ] with learning rate 0.001 and weight decay 1 e − 6.###Following [ 2 ], we set the temperature t = 0.5 and the dimension of the latent vector to 128.###Recently, self-supervised representation learning algorithms that use a contrastive loss have out-performed even supervised learning [ 2 , 14 , 16 , 24 ].###First, for CIFAR 10 [ 23 ] and STL 10 [ 6 ], we implement SimCLR [ 2 ] with ResNet-50 [ 15 ] as the encoder architecture and use the Adam optimizer [ 19 ] with learning rate 0.001 and weight decay 1 e − 6.###Chen et al. [ 2 ] extensively study verious data augmentation methods.###The bound also highlights the role of the positive and unlabeled sample sizes M and N in the objective function, in line with the observation that a larger number of negative/positive examples in the objective leads to better results [ 2 , 16 ].",impact-revealing,reporting implementation details and findings in self-supervised representation learning
513,573697c96e3b12023e6a9ac3,c8d4d601e1677ab93fa8c0a3392b152c48b94d80,How to bid the cloud,53e9afadb7602d97039fca39,TUBE: time-dependent pricing for mobile data,"Usage-based pricing can aﬀect overall demand levels, but does not even out short-term ﬂuctuations [13].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1171,,6e183b2b67fe664323bcb3c18ce3a4d49dc6b110,Measuring and Optimizing Distributed Array Programs,,,"###However, these tools are usually reported to be inefﬁcient [28, 24], because they are not build on a speciﬁc underlying infrastructure that originally designed for arrays (e.g., MLib/SystemML are based on Spark [30, 7] and SciDB [8] requires frequent disk I/O).###Spartan is a recently proposed array-based framework, which is re-ported [15] to be faster than similar systems such as Presto [28] and SciDB [8].",impact-revealing,highlighting the inefficiency of existing tools and introducing a faster alternative
2872,5f9be24691e011dcf482d8d6,842bd2d15d53e3083c110e0af55ffd7447ad03e4,Prediction-Based Power Oversubscription in Cloud Platforms,5aed146117c44a44381527e4,SmoothOperator: Reducing Power Fragmentation and Improving Power Utilization in Large-scale Datacenters.,"Others have studied hierarchical capping in production datacenters [12, 18, 43].###The prior work on power capping [16, 19, 26, 27, 29–31, 34, 36, 45] and oversubscription [12, 14, 18, 28, 38, 39, 42, 43] produced major advances in server and datacenter power management.###Researchers have proposed to use statistical oversubscription, where one profiles the aggregate power draw of multiple services and deploy them to prevent correlated peaks [12, 14, 18, 38, 42].",other,acknowledge prior work and advances in power management
1376,,84c2b67e4e52a82a9226e348d50176b3903ae170,A Privacy and Price-Aware Inter-Cloud System,,,###Parallel computing has already been widely adopted as a solution in big data processing and complex computing [17].,impact-revealing,highlighting the adoption of parallel computing in big data processing
3596,5f92ba1691e011edb3573ba0,268d347e8a55b5eb82fb5e7d2f800e33c75ab18a,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,5ede0553e06a4c1b26a841d6,Generative Pretraining From Pixels,"…for object detection (Hu et al., 2018; Carion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classiﬁcation (Wu et al., 2020), unsupervised object discovery (Locatello et al., 2020), or uniﬁed text-vision tasks (Chen et al., 2020c; Lu et al., 2019; Li et al., 2019).###Another recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers to image pixels after reducing image resolution and color space.###We leave exploration of contrastive pre-training (Chen et al., 2020b; He et al., 2020; Bachman et al., 2019; H ´ enaff et al., 2020) to future work.",other,acknowledge various applications of related models in computer vision
2555,5f0277e911dc830562231dab,6deae79dec438eaaa524bca3b82c6b8d93553b20,A General Knowledge Distillation Framework for Counterfactual Recommendation via Uniform Data,59a02623b161e8ad1a7b64f0,Controlling Popularity Bias in Learning-to-Rank Recommendation,"Recommender Systems as a feedback loop system may suffer from the bias problems such as popularity bias [1, 6], previous model bias [9, 16, 17] and position bias [3, 28].",other,highlighting bias issues in recommender systems
849,5db1765a3a55ac101c887e97,6c4b76232bb72897685d19b3d264c6ee3005bc2b,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,599c7987601a182cd2648373,Attention Is All You Need.,"Early results on transfer learning for NLP leveraged recurrent neural networks [Peters et al., 2018; Howard and Ruder, 2018], but it has recently become more common to use models based on the “Transformer” architecture [Vaswani et al., 2017].###For WMT English to German, we use the same training data as [Vaswani et al., 2017] (i.e. News Commentary v13, Common Crawl, Europarl v7) and newstest2013 as a validation set [Bojar et al., 2014].###For WMT English to German, we use the same training data as [Vaswani et al., 2017] (i.###, 2018; Howard and Ruder, 2018], but it has recently become more common to use models based on the “Transformer” architecture [Vaswani et al., 2017].###Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper [Vaswani et al., 2017] or follow-up tutorials3,4 for a more detailed introduction.###Overall, our encoder-decoder Transformer implementation closely follows its originally-proposed form [Vaswani et al., 2017].###3http://nlp.seas.harvard.edu/2018/04/03/attention.html 4http://jalammar.github.io/illustrated-transformer/
Overall, our encoder-decoder Transformer implementation closely follows its originally-proposed form [Vaswani et al., 2017].",impact-revealing,acknowledge the evolution of transfer learning methods in NLP
2039,,a80506c629f07e58185fa4a4a34afb676979513e,Association of dietary intake of fat and fatty acids with risk of breast cancer.,,,"###Postmenopausal breast cancer risk has been positively associated with BMI among women who never used hormone replacement in this cohort, and risk has been positively associated with weight gain since the age of 18 years as well.(36)###These findings strongly suggest that international correlations between fat consumption and breast cancer are severely confounded by other factors, including delayed onset of menses,(48) weight gain after the age of 18 years,(36) and hormone replacement therapy.(36) Our capacity to examine risks of breast cancer at the extremes of fat intake is limited by the small proportion of women and greater probability of misclassification of dietary intake in these categories.",impact-revealing,highlighting confounding factors in breast cancer risk analysis
1555,,e02c31f0649104c5c321c187c5c67b046580a281,Galectin-3 and incident heart failure among patients with pre-existing coronary artery disease: The ADVANCE study,,,###These codes have been previously validated by prior Kaiser Permanente studies against Framingham criteria [18] and B-type natriuretic peptide (BNP) levels [19].,impact-revealing,reporting validation of codes against established criteria
3580,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",557c14cfd19fa4669fa1a7a4,Big Data Analysis with Signal Processing on Graphs: Representation and processing of massive data sets with irregular structure,"lines; some parts of the exposition follow closely [2], [45].###Graph signal processing (GSP)1 [2], [3], [45] extends DSP to signal samples indexed by nodes of a",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3663,5da052ba3a55acfef148243e,efd81977f1e74138cf2ac3e9a42112b95f648c66,Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling,573696106e3b12023e5227c8,Neural Architectures for Named Entity Recognition.,"BLSTM-CRF models have achieved promising performance (Lample et al., 2016) and are used as our base sequence tagging model in this paper.",other,reporting prior findings on BLSTM-CRF models
3997,57d063d3ac44367354292258,db645d8a227d024121dc95a1c0eeec1b15c7fe53,A multi-player Markov stopping game for delay-tolerant and opportunistic resource sharing networks,53e9b3abb7602d9703e8c8f9,"Hey, You, Get Off Of My Cloud: Exploring Information Leakage In Third-Party Compute Clouds","(10)Since allowing multiple users to share the same physical resource on the cloud may incur security vulnerabilities [23, 24], exclusive resource assignment is assumed in this work.",other,highlighting security concerns in cloud resource sharing
3481,5a9cb66717c44a376ffb87ea,fe9b8aac9fa3bfd9724db5a881a578e471e612d7,efficient neural architecture search via parameter sharing,5a260c8417c44a4ba8a31b80,Improved Regularization of Convolutional Neural Networks with Cutout.,"When trained with a strong regularization technique, such as Shake-Shake (Gastaldi, 2016), and a data augmentation technique, such as CutOut ( DeVries & Taylor, 2017), DenseNet impressively achieves the test error of 2 .###With CutOut (DeVries & Taylor, 2017), ENAS’s error decreases to 2 .",other,highlighting the effectiveness of regularization and data augmentation techniques in improving model performance
1156,,55d49aa6bd3e6605c6510a147c1fb5bdd7af0b12,DiffusionAD: Denoising Diffusion for Anomaly Detection,,,"###A family of generative models called denoising diffusion models [21,35,43,45] are inspired by equilibrium thermodynamics [46,47].###Diffusion models [21, 45, 47], a class of generative models inspired by non-equilibrium thermodynamics [43], define a paradigm in which the forward process slowly adds random noise to the data, and the reverse constructs the desired data samples from the noise.###Our proposed noise-to-norm paradigm is analogous to the noise-to-image procedure in diffusion models [21,45], a class of probabilistic models that learn the distribution of input data and generate a desired image by gradually removing noise from a normally distributed variable.",impact-revealing,describing the concept and process of denoising diffusion models
2856,5e85c28491e0114016e821a6,21b11793b960a3e37c0eab7aae6127c28fd38e5c,Code Prediction by Feeding Trees to Transformers,5d5a76c13a55acc89e207e8f,Improve Language Modelling for Code Completion through Learning General Token Repetition of Source Code.,"These representations have ranged from linear token sequence (as for code prediction [5], [7], [32]), to paths in an AST [15], [21], [52], and sometimes even ways to convey static analysis information to the neural network [24], [49], [51], [64].",other,describing the range of representations used in argument examination
3758,5d08be648607575390f908ca,b30b850e726ee41f80a28c2a81da9c640a8541fc,QoSMT: supporting precise performance control for simultaneous multithreading architecture,53e9a03bb7602d9702922bbb,QoS for High-Performance SMT Processors in Embedded Systems,"The closest work to QoSMT is the design proposed in [6, 8].",other,acknowledge related work
1777,,21435e26f07e8a814f659678909eeb1596384c03,Effects of temporal grouping on the memory representation of inter-tone relationships,,,"###The MMN ERP (for recent reviews, see Näätänen and Winkler, 1999; Picton et al., 2000) has been widely used for investigating memory representations underlying auditory perception (Näätänen and Winkler, 1999; Winkler et al.",impact-revealing,highlighting the significance of the MMN ERP in auditory perception research
1745,,40fab05a41367a8befc93c3927ac9d2febfcbf46,Rational Design of Hyaluronic Acid-Based Copolymer-Mixed Micelle in Combination PD-L1 Immune Checkpoint Blockade for Enhanced Chemo-Immunotherapy of Melanoma,,,"###…sequence has attracted great attention for NDDS due to its biocompatibility and biodegradability, and Pluronic F68 has been widely used for construction of temperature-sensitive gels (Huang et al., 2008; Al Khateb et al., 2016; Powell et al., 2017; Patil et al., 2019; Wang et al., 2019).",impact-revealing,highlighting the significance and application of Pluronic F68 in temperature-sensitive gels
2051,,53d1796b0fcacb78d2c83b75d38e6189d7968578,Exosomes and Exosomal Cargos: A Promising World for Ventricular Remodeling Following Myocardial Infarction,,,"###It is clear that miRNAs, like miR-1 and miR-133, play an essential regulatory role in the development of cardiac hypertrophy.133,134 In a mouse model of Ang II–induced cardiac hypertrophy, CF-derived exosomal miR-21 exerts a pro-hypertrophy effect via targeting SORBS2 and PDLIM5, the Z-line-associated proteins.83 Similarly, a recent study showed that miR-27a is upregulated in infarcted hearts, and promotes cardiomyocyte hypertrophy through targeting PDLIM5, subsequently, contributing to hypertrophic gene expression.135 TAC-induced mouse model is commonly used for studying cardiac hypertrophy and HF.###In a 52-week follow-up study, intramyocardial administration of autologous MSCs was found to have a therapeutic effect on left ventricular remodeling, both on LV end-diastolic volume (LVEDV) and end-systolic volume (LVESV), in patients with advanced HF.###Targeting the p53/JMY signaling pathway, miR-218-5p and miR-363-3p inhibitor or mimic exert a significantly decreased or increased profibrotic effect both in vitro and in vivo, respectively.143 In addition, mi-320a in serum exosomes promotes the CF proliferation via regulating the PI3K/Akt/ mTOR pathway, as well as serves as a potential diagnostic biomarker of CHF.144 Simulating the mechanical stress condition after MI via TAC in vivo and stretching silicon dishes in vitro respectively, Yuan et al confirmed that miR-378 secreted from CMs inhibits excessive cardiac fibrosis by regulating the p38 MAPK pathways through a paracrine mechanism.145 Intriguingly, it has been studied that sacubitril/valsartan, the novel component of HF treatment, exerts an effect of attenuating cardiac fibrosis by downregulating the expression of miR-181a in the payload of circulating exosomes.###Microcirculation is perceived as vital for supplying oxygen and other nutrients to the heart, while the functional and structural abnormalities of microcirculation are well studied to be associated with HF.32,33 In Akt-1 activated experimental models, physiological cardiac hypertrophy is studied associated with enhanced angiogenesis, which is mainly induced by vascular endothelial growth factor (VEGF) and angiopoietin-2, while pathological myocardial remodeling companies with impaired coronary angiogenesis.10 In an autopsy study, immunohistochemistry and microscopy confirmed the significant reduction of microvascular density in patients with HF with a preserved ejection fraction (HFpEF).34 These studies confirmed that the imbalance between angiogenesis and cardiac repair may contribute to pathological VR and HF, while enhancing cardiac angiogenesis may serve as an ideal therapeutic strategy.35 Vascular endothelial cells (ECs), the most abundant non-cardiomyocytes in the heart, are getting more prominence due to their role in maintaining homeostasis and guiding organ repair.36 Expressed mainly in the endothelium of the coronary artery and capillary, endothelial Bmx tyrosine kinase plays an essential role in cardiac hypertrophy and remodeling by involving in the Ang II-induced signal transducer and activator of transcription 3 (STAT3) signaling pathway.###Increasing evidence has shown that mitochondrial dysfunction is implicated in pathological VR and HF.63 Specifically, mitochondrial dysfunction is reflected in multiple levels of ATP generation, including shifted metabolic substrate utilization, impaired mitochondrial oxidative phosphorylation (OXPHOS) system activity, increased reactive oxygen species (ROC) generation, and aberrant mitochondrial dynamics.64 Studies have confirmed that carbohydrates such as glucose and ketone act as substitutes for fatty acids to serve as the primary energy substrates in the failing heart.65,66
Recently, a study by Carley et al showed that short-chain fatty acids present an unexplored energy source in the failing heart, with a higher oxidation efficiency compared with ketone.67 Moreover, accumulating shreds of evidence have indicated that impaired mitochondrial function is involved in pathological cardiac hypertrophy.68 An example is that dual-specificity tyrosine-regulated kinase 1B (DYRK1B), which directly binds and activates STAT3, derives cardiac hypertrophy and HF by suppressing mitochondrial bioenergetics.69 Furthermore, Li et al confirmed that miR-27b-3p, which was elevated in transverse aortic constriction (TAC) and Ang II-induced cardiac hypertrophy mouse models, inhibited the OXPHOS activity to accelerate cardiac hypertrophy.70 The pathological mechanisms of ventricular remodeling are depicted in Figure 1.###Microcirculation is perceived as vital for supplying oxygen and other nutrients to the heart, while the functional and structural abnormalities of microcirculation are well studied to be associated with HF.(32,33) In Akt-1 activated experimental models, physiological cardiac hypertrophy is studied associated with enhanced angiogenesis, which is mainly induced by vascular endothelial growth factor (VEGF) and angiopoietin-2, while pathological myocardial remodeling companies with impaired coronary angiogenesis.",impact-revealing,highlighting the role of miRNAs and other factors in cardiac hypertrophy and heart failure
74,5f7ee07491e011a5faf0feb2,c54fb59319288d0d17ad09e6b7dfee6d042f83a8,Multilingual Knowledge Graph Completion via Ensemble Knowledge Transfer,53e9ba9bb7602d97046c645c,An Efficient Boosting Algorithm for Combining Preferences,"und and Schapire, 1997) and RankBoost (Freund et al., 2004), which target at classification and ranking respectively.###Freund et al. (2004) have proved that, when using RankBoost, this ranking loss is bounded as follows: where M is the number of KGs and therefore the maximum number of rounds in boosting.###Similar to RankBoost (Freund et al., 2004), given a query q, KEnSb evaluates the ranking performance of a model by checking if each of the critical entity pairs {(e, e′)} is ranked in correct order, where e is a ground truth tail and e′ is an incorrect one.###Triples: { (The Tale of Genji, country, Japan) (The Tale of Genji, genre, Monogatari) (The Tale of Genji, genre, Love Story) } Queries: Q = { q 1 = (The Tale of Genji, country, ?t) q 2 = (The Tale of Genji, genre, ?t) } Similar to RankBoost (Freund et al., 2004 Ranking loss.###Inspired by RankBoost (Freund et al., 2004), we reduce the ranking combination problem to a classiﬁer ensemble problem.###Inspired by RankBoost (Freund et al., 2004), we reduce the ranking combination problem to a classifier en-###Representative methods include AdaBoost (Fre-und and Schapire, 1997) and RankBoost (Freund et al., 2004), which target at classiﬁcation and ranking respectively.",impact-revealing,reporting existing methods for classification and ranking
3098,5c2c7a9217c44a4e7cf3161b,e6926981ef9c1d06d6c075cdae7b298d3dbf3a7d,Learning Latent Representations for Style Control and Transfer in End-to-end Speech Synthesis,5aed14d617c44a4438159115,"Style Tokens: Unsupervised Style Modeling, Control and Transfer in   End-to-End Speech Synthesis",[4] which consists of six 2-D convolutional layers followed by a GRU layer.###The hyperparameters are set according to [4].###GST model [4] with character inputs was used as our baseline model.,other,describing the model architecture and hyperparameters
2424,5edf5dd891e011bc656ded70,374f36c9081ab5dc686ab833c42a7297235cd13f,Denoising Implicit Feedback for Recommendation,5cfa5b985ced2477cb3c50fc,Neural Multi-task Recommendation from Multi-behavior Data,"Existing work either adopts the additional feedback by multi-task learning [9, 10], or leverages it to identify the true-positive interactions [32, 39].",other,acknowledge existing approaches in multi-task learning
4043,5aed147c17c44a4438153ea5,665c0dde22c2f8598869d690d59c9b6d84b07c01,domino temporal data prefetcher,558ba7c884ae6766fdee7fb2,Making Address-Correlated Prefetching Practical,"Many pieces of prior work [20], [21], [22], [23] demonstrated the effectiveness of temporal prefetching in reducing data misses and boosting the performance of processors.",other,highlighting the effectiveness of temporal prefetching in improving processor performance
2592,5fd8acf991e0119b22c1f38d,5b9d8bcc46b766b47389c912a8e026f81b91b0d8,Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting,5a260c5717c44a4ba8a294ce,Forecasting at Scale.,"We have selected 5 time-series forecasting methods as comparison, including ARIMA (Ariyo, Adewumi, and Ayo 2014), Prophet (Taylor and Letham 2018), LSTMa (Bahdanau, Cho, and Bengio 2015) and LSTnet (Lai et al. 2018) and DeepAR (Flunkert, Salinas, and Gasthaus 2017).###Baselines: We have selected five time-series forecasting methods as comparison, including ARIMA (Ariyo, Adewumi, and Ayo 2014), Prophet (Taylor and Letham 2018), LSTMa (Bahdanau, Cho, and Bengio 2015), LSTnet (Lai et al.###(4) Our proposed method achieves better results than DeepAR, ARIMA and Prophet on MSE by decreasing 20.9% (at 168), 61.2% (at 336), and 51.3% (at 720) in average.",other,reporting selected time-series forecasting methods for comparison
438,5e09a701df1a9c0c4167614c,3caf34532597683c980134579b156cd0d7db2f40,Universal Adversarial Triggers for Attacking and Analyzing NLP,5f8eab549e795e9e76f6f69e,Language Models are Unsupervised Multitask Learners,"5 In particular, our trigger causes the GPT-2 language model (Radford et al., 2019) to output racist content.###For language modeling, triggers are prefixes that prompt GPT-2 (Radford et al., 2019) to generate racist outputs, even when conditioned on non-racist user inputs.###For language modeling, triggers are preﬁxes that prompt GPT-2 (Radford et al., 2019) to generate racist outputs, even when conditioned on non-racist user inputs. ford et al., 2019) to generate racist outputs using the prompt “TH PEOPLEMan goddreams Blacks” (e.g., bottom of Table 1).###particular, our trigger causes the GPT-2 language model (Radford et al., 2019) to output racist content.",impact-revealing,highlighting the issue of biased outputs in language models
1479,,92f434efde3a03b4e1835c36402199ef709a7535,Image Declipping with Deep Networks,,,"###Elu tends to lead to faster learning.###The Elu activation function is the identity for positive x’s and ex − 1 for negative x’s.###The discriminator network architecture is inspired by the DCGAN class of GAN architectures proposed by Radford et al. [19] We used the exponential linear unit, Elu[21] activation on all discriminator and generator layers except the last.###[19] We used the exponential linear unit, Elu[21] activation on all discriminator and generator layers except the last.###The activation function is Elu.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2004,,5ecb46985e045ee86043fb0649ebcab0b4c3023f,Are Fishers Poor or Vulnerable? Assessing Economic Vulnerability in Small-Scale Fishing Communities,,,"###Moving away from the initial view of poverty as a static (low-income) condition, recent research has highlighted the importance of considering poverty as a dynamic multi-dimensional phenomenon (Dercon and Hoddinott,  2004 ; Barrett and McPeak,  2005 ).###Moving away from the initial view of poverty as a static (low-income) condition, recent research has highlighted the importance of considering poverty as a dynamic multidimensional phenomenon (Dercon and Hoddinott, 2004; Barrett and McPeak, 2005).",impact-revealing,highlighting the evolving understanding of poverty as a dynamic phenomenon
58,5edf5dd891e011bc656ded70,374f36c9081ab5dc686ab833c42a7297235cd13f,Denoising Implicit Feedback for Recommendation,5c8d2c254895d9cbc6422de6,Learning on Partial-Order Hypergraphs.,"Inspired by the dynamic adjustment ideas [7, 24], we replace the fixed threshold with a dynamic threshold function τ (T ) w.",impact-revealing,describing a methodological adjustment inspired by prior work
455,5ea2b8c391e01167f5a89e2d,38643c2926b10f6f74f122a7037e2cd20d77c0f1,Supervised Contrastive Learning,53e99924b7602d970215faff,Distance Metric Learning for Large Margin Nearest Neighbor Classification,"We also show that triplet loss [48] is a special case of our loss when only a single positive and negative are used.###Closely related to contrastive learning are metric learning and triplet losses [7, 48, 40].###Our loss is directly inspired by the family of contrastive objective functions, which have achieved excellent performance in self-supervised learning in recent years in the image and video domains [50, 25, 21, 19, 46, 6, 43] and have connections to the large literature on metric learning [48, 5].###Other contrastive losses, such as triplet loss [48], often use the computationally expensive technique of hard negative mining to increase training efﬁcacy [40].###Contrastive learning is closely related to the triplet loss [48], which is one of the widely-used alternatives to cross-entropy for supervised representation learning.",impact-revealing,providing context on contrastive learning and its relationship to triplet loss
3767,53e9b42fb7602d9703f2696f,8c34cdd2bab66623d2831004fbd1fa1cdf8a0366,Improving memory scheduling via processor-side load criticality information,53e9b52db7602d97040660bf,Thread Cluster Memory Scheduling: Exploiting Differences in Memory Access Behavior.,"Finally, we compare our scheduler against AHB [8], MORSE-P [9, 16], PAR-BS [17], and TCM [12].###The apparent discrepancy from previous TCM results [12] arises from differing workloads and target memory architectures.###We also show results for the more recent TCM proposal [12].###As a comparison, we have also implemented TCM [12], which attempts to balance system throughput ( weighted speedup ) with fairness ( maximum slowdown ).###…scheduler to three state-of-the-art memory schedulers: the adaptive history-based (AHB) scheduler proposed by Hur and Lin [8], the fairness-oriented thread cluster memory (TCM) scheduler [12], and MORSE-P, a self-optimi-zing scheduler that targets parallel application performance [9, 16].###Thread cluster memory (TCM) scheduling [12] classiﬁes threads into either a latency-sensitive or bandwidth-sen-sitive cluster.",other,comparing performance of different memory schedulers
3644,5e943b8091e0113448664118,b26f2037f769d5ffc5f7bdcec2de8da28ec14bee,dense passage retrieval for open-domain question answering,599c7987601a182cd2647a85,Triviaqa: A Large Scale Distantly Supervised Challenge Dataset For Reading Comprehension,"TriviaQA (Trivia) (Joshi et al., 2017) contains a set of trivia questions with answers that were originally scraped from the Web.###TriviaQA (Joshi et al., 2017) contains a set of trivia questions with answers that were originally scraped from the Web.",other,reporting dataset details
3473,5843777eac44360f108417ec,455afd748e8834ef521e4b67c7c056d3c33429e2,Hierarchical Attention Networks for Document Classification,53e9a806b7602d970314bc39,Learning Word Vectors for Sentiment Analysis.,"It has broad applications including topic labeling (Wang and Manning, 2012), sentiment classiﬁcation (Maas et al., 2011; Pang and Lee, 2008), and spam detection (Sahami et al., 1998).",other,highlighting the broad applications of the method
338,5d0b003a8607575390fb4f6a,43d74cd04fb22bbe61d650861766528e369e08cc,An Encoding Strategy Based Word-Character LSTM for Chinese NER,5c8ddce94895d9cbc6a97820,Chinese Ner Using Lattice Lstm,"Compared with lattice model, all of our models achieve better results, which shows that our Models P R F1 Zhang et al. (2006) 92.20 90.18 91.18 Zhou et al. (2013) 91.86 88.75 90.28 Dong et al. (2016) 91.28 90.62 90.95 Cao et al. (2018) 91.73 89.58 90.64 Lattice (Zhang and Yang, 2018) approach to integrating word information is more reasonable than lattice model.###To integrate words information into character-based model, Zhang and Yang (2018) propose a lattice-structured LSTM model to encode a sequence of input characters as well as all potential words that match a lexicon.###…we de-ﬁne its probability to be: While decoding, we use the Viterbi algorithm to ﬁnd the label sequences that obtained the highest score: Given N manually labeled data { ( s j , y j ) }| Nj =1 , we minimize the sentence-level negative log-likelihood loss to train the model: (Zhang and Yang, 2018).###For hyper-parameter conﬁgurations, we mostly refer to the settings in (Zhang and Yang, 2018).###Following (Zhang and Yang, 2018), we use the word embedding dictionary as Lexicon D in our model.###Zhang and Yang (2018) propose a lattice LSTM to exploit word information in character sequence, giving the F1 score of 73.88%.###And this is quite different from the way used in (Zhang and Yang, 2018), since they use extra shortcut paths to integrate word information into the hidden layer of LSTM.###The lexicon D is the same as the one used in (Zhang and Yang, 2018), which is built by using automatically segmented large raw text.###Another way to obtain word boundary information is proposed by (Zhang and Yang, 2018), using a lattice LSTM to integrate word information into character-based model, which is similar to what is proposed in Where ” < PAD > ” denotes padding value; ”Stgy” denotes a certain encoding strategy and ⊕…###We utilize the character and word embeddings used in (Zhang and Yang, 2018), both of which are pre-trained on Chinese Giga-Word using word2vec model.###…which shows that our Models P R F1 Zhang et al. (2006) 92.20 90.18 91.18 Zhou et al. (2013) 91.86 88.75 90.28 Dong et al. (2016) 91.28 90.62 90.95 Cao et al. (2018) 91.73 89.58 90.64 Lattice (Zhang and Yang, 2018) approach to integrating word information is more reasonable than lattice model.###Consistent with the previous re-sults, our models outperform lattice model (Zhang and Yang, 2018).###The lattice model proposed in (Zhang and Yang, 2018) is our principal comparison object, since it also utilizes the word information in character sequence.",impact-revealing,comparing model performance with existing lattice model
1350,,3efd622341d3506abef0e76fbcda8dc6b6ea7f18,A semiparametric method for estimating the progression of cognitive decline in dementia,,,"###Third, ourmodel did not consider the reversion of cognitive/functional ability (Koepsell and Monsell, 2012).",impact-revealing,acknowledge a limitation in the model
1434,,3718a46a804d484bbcf906b341b28fa30cf05126,Marathi To English Neural Machine Translation With Near Perfect Corpus And Transformers,,,"###, 2015) have been widely adopted as the state of-the-art approach for machine translation, both in the research community (Bojar et al., 2016; 2017; 2018) and for large-scale production systems (Cheng et al.###…al., 2014; Bahdanau et al., 2015) have been widely adopted as the state of-the-art approach for machine translation, both in the research community (Bojar et al., 2016; 2017; 2018) and for large-scale production systems (Cheng et al., 2016; Zhou et al., 2016; Crego et al., 2016; Hassan et al.,…",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3679,5b3d98cc17c44a510f801acc,155b7782dbd713982a4133df3aee7adfd0b6b304,Unsupervised Feature Learning via Non-parametric Instance Discrimination,599c797b601a182cd2642d21,SphereFace: Deep Hypersphere Embedding for Face Recognition,"An important technical point on metric learning for face recognition is normalization [35, 22, 43], which we also utilize in this work.",other,providing context on normalization in metric learning
1854,,a1c63079f63431b6fba33e6a61ab133eb0b91044,Use of mental practice to improve upper-limb recovery after stroke: a systematic review.,,,"###The assumption that only the internal perspective is suitable for the generation of kinesthetic imagery appears erroneous and has been questioned in the literature (Callow & Hardy, 2004).###Unfortunately, imagery modality and perspective are often considered one and the same in the literature, making conclusions regarding their benefits difficult to discern (Callow & Hardy, 2004; Glisky & Williams, 1996).",impact-revealing,highlighting a misconception in the literature regarding imagery modality and perspective
2647,59ae3c262bbe271c4c71f4a2,610cff0a09c76c43739be1a6e5b0ed7a1a24ee60,metapath2vec: Scalable Representation Learning for Heterogeneous Networks,5736973b6e3b12023e62b3f8,Panther: Fast Top-k Similarity Search in Large Networks,", skip-gram) to heterogeneous networks? By solving these challenges, the latent heterogeneous network embeddings can be further applied to various network mining tasks, such as node classi_x0080_cation [13], clustering [27, 28], and similarity search [26, 35].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
364,5e15adcb3a55ac47ab5b0b8c,aa63ac11aa9dcaa9edd4c88db18bec87e0834328,Graph Transformer Networks,5aed148b17c44a4438154f22,Deep Collective Classification in Heterogeneous Information Networks.,"Previous works [37, 43] require manually defined meta-paths and perform Graph Neural Networks on the meta-path graphs.###Then conventional GNNs can operate on the transformed homogeneous graphs [37, 43].",impact-revealing,acknowledge limitations in previous works on Graph Neural Networks
2904,53e9b0deb7602d9703b5a742,3fd5a5f29c4b956ee6272855764ed400a84649a7,Japanese NER Post-Processing Based on Improved TBL Method,53e9a3a4b7602d9702cb17ae,Applying Conditional Random Fields to Japanese Morphological Analysis,"The statistical models which were used in Japanese NER, such as Maximum Entropy Model (MEM)([2,3]), Hidden Markov Model (HMM)([4,5]), Condition Random Field (CRF)([6]), Decision Tree([7,8]), Support Vector Machine (SVM), have performed good results, but they also have some limitations.###The statistical models which were used in Japanese NER, such as Maximum Entropy Model (MEM)[2,3], Hidden Markov Model (HMM)[4,5], Condition Random Field (CRF)[6], Decision Tree[7,8], Support Vector Machine (SVM)[9~12], have performed good results, but they also have some limitations.",other,acknowledge limitations of statistical models in Japanese NER
3715,53e9afd3b7602d9703a24a4b,10c1bfa7fe190b3fc6d64c4909cb7ef0911a7b90,Dead-block prediction & dead-block correlating prefetchers,53e9a55cb7602d9702e82bac,"Selective, Accurate, And Timely Self-Invalidation Using Last-Touch Prediction","The key intuition behind why a larger history depth increases an MCP’s accuracy and coverage is that while data structures are often referenced in multiple distinct program contexts or phases [7] (e.###These results indicate that the potential for trace-based predictors to predict memory system events is beyond just predicting memory invalidation and sharing for scientific applications in multiprocessors [7].###Due to control flow irregularities in applications, multiple cache blocks may have dead-block signatures that are proper subsequences of each other resulting in subtrace aliasing [7].###In a recent paper [7], we proposed trace-based predictors that record a trace of shared memory references to predict a last reference to a cache block prior to an invalidation in a multiprocessor.###In a recent paper [7], we proposed Last-Touch Predictors (LTPs) to predict memory invalidations for shared data in a multiprocessor.###We use truncated addition (as before [7]) to maintain a fixed-size encoding for every instruction trace.",other,highlighting the significance of history depth in improving MCP accuracy and coverage
2729,5cede10dda562983788ed645,690edf44e8739fd80bdfb76f40c9a4a222f3bba8,BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer,5bbacbad17c44aecc4eb00ee,Self-Attentive Sequential Recommendation,"For sequential recommendation, Kang and McAuley [22] introduce a two-layer Transformer decoder (i.###To model such sequential dynamics in user behaviors, various methods have been proposed to make sequential recommendations based on users’ historical interactions [15, 22, 40].###Other than recurrent neural networks, various deep learning models are also introduced for sequential recommendation [3, 22, 33, 49].###, next item recommendation) task, which has been widely used in [12, 22, 49].###To ensure the quality of the dataset, following the common practice [12, 22, 40, 49], we keep users with at least five feedbacks.###For sequential recommendation, Kang and McAuley [22] introduce a two-layer Transformer decoder (i.e., Transformer language model) called SASRec to capture user’s sequential behaviors and achieve state-of-the-art results on several public datasets.###• Steam4: This is a dataset collected from Steam, a large online video game distribution platform, by Kang andMcAuley [22].###• SASRec [22]: It uses a left-to-right Transformer language model to capture users’ sequential behaviors, and achieves state-of-the-art performance on sequential recommendation.###2Here, following [22, 40], we use the relative time index instead of absolute time index for numbering interaction records.###For dataset preprocessing, we follow the common practice in [22, 40, 49].###For easy and fair evaluation, we follow the common strategy in [12, 22, 49], pairing each ground truth item in the test set with 100 randomly sampled negative items that the user has not interacted with.###• Amazon Beauty3: This is a series of product review datasets crawled from Amazon.com by McAuley et al. [34].",other,acknowledge existing methods and datasets in sequential recommendation
2031,,29b532ec6e5096fad5041b3ecd2f0103d73778f5,CXCR2-Dependent Accumulation of Tumor-Associated Neutrophils Regulates T-cell Immunity in Pancreatic Ductal Adenocarcinoma,,,"###CXCR2 is also essential for the recruitment of TANs in various cancers (32, 33).###Multiple studies have demonstrated the importance of CXCR2 in the recruitment of tumor-promoting and immunosuppressive myeloid cells in various cancers, including PDA (19, 32, 33, 44).",impact-revealing,highlighting the significance of CXCR2 in cancer research
2272,573695fe6e3b12023e5121fc,52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35,DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks,558b3daa84ae84d265c246c1,Strategies for training large scale neural network language models,"However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images.",other,highlighting the instability of architectures to perturbations
2512,5ede0553e06a4c1b26a8419c,1f3c381eedfe8914b81e93070bfdb00cf86ac943,Contrastive Multi-View Representation Learning on Graphs,634d80db90e50fcafd4ed249,Efficient graphlet kernels for large graph comparison,"…protocol, we compare our results with ﬁve graph kernel methods including shortest path kernel (SP) (Borgwardt & Kriegel, 2005), Graphlet kernel (GK) (Shervashidze et al., 2009), Weisfeiler-Lehman sub-tree kernel (WL) (Sher-vashidze et al., 2011), deep graph kernels (DGK) (Ya-nardag & Vishwana,…###Graph kernels (Borgwardt & Kriegel, 2005; Shervashidze et al., 2009; 2011; Yanardag & Vishwana, 2015; Kondor & Pan, 2016; Kriege et al., 2016) decompose graphs into sub-structures and use kernel functions to measure graph similarity between them.###Graph kernels (Borgwardt & Kriegel, 2005; Shervashidze et al., 2009; 2011; Yanardag & Vishwana, 2015; Kondor & Pan, 2016; Kriege et al., 2016) decompose graphs into substructures and use kernel functions to measure graph similarity between them.###To evaluate graph classification under the linear evaluation protocol, we compare our results with five graph kernel methods including shortest path kernel (SP) (Borgwardt & Kriegel, 2005), Graphlet kernel (GK) (Shervashidze et al., 2009), Weisfeiler-Lehman sub-tree kernel (WL) (Shervashidze et al.",other,acknowledge existing graph kernel methods for comparison
184,5f0bde8e9e795ea206ff8ef5,0feea94f89d395436bf41bd10c797447eecbc128,Unsupervised data augmentation for consistency training,599c7965601a182cd2638d24,Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning.,"(1) where CE denotes cross entropy, q(x̂ | x) is a data augmentation transformation and ✓̃ is a fixed copy of the current parameters ✓ indicating that the gradient is not propagated through ✓̃, as suggested by VAT [6].###Other works in the consistency training family mostly differ in how the noise is defined: Pseudoensemble [2] directly applies Gaussian noise and Dropout noise; VAT [6, 43] defines the noise by approximating the direction of change in the input space that the model is most sensitive to; Cross-view training [8] masks out part of the input data.###Speciﬁcally, following VAT [44], we choose to minimize the KL divergence between the predicted distributions on an unlabeled example and an augmented unlabeled example: where q (ˆ x | x ) is a data augmentation transformation and ˜ θ is a ﬁxed copy of the current parameters θ indicating that the gradient is not propagated through ˜ θ , as suggested by Miyato et al. [44].###Specifically, we compare UDA with two highly competitive baselines: (1) Virtual adversarial training (VAT) [6], an algorithm that generates adversarial Gaussian noise on input, and (2) MixMatch [28], a parallel work that combines previous advancements in semi-supervised learning.###The performance difference between UDA and VAT shows the superiority of data augmentation based perturbation.###In Appendix C, we also compare UDA with recently proposed methods including ICT [61] and mixmixup [19] which enforce interpolation smootheness similar to mixup [69] and LGA + VAT [28], an algorithm based on gradient similarity.###Since regularizing the predictions to have low entropy has been shown to be beneficial [18, 6], we sharpen predictions when computing the target distribution on unlabeled examples by using a low Softmax temperature ⌧ .###In a nutshell, consistency training methods simply regularize model predictions to be invariant to small noise applied to either input examples [6, 7, 8] or hidden states [2, 4].###In Figure 5 4 Note that the difference of UDA and VAT is essentially the perturbation process.###When compared with VAT + EntMin, a prior work on semi-supervised learning, UDA improves the top-5 accuracy from 83 .###We compare UDA with Pseudo-Label [36], an algorithm based on self-training, Virtual adversarial training (VAT) [44], an algorithm that generates adversarial Gaussian perturbations on input, Π -Model [35], which combines simple input augmentation with hidden state perturbations, Mean Teacher [58], which enforces smoothness on model parameters and MixMatch [3], a concurrent work that uniﬁes several prior works on semi-supervised learning.###, x̂ = q(x, ✏), as considered by prior works [7, 4, 6].###As explained earlier, works in this family mostly differ in how the perturbation is deﬁned: Pseudo-ensemble [2] directly applies Gaussian noise; Π -Model [35] combines simple input augmentation with hidden state noise; VAT [44, 43] deﬁnes the perturbation by approximating the direction of change in the input space that the model is most sensitive to; Cross-view training [7] masks out part of the input data; Sajjadi et al. [52] combines dropout and random max-pooling with afﬁne transformation applied to the data as the perturbations.###While the perturbations produced by VAT often contain high-frequency artifacts that do not exist in real images, data augmentation mostly generates diverse and realistic images.###In the fully supervised settings, the pre-BERT SOTAs include ULMFiT [26] for Yelp-2 and Yelp-5, DPCNN [29] for Amazon-2 and Amazon-5, Mixed VAT [51] for IMDb and DBPedia.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
814,5a260c0c17c44a4ba8a1e113,c751ab01aedc2888a7fe6e8b4f77ab1afa94072f,protein interface prediction using graph convolutional networks.,55a6cfb565ce054aad76c0d7,Progress and challenges in predicting protein interfaces.,"The prediction of those interactions, and the interfaces through which they occur, are important and challenging problems that have attracted much attention [10].###This calls for new methodologies or sources of information to be exploited"" [10].",impact-revealing,highlighting the importance and challenges of predicting interactions in research
2864,5e09cab43a55ac662f721ac6,5c109db04998e623b794b269494f44b7e5006af1,Category-Level Articulated Object Pose Estimation,5bdc315817c44a1f58a05ea5,Implicit 3D Orientation Learning for 6D Object Detection from RGB Images,"Such understanding is beyond the scope of typical 6D pose estimation algorithms, which have been designed for rigid objects [28, 23, 22, 26].",other,highlighting limitations of typical 6D pose estimation algorithms
1146,,63d53df29c1bad803955b91368fa9bbd102c6524,DAG: Depth-Aware Guidance with Denoising Diffusion Probabilistic Models,,,"###First row is unguided samples from DDIM [52], and the second row is guided samples using our guidance method, called depthaware guidance (DAG).###Note that we use two different U-Net implementations from ADM [8] and DDIM [52] because of the existence of pretrained weights, thus the selection of the feature extraction block is slightly changed.###Many attempts have been made to improve the sample quality and sampling speed through various approaches [8, 22, 37, 40, 52].###[52] proposed non-Markovian diffusion process to reduce sampling steps whereas [40] leveraged diffusion process in the discrete latent space introduced in [11] for efficient computation and faster sampling.###We use the pretrained weights from ADM [8] for LSUN-Bedroom, and for the LSUN-Church dataset, we build upon the public repository of DDIM [52] and Diffusers library [59].###We use Adam optimizer [25] to train the depth predictor MLP, and for the sampling of the diffusion model, we use a DDIM [52] sampler with a DDIM25 scheduler provided by the official repository of ADM [8] and DDIM.",impact-revealing,describing the methodology and implementation details of the sampling process
1536,,12efd932d62fb0d1208808bad0e6718fc366ea45,Talk amongst ourselves: Examining how heterosexual adults form and transform perspectives about homosexuality through dialogue with one another,,,"###Wells (2007) refers to discursive practices as “discoursing,” which he says “is at the heart of the identity construction project” (p.###As Potter & Wetherell (1987) aptly note, “Research into discourse concerns crucial###As Willig (1999) herself described: Discourse analysis as guide to reform is praxis-oriented in that it seeks to use the results of discourse analytic studies in order to develop social interventions.###concepts are summarized here according to the five organizing parts of Sears’ & Williams’ (1997) book: Foundational Issues.###In the Multidimensional Model, Worthington et al. (2002) conceived of the above two
processes, the individual and the social, as occurring within five ‘statuses,’ or sets of developmental phenomena that build on one another while at the same time remain fluid and
Talk Amongst Ourselves 32
flexible for individuals to circle back through and revisit as development progresses.###In the Multidimensional Model, Worthington et al. (2002) conceived of the above two processes, the individual and the social, as occurring within five ‘statuses,’ or sets of###concepts are summarized here according to the five organizing parts of Sears’ & Williams’ (1997) book: Foundational Issues. Sears (1997), a professor of Curriculum Studies, launches the book with a critical overview of the scholarship up to that point in time regarding definitions, components and efforts to reduce homophobia and heterosexism.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3086,5f03f3b611dc830562232090,aa2c0bd8345c7698f13ab7cc7e0fe9eaa8e4f108,Cracking Tabular Presentation Diversity for Automatic Cross-Checking over Numerical Facts,5a73cb5d17c44a0b30357397,"Automated Historical Fact-Checking by Passage Retrieval, Word Statistics, and Virtual Question-Answering.","Verifying such claims includes detecting whether a statement in check-worthy [5], retrieve information from large data source to provide related evident paragraphs, and finally give a classification [7, 10, 16].",other,describing the process of claim verification
3671,5b1642d68fbcbf6e5a9b7e77,0be19fd9896e5d40222c690cc3ff553adc7c0e27,Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods,53e9b321b7602d9703debc3f,Easy Victories and Uphill Battles in Coreference Resolution.,"1 We use three different systems as prototypical examples: the Stanford Deterministic Coref-erence System (Raghunathan et al., 2010), the Berkeley Coreference Resolution System (Durrett and Klein, 2013) and the current best published system: the UW End-to-end Neural Coreference Resolution System (Lee et al., 2017).###, 2010), feature-based (Durrett and Klein, 2013; Peng et al., 2015a), and neuralnetwork based (Clark and Manning, 2016; Lee et al.###, 2010), feature-rich, Feature, (Durrett and Klein, 2013), and end-to-end neural (the current state-ofthe-art), E2E, (Lee et al.###, 2010), the Berkeley Coreference Resolution System (Durrett and Klein, 2013) and the current best published system: the UW End-to-end Neural Coreference Resolution System (Lee et al.",other,reporting examples of coreference resolution systems
1957,,ed69dae86a7bbf42b73e4f76eb09e8795c827c57,Variant 3: Acute nonlocalized abdominal pain. Neutropenic patient. Initial imaging.,,,"###Although not frequently used for this indication, some small studies suggest US maybe more sensitive and specific than abdominal radiographs in the diagnosis of suspected SBOs and can have a similar sensitivity to CT in specialized centers [41,94].###In addition, US may be useful in selected localizing conditions, including cholecystitis, cholangitis, liver abscess, diverticulitis, appendicitis, and small-bowel inflammation, where it may be used to assess activity of Crohn disease [41,90-93].",impact-revealing,highlighting potential advantages of ultrasound in diagnosing specific conditions
2777,5d1eb9beda562961f0af981f,934d7bffdba0b560a80a518b99a791a16b3e198c,A Fourier Perspective on Model Robustness in Computer Vision,5ca600ae6558b90bfa4d76e9,Towards Deep Learning Models Resistant to Adversarial Attacks,"For any image X , we run the PGD attack [21] to generate an adversarial example C(X).",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
837,53e99845b7602d9702071dfc,178deeb441faf2892719fa24f5a0593794d704a9,matrix scheduler reloaded,53e9b4cab7602d9703fe7aa3,Select-free instruction scheduling logic,"Select-free instruction scheduling [5] and grandchild scheduling [30] both move the picker into a separate pipeline stage from wakeup.###Academic researchers have proposed dataflow prescheduling [5, 8, 21, 22] and dependence collapsing [3, 24, 25], both of which use dataflow information to reduce or eliminate the need for wait-match in the schedule loop.",impact-revealing,acknowledge existing scheduling methods and their variations
3077,5da1a6d447c8f766460688bc,9570147aee646e15affffc30a6b18be0568cfa82,SWQUE: A Mode Switching Issue Queue with Priority-Correcting Circular Queue,558ab523e4b031bae1f93377,On pipelining dynamic instruction scheduling logic,Stark et al. proposed breaking the wakeup–select loop into different pipeline stages [27].,other,reporting prior findings on pipeline stages
3882,599c7b59601a182cd272bf53,08a426042c9926419198ee22c9bf80e6e4b5791b,Soft-DTW: a Differentiable Loss Function for Time-Series,558adec1e4b037c08759a7f3,A Kernel for Time Series Based on Global Alignments,"Both DTW and GAK consider the costs of all possible alignment matrices, yet do so differently: DP Recursion.###In parallel to these developments, several authors have considered smoothed modiﬁcations of Bell-man’s recursion to deﬁne smoothed DP distances (Bahl & Jelinek, 1975; Ristad & Yianilos, 1998) or kernels (Saigo et al., 2004; Cuturi et al., 2007).###When considering kernel k γ GA and, instead, its integration over all alignments (see e.g. Lasserre 2009), Cuturi et al. (2007, Theorem 2) and the highly related formulation of Saigo et al. (2004, p.1685) use an old algorithmic appraoch (Bahl & Jelinek, 1975) which consists in (i) replacing all…###We propose in this section a uniﬁed formulation for the original DTW discrepancy (Sakoe & Chiba, 1978) and the Global Alignment kernel (GAK) (Cuturi et al., 2007), which can be both used to compare two time series x = ( x 1 , . . . , x n ) ∈ R p n and y = ( y 1 , . . . , y m ) ∈ R p m .",other,providing context on DTW and GAK methods
3780,53e9ad2db7602d97037121c7,e9090508b9073763c6693983577137db2a41a25b,multimodal fusion for multimedia analysis: a survey,53e9b2d2b7602d9703d82a37,The Banca Database And Evaluation Protocol,"For example, BANCA [14] that contains face and speech modalities; XM2VTS [82] that con-",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1866,,27f98f8bd3e6550a15403daa10d976446912e6ea,The Power of Radio to Promote Health and Resilience in Natural Disasters: A Review,,,"###A literature review, which was inspired by the integrative review method [16] and based on research studies and grey literature, was conducted.###Then, the extracted data were sorted into groups and clusters related to the aim of the study (humanitarian radio as a disaster health response intervention), and the reviewers compared and contrasted the information before summarizing the analysis into an integrated synthesis [16].###This kind of review summarizes past empirical and theoretical literature to achieve a comprehensive understanding of a specific problem or phenomenon [16].###However, the lack of randomized controlled studies does not necessarily mean that the intervention is not useful or effective, and an integrative review can contribute to building evidence on use, effects and best practice [16].###An original integrative review used quality scoring in order to conduct a quality appraisal [16].",impact-revealing,describing the methodology and significance of the integrative review process
2044,,b44d38ac58e3fbe21edf29ed3d7dee49791ff39c,Multi-Surface and Multi-Field Co-Segmentation of 3-D Retinal Optical Coherence Tomography,,,"###For finding the optimal solution we build upon the graph-search framework [11], [27], [28], called LOGISMOS [29].###obtained by the graph-search approach [11].",impact-revealing,reporting the use of a specific graph-search framework
2468,5db1765a3a55ac101c887e97,6c4b76232bb72897685d19b3d264c6ee3005bc2b,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,58437725ac44360f1082ffd7,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,"6 [Wu et al., 2016] for the WMT translation and CNN/DM summarization tasks.###Specifically, we use a beam width of 4 and a length penalty of α = 0.6 [Wu et al., 2016] for the WMT translation and CNN/DM summarization tasks.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
88,5d3ed25a275ded87f97dea9b,0ddd7c9a1955c76eb831a230ef20a68b662886bb,Mining Algorithm Roadmap in Scientific Publications,5b076eb4da5629516ce72e45,Unsupervised Does Not Mean Uninterpretable: The Case for Word Sense Induction and Disambiguation,"Inspired by word sense disambiguation methods that label super sense types for word clusters [23], we jointly predict the types for abbreviation candidates with relation extraction task to distinguish abbreviations for downstream roadmap construction.###Word sense disambiguation methods [23] have been studied to disambiguate word senses, however, deciding the sense for the abbreviation in the scientific domain is still challenging when lack of labeled data.###Word sense disambiguation [23] is a type of technique used to distinguish ambiguous word senses.",impact-revealing,highlighting the challenges in abbreviation disambiguation and the relevance of word sense disambiguation methods
2021,,3551676603f8071f83245b52a4f72157100117fa,Hyperparameter Optimization with Genetic Algorithms and XGBoost: A Step Forward in Smart Grid Fraud Detection,,,"###Particle swarm optimization is inspired by the social behavior of birds flocking, allowing for a collective exploration of the solution space through the movement and interaction of individual ‘particles’ [23].",impact-revealing,providing context on particle swarm optimization
505,5ecbc7639fced0a24b502b60,326974bd97dea3278515bcb3551a59e304d6d755,Improving Low-Resource Speech Recognition Based on Improved NN-HMM Structures,53e99d6cb7602d9702624633,A maximum-likelihood approach to stochastic matching for robust speech recognition,"003] (6) Feature-space Maximum Likelihood Linear Regression (FMLLR) was explored in [32], [33] for speaker adaptive training and it is a feature space transform where we transform acoustic features for better ﬁt to a speaker-independent (SI) model.",impact-revealing,reporting prior findings on feature-space maximum likelihood linear regression
1972,,be7cdaddded9e27c83cee4c387dc963c9d1537a7,Listening to Consumer Perspectives to Inform Addictions and Housing-Related Practice and Research.,,,"###…outcomes of consumers reported that treatment first participants were more likely to have higher rates of substance use than housing first participants (Padgett et al., 2011), and that housing first participants were more likely to remain in their programs and used fewer substance abuse services.###…been criticized for creating additional hardship for homeless people unable to maintain sobriety; in response, some advocate for the housing first approach, which does not require participation in treatment programs as a condition for long-term housing (Lincoln et al., 2009; Padgett et al., 2011).###One study that compared the outcomes of consumers reported that treatment first participants were more likely to have higher rates of substance use than housing first participants (Padgett et al., 2011), and that housing first participants were more likely to remain in their programs and used fewer substance abuse services.###This housing model has been criticized for creating additional hardship for homeless people unable to maintain sobriety; in response, some advocate for the housing first approach, which does not require participation in treatment programs as a condition for long-term housing (Lincoln et al., 2009; Padgett et al., 2011).",impact-revealing,highlighting the comparison of outcomes between treatment first and housing first approaches
2916,5eede0b791e0116a23aafe75,150f95f9c73820e0a0fa1546140e9f2bdfd25954,temporal graph networks for deep learning on dynamic graphs,5a9cb66717c44a376ffb8c25,Modeling polypharmacy side effects with graph convolutional networks.,"Graphs are ubiquitously used as models for systems of relations and interactions in many ﬁelds [5, 52, 42, 10, 16, 20, 49, 53], in particular, social sciences [68, 43] and biology [76, 62, 18].",other,acknowledge the widespread use of graphs in various fields
3385,5c04966a17c44a2c74708401,51203e9d5620abdcdf6c9be93b1e221e79cda67d,Transfer Learning of Language-independent End-to-end ASR with Language Model Fusion,5a260c8117c44a4ba8a30e87,Cold Fusion: Training Seq2seq Models Together With Language Models,"Recently, several methods to leverage an external LM during training of S2S models are proposed: deep fusion [20] and cold fusion [21].###ReLU non-linear function is inserted before the softmax layer as suggested in [21].###In [21], the authors show the effectiveness of cold fusion in a cross-domain scenario.###While shallow fusion uses the external LM only in the inference stage, cold fusion [21] uses the pre-trained LM during training of the S2S model to provide effective linguistic context.###In the original formulation in [19, 21], scores from the external LM are not used.",other,reporting recent methods for leveraging external language models in training
1063,,3088216aaed76a0ac6d5bd1bf99bb35c4dabd68d,Looking beyond the R&D effects on innovation: The contribution of non-R&D activities to total factor productivity growth in the EU,,,"###The CDM model has been frequently applied by scholars using data from the Community Innovation Survey (CIS) launched by Eurostat, such as Lööf and Heshmati (2003) for Norway, Finland and Sweden; Janz et al. (2004) for Germany and Sweden; and Griffi th et al. (2006) for France, Germany, Spain and the UK.###…model has been frequently applied by scholars using data from the Community Innovation Survey (CIS) launched by Eurostat, such as Lööf and Heshmati (2003) for Norway, Finland and Sweden; Janz et al. (2004) for Germany and Sweden; and Griffi th et al. (2006) for France, Germany, Spain and the UK.",impact-revealing,acknowledge the application of the CDM model in various studies
2868,5b3d98cc17c44a510f802054,8a564ee07fa930ebc1176019deacdc9951063a99,Collaborative Learning for Deep Neural Networks.,5a260c8417c44a4ba8a31511,mixup: Beyond Empirical Risk Minimization.,Note that increasing training time for individual learning does not improve accuracy [22].,other,highlighting a finding about training time and accuracy
695,58437735ac44360f10832276,225bcc075bc0d58c6fef43bfeb6dc427974e95c8,Yet Another Compressed Cache,55323c6d45cec66b6f9dc0bd,Skewed Compressed Caches,"This flexible sub-block allocation eliminates re-compaction overheads when a block size grows, but requires the additional area and complexity of backward pointers to maintain the decoupled mapping.###Skewed Compressed Cache (SCC) [Sardashti et al. 2014] also uses super-block tags, but eliminates DCC’s backward pointers.###• SCC models Skewed Compressed Cache [Sardashti et al. 2014] with 4-block super-blocks, and 16-byte sub-blocks.###For YACC, SCC [Sardashti et al. 2014], and DCC [Sardashti and Wood 2013], we use 4-block super-blocks (each tag tracks 1-4 neighbors), and 16-byte sub-blocks (i.e., each block compress to 0-4 sub-blocks).###Like SCC, YACC uses sparse super-block tags [Sardashti et al. 2014] to map the blocks that are compressed in the corresponding data.###SCC [Sardashti et al. 2014] eliminates DCC’s backward pointers and simplifies cache replacement, but adds the complexity of skewed associativity, complicating the tag array design and limiting the choice of replacement policy.###In practice with SCC [Sardashti et al. 2014], we addressed most of the issues of the compaction in caches: very limited tag and metadata overhead, direct tag-data matching, no need for defragmentation.###SCC [Sardashti et al. 2014] is a state-of-the-art compressed cache design that picks the best of the alternatives discussed in Section 2.2.###YACC is a new compressed cache design that seeks to preserve the good aspects of SCC, while eliminating the limitations associated with skewing.###State of the art compressed caches, Decoupled Compressed Cache (DCC) [Sardashti et al. 2013] and Skewed Compressed Cache (SCC) [Sardashti et al. 2014], try to achieve some of these goals, but at extra costs and complexities.###2005; Kim et al. 2002; Sardashti and Wood 2013; Sardashti et al. 2014].",impact-revealing,discussing various cache designs and their complexities
2200,5c04967517c44a2c74709354,04c131293bf64c67972baa0053e85a510c4aa725,Intervention Harvesting for Context-Dependent Examination-Bias Estimation,53e9bc27b7602d9704894740,A user browsing model to predict search engine click data from past observations.,"Built upon the PBM and the Cascade model, more complex models like UBM [13], DBN [8], CCM [14] and CSM [6] were proposed to infer relevance judgments from click logs.",other,acknowledge the development of complex models for relevance judgments
948,,b43372fad700357f814da54bbd24ae4d5ff1a095,FGRL-Net: Fine-Grained Personalized Patient Representation Learning for Clinical Risk Prediction Based on EHRs,,,"###Typical examples are RETAIN [5], Dipole [22], SAnD [6], ConCare [7] and COVIDCare [23].###Visit-feature process first embeds all features of each visit seperately and then deals with the embedded temporal visits [6] [5] [8] [10].###Some existing works [5] [6] [7] [8] adopted attention-based mechanism to study the importance of each feature, which is not suitable for feature correlation detection.",impact-revealing,acknowledge existing works and their methodologies
3930,5dbebb7447c8f766462c21c0,3d9baf7e87ec43f0ad486e2077824a346a58118e,Emotion-aware Chat Machine: Automatic Emotional Response Generation for Human-like Emotional Interaction,599c7968601a182cd263a565,Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory,", NLPCC 20132 and NLPCC 20143 emotion classification datasets by following [46], which contain 29, 417 manually annotated data in total, and the best performance(accuracy) of 0.###1Here we follow the work [46], where the emotion categories are {Angry, Disgust, Happy, Like, Sad, Other}.###From Table 2, we can observe that: (i) ECM performs worse than Seq2seq, the reason might be the emotion selection process is based on two-stage process, i.e., post emotion detection process and response emotion selection process, which would significantly reduce the diversity and quality of emotion response generation due to the errors of emotion classification and the transition pattern modeling procedure.###From Table 3, we can observe that: Seq2seqemb provides the worst performance as expected, this is because the generation process apparently interrupted would significantly reduce the accuracy and quality of generating responses and thus generate some hard-to-perceive sentences; ECM achieves a relatively better result, as it is good at modeling the emotion dynamics when decoding (i.e., internal memory) and assigning different generation probabilities to emotion/generic words for explicitly modeling emotion expressions (i.e., external memory); Seq2seq-emb results in a remarkable improvement over Seq2seq-emb in terms of semantic score, but it performs poorly when comparing sentiment score, which demonstrates the effectiveness of emotion injection, however the explicit two-stage procedure might reduce the smoothness of generated responses with low semantic score.###The results of EACM and ECM with (like, other) are similar and correct.###[46] successfully build an emotional chat machine (ECM) that is capable of generating emotional responses according to a pre-defined emotion category, and several similar efforts are also made by [11, 25], such as [48] proposed by Zhou et al.###However, responses given by ECM with other emotions are improper in semantics.###[46] develop an Emotional Chat Machine (ECM) model using three different mechanisms (i.###However, all of ECM with different emotions seems improper for response (especially for ECM with (Happy,other)), which reflects directly using a designated emotion for generation might be a unreasonable way for modeling the emotion interaction pattern.###In literature, Zhou et al. [46] successfully build an emotional chat machine (ECM) that is capable of generating emotional responses according to a pre-defined emotion category, and several similar efforts are also made by [11, 25], such as [48] proposed by Zhou et al. that utilizes emojis to control the emotional response generation process within conditional variational autoencoder (CAVE) framework.###In particular, we follow the work [46] to train an emotion classifier for assigning emotional labels to the sentences in the dataset.###The parameters of imemory and ememory in ECM are the same as the settings in [46].###Thereby, we manually designate a most frequent response emotion to ECM for fairness comparison.###Intuitively, the generated responses from ECM and Seq2seq-emb can be viewed as the indication of the performance of simply incorporating the EIPs for modeling the emotional interactions among the conversation pairs.###In particular, unlike [46] using solely one label for classification, we consider both of the emotion labels and thus regard it as a multi-label classification task.###For example, EACM outperforms ECM by 16.9%, 1.72% and 25.81% in terms of semantic score, sentiment score and response quality, respectively.###The emotion in the case is (Angry, other), however the responses provided by ECM with (Angry, other) is obviously incorrect in semantics, which demonstrate that simply using post’s emotion is inappropriate for response generation.###For ECM, the percentage of (0-0) degrades while the percentage of (1-0) increases as opposed to Seq2seq, which suggests that the effectivenss of EIP, i.e., themost frequent response emotions have low probability to result in emotional conflicts, In addition, the percentage of (1-1) degrades while the percentage of (1-0) increases, which reflects that directly using emotion classifier to model emotion interaction process is insufficient.###ECM [46], as mentioned, ECM model is improper to directly be as the baseline since it cannot automatically select an appropriate emotion label to the respond.###Third, it is also problematic to design a unified model that can generate plausible emotional sentence without sacrificing grammatical fluency and semantic coherence [46].###Zhou et al. [46] develop an Emotional Chat Machine (ECM) model using three different mechanisms (i.e., emotion embedding, internal memory and external memory) to generate responses according to the designated emotion category.###Seq2seq-emb [11, 46], Seq2seq with emotion embedding (Seq2seqemb) is also adopted in the same manner.",other,acknowledge prior work on emotional chat machines and their mechanisms
1161,,af3327ed734734a79d3395d5065e0f1921f1cb3f,Safe Distributed Coordination of Heterogeneous Robots through Dynamic Simple Temporal Networks,,,"###In [24] four types of activity constraints that are closely related to traditional constraints are introduced.###Mittal and Falkenhainer provide a formulation of Dynamic Constraint Satisfaction Problems (Dynamic CSPs) in [24].###conditions in which variables activate or deactivate, as described in further detail in [24].###The distributed temporal planner (DTP) presented in this thesis leverage distributed constraint satisfaction problems (distributed CSPs), because DTP utilizes dynamic constraint satisfaction problems (dynamic CSPs) [24] in a distributed fashion.###The research builds upon previous work on model-based programming [18], simple temporal networks [8], dynamic CSPs [24] and distributed CSP algorithms [23].###The main reason is that our representation of a model-based program is based on a hybrid of simple temporal constraints (STC) [8] and dynamic CSPs [24].###The choices are between functionally redundant methods [24].",impact-revealing,acknowledge and build upon previous work in dynamic constraint satisfaction problems
2286,5e09cab43a55ac662f721ac6,5c109db04998e623b794b269494f44b7e5006af1,Category-Level Articulated Object Pose Estimation,5b3d98c717c44a510f801749,Learning to Estimate 3D Human Pose and Shape from a Single Color Image,"For human pose estimation, approaches have been developed using end-to-end networks to predict 3D joint locations directly [15, 21, 17], using dense correspondence maps between 2D images and 3D surface models [2], or estimating full 3D shape through 2D supervision [13, 18].",other,reporting various approaches in human pose estimation
715,5b67b45517c44aac1c86084b,af8a8dcb74561d52d904f7bc4afcc747e079b702,modeling task relationships in multi-task learning with multi-gate mixture-of-experts,53e9ab38b7602d97034c8ad9,Adaptive Mixtures of Local Experts.,"is inspired by the Mixture-of-Experts (MoE) model [21] and the recent MoE layer [16, 31].###The Original Mixture-of-Experts (MoE) Model [21] can be formulated as:",impact-revealing,providing context for the model inspiration
574,5ecbc6199fced0a24b4eefa9,3c31c95870824736ecaba2ba01f2ccab145fd91d,Leveraging Unpaired Text Data for Training End-To-End Speech-to-Intent Systems,5db92af347c8f766462179d1,Investigating Adaptation and Transfer Learning for End-to-End Spoken Language Understanding from Speech,"[7, 11] address this problem using a curriculum and transfer learning approach whereby the model is gradually trained on increasingly relevant data until it is fine-tuned on the actual domain data.",impact-revealing,reporting prior findings on curriculum and transfer learning approaches
303,5e5e191993d709897ce5087d,674b6321ae1d12c83f28ade1850a27256c20f0d4,Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset,5bdc315017c44a1f58a05b9f,MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling.,"0 (Wen et al. 2017), FRAMES (El Asri et al. 2017), M2M (Shah et al. 2018) and MultiWOZ (Budzianowski et al. 2018).###This line of work has been facilitated by the release of multi-domain dialogue corpora such as MultiWOZ (Budzianowski et al. 2018), M2M (Shah et al.###This line of work has been facilitated by the release of multi-domain dialogue corpora such as MultiWOZ (Budzianowski et al. 2018), M2M (Shah et al. 2018) and FRAMES (El Asri et al. 2017).",impact-revealing,acknowledge the release of multi-domain dialogue corpora
3819,53e9b54ab7602d97040825b6,e4fc3adca44206ecb5dc2c4960e578fe2d0994fe,Secure Untrusted Data Repository (SUNDR),558abaae84ae84d265bf4aec,Strong security for distributed file systems,"SNAD [16] can use digital signatures for integrity, but does not guarantee freshness.",other,highlighting a limitation of SNAD
2977,5fe4094e9e795e14f30e634a,94497472eecb7530a2b75c564548c540ebd61e9b,Learning to Pre-train Graph Neural Networks,5f03f3b611dc830562232019,Meta-learning on Heterogeneous Information Networks for Cold-start Recommendation,"Finally, some optimization-based methods directly adjust the optimization algorithm to enable quick adaptation with just a few examples (Finn, Abbeel, and Levine 2017; Yao et al. 2019; Lee et al. 2019; Lu, Fang, and Shi 2020).",other,acknowledge existing optimization-based methods
1759,,00a15dda3093ac3e679f4d956fd086b6a6bfef82,Generative Design of Hardware-aware DNNs,,,"###Different HW configurations (e.g., TPU (Jouppi et al., 2017), Eyeriss (Chen et al., 2016b), and ShiDiaNao (Du et al., 2015)) have different HW resource budgets (e.g., memory capacity, bandwidth etc), and hence require different quantization strategies to fit the models.###Similar approaches such as ACGAN (Odena et al., 2017), infoGAN (Chen et al., 2016a), and others (Odena, 2016) (Ramsundar et al., 2015), task the discriminator to reconstruct the class label taken by the generator rather than feeding in condition to discriminator.###GANs have tackled the tasks of image generation (Chen et al., 2016a; Goodfellow et al., 2014; Odena et al., 2017), image prediction (Yoo et al., 2016), text-to-image synthesizing (Reed et al., 2016), and sequence generation (Yu et al., 2017).###First, we train the Quantizer, an accuracy-predicting model that serves as an auxiliary instructor in the next step, which is inspired by the auxiliary classifier in ACGAN (Odena et al., 2017) and the classification of a discriminator in infoGAN(Chen et al., 2016a).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1057,,0d88a481cbab6b99760351f3712485b9653dead6,Epigenetic biomarker development.,,,###com/methylprimerexpress) are widely used for methylation-specific primer design; BiQ Analyzer [86] and QUMA [87] support data analysis for bisulfite sequencing; commercial packages such as PyroMark Assay Design Software (http://www.###BiSearch [85] and Methyl Primer Express (http://www.appliedbiosystems.com/methylprimerexpress) are widely used for methylation-specific primer design; BiQ Analyzer [86] and QUMA [87] support data analysis for bisulfite sequencing; commercial packages such as PyroMark Assay Design Software (http://www.pyrosequencing.com/DynPage.aspx?id=7257) and EpiDesigner (http://www.epidesigner.com/) facilitate custom assay design for the respective methods; and the MassArray R package [88] provides an open-source alternative for EpiTYPER data analysis.,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2821,5736973b6e3b12023e62b0a8,97e3bb4af723f43927317e9b9f2d794a9e398e8e,PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks,5550489045ce0a409eb6f76a,A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval.,"The supervised approaches [5, 8, 23, 6] are usually based on deep neural network architectures, such as recursive neural tensor networks (RNTNs) [24] or convolutional neural networks (CNNs) [11].",other,acknowledge existing supervised approaches in deep learning
3747,5bdc31b417c44a1f58a0b894,510d98681e5e85fb1265513728f16e2543ae1b4b,Hypergraph Neural Networks,53e9b3ceb7602d9703ebd964,Video Object Segmentation By Hypergraph Cut,"In (Huang, Liu, and Metaxas 2009), hypergraph learning is further employed in video object segmentation.",other,reporting prior findings on hypergraph learning in video object segmentation
3920,5736982b6e3b12023e6fd21d,0344f9ac92006155f6fa464cab19925ac481dea4,Microarchitectural implications of event-driven server-side web applications,53e9a5b7b7602d9702ee3918,A Performance Counter Architecture For Computing Accurate Cpi Components,"Therefore, CPI stacks are used to identify sources of microarchitecture inefficiencies [36, 37].",other,providing context for the use of CPI stacks
2139,,064f91172072e33d43ef2f7d80cfa0b51ab6290b,Co-Optimization of Environment and Policies for Decentralized Multi-Agent Navigation,,,"###SPL is a stringent measure that combines navigation success rate with path length overhead, and has been widely used as a primary quantifier in comparing navigation performance [53]–[55].",impact-revealing,providing context on a specific measure used in navigation performance evaluation
773,5b3d98cc17c44a510f80212a,cb91c2f8d3cac0b655a39be318b603334eb18987,learning to optimize tensor programs,5bdc316717c44a1f58a071ff,TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.,We use primitives from an existing code generation framework [9] to form Se.,impact-revealing,reporting the use of existing code generation framework
1642,,d74cb454e8aa5240ee02837fae024860925d9d95,Understanding and Improving Athlete Mental Health: A Social Identity Approach,,,"###When subsequently developing the social identity approach, Tajfel and Turner [13] sought to build on this key finding by providing a comprehensive analysis of the wide-ranging and meaningful ways through which defining oneself as a group member (e.g. as a member of one’s sports team), and…###…team members’ similarities on such dimensions (e.g. in terms of their attitudes toward training or their sporting goals), as well as ways in which the team as a whole is positively distinct from other teams [13, 144], may be advantageous for leaders seeking to build social identification.###The social identity approach starts by recognising an individual’s capacity to define themselves both in terms of their personal identity (as ‘I’ and ‘me’) and their social identities (as members of the various groups to which they belong [13]).###In this article, we argue that efforts to understand and improve athlete mental health will be fruitfully enhanced by tackling these challenges through the lens of the social identity approach [13, 14].",impact-revealing,highlighting the significance of the social identity approach in understanding athlete mental health
2430,5f9a9af391e0114d7e7813ed,0d67d3ddca1c4e370eaf1e99ec674f612c39c66c,Graph Contrastive Learning with Adaptive Augmentation,5d9edc8347c8f76646042a37,Simplifying Graph Convolutional Networks,"Among them, considerable literature has grown up around the theme of supervised GNN [20, 23, 45, 47], which requires labeled datasets that may not be accessible in real-world applications.",other,highlighting the limitations of supervised GNN due to the lack of accessible labeled datasets
1109,,7178bbc5e8d2d9b11c890c60486ba2cc2b79b784,MotionBooth: Motion-Aware Customized Text-to-Video Generation,,,"###It builds upon earlier breakthroughs in text-to-image generation [41, 38, 19, 21, 34, 45, 56, 58] but introduces more complex dynamics by incorporating motion and time [43, 20, 18, 2, 63, 55].",impact-revealing,highlighting advancements in text-to-image generation
48,59ae3c262bbe271c4c71ea21,83e7654d545fbbaaf2328df365a781fb67b841b4,Enhanced LSTM for Natural Language Inference,5550418445ce0a409eb3bfc8,Compositional Distributional Semantics with Long Short Term Memory.,"trees of a premise and hypothesis through treeLSTM (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al.###To this end, we will also encode syntactic parse trees of a premise and hypothesis through tree-LSTM (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al., 2011).",impact-revealing,describing the method of encoding syntactic parse trees
2124,,b68cc5aa3810b89d3c213f8d26c664cd997afc3c,Evacuation Planning with Endogenous Transportation Network Degradations: A Stochastic Cell-Based Model and Solution Procedure,,,"###CTM, proposed by Daganzo (1994, 1995), is widely used as a fundamental tool to capture and analyze the realistic traffic phenomena and related problems.",impact-revealing,acknowledging the significance of CTM in traffic analysis
3695,5d245bb6da56295a28fcd54f,25fd9e491c748995e94719f52d896a41299b5b75,geometric scattering for graph data analysis,5c5ce4fd17c44a400fc385f4,Steerable Wavelet Scattering for 3D Atomic Systems with Application to Li-Si Energy Prediction.,"…Angles & Mallat, 2018) processing applications, and their advantages over learned features are especially relevant in applications with relatively low data availability, such as quantum chemistry and materials science (e.g., Hirn et al., 2017; Eickenberg et al., 2017; 2018; Brumwell et al., 2018).###, Bruna & Mallat, 2013b; Sifre & Mallat, 2014; Oyallon & Mallat, 2015; Angles & Mallat, 2018) processing applications, and their advantages over learned features are especially relevant in applications with relatively low data availability, such as quantum chemistry and materials science (e.g., Hirn et al., 2017; Eickenberg et al., 2017; 2018; Brumwell et al., 2018).",other,highlighting the relevance of processing applications in low data availability contexts
1737,,9ddb51d8b13f47afb6bcf971c343d14dbe104fc3,The Therapeutic Covenant: A Psychotheological Pathway to Dynamic Engagement,,,"###followed in his footsteps including Winnicott (1949), Heimann (1950), Balint (1950), Kohut (1971), Fonagy (1991), and Cooper-White (2004) consistently emphasized the importance of###Freud had little time for counter-transference, but those who followed in his footsteps including Winnicott (1949), Heimann (1950), Balint (1950), Kohut (1971), Fonagy (1991), and Cooper-White (2004) consistently emphasized the importance of monitoring this dimension of the therapeutic relationship.",impact-revealing,acknowledging the influence of Freud on subsequent theorists in psychotherapy
4054,5b67b45517c44aac1c860823,7f01c6fe27f57ee6191b51efa18b9199baf7b82a,Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System,5843777aac44360f10840691,Learning to Rank with Labeled Features,"Several works in information retrieval followed this direction, using weak-labeled or unlabeled data to construct test collections [2], to provide extra features [11] and labels [10] for ranking model training.",other,acknowledge existing approaches in information retrieval
2137,,33bd14880313d1e1f3aa452eeb88e3cb3570877d,SCoPE-MS: mass spectrometry of single mammalian cells quantifies proteome heterogeneity during cell differentiation,,,"###We were motivated by the realization that most proteins are present at over 50,000 copies per cell [19, 20] while modern MS instruments have sensitivity to identify and quantify ions present at hundreds of copies [21, 22].###Still, most mammalian cells are smaller (10–15 μm diameter) [19], and we were###Most of the proteins quantified in the single cells tend to be abundant, mostly above the median of the bulk sets, which corresponds to about 50,000 copies per cell [19].###tein) [19] and compares favorably to reliability for bulk datasets [40].###Indeed, the TMT manufacturer recommends 100 μg of protein per channel, almost 10(6) more than the protein content of a typical mammalian cell [19].",impact-revealing,highlighting the motivation behind the research based on protein abundance and mass spectrometry sensitivity
2756,5c04967517c44a2c7470927f,b9015d4f1e591eba7ea21c3566c919f80f7c2afe,attentive long short-term preference modeling for personalized product search,573696ce6e3b12023e5ceb9b,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.","r, the attention mechanism is usually adopted in these systems to model the user’s preference more accurately. Attention mechanisms have shown its efficiency in various tasks such as image captioning [6, 66], visual question answering [65, 67], machine translation [2] and information retrieval [5, 34, 47, 64]. A survey on the attention mechanism on those domain is out of the scope of this paper. Here we ",other,acknowledge the efficiency of attention mechanisms in various tasks
2483,555045d745ce0a409eb59fe4,460d131e081486491af532604351a0691b52b11f,User-level psychological stress detection from social media using deep neural network,53e99a85b7602d97022f7ba5,We feel fine and searching the emotional web.,"effective as ground truth data labels in emotion analysis in [19].###Our solution: Inspired by previous research [19], we have built a stressed-twitter-posting database using the “I feel stressed” sentence pattern as the ground-truth label for detecting stress from micro-blog data.",other,highlighting the inspiration from previous research for building a new database
1417,,798da6871400a119e70a46dba963e0bcdf2986d5,Dealing with the Unknown: Pessimistic Offline Reinforcement Learning,,,"###We implement PessORL on top of CQL [5], with its default hyperparameters.###We compare our algorithm to prior ofﬂine algorithms: two state-of-the-art ofﬂine RL algorithms BEAR [6] and CQL [5]; two baselines adapted directly from online algorithms, actor-critic algorithm TD3 [30] and DDQN [31]; and behavior cloning (BC).###Many prior works [5, 6, 7, 8, 9, 10] try to mitigate this problem by handling OOD actions.###Another way to get a conservative Q-function is to regularize the Q-function in the optimization problem during the learning process [5].###It is inspired by the work in [5], which uses a pes-simistic initialization and an optimistic component on the Q-function.###A representative example is Conservative Q-Learning (CQL) [5].###The other is to regularize the Q-function during the learning process [5].###…to a distribution d π β ( s ) in the dataset, we propose to solve the problem caused by state distributional shift by augmenting the policy evaluation step in CQL [5] with a regularization term scaled by a trade-off factor ε : where d φ ( s ) is a particular state distribution of our choice.###The resulting optimization problem for the policy evaluation step is: where R ( d φ ) is a regularization term inspired by [5] in order to stabilize the training.",impact-revealing,reporting on the implementation and comparison of algorithms in reinforcement learning
1504,,1e0df988af3938cdb1c663496fd1a832c6e4c9e7,Biochar as a soil conditioner for common bean plants,,,"###In this sense, biochars are widely used for the remediation of soils contaminated by heavy metals (He et al., 2019).",impact-revealing,highlighting the application of biochars in soil remediation
2281,5f058d15dfae54570ec57ea1,fb93ca1e004cbdcb93c8ffc57357189fa4eb6770,Resnest: Split-attention networks,5a260c8117c44a4ba8a30771,Squeeze-and-Excitation Networks.,"Figure 1 shows an overall comparison with SE-Net and SK-Net blocks.###SE-Net [29] introduces a channel-attention mechanism by adaptively recalibrating the channel feature responses.###Recent ResNet implementations usually apply the strided convolution at the 3× 3 layer instead of the previous 1× 1 layer to better preserve such information [25, 27].###Comparing our ResNeSt block with SE-Net [27] and SK-Net [35].###First introduced in SE-Net [29], the idea of squeeze-and-attention (called excitation in the original paper) is to employ a global context to predict channel-wise attention factors.###1: Comparing our ResNeSt block with SE-Net [30] and SK-Net [38].###Following [27, 35], a combined representation for each cardinal group can be obtained by fusing via an element-wise summation across multiple splits.###With radix = 1, our Split-Attention block is applying a squeeze-and-attention operation to each cardinal group, while the SE-Net operates on top of the entire block regardless of multiple groups.",other,comparing different model architectures and their mechanisms
2873,5c7572b7f56def97988385ce,c342c71cb23199f112d0bc644fcce56a7306bf94,active learning for convolutional neural networks: a core-set approach,53e9b7e0b7602d970438df86,UPAL: Unbiased Pool Based Active Learning,"It is also possible to perform active learning in a batch setting using the greedy algorithm via importance sampling (Ganti & Gray, 2012).",other,providing context for active learning methods
1681,,941639f9eb8e11eab8c78ca446b032c247a1af0c,Older workers’ representation and age-based stereotype threats in the workplace,,,"###In theoretical terms, a separation view of organizational age diversity has been supported by the social identity approach (Tajfel and Turner, 1979; Turner et al., 1987) and by the similarity-attraction paradigm (Byrne, 1971).###…U
N IV
E R
SI D
A D
E D
O P
O R
T O
A t 0
8: 00
1 9
Ju ly
2 01
7 (P
T )
In theoretical terms, a separation view of organizational age diversity has been supported by the social identity approach (Tajfel and Turner, 1979; Turner et al., 1987) and by the similarity-attraction paradigm (Byrne, 1971).###In this study, the role of age in the workplace builds on this background and is supported by the social identity approach, namely by social identity theory (Tajfel and Turner, 1979), and self-categorization theory (Turner et al.###…this study, the role of age in the workplace builds on this background and is supported by the social identity approach, namely by social identity theory (Tajfel and Turner, 1979), and self-categorization theory (Turner et al., 1987), as well as by the similarity-attraction paradigm (Byrne, 1971).",impact-revealing,supporting theoretical background for organizational age diversity
1138,,889e30be313b8f4e0017916a0463b391a3bedb9a,RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture,,,"###The domain of 3D content creation [18, 34, 35] has significantly improved in recent years.###Our method is motivated by recent advances in 2D content generation, especially the diffusion models [18, 31, 34, 35, 42].",impact-revealing,highlighting the motivation for the method based on advances in 2D content generation
887,5dbab2523a55acea3c05b02b,395de0bd3837fdf4b4b5e5f04835bcc69c279481,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",5bdc31b417c44a1f58a0b8c2,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Bidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classification decisions.###Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.###Masked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.###Figure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al.###BERT (Devlin et al., 2019) introduced masked language modelling, which allows pre-training to learn interactions between left and right context words.###Token Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.###Bidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classiﬁcation decisions.###Figure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).###Self-supervised methods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).",impact-revealing,highlighting the importance of bidirectional encoders in NLP tasks
3392,5736973b6e3b12023e62b0a8,97e3bb4af723f43927317e9b9f2d794a9e398e8e,PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks,5550456245ce0a409eb55cb7,#TagSpace: Semantic Embeddings from Hashtags.,"However, when compared end-to-end with sophisticated deep learning approaches such as the convolutional neural networks (CNNs) [5, 8], the performance of text embeddings usually falls short on specific tasks [30].",other,highlighting the limitations of text embeddings compared to deep learning approaches
1267,,63260c7c185ed861fa2736e423fe0d7a724de213,TripCast: Pre-training of Masked 2D Transformers for Trip Time Series Forecasting,,,"###TS2Vec [35] and CoST [34] learn representations through contrastive learning.###To tokenize the unobserved and missing values, we are inspired by TS2Vec [35] and project the input x h,c to a higher dimension latent vector z h,c and apply token-level mask to the input data.",impact-revealing,describing the method of learning representations through contrastive learning
2494,599c7ea4601a182cd28b81a7,f323036340ad0ec30cafacd6b09a3e28379e24f1,maximizing cache performance under uncertainty,57d063e0ac443673542947ae,Back to the Future: Leveraging Belady's Algorithm for Improved Cache Replacement.,"Likewise, SDBP [21], PRP [11], and Hawkeye [16] learn the behavior of different PCs.###And Hawkeye [16] emulates MIN’s past decisions.###Instead, replacement policies for processor caches are designed empirically, using heuristics based on observations of common-case access patterns [11,14,16,17,19,20,21,30,35,39].###Several recent policies break accesses into many classes, often using the requesting PC, and then adapt their policy to each class [11, 16, 21, 39].",other,acknowledge variations in existing cache replacement policies
1127,,f2ab8f7118e85aecb87eab45c9aac725dc1ca2f4,Wavelet-based Fourier Information Interaction with Frequency Diffusion Adjustment for Underwater Image Restoration,,,"###Recently, diffusion-based methods [10, 35] have garnered signiﬁcant attention due to their outstanding performance in image synthesis [23, 24, 32, 34, 52] and restoration tasks [5, 40, 46, 51].###In LDFB and HDFB, we follow the setup of [35], they can be expressed as: where (cid:15) θ ( x t , x c , t ) is the estimated value with a Unet.###Recently, Diffusion Probabilistic Models (DPMs) [10, 35] have been widely adopted for conditional image generation [5, 40, 42, 46, 50].",impact-revealing,highlighting the growing interest and performance of diffusion-based methods in image synthesis and restoration
1784,,d3b74864006a2365878a950b1a8fc255d32331ce,IVESA - Visual Analysis of Time-Stamped Event Sequences.,,,"###The incorporated metadata filtering technique was inspired by faceted search and browsing techniques [11, 93,112], whereas our clustering-based filtering approach is inspired by exploratory search approaches [18].",impact-revealing,highlighting the inspiration from existing filtering techniques
3866,5e09a83ddf1a9c0c41685fc3,90820988957e74db0fbb1df79175610e08016ba4,Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling,57a4e91dac44365e35c9851e,A Decomposable Attention Model for Natural Language Inference.,"…word context and sentence order for better sentence representations; (2) interaction and attention mechanisms (Tay et al., 2019b; Seo et al., 2017; Parikh et al., 2016; Conneau et al., 2017; Gong et al., 2018) to emphasize salient word pair interactions; (3) structure modeling (Chen et al.,…###Existing state-of-the-art techniques for SM usually comprise three major components: (1) sequential sentence encoders that incorporate word context and sentence order for better sentence representations; (2) interaction and attention mechanisms (Tay et al., 2019b; Seo et al., 2017; Parikh et al., 2016; Conneau et al., 2017; Gong et al., 2018) to emphasize salient word pair interactions; (3) structure modeling (Chen et al.###For the answer selection, paraphrase identiﬁcation, and STS tasks, we compared against the following baselines: InferSent (Conneau et al., 2017), ESIM (Chen et al., 2017b), DecAtt (Parikh et al., 2016), and PWIM (He and Lin, 2016).###, 2017b), DecAtt (Parikh et al., 2016), and PWIM (He and Lin, 2016).",other,describing components of state-of-the-art techniques for sentence modeling
2581,5db9298647c8f766461f8ecd,ce177672b00ddf46e4906157a7e997ca9338b8b9,Attention is not not Explanation,5c04967517c44a2c74708aea,Please Stop Explaining Black Box Models for High Stakes Decisions.,"Rudin (2018) defines explainability as simply a plausible (but not necessarily faithful) reconstruc-###Distinguishing between interpretability and explainability as two separate notions, Rudin (2018) argues that interpretability is more desirable but more difficult to achieve than explainability, because it requires presenting humans with a bigpicture understanding of the correlative relationship…###This aligns more closely with the more rigorous (Lipton, 2016, §3.1.1) definition of transparency, or Rudin (2018)’s definition of interpretability: human understanding of the model as a whole rather than of its respective parts.###Rudin (2018) defines explainability as simply a plausible (but not necessarily faithful) reconstruction of the decision-making process, and Riedl (2019) classifies explainable rationales as valuable in that they mimic what we as humans do when we rationalize past actions: we invent a story that…",other,discussing definitions and distinctions in explainability and interpretability
19,5d9b0cb93a55acb0384964ef,bcfdbb6b8911272139170ef4b24e31d9145093e7,Deep Anomaly Detection on Attributed Networks.,58437722ac44360f1082efeb,Semi-Supervised Classification with Graph Convolutional Networks.,"To further enable the detection of anomalous nodes, we introduce a deep autoencoder framework to reconstruct the original attributed network with the learned node embeddings from GCN.###Even though GCN emerges to be a principled tool to model attributed networks and achieves the state-of-the-art performance in the semi-supervised node classi-ﬁcation task, it remains unclear how its power can be shifted to the anomaly detection problem.###To this end, we propose a new type of attributed network encoder inspired by the graph convolutional network (GCN) model [16].###Mathematically, GCN extends the operation of convolution to networked data in the spectral domain and learns a layer-wise new latent representation by a spectral convolution function: where H ( l ) is the input for the convolution layer l , and H ( l +1) is the output after the convolution layer.###For a particular layer, the convolution operation is D̃− 1 2 ÃD̃− 1 2 XW, and its complexity is O(mdh) [16] as ÃX can be efficiently implemented using sparse-dense matrix multiplications, where m is the number of non-zero elements in matrix A and d is the number feature dimensions on the attributed network, and h is the number of feature maps of the weight matrix.###GCN, which takes the topological structure and nodal attributes as input, is able to learn discriminative node embeddings by stacking multiple layers of linear units and non-linear activation functions.###Speciﬁcally, we address the limitations of existing methods and model the attributed networks with graph convolutional network (GCN).###To this end, we pro-pose a new type of attributed network encoder inspired by the graph convolutional network (GCN) model [16].###To address the challenges above, we propose to model the attributed networks with graph convolutional network (GCN) [16].###In particular, GCN [16] takes the structure and attribute information as input, and extends the operation of convolution on network data in the spectral domain for embedding representation learning.###Speciﬁcally, GCN considers the high-order node proximity when learning the embedding representations, thus it mitigates the network sparsity issue beyond the observed links among nodes.###As GCN handles the high-order node interactions with multiple layers of nonlinear transformations, it alleviates the network sparsity issue and can capture the nonlinearity of data as well as the complex interactions between two sources of information on attributed networks.###As can be observed, the fundamental building block of Dominant is the deep autoencoder [11] and it consists of three essential components: (i) attributed network encoder - which models network structure and nodal attributes seamlessly in a joint framework for node embedding representation learning with GCN; (ii) structure reconstruction###Meanwhile, recent research advances on graph convolutional network (GCN) [16, 7, 12] demonstrate superior learning performance by considering neighbors of nodes that are multiple hops away.",impact-revealing,highlighting the proposed method's advantages and addressing limitations of existing methods
1783,,07703fb1ade6a7234ce454b97b2736af539893e7,Antimicrobial efflux pumps and Mycobacterium tuberculosis drug tolerance: evolutionary considerations.,,,"###This surprising result was explained by the finding that in Mtb, macrophage-induced tolerance to rifampicin is mediated by a bacterial efflux pump, Rv1258c, that also promotes intracellular bacterial growth in the absence of antimicrobials (Table 1) (Adams et al. 2011).###Regulation of these pumps may also be conserved, as seen with Rv1258c.###It is induced in response to sub-inhibitory concentrations of antimicrobials and mediates Rv1258c transcription in these settings.###Other candidates include piperine, a derivative of black pepper that has been proposed to inhibit Mtb efflux pumps including Rv1258c (Sharma et al. 2010; Srinivasan 2007); agents developed to counter Gram-negative efflux pumps such as the Phe-Arg-bnaphthylamine derivatives (Pages and Amaral 2009); and P-glycoprotein inhibitors originally studied in cancer such as tariquidar (Leitner et al. 2011).###Rv1258c is transcriptionally induced following macrophage residence (Table 1) (Schnappinger et al. 2003) and appears to function as a virulence factor induced in the intracellular environment that pathogenic mycobacteria encounter (Chan et al. 2002; Clay et al. 2007; Dannenberg 1993; Ramakrishnan et al. 2000).###Its association with rifampicin tolerance appears to be an epiphenomenon, and the identity of the ‘‘natural’’ substrate(s) of Rv1258c remains unknown.###Our identification of Rv1258c as a mediator of intracellular growth led us to investigate if mycobacterial efflux pumps are widely used for this critical virulence trait.###…candidates include piperine, a derivative of black pepper that has been proposed to inhibit Mtb efflux pumps including Rv1258c (Sharma et al. 2010; Srinivasan 2007); agents developed to counter Gram-negative efflux pumps such as the Phe-Arg-bnaphthylamine derivatives (Pages and Amaral 2009); and…###Rv1258c, a secondary transporter belonging to the major facilitator superfamily (MFS) of efflux pumps, is structurally related to MefA, a 12-membrane spanning MFS pump involved in macrolide resistance in Streptococcus pneumoniae (Ainsa et al. 1998; De Rossi et al. 2002; Li and Nikaido 2009; Saier et al. 2009).###This model has precedence: the macrophage-derived AMP LL-37 induces transcription of mefE, which encodes one component of a Streptococcus pneumoniae efflux pump related to Rv1258c (Zahner et al. 2010).###Furthermore, WhiB7 is itself induced by macrophage residence (Larsson et al. 2012; Rohde et al. 2012), and would also be predicted to be required for Rv1258c transcriptional induction and bacterial survival in this context.###Indeed, Rv1258c has homologs in these species and rifampicin plays an important part in their treatment (Tables 1 and 3).###Other candidates include piperine, a derivative of black pepper that has been proposed to inhibit Mtb efflux pumps including Rv1258c (Sharma et al. 2010; Srinivasan 2007); agents developed to counter Gram-negative efflux pumps such as the Phe-Arg-bnaphthylamine derivatives (Pages and Amaral 2009); and P-glycoprotein inhibitors originally studied in cancer such as tariquidar (Leitner et al.###While Rv1258c mediates tolerance to the hydrophobic antibiotic rifampicin but not the hydrophilic isoniazid, Mtr similarly mediates resistance to rifampicin and the hydrophobic erythromycin, but not the hydrophilic antibiotic streptomycin.###Similar to the Rv1258c pump, Mtr also confers antibiotic resistance (Hagman et al. 1995).",impact-revealing,highlighting the role of Rv1258c in antibiotic tolerance and its potential as a virulence factor
1623,,eb2963b975277087b956ce83c264f43baa1e5d15,Can automated program repair refine fault localization? a unified debugging approach,,,"###Therefore, besides our default Ochiai [5] Figure 4: Patch categorization tree###, Ochiai [5]) at the statement level, and then performs suspiciousness aggregation [67] to calculate the initial suspiciousness value for each program element.###spectrum-based fault localization (SBFL) techniques [5, 14, 30] compute the code elements covered by more failed tests or less passed tests as more suspicious, and pioneering mutation-based fault localization (MBFL) techniques [51, 55, 81] inject code changes (e.###tion? To address them, researchers have proposed two categories of techniques, fault localization [5, 14, 30, 42, 51, 80, 81] and program repair [32, 36, 38, 44, 45, 60, 61, 72].###, Tarantula [30], Ochiai [5], and Ample [14]) or learning techniques [9, 62–64] to the execution traces of both passed and failed tests to identify the most suspicious code elements (e.###Note that, SBFL and Metallaxis can adopt dierent SBFL formulae, and we by default uniformly use Ochiai [5] since it has been demonstrated to perform the best for both SBFL and MBFL [40, 57, 82].###We attempted to improve the eectiveness of traditional SBFL based on Ochiai formula [5], which has been widely recognized as one of the most eective SBFL formulae [40, 57, 82].###, the Ochiai [5] SBFL technique is leveraged in many recent program repair techniques [20, 29, 73].###, the default Ochiai [5]) to compute the suspiciousness for each statement, e.",impact-revealing,describing the process of suspiciousness aggregation in fault localization
2491,5f75aa6a9fced0a24b64599c,10f61ec5c6c822325a91bd7d718a81b93e9628ca,Opportunistic Early Pipeline Re-steering for Data-dependent Branches,5c7958b34895d9cbc63d780d,A case for (partially) TAgged GEometric history length branch prediction,"TAgged GEometric history length predictor (TAGE) [35] is one of the most successful branch predictor proposals.###The TAGE branch predictor, proposed in [35], has proven itself to be very efficient and is part of Championship Branch Prediction(CBP) winning predictor [33] in both 2014(CBP-4) and 2016(CBP-5).###Conditional branch predictors have evolved over time from simple tag-less bimodal predictors to long branch history based tagged prediction by partial matching (PPM) predictors [26, 35].",other,highlighting the success and efficiency of the TAGE branch predictor in competitive settings
1563,,111b18a759aa6b394e8caec969dccf394c9e89cf,Study-Abroad Experiences of Two South Korean Undergraduate Students in an English-Speaking and a Non-English-Speaking Country,,,"###Many studies emphasized the importance of student interactions with native speakers (NSs) in L2 learning (Dewey et al. 2013; Magnan and Back 2007; Segalowitz and Freed 2004), and the study-abroad (SA) context is known to offer ‘‘greater access to NSs and more varied opportunities to use the target…",impact-revealing,highlighting the significance of student interactions with native speakers in L2 learning
1163,,e6d98028bb1a590557ce55bc7e720054fee51afd,SERDP: Signature‐based and energy‐efficient relay discovery protocol for Internet of Things in cellular networks,,,"###Birthday protocol, 23 developed by using the Birthday Paradox, is inspired by the possibility that the birthdays of randomly selected people within a group are the same day.",impact-revealing,providing context about the Birthday protocol
1263,,26a3f9072e995a9dcc39b0bcb2c6ce080d99b9ef,Deep Recurrent Neural Network with Multi-scale Bi-directional Propagation for Video Deblurring,,,"###Due to the domain gap between synthetic datasets and real-world ones (Brooks and Barron 2019), the methods trained on the synthetic datasets do not generalize well on the real-world scenes as demonstrated by (Lai et al. 2016; Köhler et al. 2012; Rim et al. 2020; Zhong et al. 2020).###To better evaluate deblurring algorithms, some real-world blurry datasets (Lai et al. 2016; Köhler et al. 2012; Rim et al. 2020; Zhong et al. 2020) have been proposed, but all these works have their own drawbacks which limit their applications.###To better evaluate deblurring algorithms, some real-world blurry datasets (Lai et al. 2016; Köhler et al. 2012; Rim et al. 2020; Zhong et al. 2020) have been proposed, but all these works have their own drawbacks which limit their applications.###In (Lai et al. 2016), Lai collects 100 real blurry images without paired sharp images, thus, their dataset is not suitable for quantitative evaluation.###Due to the domain gap between synthetic datasets and real-world ones (Brooks and Barron 2019), the methods trained on the synthetic datasets do not generalize well on the real-world scenes as demonstrated by (Lai et al. 2016; Köhler et al. 2012; Rim et al. 2020; Zhong et al. 2020).",impact-revealing,highlighting the limitations of existing datasets for evaluating deblurring algorithms
3773,57aa28de0a3ac518da9896d5,36ee2c8bd605afd48035d15fdc6b8c8842363376,node2vec: scalable feature learning for networks,53e9a6bbb7602d9702fef1a7,Leveraging Label-Independent Features for Classification in Sparsely Labeled Networks: An Empirical Study.,"In networks, the conventional paradigm for generating features for nodes is based on feature extraction techniques which typically involve some seed hand-crafted features based on network properties [8, 11].",other,providing context on conventional feature generation methods in networks
1548,,1cd52fe60cc753a50be0fbd8e12f858d1f6c45f9,Inflammaging and Oxidative Stress in Human Diseases: From Molecular Mechanisms to Novel Treatments,,,"###NRF2 is instrumental in regulating expression of GSH, GPX, as well as multiple glutathione S-transferases [133,134].###NADPH is involved in the regeneration of reduced GSH from GSSG, thereby maintaining cellular antioxidant levels [40].###• Oxidative stress [16]
• Aβ deposition in astrocytes and microglia→ triggers inflammatory response→ excessive ROS formation→ activates JNK/p38 MAPK pathways→ leads to Aβ accumulation and Tau hyper-phosphorylation [12,16,63] • Aβ accumulation→ depletes Ca2+ in ER, damage mitochondrial and plasma membrane→ further ROS formation→ induces neuron cell death [62] • Increased ROS or decreased SIRT1/melatonin→ activates NF-κB pathway→ induces the expression of pro-inflammatory genes [12] • Phytochemicals and polyphenol-containing compounds [12] • IFNβ1a [64] • LW-AFC [65]
PD
• Loss of dopaminergic neurons in the SNpc area of the brain [66,67] • Intracellular deposition of misfolded α-synuclein [66,67]
• Aging, environmental toxicities, oxidative stress→ buildup of NM→ neuron cell death • Complex I interruption and mitochondrial dysfunction → increased ROS accumulation→ leads to oxidative damage on lipids, proteins, and DNA→ neuron cell death [62,68] • Pro-inflammatory cytokines such as IFN-γ, TNF-α or TLR activation→ activates microglia→ induce the release of ROS, nitrogen species, MMP, and pro-inflammatory cytokines [69,70] • Inflammaging→ increases inflammatory cytokines→ favor BBB permeabilization→ infiltration of lymphocytes and macrophages to CNS [69–71] • Lipoic acid and GSH [58] • Melatonin [72,73]
Int.###Disease Causative Factors Roles of Inflammaging and ROS Potential Treatments
COPD • Cigarette smoke [74] • Toxic gases [74]
• Oxidative stress→ inactivates anti-proteases→ causes protease/anti-protease imbalance→ emphysema and elastin degradation [75] • ROS→ increase pro-inflammatory mediators such as IL-1 and TNF-α→ inflammation [56,76] • Oxidative stress→ reduces SIRT1 activity→ acetylation of p53, NF-κB, and FOXO→ results in
inflammation [74,77] • Activation of p53→ inhibits PGC-1α and PGC-1β transcription→ diminished mitochondrial
function→ contributes to COPD pathogenesis and may contribute to carcinogenesis in COPD [78–81]
• SIRT1 activators and polyphenols [82] • Zinc, vitamin E, vitamins C, D,
and carotenoids [16,83] • Pharmacological and plant elements
such as theophylline, sulforaphane, nortriptyline, baicalin, quercetin, erythromycin, and curcumin [84]
Diabetes
• Vascular aging • Genetic insulin
resistance [85] • Obesity, lack of physical
exercise, pregnancy, hormone excess [85] • Oxidative stress [85]
• Increased glucose levels, especially glucose fluctuation→ leads to increased mitochondrial ROS formation and glycation of proteins→ oxidative stress→ decreased enzyme activity [85] • Advanced diabetes→ decreased antioxidants such as vitamin E, α-lipoic acid, and SOD→ oxidative stress [85] • ROS→ cause oxidative stress in β cell→ decreased insulin secretion [86] • Elevated O2•− formation→ results in increased polyol activity, increased hexosamine pathway
flux, and activation of PKC isoform→ lead to β cell dysfunction [86] • Inflammaging→ increased TNF-α and IL-6→ activate multiple Ser/Thr kinase→ catalyzes serine
phosphorylation of IRS1→ disrupts the capability of IRS1 to mobilize phosphatidylinositol-3-kinase and Akt→ disturbs insulin processing mechanism [87]
• Increased IL-1β, IL-6, and IL-8 in pancreatic islets→ result in down-regulation of insulin gene expression and increase of macrophages in pancreas→ leads to β-cell apoptosis [87]
• SIRT1 activators [88] • p38-MAPK inhibitors [88] • Limited caloric intake [88] • Antioxidants like SOD, catalase,
and GPX [89] • Fecal microbiota transplantation [39]
RA
• Genotype • Certain triggers from the
environment such as cigarette smoke and infectious agents [90]
• REL allele→ leukocyte activation through NF-κB pathway→ increase autoimmune response/inflammation [90,91] • Maladaptive T cells→ secrete pro-inflammatory cytokines such as TNF-α and IFN-γ→ promote inflammation and leads to autoimmune disorder [92–94] • Neutrophils, macrophages, and lymphocytes in RA→ increased ROS formation→ causes abnormalities in T cell signaling and proliferation [95] • O2•− reacts with NO→ form ONOO− → decreased GSH levels→ activate NF-κB signaling pathways→ increased inflammatory mediators such as cyclo-oxygenase 2, cytosolic phospholipase A2, IL-1β, iNOS, and TNF-α→ promote inflammation and leads to autoimmune disorder [96] • Adipose tissue→ secrets leptin→ causes angiogenesis→ induces ROS expression and aids in inflammation [97] • Fenofibrate [98] • Canakinumab [99]
AD, Alzheimer’s disease; Akt, protein kinase B; Aβ, amyloid beta plaques; BBB, blood brain barrier; BDB, 3-bromo-4, 5-dihydroxybenzaldehyde; CCL, CC chemokine ligand; CNS, central nervous system; COPD, chronic obstructive pulmonary disease; CPCs, circulating progenitor cells; CVD, cardiovascular disease; EGFR, epidermal growth factor receptor; ER, endoplasmic reticulum; ERK, extracellular regulated protein kinases; G-MDSCs, granylocytic-myeloid derived suppressor cells; GSH, glutathione; IFN, interferon; IP-10, IFN-γ-induced protein 10; IL, interleukin; iNOS, inducible nitric oxide synthase; IRS1, insulin receptor substrate 1; KCNB1, Voltage-gated potassium (K+) channel sub-family B member 1; LTL, leukocyte telomere length; LW-AFC, Active fraction combination from Liuwei Dihuang decoction; MMP, matrix metalloprotease; MPO, myeloperoxidase; LDL, low-density lipoprotein; MS, multiple sclerosis; NF-κB, Nuclear factor kappa-light-chain-enhancer of activated B; NFT, neurofibrillary tangles; NLRP3, pryin domain containing-3 protein; NM, neuromelanin; NRF-2, nuclear factor erythroid 2-related factor 2; PD, Parkinson’s disease; PGC, peroxisome proliferator-activated receptor-γ coactivator; PKC, protein kinase C; RA, rheumatoid arthritis; ROS, reactive oxygen species; SIRT1, Sirtuin-1; snPC, substantia nigra pars compacta; SOD, superoxide dismutase; Th1, T helper cell 1; TLR, toll-like receptor; TNF-α, tumor necrosis factor-α; VSMC, vascular smooth-muscle cells; XO, xanthine oxidase.###PD • Loss of dopaminergic neurons in the SNpc area of the brain [66,67] • Intracellular deposition of misfolded α-synuclein [66,67] • Aging, environmental toxicities, oxidative stress→ buildup of NM→ neuron cell death • Complex I interruption and mitochondrial dysfunction → increased ROS accumulation→ leads to oxidative damage on lipids, proteins, and DNA→ neuron cell death [62,68] • Pro-inflammatory cytokines such as IFN-γ, TNF-α or TLR activation→ activates microglia→ induce the release of ROS, nitrogen species, MMP, and pro-inflammatory cytokines [69,70] • Inflammaging→ increases inflammatory cytokines→ favor BBB permeabilization→ infiltration of lymphocytes and macrophages to CNS [69–71] • Lipoic acid and GSH [58] • Melatonin [72,73]###Sulforaphane, present in cruciferous vegetables such as broccoli sprouts, possesses anticarcinogenic and antioxidant functions [58,59].###Cancer Oxidative stress [40] • ROS→ activate p38 MAPK→ initiates/blocks tumor development based on degree of activation and cancer type [41,42] • ROS→ activate NRF2→ anabolic purine synthesis pathways→ tumor cell proliferation [43] • Oxidative stress→ activates p53 pathway→ induces tumor cell apoptosis [44–46] • Inflammaging→ increased levels of pro-inflammatory cytokines including IL-1, IL-6, and TNF-α → stimulate ROS formation→ DNA mutagenesis→ tumorigenesis [46–50] • Mitochondrial ROS→ upregulate IL-1, IL-6, and TNF-α→ activates NF-κB signaling→ increased oncogenic K-Ras levels→ cancer progression [51–56] • G-MDSCs→ produce ROS→ suppress CD8+ T cells→ promote tumor growth and progression [57] • Sulforaphane, present in cruciferous vegetables such as broccoli sprouts [58,59] • Mediterranean diet [47] • Polyphenols [36]",impact-revealing,highlighting the role of oxidative stress and inflammation in various diseases
3603,5c2c7a9217c44a4e7cf317b4,61946177ea5d44ad18ff09de7929556d7b34cd8b,simd-x: programming and processing of graph algorithms on gpus.,53e99c83b7602d97025327fe,Managing Large Graphs on Multi-Cores with Graph Awareness.,"Recent advance in graph computing falls in algorithm innovation [39, 72], framework developments [37, 14, 33, 28, 30, 75, 77, 18, 53, 51, 49, 19, 42, 48, 61, 6, 66, 68, 52, 73, 62, 43, 74, 70, 69, 3, 64, 40, 17, 8] and accelerator optimizations [63, 29, 38, 25, 71, 47].",other,acknowledge advancements in graph computing
2678,5dde4b463a55ac4c42972afc,43a8b2fd651c3783723f4265d7641f9601a5a6f4,One Man’s Trash Is Another Man’s Treasure: Resisting Adversarial Examples by Adversarial Examples,5b1642388fbcbf6e5a9b57aa,Learning to Attack: Adversarial Transformation Networks.,"This conclusion also echos the prior studies [4, 48], which show that learning-based adversarial attacks usually perform worse than gradient-based attacks.",other,highlighting consistency with prior studies on adversarial attacks
3831,58437725ac44360f1082f7f7,79da740db9006b2aa3e7b571d038ec895e323121,Accelerating the Super-Resolution Convolutional Neural Network,565711440cf20caa7d695f28,Accelerating Very Deep Convolutional Networks for Classification and Detection.,"This is the main difference between our method and other CNN acceleration works [20,21].###[21] make attempts to accelerate very deep CNNs for image classfication.",other,highlighting the distinction between methods in CNN acceleration
1542,,d9528300d690bb1e1e4cf1c39c9bd7bace98fb90,SEROTONIN IN EMOTIONAL MEMORY PROCESSES: NEUROPHARMACOLOGICAL STUDIES WITH EMPHASIS ON DEPRESSION,,,"###Supports for a role of the medial temporal lobe in explicit, but not implicit, memory functions were based on observations after pathological or surgical damages in the nervous system, but have in later years been confirmed and extended by imaging studies in humans (Scoville and Milner, 1957; Squire et al., 1992; Grasby et al., 1993; Kapur, 1995; Maguire, 1995; Ghaem, 1997; Eichenbaum, 2001; Squire, 2004; Naya and Suzuki, 2011).###The hippocampus plays a critical role in these processes (Nyberg, 1996; Schacter et al., 1995; Dolan and Fletcher, 1997; Holland and Bouton, 1999; LeDoux, 2000; Eichenbaum, 2001; McGaugh et al., 2004; Paul et al., 2005; Tse et al., 2011).",impact-revealing,highlighting the evolution and confirmation of the role of the medial temporal lobe in memory functions
3661,5a9cb66717c44a376ffb87ea,fe9b8aac9fa3bfd9724db5a881a578e471e612d7,efficient neural architecture search via parameter sharing,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"The decision of what previous nodes to connect to allows the model to form skip connections (He et al., 2016a; Zoph & Le, 2017).###Each convolution in our model is applied in the order of relu-conv-batchnorm (Ioffe & Szegedy, 2015; He et al., 2016b).",other,providing context on model architecture and connections
3088,5b67b46b17c44aac1c861fc4,f6d06993e003fa6fec5bf630efded9e4fd90a030,fluency boost learning and inference for neural grammatical error correction,58d82fd2d649053542fd77e5,JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction.,"Moreover, we evaluate our approach on JFLEG corpus (Napoles et al., 2017).",other,reporting evaluation on a specific corpus
507,5ecbc7639fced0a24b502b60,326974bd97dea3278515bcb3551a59e304d6d755,Improving Low-Resource Speech Recognition Based on Improved NN-HMM Structures,5736960d6e3b12023e52030a,Adding Gradient Noise Improves Learning for Very Deep Networks.,"In [40], the authors consider a simple technique of adding time-dependent Gaussian noise to the gradient at every training step.###Inspired by [40], we investigated a new label encoding method named ‘‘Soft One-hot Label (SOL)’’.",impact-revealing,drawing inspiration from a prior technique for a new method
1651,,b4db0f724e6f8c117dc1fc4a52e87a6326d50218,Advancing a social identity perspective on interoperability in the emergency services: Evidence from the Pandemic Multi-Agency Response Teams during the UK COVID-19 response,,,"###The approach comprises two inter-related theories: social identity theory [14] and self-categorization theory [15,16].###Both theories build upon a foundational insight that people define themselves not only in terms of their personal identity as individuals [17], but also in terms of their social identity as members of groups [14].",impact-revealing,providing context for the theoretical framework
1749,,d11f50432493aebbab682e6657c291b3b768ef26,Prognostic significance of tumor-infiltrating lymphocytes and macrophages in nasopharyngeal carcinoma: a systematic review and meta-analysis,,,"###NPC Nasopharyngeal carcinoma TILs Tumor-infiltrating lymphocytes TME Tumor microenvironment TIMs Tumor-infiltrating macrophages AJCC American Joint Committee on Cancer CI Confidence interval
DFS Disease-free survival HR Hazard ratio OS Overall survival PFS Progression-free survival HNSCC Head and neck squamous cell carcinoma###NPC is a type of lymphoepithelioma characterized by substantial infiltration of leukocytes into the TME [32].###CD4+ T cells play
1 3
crucial roles in orchestrating the anti-tumor response via secretion of cytokines that affect immune, tumor, and stromal cells within the TME.###Since CD8+ T cells are the major cytotoxic antigen-specific cells, their
recruitment to the TME may represent a more robust prognostic marker than the number of total CD+ TILs [40].###Given that immune checkpoint blockade is more likely to be successful in cancers with high levels of TILs in the TME [34], our study provides some support for the use of such immunotherapy in NPC.###Tumor-infiltrating lymphocytes (TILs), which include CD4+ , CD8+ , and FOXP3+ T cells, B cells, and CD56+ natural killer (NK) cells and CD163+ and CD68+ tumor-infiltrating macrophages (TIMs), represent the most abundant immune cells within the TME and play critical roles as regulators of tumor proliferation, metastasis, and angiogenesis [7].###In turn, TIMs can recruit Tregs to the TME, thereby promoting cancer progression [48].###Recent advances in tumor immunotherapy with agents such as monoclonal antibodies against the T-cell receptor PD-1 and its ligand PD-L1 have clearly demonstrated the importance of immune system in the tumor microenvironment (TME) [6].",impact-revealing,highlighting the significance of tumor-infiltrating lymphocytes and their role in immunotherapy for nasopharyngeal carcinoma
2238,5d0616bd8607575390f86730,635b079447ee46acc813a08305988e77dfe3d1dd,Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi,5bdc315017c44a1f58a0565c,Towards Environment Independent Device Free Human Activity Recognition.,"For example, recent works [20, 50] borrow the ideas from machine learning, such as transfer learning and adversarial learning, and apply advanced learning methodologies to improve cross-domain recognition performance.###0 against several alternative state-of-the-arts methodologies, CARM[44], EI[20] and CrossSense[50], where the latter two are feasible for cross-domain recognition.###EI [20] incorporates an adversarial network to obtain domain-independent features from CSI.###As an example, we evaluate the performance of an adversarial learning based model, EI [20] over different domain factors (e.g., environment, location and orientation of the person).###Tempts to adapt recognition schemes in various domains fall into two categories: virtually generating features for target domains [39, 40, 50, 53] and developing domain-independent features [9, 20, 37].###Cross-domain learning methods such as transfer learning [50] and adversarial learning [20] latently generate features of data samples in the target domain, either by translating samples from the source domain, or learning domain-independent features.",other,highlighting the application of machine learning ideas to improve cross-domain recognition performance
2059,,c7dbe95e461199659f60881fd1ffc7f56a96d82a,Further validation of the Treatment Self-Regulation Questionnaire for assessing motivations for responsible drinking: A test of self-determination theory.,,,"###Further, a harm reduction approach (e.g., Marlatt & Witkiewitz, 2002) has been widely adopted for addressing the public health burden of alcohol misuse among college students, which focuses on reducing the harmful consequences of alcohol misuse as opposed to abstaining from alcohol use.",impact-revealing,highlighting the significance of harm reduction approaches in public health
3146,5550446645ce0a409eb4d54a,06e122f475a21d92dba137609c40f35690217475,Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification,53e9b0abb7602d9703b188ae,Dependency tree-based sentiment classification using CRFs with hidden variables,"The features used in traditional learning-based methods (Pang et al., 2002; Nakagawa et al., 2010) are independent to the targets, hence the results are computed despite what the targets are.",other,highlighting limitations in traditional learning-based methods
704,5d3c234c3a55acd386d4e112,a9ec03dbe702f6909acd1f1f14a3395d0141043b,generative models for graph-based protein design,599c7987601a182cd2648373,Attention Is All You Need.,"Optimization We trained models using the learning rate schedule and initialization of the original Transformer paper [7], a dropout rate of 10% [42], a label smoothing rate of 10%, and early stopping based on validation perplexity.###We introduce our Structured Transformer model in Section 2.2.###This concatenation and masking structure ensures that sequence information only ﬂows to position i from positions j   i , but still allows position i to attend to subsequent structural information unlike the standard Transformer decoder.###In this work, we introduce a Structured Transformer model that draws inspiration from the selfattention based Transformer model [7] and is augmented for scalable incorporation of relational information (Figure 1).###3 Relative positional encodings Taking a cue from the original Transformer model, we obtain positional embeddings e ( p ) ij that encode the role of local structure around node i .###Self-Attention Our model extends the Transformer [33] to capture sparse, pairwise relational information between sequence elements.###Each layer of the encoder implements a multi-head self-attention component, where head ` ∈ [L] can attend to a separate subspace of the embeddings via learned query, key and value transformations [7].###Thus, our model generalizes Transformer to spatially structured settings.###Our model augments the traditional sequence-level self-attention of Transformers [7] with relational 3D structural encodings and is able to leverage the spatial locality of dependencies in molecular structures for efﬁcient computation.###We update the embeddings with this residual, and alternate between these self-attention layers and position-wise feedforward layers as in the original Transformer [7].###These relative distances contrast the absolute positional encodings of the original Transformer, and instead matches the relative encodings in [34].###In this work, we introduce a Structured Transformer model that draws inspiration from the self-attention based Transformer model [7] and is augmented for scalable incorporation of relational information (Figure 1).###In our architecture, an encoder develops a sequence-independent representation of 3D structure via multi-head self-attention [7] on the spatial k-nearest neighbors graph.###Optimization We trained models using the learning rate schedule and initialization of the original Transformer paper [7], a dropout rate of 10% [42], a label smoothing rate of 10% , and early stopping based on validation perplexity.###Our model augments the traditional sequence-level selfattention of Transformers [7] with graph-based, 3D structural encodings and is able to leverage the spatial locality of dependencies in molecular structures for efficient computation.###We found that our Structured Transformer model signiﬁcantly improved upon the perplexities of SPIN2 (Table 2).###In particular, our Structured Transformer model attained a perplexity of ∼ 7 on the full test set.###Specifically, we augment the autoregressive self-attention of recent sequence models [7] with graph-based representations of 3D molecular structure.",impact-revealing,describing the development and features of a new Structured Transformer model
2682,55a49d7565ceb7cb02d42720,d897b2aafee70e1b52a5f064f8d8c9655a9282a0,data-driven prediction of drug effects and interactions,55a4805265ce31bc877d04a5,Predicting adverse drug events using pharmacological network models.,"More recently, network and chemical properties have been combined together into predictive models of drug effects ( 19 ); these approaches all rely on a comprehensive database of known drug effects.",other,highlighting the integration of network and chemical properties in drug effect prediction
789,5bdc315817c44a1f58a05e88,9ea992f009492888c482d5f4006281eaa8b758e7,"X2Face: A network for controlling face generation by using images, audio, and pose codes",5a260c8417c44a4ba8a3160d,ExprGAN: facial expression editing with controllable expression intensity,") by conditioning the generated image on known ground truth information which may be head pose, expression, or landmarks [5, 12, 21, 30, 42, 44].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
4041,5c0495ae17c44a2c747019af,8e37a3b227b68953f8067215828dc8b8714cb21b,Boosting Adversarial Attacks with Momentum,53e99f09b7602d97027d3863,Ensemble selection from libraries of models,"Ensemble methods have been broadly adopted in researches and competitions for enhancing the performance and improving the robustness [6, 8, 2].",other,acknowledge the use of ensemble methods in research
3823,5efdaf7b91e01191d3d28242,6ff1eb9cdf64a464bf43b54d852456e9ddf55b28,Debiased Contrastive Learning,53e9b009b7602d9703a647cb,Learning classifiers from only positive and unlabeled data.,"Common applications of PU learning are retrieval or outlier detection [13, 12, 11].",other,reporting applications of PU learning
2908,5aed14d617c44a4438159123,921196c32213a229245a9705ee4768bc941e7a26,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,57a4e921ac44365e35c990e6,Layer Normalization,"…techniques for recurrent networks have been introduced and continue to be actively explored (El Hihi & Bengio, 1995; Schuster & Paliwal, 1997; Gers et al., 2002; Koutnik et al., 2014; Le et al., 2015; Ba et al., 2016; Wu et al., 2016; Krueger et al., 2017; Merity et al., 2017; Campos et al., 2018).",other,acknowledge ongoing research in recurrent networks
1639,,225bf68698720d3ecb1a103ee5833355ff5c0b2f,Everolimus-eluting stents in interventional cardiology,,,"###Indeed, stent implantation not only reduced the rate of acute vessel closure, but also reduced restenosis by approximately 30% compared with PTCA.2,8,9 Despite these improvements restenosis continued to occur at a rate of 20%–30%.2,8,10 This restenosis, resulting from vessel injury and a predictable, insidious proliferation of smooth muscle cells, typically occurs within 5–6 months after stent placement.11###While modern medical therapy is appropriate as an initial strategy in stable patients with mild angina, roughly one third of patients develop progressive or refractory angina and require invasive treatment over time.6 The RITA-2 (second Randomised Intervention Treatment of Angina) study compared PTCA with medical therapy and demonstrated reduced angina and dyspnea, improved exercise capacity, and a reduced need for antianginal medications with PTCA.1
The success of PTCA was limited by concerns over safety and efficacy.###Indeed, stent implantation not only reduced the rate of acute vessel closure, but also reduced restenosis by approximately 30% compared with PTCA.(2,8,9) Despite these improvements restenosis continued to occur at a rate of 20%–30%.",impact-revealing,highlighting the effectiveness of stent implantation compared to PTCA in reducing restenosis
3608,5c04967517c44a2c7470927f,b9015d4f1e591eba7ea21c3566c919f80f7c2afe,attentive long short-term preference modeling for personalized product search,53e9b7c6b7602d970436f979,Implicit user modeling for personalized search,"…profile; methods in [58, 61] determine the extent of using context (e.g., prior queries and clicks) and investigate the combination of the query and the context to model the user intention; and Shen et al. [52] captured users’ interests by analyzing the immediate contexts and implicit feedback.",other,acknowledge existing methods for user intention modeling
2466,5ede0553e06a4c1b26a8419c,1f3c381eedfe8914b81e93070bfdb00cf86ac943,Contrastive Multi-View Representation Learning on Graphs,58d82fcbd649053542fd6482,Variational Graph Auto-Encoders.,"Graph autoencoders (GAE) (Kipf & Welling, 2016; Garcia Duran & Niepert, 2017; Wang et al., 2017; Pan et al., 2018; Park et al., 2019) train encoders that impose the topological closeness of nodes in the graph structure on the latent space by predicting the ﬁrst-order neighbors.###GAEs over-emphasize proximity information (Velikovi et al., 2019) and suffer from unstructured predictions (Tian et al., 2019).###…classiﬁcation under the clustering protocol, we compare our model with models reported in (Park et al., 2019) including: variational GAE (VGAE) (Kipf & Welling, 2016), marginalized GAE (MGAE) (Wang et al., 2017), adversarially regularized GAE (ARGA) and VGAE (ARVGA) (Pan et al., 2018), and…###We also train a GAE (Kipf & Welling, 2016), a variant of DGI with a GDC encoder, and a variant of VERSE (Tsitsulin et al., 2018) by minimizing KL-divergence between node representations and diffusion matrix.###To address this, unsupervised approaches such as reconstruction based methods (Kipf & Welling, 2016) and contrastive methods (Li et al., 2019) are coupled with GNNs to allow them learn representations without relying on supervisory data.###To evaluate node classiﬁcation under the clustering protocol, we compare our model with models reported in (Park et al., 2019) including: variational GAE (VGAE) (Kipf & Welling, 2016), marginalized GAE (MGAE) (Wang et al., 2017), adversarially regularized GAE (ARGA) and VGAE (ARVGA) (Pan et al., 2018), and GALA (Park et al., 2019).###…node classiﬁcation under the clustering protocol, we compare our model with models reported in (Park et al., 2019) including: variational GAE (VGAE) (Kipf & Welling, 2016), marginalized GAE (MGAE) (Wang et al., 2017), adversarially regularized GAE (ARGA) and VGAE (ARVGA) (Pan et al., 2018), and…",other,acknowledge existing research on graph autoencoders and their limitations
2311,5ac1827b17c44a1fda915855,f96a5a9cfa1dbb01df6df749e093c0bad20240a8,Control Flow Checking at Virtual Edges.,557c69d508b02739a5ca68b7,Out of Control: Overcoming Control-Flow Integrity,"Some of these vulnerabilities are due to the complexity and architectural constraints of the underlying execution environment (CPU hardware and commodity operating systems), some are due to poor software development practices and lack of software security in applications [2].",other,highlighting the causes of vulnerabilities in software and hardware environments
264,5f8d6be69fced0a24bbaaf7b,6427b12aa3ddb4c89b7879c43267cd4a9f0ad1c7,DE-RRD: A Knowledge Distillation Framework for Recommender System,5550417545ce0a409eb3b767,Distilling the Knowledge in a Neural Network.,"Knowledge distillation (KD) is a modelagnostic strategy to improve the learning and the performance of a new “compact” model (student) by transferring knowledge from a previously trained “large” model (teacher) [3, 5, 7, 22].###An early work [7] matches the softmax distribution of the teacher and the student.###KD is a model-agnostic strategy to accelerate the learning of a new compact model (student) by transferring knowledge from a previously trained large model (teacher) [7].",impact-revealing,describing the concept and purpose of knowledge distillation
2831,5b67b47917c44aac1c8637c6,5aea95e1ae78a66474051a330ded374e199b658c,Representation Learning on Graphs with Jumping Knowledge Networks,55323b9045cec66b6f9da283,Computing personalized PageRank quickly by exploiting graph structures,"Social and web networks usually consist of an expander-like core part and an almost-tree (bounded treewidth) part, which represent well-connected entities and the small communities respectively (Leskovec et al., 2009; Maehara et al., 2014; Tsonis et al., 2006).",other,providing context on the structure of social and web networks
3741,5e3a93a93a55ac06c6119df5,cad9e682ddec3b1dd532cb8301737109d9eda7d7,Collaborative Distillation for Top-N Recommendation,5b67b45517c44aac1c860823,Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System.,"Illustration of rank distillation (RD) [11].###Then, we explain the concept of knowledge distillation (KD) [10] and present rank distillation (RD) [11] that applies knowledge distillation to recommender models.###Recently, Tang and Wang [11] proposed a KD model to address the ranking problem, called rank distillation (RD).###• RD [11] and RD-Rank: We set ρ to be 0.###Note that the improvement gap for RD is somewhat different from that in [11].###For Caser, we used the public PyTorch implementation 6 provided in [11].###Tang and Wang [11] proposed ranking distillation (RD) that applies KD for ranking models.###Also, the gain indicates how additional accuracy achieved by the proposed model over that of RD [11].###Since RD [11] is the state-of-the-art KD###• RD [11]: To define the KD loss in equation (4), this utilizes only the top-K items of the soft target by quantizing their values to 1.",other,describing the concept of rank distillation and its application
2399,5de7997c9e795e77580692f9,c919ae4366f5cc4901b854cc259101ccc13e6f3f,Constrained Reinforcement Learning Has Zero Duality Gap,53e99ffcb7602d97028dd8ab,A Geometric Approach To Multi-Criterion Reinforcement Learning,"Although effective, the multi-objective problem [5] has several downsides.",other,highlighting limitations of the multi-objective problem
2403,5736974d6e3b12023e6388bf,1d7e580740a3f96f1529e3d771915af4e996a259,Verbal and Nonverbal Clues for Real-life Deception Detection,53e9988bb7602d97020c26a7,Finding Deceptive Opinion Spam by Any Stretch of the Imagination,", 2010; Joinson and Dietz-Uhler, 2002), social networks (Ho and Hollister, 2013), and consumer report websites (Ott et al., 2011; Li et al., 2014).###As noted before (Ott et al., 2011), this low
3Inter-rater agreement with multiple raters and variables. https://mlnl.net/jg/software/ira/
agreement can be interpreted as an indication that people are poor judges of deception.###It has been successfully used in previous work on deception detection (Newman et al., 2003; Mihalcea and Strapparava, 2009; Ott et al., 2011).###…of deceptive content in a variety of domains, including online dating websites (Toma and Hancock, 2010; Guadagno et al., 2012), forums (Warkentin et al., 2010; Joinson and Dietz-Uhler, 2002), social networks (Ho and Hollister, 2013), and consumer report websites (Ott et al., 2011; Li et al., 2014).",other,acknowledge existing research on deception detection across various domains
731,5ecbc8889fced0a24b51eb0e,57c38661af2d1ac5ac79cc51a443f5f1cca4b03b,single shot video object detector,5a260c8b17c44a4ba8a3289f,"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume","Unlike the works [21], [24], [45] which capitalize on FlowNet-s [59] to produce optical ﬂow, PWC-Net [60] is particularly remould in our motion stream.###For motion stream, we utilize PWC-Net [60] pre-trained on Flying Chairs dataset for optical ﬂow estimation.###This is due to the fact that the receptive ﬁeld in sampling stream for offset prediction is smaller than that in PWC-Net [60] for optical ﬂow generation.",impact-revealing,highlighting the differences in methods for optical flow estimation
3544,5ce3ad3fced107d4c65b6bd9,eeecea3097cf5629eb72a06e5caaf24d774adce7,Unsupervised label noise modeling and loss correction,5aed14e217c44a4438159ac5,Unsupervised Representation Learning by Predicting Image Rotations.,"Automatically obtained noisy labels have previously been demonstrated useful for learning visual representations (Pathak et al., 2017; Gidaris et al., 2018); however, a recent study on the generalization capabilities of deep networks (Zhang et al., 2017) demonstrates that noisy labels are easily ﬁt…###Automatically obtained noisy labels have previously been demonstrated useful for learning visual representations (Pathak et al., 2017; Gidaris et al., 2018); however, a recent study on the generalization capabilities of deep networks (Zhang et al.",other,highlighting the utility and challenges of noisy labels in learning
3844,5f64211c9e795e0286c313a2,f5316f15c665e5a5f89b8b70de13438892e21207,ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks,5e85c28491e0114016e8221a,Regularizing Class-wise Predictions via Self-knowledge Distillation,"Regarding self KD, two examples of the same class are constrained to have consistent output distributions [51, 56].###First, self KD methods [51, 56, 57] maximise the consistency of intraclass images’ predictions or the consistency of different classifiers.",other,describing self knowledge distillation methods
3436,5aed14c317c44a4438157aca,36d442f59c61ea2912d227c24dee76778c546b0a,"graph signal processing: overview, challenges, and applications",53e9abbeb7602d970356e10f,"Digraphs: theory, algorithms and applications.","Acyclic graphs [31], [32] represent Bayesian networks, and undirected graphs represent Markov random fields [33], [34], [35].",other,providing context on graph representations in Bayesian networks and Markov random fields
1855,,9e1c805cc00f9661f4ac936cb32e32d7a83dbb14,Preclinical Transplacental Transfer and Pharmacokinetics of Fipronil in Rats,,,"###Fipronil, a member of the phenyl-pyrazole chemical family, is commonly used as an insecticide and pesticide to eliminate fleas, lice, and ticks (Tingle et al., 2003).",impact-revealing,providing context about the use of Fipronil as an insecticide
246,5db6c73a3a55acec0731cd68,73a5605ce482bd639078ebbb19baac7b903017e2,A Unified MRC Framework for Named Entity Recognition,599c798c601a182cd264a01a,Zero-Shot Relation Extraction via Reading Comprehension.,"For example, Levy et al. (2017) transformed the task of relation extraction to a QA task: each relation type R(x, y) can be parameterized as a question q(x) whose answer is y. For example, the relation EDUCATED-AT can be mapped to “Where did x study?”. Given a question q(x), if a non-null answer y can be extracted from a sentence, it means the relation label for the current sentence is R. McCann et al. (2018) transformed NLP tasks such as summarization or sentiment analysis into question answering. For example, the task of summarization can be formalized as answering the question “What is the summary?”. Our work is significantly inspired by Li et al. (2019), which formalized the task of entity-relation extraction as a multiturn question answering task. Different from this work, Li et al. (2019) focused on relation extraction rather than NER.###For example, Levy et al. (2017) transformed the task of relation extraction to a QA task: each relation type R(x, y) can be parameterized as a question q(x) whose answer is y.###For example, Levy et al. (2017) transformed the task of relation extraction to a QA task: each relation type R(x, y) can be parameterized as a question q(x) whose answer is y. For example, the relation EDUCATED-AT can be mapped to “Where did x study?”. Given a question q(x), if a non-null answer y can be extracted from a sentence, it means the relation label for the current sentence is R. McCann et al. (2018) transformed NLP tasks such as summarization or sentiment analysis into question answering. For example, the task of summarization can be formalized as answering the question “What is the summary?”. Our work is significantly inspired by Li et al. (2019), which formalized the task of entity-relation extraction as a multiturn question answering task.###For example, Levy et al. (2017) transformed the task of relation extraction to a QA task: each relation type R ( x, y ) can be parameterized as a question q ( x ) whose answer is y .###…time and the intensiveness in developing hand-crafted features, etc. Inspired by the current trend of formalizing NLP problems as question answering tasks (Levy et al., 2017; McCann et al., 2018; Li et al., 2019), we propose a new framework that is capable of handling both ﬂat and nested NER.###• ARN: Lin et al. (2019a) proposes AnchorRegion Networks by modeling and levraging the head-driven phrase structures of entity mentions.###Inspired by the current trend of formalizing NLP problems as question answering tasks (Levy et al., 2017; McCann et al., 2018; Li et al., 2019), we propose a new framework for NER that is caar X iv :1 91 0.###For example, Levy et al. (2017) transformed the task of relation extraction to a QA task: each relation type R(x, y) can be parameterized as a question q(x) whose answer is y. For example, the relation EDUCATED-AT can be mapped to “Where did x study?”. Given a question q(x), if a non-null answer y can be extracted from a sentence, it means the relation label for the current sentence is R. McCann et al. (2018) transformed NLP tasks such as summarization or sentiment analysis into question answering.",impact-revealing,highlighting the transformation of relation extraction tasks into question answering tasks
2544,5ea2b8c391e01167f5a89e2d,38643c2926b10f6f74f122a7037e2cd20d77c0f1,Supervised Contrastive Learning,5b8c9f4a17c44af36f8b6df7,Improving Generalization via Scalable Neighborhood Component Analysis,"Most similar to our supervised contrastive is the soft-nearest neighbors loss introduced in [39] and used in [49].###We further improve on [49] by the increased use of data augmentation, a disposable contrastive head and two-stage training (contrastive followed by cross-entropy).###In [49] introduces the approximation of only backpropagating through part of the loss, and also the approximation of using stale representations in the form of memory bank.###Similar to [49], we improve upon [39] by normalizing the embeddings and replacing euclidean distance with inner products.",other,acknowledge improvements and variations in supervised contrastive methods
986,,7da67611b7bbcf39db0a44cb3df06f2004de6234,Upper motor neuron burden measurement in motor neuron diseases: Does one scale fit all?,,,###This builds on prior knowledge that the CNS-LS subscale has good reliability by itself.(17) The,impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3878,5ce3af25ced107d4c65f15d4,abb8c7535d62e5cfaa7332df2479312779988fb4,using deep learning to annotate the protein universe,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"For the embedding network, ProtCNN uses convolutional residual networks (ResNets [44], a variant of convolutional neural networks that in practice train quickly and are stable, even with many layers [44]).",other,describing the architecture of the embedding network
2817,555048d345ce0a409eb71be1,eba36ac75bf22edf9a1bfd33244d459c75b98305,Recurrent convolutional neural networks for text classification,53e99b26b7602d97023bdaff,Why Does Unsupervised Pre-training Help Deep Learning?,Recent research (Hinton and Salakhutdinov 2006; Erhan et al. 2010) shows that neural networks can converge to a better local minima with a suitable unsupervised pre-training procedure.###The learning rate for that layer is divided by “fan-in”.,other,highlighting the benefits of unsupervised pre-training in neural networks
6,5ce2d0b2ced107d4c63a8bff,f02420c1d814ed6bdeae1a1c0923ee9d8eeec8dd,Hierarchical Reinforcement Learning for Course Recommendation in MOOCs,53e9a914b7602d97032679a1,Reinforcement learning with hierarchies of machines,"Inspired by the theory of hierarchical abstract machines (Parr and Russell 1998), we cast the task of profile reviser as a hierarchical Markov Decision Process (MDP).",impact-revealing,drawing on theoretical foundations to frame the task
3521,53e9bb36b7602d970477616b,d2cbad37a383b894cc0c0f904c76b06a73dbb738,a survey of flash translation layer,53e99dabb7602d970266b72e,Algorithms and data structures for flash memories,Toledo [ 7 ] also provided algorithms and data structures for flash memory systems.,other,acknowledge prior work on algorithms and data structures
3926,5eccb534e06a4c1b26a83514,a9682a89b2fef793507c365a577f1521745db96c,Boosting the Transferability of Adversarial Samples via Attention,599c796c601a182cd263b7c7,Ensemble Adversarial Training: Attacks and Defenses,"When it comes to the defended models, we adopt multiple state-of-the-art adversarially trained models as remote targets [37, 19], since adversarial training is arguably the most promising and effective defense to date [22].###Moreover, exploiting the malicious examples tailored for diverse hold-out models can further strengthen defense and confer robustness to transferbased black-box attacks [37].",other,highlighting the effectiveness of adversarial training in model defense
3035,5c04967517c44a2c7470927f,b9015d4f1e591eba7ea21c3566c919f80f7c2afe,attentive long short-term preference modeling for personalized product search,599c7cc6601a182cd27d7616,Emphasizing temporal-based user profile modeling in the context of session search.,"Approaches in the first category [15, 29, 55] capture the short-term user preference from search sessions.###2A session comprises a set of previous interactions containing past submitted queries and clicked records within a specific time limit [29].",other,describing user preference capture methods in search sessions
2387,5db6c73a3a55acec0731cd68,73a5605ce482bd639078ebbb19baac7b903017e2,A Unified MRC Framework for Named Entity Recognition,599c7953601a182cd263079b,Reading Wikipedia To Answer Open-Domain Questions,"MRC models (Seo et al., 2016; Wang et al., 2016; Wang and Jiang, 2016; Xiong et al., 2016, 2017; Wang et al., 2016; Shen et al., 2017; Chen et al., 2017) extract answer spans from passages given questions.###MRC models (Seo et al., 2016; Wang et al., 2016; Wang and Jiang, 2016; Xiong et al., 2016, 2017; Wang et al., 2016; Shen et al., 2017; Chen et al., 2017) extract answer spans from a passage through a given question.",other,reporting existing methods in machine reading comprehension
1307,,56c8d4879369a1e7d665cabb3f5cb1e4a29f1491,The Female Expatriate Manager Experience,,,"###""Face"" is widely recognized as being of pressing significance in Chinese culture (Ho, 1976) and in organizational contexts specifically (Kirkbride et al.",impact-revealing,highlighting the cultural significance of 'Face' in Chinese culture and organizational contexts
606,5e16fa233a55acac60fd363d,f3058ac35927720d2a229984b10524e36d87d7dc,HyGCN: A GCN Accelerator with Hybrid Architecture,599c7988601a182cd2648a09,Inductive Representation Learning on Large Graphs.,"The execution time breakdown of GCN (GCN) [12], GraphSage (GSC) [5], and GINConv (GIN) [25] on several datasets [27] is illustrated in Fig.###For example, the Sampling access volume of GSC is 56.5GB on the RD dataset.###GCNs follow a neighborhood aggregation scheme, where the feature vector of each vertex is computed by recursively aggregating and transforming the representation vectors of its neighbor vertices [1, 5, 25].###Sampling is used to sample a subset from neighbors, which can be done during preprocessing [11] or with random selection during runtime [5].###On the CL dataset for GCN, GSC, and GIN, multiple graphs are assembled to form a larger one before being processed, which results in intensive sparsity.###On CPU, the datasets with more than one graphs are tested by assembling randomly selected 128 graphs into a large graph before processing for GCN, GSC, and GIN or batching the same number of graphs for DFP.###The following evaluations are measured in GSC model.###GraphSage further adopts uniform neighbor sampling to alleviate receptive field expansion that effectively trades off accuracy and execution time [5].###The GSC model consumes signiﬁcant time on the Sampling operation in a preprocessing step, which is not included in the result of PyG-CPU and PyG-GPU.###In order to decrease the computational complexity, the Sample function is usually applied before the Aggregate function to sample a subset from the neighbor vertices of each vertex [5, 26] as the new neighbors, specifically, S(v) = Sample ( N(v) ) .###The execution time breakdown of GCN (GCN) [25], GraphSage (GSC) [18], and GINConv (GIN) [39] on several datasets [23] is illustrated in Fig.###For example, the Sampling energy of GSC is 2715J on the RD dataset.",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3363,5eccb534e06a4c1b26a83a46,e582444763f8b1e3e2c725aab1f37c5d64b69123,Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations,573696f46e3b12023e5f0ef7,FlowNet: Learning Optical Flow with Convolutional Networks.,"Deep neural networks (DNNs) have shown impressive performance in numerous applications, ranging from image classiﬁcation [16, 48] to motion regression [8, 47].",other,highlighting the broad applications and effectiveness of deep neural networks
2299,58d82fd2d649053542fd75bc,e0b207e96351671453aa8bf05b7225c8a340a0b2,towards end-to-end speech recognition with deep convolutional neural networks,53e9b850b7602d9704411f4e,Improvements To Deep Convolutional Neural Networks For Lvcsr,"Following the suggestion from [4], we only perform pooling along the frequency band on the first convolutional layer.###Recently, Convolutional Neural Networks (CNNs) [1] have achieved great success in acoustic modeling [2, 3, 4].###We do pooling only along the frequency axis since it helps to reduce spectral variations within the same speaker and between different speakers [28], while pooling in time has been shown to be less helpful [4].###Most of the CNN models [2, 3, 4] in the speech domain have large filters and use limited weight sharing which splits the features into limited frequency bands while performing convolution separately and the convolution is usually applied with no more than 3 layers.###As shown in Figure 3, we follow the suggestions from [4] that the max pooling is performed only once after the first convolutional layer.###Unlike the time windows applied in DNN systems [2, 3, 4], the temporal modeling is deployed within convolutional layers, where we perform a 2D convolution similar to vision tasks, and multiple convolutional layers are stacked to provide a relatively large context window for each output prediction of the highest layer.###In the context of Automatic Speech Recognition, CNNs are usually combined with HMMs/GMMs [5, 6], like regular Deep Neural Networks (DNNs), which results in a hybrid system [2, 3, 4].",other,describing the architecture and methodology of CNNs in acoustic modeling
1720,,30faaa2abf5e268d0d57113d0c320bef7b99ddd3,Angiogenesis and vasculogenic mimicry as therapeutic targets in ovarian cancer,,,"###Potential hypoxia target molecues containing functional HREs such as VE-cadherin, VEGF-A, VEGFR-1, EphA2, and Twist influence VM (57).###protein kinase Lyn (Lyn), proto-oncogene tyrosine-protein kinase Src (Src), and receptor tyrosine kinases such as PDGFR, FGFR, FLT3, and VEGFR (27).###Pazopanib is a selective multi-targeted receptor tyrosine kinase inhibitor that inhibits VEGFR, platelet-derived growth
293http://bmbreports.org BMB Reports
factor receptor (PDGFR), c-KIT, c-Fms, and fibroblast growth factor receptor (FGFR), and tumor growth and angiogenesis (31, 32).###Bevacizumab inhibits VEGF-A from binding to VEGFR.###Therapeutics that involve targeting anti-angiogenic drugs through VEGF/VEGFR signaling are summarized below.###Vascular endothelial growth factor (VEGF) and its receptor (VEGFR) are major drivers of tumor angiogenesis.###Nintedanib competitively inhibits non-receptor tyrosine kinases such as lymphocyte-specific protein tyrosine kinase (Lck), tyrosineprotein kinase Lyn (Lyn), proto-oncogene tyrosine-protein kinase Src (Src), and receptor tyrosine kinases such as PDGFR, FGFR, FLT3, and VEGFR (27).###Inhibitors of VEGF and VEGFR are widely used for the treatment of solid tumors (7).###which has been shown to have disappointing efficacy in many cancers and increased toxicity such as diarrhea, neutropenia, hypertension, and voice changes (27, 28).###VEGFR targeting agents: Cediranib, pazopanib, nintedanib Cediranib is an inhibitor of VEGFR (VEGFR1-3) tyrosine kinase which has been shown to have disappointing efficacy in many cancers and increased toxicity such as diarrhea, neutropenia, hypertension, and voice changes (27, 28).###However, bevacizumab has been shown to improve median progression-free survival (PFS) for 2-4 months (27).###VEGF proteins promote angiogenesis by binding to VEGFR and subsequently activating signaling cascades (7).",impact-revealing,discussing the role of various molecules and inhibitors in tumor angiogenesis
2961,5bdc315017c44a1f58a05e13,1717255b6aea01fe956cef998abbc3c399b5d7cf,AMC: AutoML for Model Compression and Acceleration on Mobile Devices,573696106e3b12023e523461,EIE: Efficient Inference Engine on Compressed Deep Neural Network,"Fine-grained pruning method prunes neural networks based on individual connections to achieve sparsity in both weights and activations, which is able to achieve higher compression ratio and can be accelerated with specialized hardware such as [18, 17, 39].###However, such algorithms result in an irregular pattern of sparsity, and it requires specialized hardware such as EIE [18] for speed up.###Extensive works [20, 19, 34, 12, 18, 17] have been done on accelerating neural networks by compression.",other,highlighting the challenges and requirements of fine-grained pruning methods in neural networks
902,5fdb279d91e0118a02c4f4ef,cbf8d76c8286b536f431778e974ba9cd583d692e,SimpleChrome: Encoding of Combinatorial Effects for Predicting Gene Expression,62376b385aee126c0f09faaf,Generative Adversarial Nets,"Two of the most popular methods are Generative Adversarial Networks (GANs) by Goodfellow et al. (2014) and Variational
Autoencoders (VAEs) by Kingma and Welling (2013).###Two of the most popular methods are Generative Adversarial Networks (GANs) by Goodfellow et al. (2014) and Variational Autoencoders (VAEs) by Kingma and Welling (2013). While GANs were shown to generate high quality samples, the lack of theoretical support and the difficulty in adversarial learning hinders its utilities in the field, as seen by Tolstikhin et al.###Two of the most popular methods are Generative Adversarial Networks (GANs) by Goodfellow et al. (2014) and Variational Autoencoders (VAEs) by Kingma and Welling (2013). While GANs were shown to generate high quality samples, the lack of theoretical support and the difficulty in adversarial learning hinders its utilities in the field, as seen by Tolstikhin et al. (2017). In this paper, we focus on Variational Autoencoders (VAEs) which are easier to train and enjoy strong theoretical support from Bayesian inference.###Two of the most popular methods are Generative Adversarial Networks (GANs) by Goodfellow et al. (2014) and Variational Autoencoders (VAEs) by Kingma and Welling (2013).",impact-revealing,acknowledge popular methods in generative modeling
1283,,ac712af7e4870d63261da6ed9925c40ea9b1a636,Causal explanation of individual differences in human sensorimotor memory formation,,,###Relative Cramér-Rao Lower Bounds (CRLB) is commonly used as a quality-filtering criterion to identify and discard ‘bad quality’ data (Provencher 1993; Provencher 2001; Scholz et al. 2009; Kim et al. 2014; Antonenko et al. 2017).###The fitted LCModel (red) is overlaid on the raw data (black).###MR data analysis MR spectroscopy Metabolites were quantified using LCModel (Provencher 1993; Provencher 2001; Provencher 2012) performed on all spectra within the chemical shift range 0.5 to 4.2 ppm.,impact-revealing,reporting the use of a quality-filtering criterion in data analysis
4029,5cede0f6da562983788d5a5f,31c343d741b31daeab7cce6ddb768767523d185e,Relational Graph Attention Networks.,58d82fc8d649053542fd59aa,Geometric deep learning on graphs and manifolds using mixture model CNNs,"Spatial approaches are limited by an absence of shift invariance and lack of coordinate system (Duvenaud et al., 2015; Atwood and Towsley, 2016; Monti et al., 2017).",other,highlighting limitations in spatial approaches
2918,5c6a37d03a69b1c9e12a9fc4,81b6d24e8f313fd88b0fe5ff6c21dd154fbe32d2,SimGNN: A Neural Network Approach to Fast Graph Similarity Computation,5b67b4b917c44aac1c867dbc,Hierarchical Graph Representation Learning with Differentiable Pooling.,"A more sophisticated way to represent graphs can be achieved by viewing a graph as a hierarchical data structure and applying graph coarsening [3, 9, 44, 54].###However, once moving to the graph-level tasks, most existing works deal with the classi cation of a single graph [9, 14, 34, 44, 54, 57].",other,highlighting the limitations in existing graph-level tasks
3216,5736986b6e3b12023e72fc2d,51a55df1f023571a7e07e338ee45a3e3d66ef73e,Character-level convolutional networks for text classification,53e9bb9ab7602d97047e2e2f,Rcv1: A New Benchmark Collection For Text Categorization Research,"However, most open datasets for text classification are quite small, and large-scale datasets are splitted with a significantly smaller training set than testing [21].",other,highlighting limitations in open datasets for text classification
3308,5a260c8117c44a4ba8a30a57,908272f8e6340971600148d4e73f50e1e8843aaf,"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and node2vec",59ae3c262bbe271c4c71f4df,Unsupervised Feature Selection in Signed Social Networks.,"…[7, 11, 17, 34], direct network embedding [27], semi-supervised network embedding [18, 42, 49], network embedding with rich vertex attributes [41, 47], network embedding with high order structure [5, 15], network embedding via deep neural network [6, 22, 44], signed network embedding [9], etc.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2862,5def6ca63a55ac6095fe0607,2a6f2656c716082e34f9b86d9e589390842e0853,DistTC: High Performance Distributed Triangle Counting,5a260c4617c44a4ba8a26e8a,TriX: Triangle counting at extreme scale,"Our implementation leverages the binary search based intersection method [16] to improve performance on a single host which in turn improves overall distributed runtime.###[16] presented a distributed implementation of triangle counting, and they noted that even on a single GPU, a binary search based intersection method for finding triangles can speed up computation on GPUs due to improved exploitation of memory bandwidth.###It uses binary search to increase coalesced memory accesses [16] and employs load balancing by dynamically assigning independent units of work (created during preprocessing) to GPUs.",other,reporting on a method to improve performance in distributed computing
957,,9c715e50bd4612b00533f4cfa273ddfb11f7e622,Effects of Photo-Depicted Pupil Diameter on Judgments of Others’ Attentiveness and on Facial Recognition Memory,,,"###Since we have a natural disposition to follow (Batki, Baron-Cohen, Wheelwright, Connellan, & Ahluwalia, 2000) and fixate on the eyes of others (Heisz & Shore, 2008; Janik, Wellens, Goldberg, & Dell’Osso, 1978), and monitoring eye-gaze is instrumental for effective interpersonal communication (Kleinke, 1986), it seems reasonable to suppose that we monitor others’ pupil diameters, either implicitly or explicitly.###…on the eyes of others (Heisz & Shore, 2008; Janik, Wellens, Goldberg, & Dell’Osso, 1978), and monitoring eye-gaze is instrumental for effective interpersonal communication (Kleinke, 1986), it seems reasonable to suppose that we monitor others’ pupil diameters, either implicitly or explicitly.",impact-revealing,providing context for the importance of monitoring pupil diameters in interpersonal communication
642,558ab239e4b037c08758a249,c1757f34c23241960a0089c5117fa3b676902951,the effect of code reordering on branch prediction,53e99822b7602d9702043cce,Software Trace Cache,"Both factors prove important at increasing the fetch performance, as shown in [19, 18].###We examine the interaction of these optimizations with both static and dynamic branch predictors using the Software Trace Cache layout optimization [19].###In order to simulate the optimized code layout we generate an address translation table using the Software Trace Cache algorithm [19] and feed the simulator with translated PC’s and recomputed branch outcomes.###Code layout optimizations usually target a better utilization of the instruction cache, and use profile data or heuristics to lay out the routines in a program [17, 7, 6], and the basic blocks in a routine [8, 17, 25, 19] to minimize the number of conflict misses.###We optimize the code layout using the Software Trace Cache (STC) algorithm [19], which targets an increase in the sequentiality of the code, that is, it reorders basic blo ks so that branches tend to be not taken.###By aligning basic blocks so that they execute sequentially, we can further increase spatial loca ity increasing both cache performance and fetch bandwidth [8, 17, 25, 19].",impact-revealing,highlighting the importance of code layout optimizations for performance
593,5e16fa233a55acac60fd369d,0b5f443d9fae1d5cb90994025e0ce21ddc49c21c,iDLG: Improved Deep Leakage from Gradients,5d1eb9deda562961f0b18cb3,Deep Leakage from Gradients.,"For DLG [1], as described by the authors, we start the procedure with the randomly initialized dummy data and outputs ( x (cid:48) , y (cid:48) ) , then iteratively update them to minimize the gradient matching objective.###Recent work by Zhu et al. [1] presents an approach (DLG) to steal the proprietary data protected by the participants in distributed learning from the shared gradients.###In this section, we empirically demonstrate the advantages of our (iDLG) method over DLG [1].###However, recent work by Zhu et al. , “Deep Leakage from Gradient” (DLG) [1] showed the possibility to steal the private training data from the shared gradients of other participants.###…for 300 iterations, and evaluate the performance in terms of (i) the accuracy of the extracted labels c (cid:48) , and (ii) the ﬁdelity of the extracted Dataset DLG iDLG MNIST 89.9% 100.0% CIFAR-100 83.3% 100.0% LFW 79.1% 100.0% Table 1: Accuracy of the extracted labels for DLG [1] and iDLG.###Following the settings in [1], we use the randomly initialized LeNet for all experiments.###• We empirically demonstrate the advantages of iDLG over DLG [1] via comparing the accuracy of extracted labels and the ﬁdelity of extracted data on three datasets.###This enables us to always extract the ground-truth labels and signiﬁcantly simplify the objective of DLG [1] in order to extract good-quality data.",impact-revealing,demonstrating the advantages of a new method over existing approaches
242,56d86073dabfae2eee7807ea,6928b1bf7c54a4aa8d976317c506e5e5f3eae085,Deception detection using real-life trial data,5550488e45ce0a409eb6f579,Deception detection using a multimodal approach,"A multimodal deception dataset consisting of linguistic, thermal, and physiological features was introduced in [35], which was then used to develop a multimodal deception detection system [2].###However, while these are great features to consider in order to gain insights into the semantic categories of words that represent useful clues for deception, their performance is often similar to that of the n-grams features [2].",impact-revealing,highlighting the limitations of multimodal features in deception detection
1278,,499efc8bcadd5965ed35e1b2c32a5172c5fe8fc3,"Development of mathematical models of everyday life factors on glycaemia, identifiable in outpatients with type 1 diabetes",,,"###The concept of practical identifiability takes into consideration the role of data quality in the unique observability of behaviours (Raue et al., 2009; Docherty et al., 2011).###Early research describing practical identifiability was presented by Raue et al. (2009).###These methods determine when the data quantity and quality is insufficient for the size of a model, resulting in mutual interference of two or more parameters (Docherty et al., 2011; Raue et al., 2009; Saccomani, 2013).###This is especially true if there is no recognition of the limitations in the unique observability of features, since the realistic expectations of the model and data can be lost amongst the mathematics (Docherty et al., 2011; Raue et al., 2009; Saccomani, 2013).###Infinite confidence intervals indicate practical non-identifiability (Raue et al., 2009) and since identifiability is a continuous artefact (Docherty et al.###Identified model parameter values describing similar but distinct behaviours can trade off if measurement noise in the data is sufficient (Raue et al., 2009; Docherty et al., 2011).###However increasing the size and complexity of the models also increases risk of model structural (Audoly et al., 2001, 1998; Bellman and Åström, 1970; Bellu et al., 2007) and practical (Docherty et al., 2011; Saccomani, 2013; Raue et al., 2009, 2014)
33
non-identifiability.###Furthermore, models must be well-suited to the quality of data they would be coupled with to ensure successful parameter identification (Docherty et al., 2011; Raue et al., 2009; Saccomani, 2013).###28
3.3 A demonstration of the method of Raue et al. (2009) to
determine identifiability of a parameter using the maximum likelihood profile.###3.3.2 Practical identifiability
Practical identifiability is an emerging field (Raue et al., 2009, 2012, 2014; Docherty et al., 2011; Saccomani, 2013).",impact-revealing,highlighting the significance of practical identifiability in model data quality
2392,5e5e18ca93d709897ce315f0,68f86237dadcf2f570f0cd5b5e56161693619a74,Residual Energy-Based Models for Text Generation,53e9a645b7602d9702f7362e,Noise-contrastive estimation: A new estimation principle for unnormalized statistical models,"With the theoretical guarantee of NCE, we can show that the optimum of the above objective is reached at data distribution with inﬁnite amount of data and model with enough capacity, which is also proved in Ma & Collins (2018) 2 .###Instead, we train our residual energy function using Noise Contrastive Estimation (NCE) (Gutmann & Hyv¨arinen, 2010), and more speciﬁcally its conditional version (Ma & Collins, 2018).###NCE then trains a binary classiﬁer on the difference of log-probability scores of these two models.###NCE requires two distributions: The model distribution and a noise distribution.###In particular, we adopt the conditional noise contrastive estimation (NCE) objec-tive (Ma & Collins, 2018; Gutmann & Hyv¨arinen, 2010) to our residual model energy function and then sample from the joint model using importance sampling.###While Ma & Collins (2018) used conditional NCE to predict the next word in a sequence, we apply it to produce a whole sequence at once with the pretrained auto-regressive language model as the noise distribution.",other,describing the application of Noise Contrastive Estimation in model training
1094,,21b86f315411ddb33e0bea3e4b948e240451401c,Deep Bayesian network architecture for Big Data mining,,,"###The joint probability distribution of a given Bayesian Network is computed Example of a Bayesian network using the following expression: where Parents ( V i ) is the set of the parents of the variable V i in the graph G and n is the number of nodes in the corresponding Bayesian Network.###23 In the scope of this paper, we were inspired by both the Deep Learning's principle and the Hierarchical Bayesian Networks in order to provide a newmultilayeredBayesiannetworkarchitecturewithlatentvariables.###We also have adopted the principle of HBN in order to make use of the edges' signification, the causality, and the uncertainty of the Bayesian Network, hence the meaningfulness of the hidden layers and the latent variable's connections.###Our contribution consists on providing a new Deep Bayesian Network architecture.###Besides,wehaveadoptedtheprincipleofHBNinordertomakeuseoftheedges' signification, the causality, and the uncertainty of the Bayesian Network, hence the meaningfulness of the hidden layers and the latent variable's connections.###Unfortunately, these efforts are more focusing on optimizing the structure learning 11 , 18 , 19 rather than ensuring the analyzability of the dependencies between thefeatures.###We point out the Hierarchical Bayesian Networks and the Deep Belief Networks as the most similar related work.###Werecallthattheobjectiveofthispaperistoprovideanew Bayesian Network architecture that supports the characteristics of Big Data.###The main motivation behind considering the Deep Bayesian Network (Deep-BN) is the need to provide a simplified and accurate modeling of high-dimensional data.###26 It is substantial to mention that the latent structure of the Hierarchical Bayesian Network (HBN) is highly dependent on the complexity of the latentvariable(ie,thenumberofdiscretestatesofthelatentvariable).###Therefore, our newly proposed architecture has the advantage of being simply learnt and easily interpreted because of the graphical aspect of the Bayesian Network.###21 Generally, the Bayesian network with latent variables are called the Hierarchical Bayesian Networks.###Inanutshell,althoughtheDeep-BNdidnotremarkablyoutperformtheaverageaccuracyoftheDBN,itsgraphicalandprobabilistic foundations are very useful when dealing with the analyzabilityof the provided model (ie, a user-friendly Bayesian Network model whose analysis, understanding, information preservation, and querying are straightforward).###TheBayesiannetworkisusuallylearntfromatrainingdatasetthroughtwosteps:the structurelearning ,whichestablishesadirectedacyclicgraph offeatures, 11 - 14 andthe parameterlearning ,whichestimatestheprobabilitydistributionofeachfeaturegivenitsparentsinthestructure.###Following this context, we focus on the Bayesian Network 11 as one of the most suitable datamining methods that provide an understandable model.",impact-revealing,describing a new Bayesian Network architecture and its advantages
29,58437722ac44360f1082f15c,cc16e43cce64b649da00892d1493425620c2d61c,Learning to Match Using Local and Distributed Representations of Text for Web Search.,53e9affab7602d9703a4f291,Learning deep structured semantic models for web search using clickthrough data,"Both the deep structured semantic model (DSSM) [16] and its convolutional variant CDSSM [38] consider only the document title for matching with the query.###[16], but unlike their approach we don’t limit our input representation to n-graphs of a fixed length.###Both the deep structured semantic model (DSSM) [16] and its convolutional variant CDSSM [37] consider only the document title for matching with the query.###Among the baseline models, including both traditional and neural network based models, CDSSM and DESM achieve the highest NDCG at position one and ten, respectively, on the weighted test set.###[16] learn a distributed representation of query and title, for document ranking.###These anecdotal examples agree with the results in in Table 2 that show that on the weighted test set all the neural models whose main focus is on learning distributed representations of text (duet model, distributed model, DESM, DSSM, and CDSSM) perform better than the models that only look at patterns of term matches (local model and DRMM).###Unlike some previous work [16, 36, 37] that train on clickthrough data with randomly sampled documents as negative examples, we train our model on human-judged labels.###Our choice of a window-based max-pooling strategy, instead of global max-pooling as employed by CDSSM [38], is motivated by the fact that the window-based approach allows the model to distinguish between matches in different parts of the document.###For the CDSSM model, we concatenated the trigraph hash vectors of the ﬁrst T terms of the body text followed by a vector that is a sum of the trigraph hash vectors for the remaining terms.###While some papers have reported negative performances for title-based DSSM and CDSSM on the ad hoc document retrieval tasks [12, 30], we included document-based variants appropriately retrained on the same set of positive query and document pairs as our model.",impact-revealing,providing context on document matching models and their performance
975,,8e61aacb86c6c202bc5d6262be3431812b6825b1,Measuring Situation Awareness during Command and Control Activity : A Comparison of Measures Study,,,"###Salas et al (1995) subsequently defined team SA as “the shared understanding of a situation among team members at one point in time” (Salas et al, 1995, p.131).###Salas et al (1995) also highlighted the importance of communication in team SA acquisition.###Based on a review of the literature Salas et al (1995) proposed a framework of team SA, suggesting that it comprises two critical, but poorly understood, processes: individual SA and team processes.",impact-revealing,reporting prior findings on team situational awareness and its components
3795,5f803c8f91e01119a5df749b,39ca8f8ff28cc640e3b41a6bd7814ab85c586504,deformable detr: deformable transformers for end-to-end object detection,573696026e3b12023e515eec,Deep Residual Learning for Image Recognition,"The inputs are of ResNet feature maps (with encoded positional embeddings).###Given the input feature maps x ∈ R C × H × W extracted by a CNN backbone (e.g., ResNet (He et al., 2016)), DETR exploits a standard Transformer encoder-decoder architecture to transform the input feature maps to be features of a set of object queries.###In encoder, we extract multi-scale feature maps { x l } L − 1 l =1 ( L = 4 ) from the output feature maps of stages C 3 through C 5 in ResNet (He et al., 2016) (transformed by a 1 × 1 convolution), where C l is of resolution 2 l lower than the input image.###ImageNet (Deng et al., 2009) pre-trained ResNet-50 (He et al., 2016) is utilized as the backbone for ablations.###As discussed in Section 4.1 and illustrated in Figure 3, the input multi-scale feature maps of the encoder { x l } L − 1 l =1 ( L = 4 ) are extracted from the output feature maps of stages C 3 through C 5 in ResNet (He et al., 2016) (transformed by a 1 × 1 convolution).",other,providing context on the input feature maps and model architecture
543,5aed14d617c44a4438159123,921196c32213a229245a9705ee4768bc941e7a26,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,599c7959601a182cd263387c,Convolutional Sequence to Sequence Learning.,"Particularly inspiring for our work are the recent applications of convolutional architectures to machine translation (Kalchbrenner et al., 2016; Gehring et al., 2017a;b), audio synthesis (van den Oord et al., 2016), and language modeling (Dauphin et al., 2017).###On the other hand, recent research indicates that certain convolutional architectures can reach state-of-the-art accuracy in audio synthesis, word-level language modeling, and machine translation (van den Oord et al., 2016; Kalchbrenner et al., 2016; Dauphin et al., 2017; Gehring et al., 2017a;b).###…is informed by recent convolutional architectures for sequential data (van den Oord et al., 2016; Kalchbrenner et al., 2016; Dauphin et al., 2017; Gehring et al., 2017a;b), but is distinct from all of them and was designed from ﬁrst principles to combine simplicity, autoregressive prediction,…",impact-revealing,highlighting inspiration from recent convolutional architectures in various applications
2224,5fb24ee191e01186d3f5decc,050fecf6e3e21faeeb7629f179ba839c4dfd1ead,EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints,599c7ec3601a182cd28c525e,Findings of the 2014 Workshop on Statistical Machine Translation.,"…on three language pairs spanning different language families and data conditions (Table 1): Romanian-English (Ro-En) from WMT16 (Bo-jar et al., 2016), English-German (En-De) from WMT14 (Bojar et al., 2014), and English-Japanese (En-Ja) from WAT2017 Small-NMT Task (Nakazawa et al., 2017).###The LevT base-line in Susanto et al. (2020) achieves higher BLEU than ours on the small Wiktionary and IATE test sets, while it underper-forms our LevT on the full WMT14 test set (26.5 vs. 26.9). improve BLEU.###Dataset Following Gu et al. (2019), we exper-iment on three language pairs spanning different language families and data conditions (Table 1): Romanian-English (Ro-En) from WMT16 (Bo-jar et al., 2016), English-German (En-De) from WMT14 (Bojar et al., 2014), and English-Japanese (En-Ja) from WAT2017 Small-NMT Task (Nakazawa et al., 2017).",other,reporting comparative results on translation tasks
927,5f7d9bfd91e011346ad27f0c,2051548f7681c96d603de932ee23406c525276f9,a transformer-based framework for multivariate time series representation learning,5da056bb47c8f76646056c7b,Learning representations of multivariate time series with missing data.,"As a novel take on autoencoding, and with the goal of dealing with missing data, Bianchi et al. (2019) employ a stacked bidirectional RNN encoder and stacked RNN decoder to reconstruct the input, and at the same time use a user-provided kernel matrix as prior information to condition internal…###As a novel take on autoencoding, and with the goal of dealing with missing data, Bianchi et al. (2019) employ a stacked bidirectional RNN encoder and stacked RNN decoder to reconstruct the input, and at the same time use a user-provided kernel matrix as prior information to condition internal representations and encourage learning similarity-preserving representations of the input.",impact-revealing,describing a novel approach to autoencoding for missing data
3487,5eede1bc91e0116a822a4942,156d11ca27740b591fb827e143cc71e9795a9745,Riptide: Fast End-to-End Binarized Neural Networks,5b3d98d617c44a510f8023fd,Heterogeneous Bitwidth Binarization In Convolutional Neural Networks,"This increased efficiency typically comes at the cost of reduced inference accuracy, and a slew of recent work (Courbariaux et al., 2016; Zhou et al., 2016; Cai et al., 2017; Hubara et al., 2016; Tang et al., 2017; Dong et al., 2017; Fromm et al., 2018; Choi et al., 2019) has therefore focused on closing the accuracy gap.###…typically comes at the cost of reduced inference accuracy, and a slew of recent work (Courbariaux et al., 2016; Zhou et al., 2016; Cai et al., 2017; Hubara et al., 2016; Tang et al., 2017; Dong et al., 2017; Fromm et al., 2018; Choi et al., 2019) has therefore focused on closing the accuracy gap.",other,highlighting the trade-off between efficiency and accuracy in recent research
1657,,3cf77b7bc2a4ff577b58ba805d723d1434bce16c,"Sex Differences in the Attitudes of Australian and Indian Heterosexual Individuals toward Gay Men, Lesbians, Bisexual Men and Bisexual Women",,,"###Identifying with one’s own in-group can be reinforced by discriminating against other social groups that an individual does not belong to (Tajfel & Turner, 1979).",impact-revealing,providing context on social identity theory
3304,5a260c8117c44a4ba8a30f54,33998aff64ce51df8dee45989cdca4b6b1329ec4,Graph Attention Networks,57a4e91dac44365e35c9830c,Learning Convolutional Neural Networks for Graphs,"…degree (Duvenaud et al., 2015), using the powers of a transition matrix to deﬁne the neighborhood while learning weights for each input channel and neighborhood degree (Atwood & Towsley, 2016), or extracting and normalizing neighborhoods containing a ﬁxed number of nodes (Niepert et al., 2016).###ﬁne the neighborhood while learning weights for each input channel and neighborhood degree (Atwood &amp; Towsley, 2016), or extracting and normalizing neighborhoods containing a ﬁxed number of nodes (Niepert et al., 2016). More recently, Hamilton et al. (2017) introduced GraphSAGE, a method for computing node representations in an inductive manner. This technique operates by sampling a ﬁxed-size neighborhood of each n",other,acknowledge prior methods in graph representation learning
1135,,bd5c066a63ae6a5fcf99e53551161446baf54a27,Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models,,,"###To put our contribution in context, we begin with an overview of two recent text-to-image personalization approaches: Textual Inversion [Gal et al. 2022] and E4T [Gal et al. 2023] which serve as a basis for our work.",impact-revealing,providing context for the current research contribution
3735,5ed12ca69e795e8ab1c11568,b3b60a15502abca31ae7b20ce16bf50049183c5a,knowledge enhanced personalized search,555044ed45ce0a409eb52210,Temporal Latent Topic User Profiles for Search Personalisation.,"Other works [3, 9, 17, 23, 29, 34, 35, 38] extracted the topics features from user’s search history to predicted document relevance.###Previous research often personalizes the search results by matching documents with user profiles, which represent user’s intents and are constructed using user’s search history [16, 18, 22, 30, 34, 35].",other,acknowledge existing methods in document relevance prediction
2223,5d5e6b9a3a55acfce79a16dd,6303bac53abd725c3b458190a6abe389a4a1e72d,Deep High-Resolution Representation Learning for Human Pose Estimation,57a4e91dac44365e35c98b31,Deeply-Fused Nets,"then a reﬁnenet combines the low-to-high level features that are processed through convolutions. Our approach repeats multi-scale fusion, which is partially inspired by deep fusion and its extensions [67,73,59,80,82]. Intermediate supervision. Intermediate supervision or deep supervision, early developed for image classiﬁcation [34,61], is also adopted for helping deep networks training and improving the heatmap ",other,providing context for the proposed approach
3692,5d8dded23a55acd1b54967df,1ecbaf7a2cd3059e07261e72a1195a7c70b3d664,GraphMix: Regularized Training of Graph Neural Networks for Semi-Supervised Learning,58d83014d649053542fe1979,Edge Weight Prediction in Weighted Signed Networks,"s and edge connectivity of all the nodes but has access to the class labels of only a few of the nodes. For semi-supervised link classiﬁcation, we use two datasets Bitcoin Alpha and Bitcoin OTC from (Kumar et al., 2016; 2018). The nodes in these datasets correspond to the bitcoin users and the edge weights between them correspond to the degree of trust between users. Following (Qu et al., 2019), we treat edges with###For semi-supervised link classiﬁcation, we use two datasets Bitcoin Alpha and Bitcoin OTC from (Kumar et al., 2016; 2018).",other,reporting datasets used for semi-supervised link classification
1348,,6113cf8b24d4b963667d2eb3d33877a59aa212b1,Learning-based image restoration for compressed images,,,"###It is also constrained that the filed G varies slowly [18,19], which means that the neighboring attenuation RF appears nearly the same.###DCT [1–3], overcomplete wavelet representation (OWR) [4–6]), spatial domain [7–10,12–14], or the combinations [15–21].###[16] proposed a simple yet efficient image degradation model, which have been widely used for image restoration [16,17] and image quality evaluation [18,19].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2181,,a3a2a96537a5611aba20fdcab1976bf792370c7f,Avoid Deadlock Resource Allocation (ADRA) Model V VM-out-of-N PM,,,"###In the past, grid computing and batch scheduling had both been commonly used for large scale computation [1].",impact-revealing,providing historical context on computation methods
258,5b67b45517c44aac1c860823,7f01c6fe27f57ee6191b51efa18b9199baf7b82a,Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System,53e9b7d3b7602d97043808b0,Improving pairwise learning for item recommendation from implicit feedback,"s weight wa should be inversely proportional to the rank: wa r ∝r −1 and r ∈[1,K], (6) where r is the rank range from 1 to K. As pointed out above, this scheme pre-determines the weight. Rendle et al [29] proposed an empirical weight for sampling a single position from a top-K ranking, following a geometric distribution: wa r = ρ(1 −ρ)r and ρ∈(0,1). (7) Following their work, we use a parametrized geom",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2524,5c5ce4fd17c44a400fc38abb,d524f10d653ba09e36456475da0aed92d244f795,Image Super-Resolution As A Defense Against Adversarial Attacks,58437722ac44360f1082f1c4,Learning a Driving Simulator.,"a pivotal role in designing many critical real-world systems, including self-driving cars [12] and models for disease diagnosis [13], which necessitates their robustness in such situations.",other,highlighting the importance of robustness in critical real-world systems
3433,5c8f2a8b4895d9cbc62ecf7c,10ab21b120e305b6d3cbf81c5a906d36521152f1,Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors,5a260bfb17c44a4ba8a1c5dd,Simple Black-Box Adversarial Attacks on Deep Neural Networks.,"Soon after, Narodytska et. al [NK17] described the ﬁrst black-box attack on deep neural networks; the algorithm uses a greedy search algorithm that selectively changes individual pixel values.",other,reporting prior findings on black-box attacks on deep neural networks
123,5ee8986891e011e66831c556,73366d75289c5e37481639fb54fdee28a664e2b3,GNNGUARD: Defending Graph Neural Networks against Adversarial Attacks,53e9ad82b7602d9703778d21,Birds of a Feather: Homophily in Social Networks,"he former component dynamically adjusts the relevance of nodes’ local network neighborhoods, prunes likely fake edges, and assigns less weight to suspicious edges based on network theory of homophily [14]. The latter components stabilizes the evolution of graph structure by preserving, in part the memory from a previous layer in the GNN. We compare GNNGUARD to three state-of-the-art GNN defenders acro###n the sense that it allows for successful routing of GNN’s messages. In contrast to attention mechanisms (e.g., GAT [19, 40]), GNNGUARD determines importance weights using theory of network homophily [14], positing that similar nodes (i.e., nodes with similar features) are more likely to interact than dissimilar nodes. To this end, we quantify similarity sk uv between uand its neighbor vin the k-th la",impact-revealing,describing the functionality of GNNGUARD and its comparison to existing methods
1116,,e7ecb41e7f0eddd1702a4ad0e2a11ca44ce6db07,Momentum-Imbued Langevin Dynamics (MILD) for Faster Sampling,,,"###Recent works that tackle the problem of optimizing the sampler for diffusion models [21, 27–31] are motivated by the theory of numerical methods applied to stochastic differential equations (SDEs).###Denoising diffusion implicit models (DDIMs) [21] proposed a generalization involving non-Markovian diffusion processes, achieving a 10 to 50 speedup over Denoising Diffusion Probabilistic Models [4].",impact-revealing,highlighting advancements in diffusion models and their optimization
3682,53e99c66b7602d9702514d51,8e8e622d5fab4c1d2a5bc7783db84e62cc570f9a,disaggregated memory for expansion and sharing in blade servers,56d82136dabfae2eeebd910a,Power aware page allocation,"Keywords Memory capacity expansion, disaggregated memory, power and cost efficiencies, memory blades.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
2011,,e552d709a24db33ef8b77527af5ae7bdbcfbff68,Reassessing the alcohol-violence linkage: Results from a multiethnic city,,,"###, 2000; Sampson & Raudenbush, 1999), and several scholars (Alaniz et al., 1998; Gorman et al., 2001; Speer et al., 1998) have suggested that smaller units of analysis (less than the city level) are best for studying the alcohol-violence relationship.###1° Although other studies (e.g., Peterson et al., 2000; Roncek & Maier, 1991; Scribner etal . , 1999; Speer et al., 1998) also demonstrated the importance of alcohol outlets in various locations, Miami offers an important context unlike those that were previously examined: It is a multiethnic, majority Latino city",impact-revealing,highlighting the unique context of Miami in studying the alcohol-violence relationship
2384,53e9b9fbb7602d97045fabff,f89facea7ae5a3f51af96b549f04f3dbe8c884b3,SHIFT: Shared history instruction fetch for lean-core server processors,558c6c5ce4b02b9f07a70396,NOC-Out: Microarchitecting a Scale-Out Processor,"To quantify the overall power overhead induced by SHIFT, we use CACTI [24] to estimate the LLC power (both for index pointers in the tag array and history buffers in the data array) and custom NoC power models to estimate the link, router switch fabric and buffer power in the NoC [21].###To quantify the overall power overhead induced by SHIFT, 
we use CACTI [24] to estimate the LLC power (both for index pointers in the tag array and history buffers 
in the data array) and custom NoC power mod­els to estimate the link, router switch fabric and buffer 
power in the NoC [21].",other,providing context for power overhead estimation methods
2545,5a4aef9e17c44a2190f7a8b8,ff772950f66ac6a57f4201ce1f02f0013ccdc1bb,Receptive Field Block Net for Accurate and Fast Object Detection,58437722ac44360f1082f424,PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection.,"Its variants [35, 33, 17] achieve competitive results in object detection and classification tasks.",other,reporting competitive results of variants in object detection and classification tasks
1803,,7d70326068cba290c18e809735d2a425894380a8,Refinement-based context-sensitive points-to analysis for Java,,,"###In previous work [36], we showed that context-insensitive points-to analysis for Java is a balanced parentheses CFL-reachability problem.###Section 3 presents our CFL-reachability formulation of context-insensitive points-to analysis for Java [36], and Section 4 deﬁnes our context-sensitive formulation.###A demand-driven points-to analysis [14, 37] performs only the work necessary to answer a query, i.###Finally, the “Shown to Scale” column indicates whether the algorithm has been shown to scale to large Java benchmarks; “1.1 lib” means the smaller Java 1.1 libraries were analyzed, and k-limiting [35] is indicated where used. strings, exploiting typical object-oriented code structure for greater precision and scalability.###In this section, we formulate ﬁeld-sensitive, context-insensitive points-to analysis for Java in CFL-reachability, adapted from [36].###However, they analyze the Java 1.1 libraries, which are signiﬁcantly smaller than the Java 1.3 libraries used in the present work.###In the previous algorithm, the balanced parentheses property of Java points-to analysis was used to create an analysis that traversed a small portion of the graph to compute an answer, for use with tight time budgets.###Finally, Section 2.4 illustrates how the analysis works on a Java code example.###LF is described in greater detail in [37].###We have also added precise handling of recursive ﬁelds in the current algorithm, which is more important in the context-sensitive setting for handling data structures like Java’s LinkedList .###In previous work [37], we showed that contextinsensitive points-to analysis for Java is a balanced parentheses CFL-reachability problem.###Section 3 presents our CFL-reachability formulation of contextinsensitive points-to analysis for Java [37], and Section 4 defines our context-sensitive formulation.###The size of the benchmarks are comparable to those used in other recent Java points-to analysis studies [20, 42].###Our refinement analysis is best run with a budget: after some fixed amount of time for each query, the analysis terminates and returns a conservative result to the client [37].###The numbers include the reachable portions of the Java library, determining using a call graph constructed on the ﬂy with Andersen’s analysis [3] by Spark [20]. context-insensitive analysis time in all presented running times for our analysis.###We have developed a reﬁnement-based approach to points-to analysis that succeeds by simultaneously reﬁning the two key axes of precision for Java points-to analysis: handling of method calls, and handling of heap accesses, i.e. , reads and writes to object ﬁelds.###Our previous work [36] showed that precise handling of Java heap accesses can be formulated with L F being a language of balanced parenthe-ses .###• We observe that balanced parentheses in the CFL-reachability formulation of context-insensitive Java points-to analysis [37] can be used to guide refinement of both heap access and method call handling.###We include the SPECjvm98 suite, soot-c and sablecc-j from the Ashes suite [1], several benchmarks from the DaCapo suite version beta050224 [2], and the Polyglot Java front-end [25].###Constraint-based analyses have been developed to convert legacy Java programs to use Java 5 generics, and they have been shown to prove many downcasts safe [8, 10].###The key difference with our work is that their analysis adds sensitivity to all possibly polluting statements when imprecision is detected; this approach does not scale for Java, as it requires too much code to be treated precisely.###These analyses rely on the generics annotations of Java 5 java.util classes to model their behavior.###Consider a query for the objects possibly returned by a call to this simpliﬁed version of Vector.elementAt() from the Java standard library: Object elementAt( int index) { if (index >= numElements) { throw new OutOfBoundsException(index + "" too big"")); } return elems[index]; } Our analysis considers only the array access and the read of the elems ﬁeld in the return statement, ignoring the calls to the OutOfBoundsException constructor and String and StringBuffer methods from the string concatenation.###Intuitively, ﬁeld accesses are balanced since Java instance ﬁelds cannot be accessed through lower-level pointer operations like C’s dereference ( * ) and address-of ( & ) operators.###This paper makes the following contributions: • We observe that balanced parentheses in the CFL-reachability formulation of context-insensitive Java points-to analysis [36] can be used to guide reﬁnement of both heap access and method call handling.###In this section, we formulate field-sensitive, context-insensitive points-to analysis for Java in CFL-reachability, adapted from [37].###Our previous work [37] showed that precise handling of Java heap accesses can be formulated with LF being a language of balanced parentheses.###We were unable to sufﬁciently scale the algorithm variant with context-sensitive call graph construction to analyze our benchmarks; the most similar published analysis for Java [26] had similar scalability issues.###The current work builds on our own previous work on demand-driven points-to analysis [37].###To compare with an analysis that handles assignments with equality constraints, we also implemented data structure analysis [17], a context-sensitive analysis for C that we adapted to Java.",impact-revealing,highlighting the contributions and advancements in points-to analysis for Java
226,58437722ac44360f1082efeb,36eff562f65125511b5dfab68ce7f7a943c27478,Semi-Supervised Classification with Graph Convolutional Networks,57a4e91aac44365e35c97c6e,Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.,"Defferrard et al. (2016) use this K -localized convolution to deﬁne a convolutional neural network on graphs.###Firstly, we introduce a simple and well-behaved layer-wise propagation rule for neural network models which operate directly on graphs and show how it can be motivated from a ﬁrst-order approximation of spectral graph convolutions (Hammond et al., 2011).###We leave memory-efﬁcient extensions with mini-batch stochastic gradient descent for future work.###In the following, we show that the form of this propagation rule can be motivated 1 via a ﬁrst-order approximation of localized spectral ﬁlters on graphs (Hammond et al., 2011; Defferrard et al., 2016).",impact-revealing,reporting prior findings on graph convolutional networks
1790,,86d1288fcfc9810b0b277e4a8ac5ee848df5c13d,Large-Scale Screening of HCMV-Seropositive Blood Donors Indicates that HCMV Effectively Escapes from Antibodies by Cell-Associated Spread,,,"###However, in situations where their use is limited by adverse effects or the occurrence of resistant strains [11,12], alternatives with a different mode of action may be required.",impact-revealing,highlighting the need for alternative solutions due to limitations
2440,5db9295f47c8f766461f5135,2c1006c856fefdbd6cd710e840e57153f2d6cd04,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,5a260c8117c44a4ba8a30f20,PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples.,"In the mean time, many efforts have been devoted to defending against adversarial examples [38, 37, 63, 25, 33, 50, 53, 46, 35].",other,highlighting ongoing efforts to defend against adversarial examples
2937,5cf48a45da56291d582a8448,f937d6482ad162f55022c7dce5857855ece27c1e,Knowledge Graph Convolutional Networks for Recommender Systems with Label Smoothness Regularization,53e99991b7602d97021d5b2b,Video suggestion and discovery for youtube: taking random walks through the view graph.,"Based on different settings of edge weights in the input graph, these methods are classified as: (1) Edge weights are assumed to be given as input and therefore fixed [1, 36, 37]; (2) Edge weights are parameterized and therefore learnable [9, 20, 34].",other,describing classification of methods based on edge weight settings
238,5a73cbcc17c44a0b3035f1d3,14058c2ebe9905fe5c3acf8b1bfcd5390fe32a28,Deception Detection in Videos,5550414e45ce0a409eb3a100,A robust and efficient video representation for action recognition.,"…ﬁrst introduced in (Jaakkola and Haussler 1999) to combine the advantages of generative and discriminative models, and are widely used in other computer vision tasks, such as image classiﬁcation (Perronnin and Dance 2007), action recognition (Wang et al. 2016) and video retrieval (Han et al. 2017).",impact-revealing,highlighting the widespread application of generative-discriminative models in various computer vision tasks
750,58d82fcbd649053542fd6178,515a21e90117941150923e559729c59f5fdade1c,the concrete distribution: a continuous relaxation of discrete random variables,5550443b45ce0a409eb4c39d,Stochastic Backpropagation and Approximate Inference in Deep Generative Models.,"Here, the observation that AD “just works” when stochastic nodes1 can be reparameterized into deterministic functions of their parameters and a fixed noise distribution [23, 35], has liberated researchers in the development of large complex stochastic architectures [e.###…works” when stochastic nodes1 can be reparameterized into deterministic functions of their parameters and a fixed noise distribution (Kingma & Welling, 2013; Rezende et al., 2014), has liberated researchers in the development of large complex stochastic architectures (e.g. Gregor et al., 2015).###(7)
The reparameterization trick, introduced in the context of variational inference independently by Kingma & Welling (2014), Rezende et al. (2014), and Titsias & Lázaro-Gredilla (2014), is usually the estimator of choice when it is applicable.###The reparameterization trick, introduced in the context of variational inference independently by [24], [35], and [41], is the method of choice for training variational autoencoders and related models with continuous latent variables.",impact-revealing,highlighting the significance of the reparameterization trick in variational inference
3224,5ee9f15b91e01152af022eb9,6360aaece0d6bf153183b9ecd075f42f7b127cc9,Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting,53e99acab7602d970234a328,Weisfeiler-Lehman Graph Kernels,"Similarly, we obtain the WLK (Shervashidze et al., 2011) and GIN (Xu et al.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
3299,5efcb8cd91e0115203245887,81a5cdc8fb5c58e7876b60fb735a785a9b16f62f,graph clustering with graph neural networks,53e9acf0b7602d97036cf16e,Comparing Community Structure to Characteristics in Online Collegiate Social Networks,"nomena in the underlying graph, for example to education [56] or employment [40] in social graphs.",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
350,5e71f49891e0115656f5d0a5,fc99c6cc77f12cf252a9dcaa4dd2f2987e029375,closed-loop matters: dual regression networks for single image super-resolution,5c8f22cb4895d9cbc62c6c05,Image Super-Resolution Using Very Deep Residual Channel Attention Networks,"[51] propose the channel attention mechanism to build a deep model called RCAN to further improve the performance of SR.###Unlike the baseline U-Net, we build each basic block using B residual channel attention block (RCAB) [51] to improve the model capacity.###Many efforts have been made to improve the performance of SR, including the interpolation-based approaches [19] and reconstructionbased methods [16, 25, 51].###Based on DNNs, many methods have been proposed to improve SR performance [51, 26, 10, 12, 49].",impact-revealing,acknowledge existing methods and improvements in super-resolution
3647,5a73cb3517c44a0b303556bf,eac48f406c46527f5ca821de7fe8d62d6db56a27,"Exploring architectures, data and units for streaming end-to-end speech recognition with RNN-transducer",5736960c6e3b12023e51fb74,TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems.,All RNN-T models are trained with LSTM networks in the tensorflow [26] toolkit with asynchronous stochastic gradient descent.,other,describing the training method for RNN-T models
1374,,e1b81807da3af89330721423f46e9436d4035f9d,Restorative Justice and emotional literacy : using restorative conversation as a targeted intervention for students who get involved in peer conflict : a multiple base line design,,,"###Although the concept of ‘Emotional Literacy’ (EL) is not new, the coining of
the phrase is relatively recent and mainly used in the United Kingdom (Weare & Gray, 2003).###…an environment where problems are solved in a positive way, can model a culture of violence and aggression, tending to dehumanize the people involved by bringing shame and harm on students who have probably already been hurt (Hostetler, 2014; Vernon, 2014, pg. 52; Weare & Gray, 2003, p.23).###An evaluation conducted for 3 years at a middle school in the United States
found that a whole-school approach to restorative discipline yielded promising outcomes with school suspension falling by 30% and off-campus incidents by 84% during the initial 2 years of the study (Armour, 2013).###There is an increasing recognition of the importance of evidence-based
interventions in education and even though there has been a considerable amount of work in the field of social and emotional wellbeing in England, most of the evidence still comes from the United States (Weare & Gray, 2003).###Although EL is the preferred term used in the UK, it is not without drawbacks
(Weare & Gray, 2003).###Positive Youth Development in the United States: Research Findings on Evaluations of Positive Youth Development Programs.###These teachers also seem to issue fewer exclusionary referrals in general but especially to vulnerable groups such as Latinos and African Americans in the United States, showing promising evidence for narrowing the racial discipline gap (Gregory, Clawson, Davis & Gerewitz, 2016).###It has been argued that behavioural approaches that concentrate on dealing with the behaviour alone by simply punishing it, instead of aiming to understand the emotional, social and environmental causes of behaviour, are destined to fail (Vernon, 2014; Weare & Gray, 2003).###Catalano, R.F., Berglund, L., Ryan, A.M., Lonczak, H.S. and Hawkins, J. (2002)
‘Positive Youth Development in the United States: Research Finding on
Evaluations of Positive Youth Development Programmes’.###Lastly, the teaching of EL has been considered the ‘organising framework’ in
some Local Authorities in the UK by promoting these skills as of equal importance to literacy and numeracy in the school curriculum (Weare & Gray, 2003).###According to Weare and Gray (2003), most effective EL programmes use guidelines or manuals to guide the consistency of delivery.###This should include ‘a regular and predictable work routine to develop specific skills across the curriculum, and reinforce these skills by pupils’ real life experience across the whole school’ (Weare & Gray, 2003, p.68).###Some authors use the above terms interchangeably but in this review the
phrase ‘emotional literacy’ has been chosen as its meaning relates to an educational context and it has been broadly adopted by Educational Psychologists and Local Authorities in the UK (Weare & Gray, 2003).",impact-revealing,highlighting the significance of emotional literacy in education and its evidence-based interventions
3945,5f76f20a91e011f31b98056c,645bd6eadc247989abc5e0b0aa0be79ec8b11ea6,CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,599c7978601a182cd2641b24,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,"The prompts are either premise sentences taken from MultiNLI’s fiction genre (Williams et al., 2018) or 2–3 sentence story openings taken from examples in ROCStories (Mostafazadeh et al., 2016).###For each example, a crowdworker wrote standalone sentences inspired by a prompt that was drawn
from either MultiNLI (Williams et al., 2018) or ROCStories (Mostafazadeh et al., 2016).",other,describing the sources of prompts used in the study
0,5db80dc83a55acd5c14a24b9,0af061849aa325b41a213e8730b3d1e84aa26c0d,CONNA: Addressing Name Disambiguation on the Fly,599c7968601a182cd263a485,End-to-End Neural Ad-hoc Ranking with Kernel Pooling,"The hyper-parameters of the RBF kernel functions are set the same as [44].###Thus we adopt an RBF kernel aggregation function [44] to extract features.###based models [6], [13], [44] widely used in information retrieval.",impact-revealing,reporting prior findings on RBF kernel functions
3519,5ea16b2b91e011fa08b8f6e3,787c17aa8c3d4788e8144e0717d296754a02c244,Single-Step Adversarial Training With Dropout Scheduling,5b8c9f4a17c44af36f8b6b6c,Evaluating and Understanding the Robustness of Adversarial Logit Pairing.,[11] demonstrated that the performance of models trained using certain adversarial training methods degrade significantly with increase in the number of steps of PGD attack.,other,reporting findings on adversarial training methods
1145,,84740e1014ef0e4e9e1421737eab3791bc634851,HumanMAC: Masked Motion Completion for Human Motion Prediction,,,"###We train HumanMAC on both datasets with a 1000-step diffusion model and sample with a 100-step DDIM [65].###In our diffusion model, the proposed method needs 100 steps DDIM sampling steps, which makes it limited to being a real-time system.###2 Denoising Diffusion Models Denoising diffusion probabilistic models [23, 43, 60, 35, 10, 67] are motivated by the second law of thermodynamics.###Note that traditional diffusion models [23, 60, 34, 43, 35] have to make the prediction from random noise with a denoising procedure, which cannot make use of observed motions, resulting in uncontrollable results.###We train HumanMAC on both datasets with a 1000-step diffusion model and sample with a 100-step DDIM [60].",impact-revealing,describing the training process and limitations of the proposed method
1558,,e9643a3d06d3a45e4fde6725119f711bbbec1a61,"The involvement of the apoptosis-modulating proteins ERK 1/2, Bcl-xL and Bax in the resolution of acute inflammation in vivo.",,,"###Since then, the carrageenan-induced pleurisy model has been further developed and widely used as an acute resolving model of inflammation.(24) Briefly, the animals were anesthetized with halothane, a small incision was made to expose the musculature and 0.",impact-revealing,acknowledging the development and application of a specific inflammation model
3356,573698456e3b12023e70ee1b,524664475292ad6cdbdda3992fe5dc8f036b6ce5,Deep learning for emotion recognition on small datasets using transfer learning,53e9a80cb7602d970314d95d,"Static Facial Expression Analysis In Tough Conditions: Data, Evaluation Protocol And Benchmark","Images are selected from movies, in a semi-automated way, via a system based on subtitles [5, 6].",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
458,5f9a9af391e0114d7e7813ed,0d67d3ddca1c4e370eaf1e99ec674f612c39c66c,Graph Contrastive Learning with Adaptive Augmentation,5e3a92413a55ac054d0cdbf7,Graph Representation Learning via Graphical Mutual Information  Maximization,"GI, previous work [51] proposes to not rely on an explicit graph embedding, but rather focus on maximizing the agreement of node embeddings across two corrupted views of the graph. Following DGI, GMI [30] employs two discriminators to directly measure MI between input and representations of both nodes and edges without data augmentation; MVGRL [15] proposes to learn both node-level and graph-level rep###s including DeepWalk [32] and node2vec [11] and (2) deep learning methods including Graph Autoencoders (GAE, VGAE) [21], Deep Graph Infomax (DGI) [44], Graphical Mutual Information Maximization (GMI) [30], and Multi-View Graph Representation Learning (MVGRL) [15]. Furthermore, we report the performance obtained using a logistic regression classifier on raw node features and DeepWalk with embeddings co###original graph by simply shuffling node features. Then, an objective based on MI maximization is proposed to maximize the MI between node embeddings and a global summary embedding. Following DGI, GMI [30] proposes two node-level contrastive objectives to directly measure MI between input and representations of nodes and edges respectively, without explicit data augmentation. Moreover, to supplement th###form — GMI Node–node — — MVGRL Node–global Uniform — GCA Node–node Adaptive Adaptive proposed GCA and other state-of-the-art graph contrastive representation learning methods, including DGI [44], GMI [30], and MVGRL [15] in Table1, where the two columns “Topology” and “Attribute” denote data augmentation strategies at both levels. It is seen that the proposed GCA method simplifies previous node– globa",impact-revealing,reporting on various graph representation learning methods and their performance
2726,5aed147c17c44a4438153ea5,665c0dde22c2f8598869d690d59c9b6d84b07c01,domino temporal data prefetcher,53e9bdfdb7602d9704aafae1,Using a user-level memory thread for correlation prefetching,"One of the promising prefetching techniques is temporal prefetching [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17].###Thread-based prefetching techniques [12], [45], [46], [47], [48], [49], [50], [51] exploit idle thread contexts to execute threads that prefetch for the main program thread.",other,acknowledge existing prefetching techniques
3871,5d0616bd8607575390f86730,635b079447ee46acc813a08305988e77dfe3d1dd,Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi,573696966e3b12023e59fe3d,Capturing the human figure through a wall,"…existing approaches extract various parameters of signals reflected or shadowed by human, including DFS [26, 32, 44], ToF [2–4, 21], AoA / AoD [2 , On the human side, existing model-based works only tracks coarse human motion status, such as location [4, 41], velocity [26, 32], gait [43, 49]…###On the signal side, existing approaches extract various parameters of signals reflected or shadowed by human, including DFS [26, 32, 44], ToF [2–4, 21], AoA / AoD [2 , On the human side, existing model-based works only tracks coarse human motion status, such as location [4, 41], velocity [26, 32],…###…extract various parameters of signals reflected or shadowed by human, including DFS [26, 32, 44], ToF [2–4, 21], AoA / AoD [2 , On the human side, existing model-based works only tracks coarse human motion status, such as location [4, 41], velocity [26, 32], gait [43, 49] and figure [2, 19].",other,acknowledge existing approaches and their limitations
2228,555048d345ce0a409eb71be1,eba36ac75bf22edf9a1bfd33244d459c75b98305,Recurrent convolutional neural networks for text classification,5550446445ce0a409eb4d451,"Don'T Count, Predict! A Systematic Comparison Of Context-Counting Vs. Context-Predicting Semantic Vectors","In this work, we use the Skip-gram model to pre-train the word embedding. this model is the state-of-the-art in many NLP tasks (Baroni, Dinu, and Kruszewski 2014).",other,reporting the use of a state-of-the-art model for word embedding
2183,,3e11019834023b40827c5af33211e7183eb534f4,Integration of Qos Aspects in the Cloud Service Research and Selection System,,,"###Another definition based on the concepts Cloud Computing is built on was proposed by Vouk in [7], stating that Cloud Computing "" embraces cyber infrastructure and builds upon decades of research in virtualization, distributed computing, grid computing, utility computing, and, more recently,…",impact-revealing,providing a definition and context for Cloud Computing
2688,53e9b253b7602d9703cf4028,fff114cbba4f3ba900f33da574283e3de7f26c83,DeepWalk: online learning of social representations,53e9b57cb7602d97040bdd6d,LIBLINEAR: A Library for Large Linear Classification,For all models we use a one-vs-rest logistic regression implemented by LibLinear [11] extended to return the most probable labels as in [39].,other,describing the model implementation for label prediction
2613,5fc61cdb91e0118947381abc,c9d736dd9f967844d2391bb13c4cb477576ab373,On Disambiguating Authors: Collaboration Network Reconstruction in a Bottom-up Manner,53e99a14b7602d97022654df,Constraint-based entity matching,"Our work is related to record linkage [1]–[3], entity resolution [4]–[7], object identification [8], duplicate detection [9]– [11] and entity matching [12]–[15], etc.",other,acknowledge related fields of study
3071,5d9ed30647c8f76646f7f04c,f160c69c428122e8fa7ba96f220b4ded5f8761f4,ARNOR: Attention Regularization based Noise Reduction for Distant Supervision Relation Classification,57d063b9ac4436735428eaae,Attention-Based Bidirectional Long Short-Term Memory Networks For Relation Classification,"For RC task, various models are recently proposed based on different neural architectures, such as convolutional neural networks (Zeng et al., 2014, 2015) and recurrent neural network (Zhang et al., 2015; Zhou et al., 2016).###In order to capture the key feature words for identifying relations, we apply an attention mechanism over a BiLSTM Encoder, which is ﬁrst introduced in (Zhou et al., 2016) for RC.###BiLSTM+ATT (Zhou et al., 2016) adds an attention mechanism into BiLSTM to capture the most important features for identifying relations.",other,acknowledge various models proposed for relation classification
721,5f61e88391e011fae8fd6b13,f30444fbb6ad806168e2564db4815cd27faa7fd9,It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners,6085415691e01180c31e92d5,Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference.,"As ﬁnding ways to reformulate tasks as cloze questions that are understood well by LMs is difﬁ-cult (Jiang et al., 2019), Schick and Sch¨utze (2020a) propose P ET , a method that uses knowledge distillation (Hinton et al., 2015) to easily combine several reformulations.###…only a powerful pretraining objective, but many tasks can be reformulated as cloze questions (e.g., by appending phrases such as “the correct answer is ”), allowing pretrained LMs to solve them without any or with only very few labeled examples (Radford et al., 2019; Schick and Sch¨utze, 2020a).###To give MLMs trained on different patterns further opportunity to learn from one another, Schick and Sch¨utze (2020a) also propose iP ET , an iterative variant of P ET in which several generations of models are trained on datasets of increasing size that are labeled by previous generations.###Un-less explicitly stated differently, we use the exact same set of hyperparameters as Schick and Sch¨utze (2020a) with the only difference that for iP ET , we only train 3 generations of models to speed up training.###…deﬁne two verbalizers mapping questions containing a true statement to yes / true and others to no / false , respectively, for a total of 6 PVPs. CB (De Marneffe et al., 2019) and RTE (Dagan et al., 2006) are textual entailment tasks like MNLI, so we use PVPs similar to Schick and Sch¨utze (2020a).###An alternative to priming is pattern-exploiting training (P ET ) (Schick and Sch¨utze, 2020a), which combines the idea of reformulating tasks as cloze questions with regular gradient-based ﬁnetuning.###In practice, Schick and Sch¨utze (2020a) train three MLMs per pattern as performance can vary substantially between runs.###Given 32 examples, P ET clearly outperforms both baselines, which is in line with ﬁndings by Schick and Sch¨utze (2020a).###5 We run P ET on the FewGLUE training sets for all SuperGLUE tasks using the exact same setup as Schick and Sch¨utze (2020a).###Our implementation extends the original implementation of P ET by Schick and Sch¨utze (2020a) which, in turn, is based on the Transformers library (Wolf et al., 2019) and PyTorch (Paszke et al., 2017).###It is also commonly used for probing the knowledge contained within LMs (Trinh and Le, 2018; Petroni et al., 2019; Talmor et al., 2019; Schick and Sch¨utze, 2020b; Ettinger, 2020, i.a. ).",impact-revealing,describing the method and its implementation details
763,573697316e3b12023e621b53,a5a927193260bad591e436131f4d971b9d4c5c3a,optimal cache partition-sharing,53e9aee4b7602d970390d30f,Optimal Partitioning of Cache Memory,"The allocation is optimal if the miss-rate derivatives are as equal as possible [9].###was designed to maximize performance (minimal group miss ratio) and required the assumption that the individual miss ratio curve be convex [9].###single-threaded programs by partitioning the cache between instructions and data, and for multiprogramming by giving each process a partition [9].",impact-revealing,discussing optimal allocation in relation to miss-rate derivatives
1238,,4b91b988c44cde99650df48885728bbb8265f5b8,Immunoinformatics,,,"###In this chapter, we will use position-specific scoring matrices (PSSMs) as the predictor of peptide–MHC binding (11,12) and describe in detail the generation of supertypes using, for example, a selection of HLA class I (HLA I) molecules for which PSSMs are readily available.###(11) used clustering procedures for grouping HLA alleles into putative supertypes (where different members bind similar peptides, yet exhibiting distinct repertoires).###19
3 The IMGT/HLA Database James Robinson and Steven G. E. Marsh . . . . . . . . . . . . . . . . . . . . . . . . . .###It has been extended to the C-DOMAINs of the IG and TR and to the C-LIKE-DOMAINs of IgSF proteins other than IG and TR (11,26,27).###A variation of the PCA, CPCA, is also commonly used for calculations with multiple probes (11).###By this strategy, PBMCs of HCMV-seropositive donors are stimulated with virion-infected fibroblasts to increase the precursor frequency of virus-specific T cells (11).###Currently, the IPD-MHC sequence alignments are limited to species-specific alignments; however, we are working to allow cross-species alignments and the inclusion of human sequences from the IMGT/HLA Database (11) for comparative purposes.###The IMGT Collier de Perles is based on the IMGT unique numbering for C-DOMAIN and C-LIKE-DOMAIN (11).###(11) used clustering procedures for grouping HLA alleles into putative supertypes.###In our study PSSMs derived from aligned MHC ligands as the predictors of peptide–MHC binding (11,12).###Early methods attempted to predict epitopes directly, and in the absence of knowledge of the peptide preferences of MHC restriction, were not very successful (11).",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
1937,,79342ec9bdf2c6310d9e8e2136c180b49d9274a8,Is Chinese Special? Four Aspects of Chinese Literacy Acquisition that Might Distinguish Learning Chinese from Learning Alphabetic Orthographies,,,"###…in Chinese has underscored the importance of both semantic radicals (Ho et al. 2002; McBride-Chang et al. 2011a, b, c; Shu et al. 2006) and morphological awareness (e.g., Lei et al. 2011; McBride-Chang et al. 2011a, b, c, 2012; Shu et al. 2006) for distinguishing those with and without dyslexia.###Research on dyslexia in Chinese has underscored the importance of both semantic radicals (Ho et al. 2002; McBride-Chang et al. 2011a, b, c; Shu et al. 2006) and morphological awareness (e.g., Lei et al. 2011; McBride-Chang et al. 2011a, b, c, 2012; Shu et al. 2006) for distinguishing those with and…###, semantic radicals, morphological awareness, different scripts, and changes in learning patterns apart from literacy itself, impact how dyslexia is understood in Chinese? Research on dyslexia in Chinese has underscored the importance of both semantic radicals (Ho et al. 2002; McBride-Chang et al. 2011a, b, c; Shu et al. 2006) and morphological awareness (e.###Studies also indicate that in places using a phonological coding system such as Mainland China, phonological abilities are crucial for distinguishing those with and without dyslexia, just as morphological skills are (e.g., Lei et al. 2011; Shu et al. 2006).###2006) and morphological awareness (e.g., Lei et al. 2011; McBride-Chang et al. 2011a, b, c, 2012; Shu et al. 2006) for distinguishing those with and without dyslexia.",impact-revealing,highlighting the significance of semantic radicals and morphological awareness in understanding dyslexia in Chinese
3964,5f0d85c69fced0a24be4f028,5d9073cfec34aea00247ec625fa94f6279ff580d,tailored page sizes,53e9bb01b7602d9704739803,Going the distance for TLB prefetching: an application-driven study,"Prior work has proposed hardware PTE prefetchers [13], [33], [52].",other,reporting prior findings on hardware PTE prefetchers
1199,,81567f7a1babf6daed60b98001680c02ce1b24fa,Three-way decision support for diagnosis on focal liver lesions,,,"###Besides the definition of attribute reducts, Yao also analyzed the properties of attribute reduction in DTRS model, such as the monotonicity of positive region, confidence, coverage and generality of attribute reduts, and costs of decisions [61].###AC CE PT ED M AN US CR IP T making [61, 62].###The methodology of three-way decision support is motivated by the theory of Three-way Decision (3WD) [22, 61].",impact-revealing,reporting on the analysis of attribute reduction properties in DTRS model
498,57d063e8ac443673542950ad,0509facb30efa81c4730f98bfc28cdbd6178822e,"Image sentiment analysis using latent correlations among visual, textual, and sentiment views",53e9b3bcb7602d9703ea484a,Random Features for Large-Scale Kernel Machines,"In contrast, recent advances of explicit feature maps [12, 13] can convert nonlinear problems to linear problems, which can be solved by linear frameworks with a low computation cost [9,23].###For GIST features, attribute features, and SentiBank features, we use the random Fourier feature mapping [12] to approximate the Gaussian kernel.###To reduce the computation complexity, one can use explicit feature maps [12,13].###Speciﬁcally, to capture the non-linear relationship between features, we introduce explicit feature maps [12,13] to CCA.",impact-revealing,highlighting the advantages of explicit feature maps in reducing computation complexity
2854,5bdc315017c44a1f58a05e13,1717255b6aea01fe956cef998abbc3c399b5d7cf,AMC: AutoML for Model Compression and Acceleration on Mobile Devices,53e9ba54b7602d970466c10f,Speeding up Convolutional Neural Networks with Low Rank Expansions.,"[26] proposed to factorize layers into 1×3 and 3×1; and Zhang et al .###Many works have been proposed to improve the hardware efficiency of neural networks by model compression [26, 19, 22].",other,acknowledge existing methods for improving hardware efficiency
2560,5aed14d617c44a4438159040,ebc96892b9bcbf007be9a1d7844e4b09fde9d961,YOLOv3: An Incremental Improvement,5c7939c44895d9cbc62f169f,Best Of Both Worlds: Human-Machine Collaboration For Object Annotation,"[18] If humans have a hard time telling the difference, how much does it matter?",other,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
901,5fdb279d91e0118a02c4f4ef,cbf8d76c8286b536f431778e974ba9cd583d692e,SimpleChrome: Encoding of Combinatorial Effects for Predicting Gene Expression,53e99a48b7602d97022a8346,Auto-Encoding Variational Bayes.,"Two of the most popular methods are Generative Adversarial Networks (GANs) by Goodfellow et al. (2014) and Variational
Autoencoders (VAEs) by Kingma and Welling (2013).###While the expectation of the likelihood term can be hard to derive in practice, Kingma and Welling (2013) introduce a Stochastic Gradient Variational Bayes (SGVB) method to approximate the expectation to overcome such an issue.###Autoencoders (VAEs) by Kingma and Welling (2013). While GANs were shown to generate high quality samples, the lack of theoretical support",impact-revealing,acknowledge popular methods in generative modeling
575,5b1643998fbcbf6e5a9bc25e,080e1bb6bbebeb78f822b3998b7ed898ab6457aa,End-to-End Speech Separation with Unfolded Iterative Phase Reconstruction,5c861c764895d9cbc61c4386,An Iterative Phase Recovery Framework with Phase Mask for Spectral Mapping with An Application to Speech Enhancement,"There are some previous attempts at naively applying such iterative algorithms as a post-processing step on the magnitudes produced by deep learning based speech enhancement and separation [18, 19, 20, 3].",impact-revealing,acknowledge prior attempts in speech enhancement
1808,,7b58b67effb5e513a9539facfbb8f029797541eb,Intervention Effectiveness Research in Adolescent Health Psychology: Methodological Issues and Strategies,,,"###…psychology work on scienti fi c reasoning and evidence appraisal, Koslowski similarly emphasized the importance of one’s network of evidentially relevant collateral information to thinking in general and to scienti fi c explanation in particular (Koslowski, 1996 ; Koslowski & Thompson, 2002 ) .###…& Brewer, 2001 ; Evans, 1989 ; Gigerenzer, 2009 ) of the nature of scienti fi c research evidence and its use is that “neither theory nor data alone is suf fi cient to achieve scienti fi c success; each must be evaluated in the context of, and constrained by, the other” (Koslowski, 1996 , p. 252).",impact-revealing,highlighting the importance of contextual information in scientific reasoning
3901,53e99f7fb7602d9702853a66,c37f1baac3c8ba30250084f067167ac3837cf6fd,a survey of monte carlo tree search methods,53e99a48b7602d97022a66e7,Amazons Discover Monte-Carlo,"Another approach called dynamic exploration , proposed by Bourki et al. [25], tunes parameters based on patterns in their Go program M O G O .",other,describing a specific approach in parameter tuning
1917,,90fd8b9b1de5261d0c6819e2d1362c22fea85473,International Review of Research in Open and Distributed Learning Using the Community of Inquiry Framework to Scaffold Online Tutoring,,,"###Cognitive presence is taken to mean the extent to which the students are able to construct meaning through sustained communication (Garrison, Anderson, & Archer, 2000).###Learners are often able to create and shape relationships as part of their participation within the emerging community (Garrison et al., 2000).###The existence of the three principal types of presence— social presence, teaching presence and cognitive presence —was first validated by the development and measurement of a set of indicators, as shown in Table 1 (see Garrison et al., 2000).###The goal of social presence is to develop a supportive learning environment, in which students will feel comfortable and safe and be able to express their ideas without prejudice (Garrison et al., 2000).###This has been widely measured in three dimensions, namely: group cohesion, open communication, emotional expression (Garrison, Anderson, & Archer, 2000).",impact-revealing,providing context and definitions related to cognitive presence in educational settings
194,5ee8986891e011e66831c3b6,38f93092ece8eee9771e61c1edaf11b1293cae1b,Bootstrap Your Own Latent A New Approach to Self-Supervised Learning,573696096e3b12023e51cb6b,Continuous control with deep reinforcement learning,"While most RL methods use fixed target networks, BYOL uses a weighted moving average of previous networks (as in [54]) in order to provide smoother changes in the target representation.###The target network provides the regression targets to train the online network, and its parameters ξ are an exponential moving average of the online parameters θ [54].",impact-revealing,describing the mechanism of a specific reinforcement learning method
1386,,7a88a591592f09dcf0e6ed2716f9c1818bbf9d7c,Sémantique géométrique pour la calculabilité asynchrone,,,"###Once we make those two assumptions, the possibility of crashing processes often becomes irrelevant: this is for example the case for the usual impossibility of solving consensus [63] and setagreement [112] using read/write registers.###In order to prove impossibility results in this context, people started developing powerful mathematical tools based on algebraic topology [14, 112].###Soon thereafter, people started developing powerful mathematical tools based on algebraic topology in order to prove impossibility results [14, 112, 70].###In two 1993 papers [14, 112], this discovery was motivated by the study of the k-set agreement task, a weaker form of consensus where the processes must agree###It has been remarked from the very beginning of the topological approach to task solvability that the protocol complex approach has an intuitive explanation in terms of the knowledge that the agents need to gain in order to solve a task [112].",impact-revealing,highlighting the development of mathematical tools for proving impossibility results
1342,,cd7643d0ef9621875443c5ddda44cb7d7521ea1a,Enhancing LTW image encoder with perceptual coding and GPU-optimized 2D-DWT transform,,,"###One of the best behaving objective quality assessment metrics is visual information fidelity (VIF) [7], which has been proven [17,19] to have a better correlation with subjective perception than other metrics that are commonly used for encoder comparisons [14,20].###A comprehensive review of HVS models for quality assessment/image compression is found in [7].###The most widely used characteristic is the contrast adaptability of the HVS, because HVS is more sensitive to contrast than to absolute luminance [7].",impact-revealing,highlighting the effectiveness of visual information fidelity as a quality assessment metric
1099,,9901c4b8923a9ab7b59cbbceacc5897f047ad5c8,Estradiol Stimulates Vasodilatory and Metabolic Pathways in Cultured Human Endothelial Cells,,,"###Microarrays are high-throughput genomic tools that allow the comparison of global expression changes in thousands of genes between different experimental conditions in cell/tissue analysis, and they have been widely adopted for analyzing the global gene expression profiles in vivo and in vitro [10].",impact-revealing,providing context on the use and significance of microarrays in genomic analysis
2733,5d3c234c3a55acd386d4e112,a9ec03dbe702f6909acd1f1f14a3395d0141043b,generative models for graph-based protein design,5b67b4b417c44aac1c867387,"Relational inductive biases, deep learning, and graph networks","Given this graph-structured self-attention, our model may also be reasonably cast in the framework of message-passing or graph neural networks [36, 37] (Section 4.",other,providing context for model framework
127,5e5e189993d709897ce1ddbc,055fd6a9f7293269f1b22c1470e63bd02d8d9500,Reformer: The Efficient Transformer,599c7987601a182cd2648373,Attention Is All You Need.,"In the two plots on the right in Figure 3, we compare a regular Trans-former per Vaswani et al. (2017) with the reversible one describe in Section 3.###We also evaluate on the WMT 2014 English-to-German translation task, following the hyperparameters of Vaswani et al. (2017).###The Transformer architecture (Vaswani et al., 2017) is widely used in natural language processing and yields state-of-the-art results on a number of tasks.###The Transformer model introduced in (Vaswani et al., 2017) has been used widely in natural language tasks and further extended to model diverse data such as music scores (Huang et al., 2018), and images (Parmar et al., 2018; Ramachandran et al., 2019).###We start by making both the encoder and the decoder fully reversible in the Transformer-base architecture, and see that the resulting model performs comparably to Vaswani et al. (2017) when trained for 100K steps.###The standard attention used in the Transformer is the scaled dot-product attention (Vaswani et al., 2017).",impact-revealing,comparing the performance of different Transformer architectures
2705,5da2f8a647c8f76646083cd9,b789abc47b7a92596050f6055a93c8fe1929db2a,Dynamic Multicontext Segmentation of Remote Sensing Images Based on Convolutional Networks,53e99d0bb7602d97025bde89,Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data,"related to the proposed work do not use any postprocessing, such as CRFs [35].###proposed a multicontext semantic segmentation by combining ConvNets, hand-crafted descriptors, and conditional random fields (CRFs) [35].",other,acknowledge differences in existing methods
1775,,7b30ff7e197669061cbcbda18daf13efb3a8495b,Stressing emotions : A single subject design study testing an emotion-focused transdiagnostic treatment for stress-related ill health,,,"###Visual inspection is commonly used as a means to analyze the data gathered within clinical research using a single subject design (Kazdin, 2010).###This does make the connection between treatment and improvement less clear and increases the plausibility that other factors, like the mere passage of time could actually explain the effect, as with statistical regression (Kazdin, 2010).###Moreover, using multiple measurements can cause reactivity (Kazdin, 2010; Kazdin, 2011).###Visual inspection is commonly used as a means to analyze the data
gathered within clinical research using a single subject design (Kazdin, 2010).###Lastly, ideally, in addition to continued post-treatment baseline measures, a rigorous
follow up assessment is to be conducted, as it facilitates detection of long-lasting treatment effects (Kazdin, 2010).###Lastly, ideally, in addition to continued post-treatment baseline measures, a rigorous follow up assessment is to be conducted, as it facilitates detection of long-lasting treatment effects (Kazdin, 2010).###For an in-depth discussion of the principles of single subject designs, see the works by for example Barlow et al. (2009), Kazdin (2010) and Kazdin (2011).",impact-revealing,highlighting the common use and challenges of visual inspection in clinical research
2768,5e6cae3493d709897ccff2f9,804f23ac1a4a56b8dc5bb7201dab7b8cece76a70,classifying memory access patterns for prefetching,53e99b26b7602d97023bf6ae,Pointer cache assisted prefetching,"There exist several works that analyze more complex recursive data structures such as linked-lists at compile time to insert jump pointers [11, 33, 40, 41] pointing to an element within the same data structure at some distance e.",other,acknowledge existing research on recursive data structures
1147,,143e118b5b3abaf3a63b78c4dee3df05538a7ed4,Soft Diffusion: Score Matching for General Corruptions,,,"###Our method is inspired by the continuous formulation of diffusion models that is introduced in Song et al. (2020b). To develop our idea, we start with the toy setting where the dataset contains one single image, α ∈ R.###In fact, if there is no blur (Ct = I), our sampler becomes exactly the sampler used for the Variance Exploding (VE) SDE in Song et al. (2020b).###Formally, let {q′ t}(1)t=0 be the (noisy) distributions used in Song et al. (2020b) for the Variance Exploding (VE) SDE and let {qt}(1)t=0 the blurry (and noisy) distributions we want to select.###This is a general class of diffusion processes, that includes (as special cases) the VE, VP and subVP SDEs used in Song et al. (2020b). Our diffusion is the sum of a deterministic linear corruption of x0 and a stochastic part that progressively adds noise. For any corruption process of this family, we are interested in learning the scores Song & Ermon (2019), i.e. ∇xt log qt(xt) for all t. For the vanilla Gaussian denoising diffusion, the celebrated result of Vincent (2011) shows that to learn the score, ∇xt log qt(xt), one only needs access to the gradient of the conditional loglikelihood,∇xt log qt(xt|x0).###This is a general class of diffusion processes, that includes (as special cases) the VE, VP and subVP SDEs used in Song et al. (2020b). Our diffusion is the sum of a deterministic linear corruption of x0 and a stochastic part that progressively adds noise. For any corruption process of this family, we are interested in learning the scores Song & Ermon (2019), i.###This is a general class of diffusion processes, that includes (as special cases) the VE, VP and subVP SDEs used in Song et al. (2020b). Our diffusion is the sum of a deterministic linear corruption of x0 and a stochastic part that progressively adds noise.###For all our experiments, we are using the architecture and the training hyperparameters from Song et al. (2020b). We are using blur as the main corruption mechanism.###This is a general class of diffusion processes, that includes (as special cases) the VE, VP and subVP SDEs used in Song et al. (2020b). Our diffusion is the sum of a deterministic linear corruption of x0 and a stochastic part that progressively adds noise. For any corruption process of this family, we are interested in learning the scores Song & Ermon (2019), i.e. ∇xt log qt(xt) for all t. For the vanilla Gaussian denoising diffusion, the celebrated result of Vincent (2011) shows that to learn the score, ∇xt log qt(xt), one only needs access to the gradient of the conditional loglikelihood,∇xt log qt(xt|x0). By revisiting the proof of Vincent (2011), we find that this is actually true for a wide set of corruption processes as long as some mild technical conditions are satisfied.",impact-revealing,drawing inspiration from previous work on diffusion models
3700,5da1a6d447c8f7664606888d,91a4a5a1184a12821e7f5ddf5372b259ded96feb,Directed Statistical Warming through Time Traveling,53e9a82cb7602d9703175aef,Pinpointing data locality bottlenecks with low overhead,"Liu and Mellor-Crummey [17] use a technique based on shadow profiling that forks off a redundant copy of an application, instrumented by Pin, to measure the stack distances for a selected set of references [17].",other,reporting prior findings on stack distance measurement technique
234,5f03f3b611dc83056223205d,639206a9a32d91386924f1c94e9760dfb43df72e,Towards Deeper Graph Neural Networks,5a9cb66717c44a376ffb8c0f,Deeper insights into graph convolutional networks for semi-supervised learning,"Several recent works attribute this performance degradation to the oversmoothing issue [3, 15, 33], which states that representations from different classes become inseparable due to repeated propagation.###According to [15], the node representations suffering from the oversmoothing issue will converge to the same value or be proportional to the square root of the node degree, where the corresponding smoothness metric value should be close to 0, computed by our quantitative metric.###To our knowledge, [15] is the first attempt to demystify the over-smoothing issue in the GCN model.###[15] applies co-training and self-training to overcome the limitation of shallow architectures.###Several studies [3, 15] attribute this performance degradation phenomenon to the over-smoothing issue.###The previous descriptions of the over-smoothing issue simplify the assumption of non-linear activation function [15, 33] or make approximations of different probabilities [33].",impact-revealing,highlighting the significance of the oversmoothing issue in GCN models and its implications for performance
3247,5da1a6d447c8f7664606888c,404d82a8da658f16c46714442155490d79a413f1,temporal prefetching without the off-chip metadata,53e99b1bb7602d97023b2390,Linearizing irregular memory accesses for improved correlated prefetching.,"First, most of the coverage for state-of-the-art temporal prefetchers [24, 47] comes from a small number of metadata entries, so it is possible to get substantial coverage without storing megabytes of metadata (see Figure 1).###In particular, compared to other metadata organizations [24, 45, 47], our table-based organization avoids metadata redundancy by representing each correlated address pair only once.###Triage is the first data prefetcher that reaps the benefit of PC-localized address correlationÐthe most powerful form of temporal prefetching [24]Ð without using any off-chip metadata.###We present single-core results for a subset of SPEC2006 benchmarks that are memory bound [18] and are known to have irregular access patterns [24].###By contrast, previous solutions [24, 45] introduce varying degrees of metadata redundancy to facilitate off-chip metadata management.###Other prefetchers exploit address correlation by storing metadata in off-chip memory [24, 45, 47], but the use of off-chip metadata has limited the commercial viability of such prefetchers.###The Irregular Stream Buffer (ISB) combines address correlation with PC-localization by proposing a new off-chip metadata organization [24, 47].###More recently, the Irregular Stream Buffer (ISB) [24] introduced an alternative metadata representation that enabled portions of the metadata to be cached on chip.",other,highlighting the advantages of the proposed data prefetcher and its metadata organization
215,5eccb534e06a4c1b26a83a1b,afdac86a934df38748d6ded69a5ff48b06a40053,Defending and Harnessing the Bit-Flip Based Adversarial Weight Attack,5de0e865df1a9c0c415cffe1,Bit-Flip Attack: Crushing Neural Network With Progressive Bit Search,"Moreover, results reported in this figure use 8-bit post-training weight quantization [17].###Adversarial weight attack [17] is a recently developed security threat model for modern DNN.###Nevertheless, in general, regularization techniques such as dropout are expected to prevent the network from over-fitting [13], subsequently slightly improve network resistance against both adversarial input [23] and weight attacks [17].###To evaluate the effectiveness of the proposed defense methods, the code from [17] is utilized with further modification.###BitFlip Attack (BFA) [17], is an adversarial attack variant which performs weight fault injection through flipping the bits.###Thus, the bit searching in iteration i can be formulated as an optimization process [17]:###However, a newly proposed Bit-Flip Attack (BFA) [17] whose progressive bit searching algorithm can###2, the progressive bit search proposed in BFA [17] is prone to identify vulnerable bit in the weight whose absolute value has a small magnitude (i.###To clarify, we use the same threat model as in prior work [17], which is listed in Table 1.###Note that, all the quantized DNN reported hereafter still uses the uniform quantizer as in [17], but with quantization-aware training instead of post-training quantization.",impact-revealing,providing context on adversarial weight attacks and their implications
272,5d3ed25a275ded87f97deb36,37f7eaf57b88ef03efd4196f2f57ae0b4c657a79,Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommendation,5bdc316717c44a1f58a06ed8,RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems.,"But in contrast to hybrid methods such as RKGE [18] or RippleNet [24], the computation complexity of our model scales well with the increase of KG size.###• RippleNet [24] is a representative of hybrid methods, which is a memory-network-like approach that propagates users’ preferences on the KG for recommendation.###The learning rate are the same as in SVD. • RippleNet [24] is a representative of hybrid methods, which is a memory-network-like approach that propagates users’ preferences on the KG for recommendation.###(3) Hybrid methods [18, 24] combine the above two categories and learn user/item embeddings by exploiting the structure of KGs.###Existing KG-aware recommender systems can be classified into path-based methods [8, 33, 36], embedding-based methods [9, 26, 27, 34], and hybrid methods [18, 24, 28].###For RippleNet, d = 8, H = 2, λ1 = 10−6, λ2 = 0.01, η = 0.01 for MovieLens20M; d = 16, H = 3, λ1 = 10−5, λ2 = 0.02, η = 0.005 for
Last.",impact-revealing,comparing the computational complexity of different recommendation methods
3950,5f8cf5159e795ea21aee7f07,0ee0801ba010a441403f9ed666ef9bf006b3aa07,Adaptive Universal Generalized PageRank Graph Neural Network,5736977f6e3b12023e66646c,Provably Fast Inference Of Latent Features From Networks With Applications To Learning Social Circles And Multilabel Classification,"Homophily is also a common assumption in graph clustering (Von Luxburg, 2007; Tsourakakis, 2015; Dau & Milenkovic, 2017) and in many GNNs design (Klicpera et al., 2018).",other,acknowledge common assumptions in graph clustering and GNN design
871,5a260c8117c44a4ba8a30adf,ecf6c42d84351f34e1625a6a2e4cc6526da45c74,representation learning on graphs: methods and applications,53e9b9d3b7602d97045c8726,Latent Space Approaches to Social Network Analysis,"We refer the reader to [32], [42], [37], and [7] for comprehensive overviews of these areas.###…there are other lines of closely related and relevant work, which we do not review in detail here—including latent space models of social networks [32], embedding methods for statistical relational learning [42], manifold learning algorithms [37], and geometric deep learning [7]—all of which…###These low-dimensional embeddings can be viewed as encoding, or projecting, nodes into a latent space, where geometric relations in this latent space correspond to interactions ( e.g. , edges) in the original graph [32].",impact-revealing,the citation phrase doesn't include enough information to judge so it will be considered not impact-revealing
315,5dbebb7447c8f766462c2328,b0d941cfa0a3c43703e5222221addb1ad0f9e68d,Rethinking the Item Order in Session-based Recommendation with Graph Neural Networks,5a260c8117c44a4ba8a30f54,Graph Attention Networks.,"In recent years, some baseline methods on GNN, for example, GCN [12] and GAT [33], are demonstrated to be capable of extracting features of the graph.###In recent years, many GNN methods [6, 12, 18, 33, 39] work under the mechanism that is similar to message passing network [3] to compute the information flow between nodes via edges.###, GCN [12], GAT [33] and gated graph networks [18, 38].###As suggested in previous work [32, 33], the multi-head attention can help to stabilize the training of the self-attention layers.",impact-revealing,acknowledge existing methods and their capabilities in graph feature extraction
